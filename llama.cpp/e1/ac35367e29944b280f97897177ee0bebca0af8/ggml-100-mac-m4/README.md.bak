### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.29 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.24 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.49 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.86 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.99 sec*proc (28 tests)

Total Test time (real) = 222.00 sec

real	3m42.031s
user	7m40.658s
sys	0m6.408s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.62 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.28 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.44 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.27 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.32 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.21 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.40 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.24 sec*proc (28 tests)

Total Test time (real) =  52.25 sec

real	0m52.262s
user	1m11.871s
sys	0m5.416s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.947 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.161 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.170 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.172 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.172 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.173 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.175 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.175 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.176 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.177 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.177 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.181 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.182 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.182 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.183 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.184 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.184 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.185 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.877 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.879 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.879 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.880 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.880 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.881 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.881 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.882 I llama_model_loader: - type  f32:  124 tensors
0.00.028.882 I llama_model_loader: - type  f16:   73 tensors
0.00.033.311 I llm_load_vocab: special tokens cache size = 5
0.00.035.447 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.451 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.451 I llm_load_print_meta: arch             = bert
0.00.035.452 I llm_load_print_meta: vocab type       = WPM
0.00.035.452 I llm_load_print_meta: n_vocab          = 30522
0.00.035.452 I llm_load_print_meta: n_merges         = 0
0.00.035.453 I llm_load_print_meta: vocab_only       = 0
0.00.035.453 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.453 I llm_load_print_meta: n_embd           = 384
0.00.035.453 I llm_load_print_meta: n_layer          = 12
0.00.035.457 I llm_load_print_meta: n_head           = 12
0.00.035.458 I llm_load_print_meta: n_head_kv        = 12
0.00.035.458 I llm_load_print_meta: n_rot            = 32
0.00.035.458 I llm_load_print_meta: n_swa            = 0
0.00.035.458 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.458 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.459 I llm_load_print_meta: n_gqa            = 1
0.00.035.460 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.461 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.462 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.462 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.463 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.463 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.463 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.464 I llm_load_print_meta: n_ff             = 1536
0.00.035.464 I llm_load_print_meta: n_expert         = 0
0.00.035.464 I llm_load_print_meta: n_expert_used    = 0
0.00.035.465 I llm_load_print_meta: causal attn      = 0
0.00.035.465 I llm_load_print_meta: pooling type     = 2
0.00.035.465 I llm_load_print_meta: rope type        = 2
0.00.035.465 I llm_load_print_meta: rope scaling     = linear
0.00.035.469 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.469 I llm_load_print_meta: freq_scale_train = 1
0.00.035.469 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.470 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.470 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.470 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.470 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.470 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.497 I llm_load_print_meta: model type       = 33M
0.00.035.497 I llm_load_print_meta: model ftype      = F16
0.00.035.497 I llm_load_print_meta: model params     = 33.21 M
0.00.035.498 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.498 I llm_load_print_meta: general.name     = Bge Small
0.00.035.499 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.499 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.500 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.502 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.502 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.502 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.503 I llm_load_print_meta: max token length = 21
0.00.037.603 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.608 I llm_load_tensors: offloading output layer to GPU
0.00.037.609 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.636 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.637 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.187 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.188 I llama_new_context_with_model: n_ctx         = 512
0.00.038.189 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.189 I llama_new_context_with_model: n_batch       = 2048
0.00.038.189 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.189 I llama_new_context_with_model: flash_attn    = 0
0.00.038.190 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.190 I llama_new_context_with_model: freq_scale    = 1
0.00.038.191 I ggml_metal_init: allocating
0.00.038.202 I ggml_metal_init: found device: Apple M4
0.00.038.205 I ggml_metal_init: picking default device: Apple M4
0.00.039.058 I ggml_metal_init: using embedded metal library
0.00.043.205 I ggml_metal_init: GPU name:   Apple M4
0.00.043.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.209 I ggml_metal_init: simdgroup reduction   = true
0.00.043.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.209 I ggml_metal_init: has bfloat            = true
0.00.043.209 I ggml_metal_init: use bfloat            = true
0.00.043.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.055.961 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.056.576 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.578 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.579 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.376 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.378 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.378 I llama_new_context_with_model: graph nodes  = 429
0.00.057.379 I llama_new_context_with_model: graph splits = 2
0.00.057.391 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.057.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.968 I 
0.00.063.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.064.649 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.149 I llama_perf_context_print:        load time =      45.02 ms
0.00.068.150 I llama_perf_context_print: prompt eval time =       3.37 ms /     9 tokens (    0.37 ms per token,  2669.04 tokens per second)
0.00.068.150 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.151 I llama_perf_context_print:       total time =       4.18 ms /    10 tokens
0.00.068.292 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.049s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.101 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.151 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.156 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.156 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.157 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.157 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.158 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.158 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.159 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.159 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.159 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.161 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.161 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.162 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.162 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.162 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.163 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.165 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.322 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.323 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.323 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.324 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.324 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.324 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.325 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.325 I llama_model_loader: - type  f32:  124 tensors
0.00.014.325 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.819 I llm_load_vocab: special tokens cache size = 5
0.00.018.133 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.136 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.136 I llm_load_print_meta: arch             = bert
0.00.018.137 I llm_load_print_meta: vocab type       = WPM
0.00.018.137 I llm_load_print_meta: n_vocab          = 30522
0.00.018.137 I llm_load_print_meta: n_merges         = 0
0.00.018.137 I llm_load_print_meta: vocab_only       = 0
0.00.018.137 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.138 I llm_load_print_meta: n_embd           = 384
0.00.018.138 I llm_load_print_meta: n_layer          = 12
0.00.018.140 I llm_load_print_meta: n_head           = 12
0.00.018.141 I llm_load_print_meta: n_head_kv        = 12
0.00.018.142 I llm_load_print_meta: n_rot            = 32
0.00.018.142 I llm_load_print_meta: n_swa            = 0
0.00.018.142 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.142 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.147 I llm_load_print_meta: n_gqa            = 1
0.00.018.148 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.148 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.149 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.150 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.167 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.169 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.169 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.170 I llm_load_print_meta: n_ff             = 1536
0.00.018.170 I llm_load_print_meta: n_expert         = 0
0.00.018.170 I llm_load_print_meta: n_expert_used    = 0
0.00.018.172 I llm_load_print_meta: causal attn      = 0
0.00.018.172 I llm_load_print_meta: pooling type     = 2
0.00.018.172 I llm_load_print_meta: rope type        = 2
0.00.018.172 I llm_load_print_meta: rope scaling     = linear
0.00.018.172 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.173 I llm_load_print_meta: freq_scale_train = 1
0.00.018.173 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.174 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.174 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.174 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.174 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.174 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.174 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.182 I llm_load_print_meta: model type       = 33M
0.00.018.182 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.182 I llm_load_print_meta: model params     = 33.21 M
0.00.018.183 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.184 I llm_load_print_meta: general.name     = Bge Small
0.00.018.184 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.184 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.184 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.185 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.185 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.185 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.186 I llm_load_print_meta: max token length = 21
0.00.019.497 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.497 I llm_load_tensors: offloading output layer to GPU
0.00.019.498 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.506 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.506 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.862 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.863 I llama_new_context_with_model: n_ctx         = 512
0.00.019.863 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.863 I llama_new_context_with_model: n_batch       = 2048
0.00.019.863 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.863 I llama_new_context_with_model: flash_attn    = 0
0.00.019.864 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.864 I llama_new_context_with_model: freq_scale    = 1
0.00.019.865 I ggml_metal_init: allocating
0.00.019.869 I ggml_metal_init: found device: Apple M4
0.00.019.872 I ggml_metal_init: picking default device: Apple M4
0.00.020.498 I ggml_metal_init: using embedded metal library
0.00.023.033 I ggml_metal_init: GPU name:   Apple M4
0.00.023.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.036 I ggml_metal_init: simdgroup reduction   = true
0.00.023.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.036 I ggml_metal_init: has bfloat            = true
0.00.023.036 I ggml_metal_init: use bfloat            = true
0.00.023.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.392 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.033.869 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.871 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.873 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.483 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.484 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.484 I llama_new_context_with_model: graph nodes  = 429
0.00.034.485 I llama_new_context_with_model: graph splits = 2
0.00.034.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.048 I 
0.00.039.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.570 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.962 I llama_perf_context_print:        load time =      29.94 ms
0.00.042.964 I llama_perf_context_print: prompt eval time =       3.27 ms /     9 tokens (    0.36 ms per token,  2756.51 tokens per second)
0.00.042.965 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.965 I llama_perf_context_print:       total time =       3.91 ms /    10 tokens
0.00.043.164 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.243 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.379 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.716 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.721 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.724 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.031.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.729 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.031.729 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.031.730 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.031.732 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.031.733 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.031.733 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.031.734 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.031.735 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.031.738 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.031.739 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.031.740 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.031.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.184 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.184 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.184 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.185 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.185 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.185 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.186 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.186 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.187 I llama_model_loader: - type  f32:   40 tensors
0.00.047.187 I llama_model_loader: - type  f16:   30 tensors
0.00.065.485 W llm_load_vocab: empty token at index 5
0.00.069.999 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.285 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.316 I llm_load_vocab: special tokens cache size = 5
0.00.336.753 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.336.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.336.760 I llm_load_print_meta: arch             = jina-bert-v2
0.00.336.760 I llm_load_print_meta: vocab type       = BPE
0.00.336.760 I llm_load_print_meta: n_vocab          = 61056
0.00.336.760 I llm_load_print_meta: n_merges         = 39382
0.00.336.761 I llm_load_print_meta: vocab_only       = 0
0.00.336.761 I llm_load_print_meta: n_ctx_train      = 8192
0.00.336.761 I llm_load_print_meta: n_embd           = 384
0.00.336.761 I llm_load_print_meta: n_layer          = 4
0.00.336.767 I llm_load_print_meta: n_head           = 12
0.00.336.767 I llm_load_print_meta: n_head_kv        = 12
0.00.336.767 I llm_load_print_meta: n_rot            = 32
0.00.336.768 I llm_load_print_meta: n_swa            = 0
0.00.336.768 I llm_load_print_meta: n_embd_head_k    = 32
0.00.336.768 I llm_load_print_meta: n_embd_head_v    = 32
0.00.336.772 I llm_load_print_meta: n_gqa            = 1
0.00.336.773 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.336.774 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.336.775 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.336.775 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.336.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.336.776 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.336.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.336.777 I llm_load_print_meta: n_ff             = 1536
0.00.336.778 I llm_load_print_meta: n_expert         = 0
0.00.336.778 I llm_load_print_meta: n_expert_used    = 0
0.00.336.778 I llm_load_print_meta: causal attn      = 0
0.00.336.778 I llm_load_print_meta: pooling type     = -1
0.00.336.778 I llm_load_print_meta: rope type        = -1
0.00.336.778 I llm_load_print_meta: rope scaling     = linear
0.00.336.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.336.779 I llm_load_print_meta: freq_scale_train = 1
0.00.336.779 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.336.779 I llm_load_print_meta: rope_finetuned   = unknown
0.00.336.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.336.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.336.780 I llm_load_print_meta: ssm_d_state      = 0
0.00.336.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.336.780 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.336.781 I llm_load_print_meta: model type       = 33M
0.00.336.781 I llm_load_print_meta: model ftype      = F16
0.00.336.781 I llm_load_print_meta: model params     = 32.90 M
0.00.336.782 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.336.782 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.336.782 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.336.783 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.336.783 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.336.783 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.336.783 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.336.784 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.336.784 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.336.784 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.336.784 I llm_load_print_meta: max token length = 45
0.00.338.049 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.338.049 I llm_load_tensors: offloading output layer to GPU
0.00.338.050 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.338.073 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.074 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.338.954 I llama_new_context_with_model: n_seq_max     = 1
0.00.338.955 I llama_new_context_with_model: n_ctx         = 8192
0.00.338.956 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.338.956 I llama_new_context_with_model: n_batch       = 2048
0.00.338.956 I llama_new_context_with_model: n_ubatch      = 2048
0.00.338.956 I llama_new_context_with_model: flash_attn    = 0
0.00.338.957 I llama_new_context_with_model: freq_base     = 10000.0
0.00.338.957 I llama_new_context_with_model: freq_scale    = 1
0.00.338.958 I ggml_metal_init: allocating
0.00.338.960 I ggml_metal_init: found device: Apple M4
0.00.338.962 I ggml_metal_init: picking default device: Apple M4
0.00.339.968 I ggml_metal_init: using embedded metal library
0.00.342.874 I ggml_metal_init: GPU name:   Apple M4
0.00.342.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.876 I ggml_metal_init: simdgroup reduction   = true
0.00.342.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.876 I ggml_metal_init: has bfloat            = true
0.00.342.876 I ggml_metal_init: use bfloat            = true
0.00.342.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.289 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.354.722 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.725 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.729 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.352 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.353 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.353 I llama_new_context_with_model: graph nodes  = 154
0.00.355.353 I llama_new_context_with_model: graph splits = 2
0.00.355.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.561 I 
0.00.368.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.849 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.850 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.852 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.853 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.856 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.856 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.369.403 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.373.127 I llama_perf_context_print:        load time =     347.18 ms
0.00.373.128 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16689.10 tokens per second)
0.00.373.128 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.130 I llama_perf_context_print:       total time =       4.57 ms /    63 tokens
0.00.373.360 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.348s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.081 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.177 I main: llama backend init
0.00.000.182 I main: load the model and apply lora adapter, if any
0.00.039.056 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.613 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.631 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.631 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.631 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.633 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.647 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.065.979 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.980 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.982 I llama_model_loader: - type  f32:  194 tensors
0.00.065.982 I llama_model_loader: - type  f16:   98 tensors
0.00.089.691 I llm_load_vocab: special tokens cache size = 25
0.00.095.929 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.934 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.934 I llm_load_print_meta: arch             = gptneox
0.00.095.934 I llm_load_print_meta: vocab type       = BPE
0.00.095.935 I llm_load_print_meta: n_vocab          = 50304
0.00.095.935 I llm_load_print_meta: n_merges         = 50009
0.00.095.935 I llm_load_print_meta: vocab_only       = 0
0.00.095.935 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.935 I llm_load_print_meta: n_embd           = 2048
0.00.095.935 I llm_load_print_meta: n_layer          = 24
0.00.095.940 I llm_load_print_meta: n_head           = 16
0.00.095.941 I llm_load_print_meta: n_head_kv        = 16
0.00.095.943 I llm_load_print_meta: n_rot            = 32
0.00.095.944 I llm_load_print_meta: n_swa            = 0
0.00.095.944 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.944 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.944 I llm_load_print_meta: n_gqa            = 1
0.00.095.945 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.946 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.946 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.946 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.947 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.947 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.947 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.948 I llm_load_print_meta: n_ff             = 8192
0.00.095.948 I llm_load_print_meta: n_expert         = 0
0.00.095.948 I llm_load_print_meta: n_expert_used    = 0
0.00.095.948 I llm_load_print_meta: causal attn      = 1
0.00.095.948 I llm_load_print_meta: pooling type     = 0
0.00.095.948 I llm_load_print_meta: rope type        = 2
0.00.095.948 I llm_load_print_meta: rope scaling     = linear
0.00.095.949 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.950 I llm_load_print_meta: freq_scale_train = 1
0.00.095.950 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.951 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.951 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.951 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.951 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.951 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.952 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.953 I llm_load_print_meta: model type       = 1.4B
0.00.095.953 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.954 I llm_load_print_meta: model params     = 1.41 B
0.00.095.954 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.954 I llm_load_print_meta: general.name     = 1.4B
0.00.095.955 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.955 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.955 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.955 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.955 I llm_load_print_meta: LF token         = 128 ''
0.00.095.956 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.956 I llm_load_print_meta: max token length = 1024
0.00.098.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.486 I llm_load_tensors: offloading output layer to GPU
0.00.098.486 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.506 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.507 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.441 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.442 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.442 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.442 I llama_new_context_with_model: n_batch       = 2048
0.00.099.442 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.443 I llama_new_context_with_model: flash_attn    = 0
0.00.099.443 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.443 I llama_new_context_with_model: freq_scale    = 1
0.00.099.444 I ggml_metal_init: allocating
0.00.099.447 I ggml_metal_init: found device: Apple M4
0.00.099.449 I ggml_metal_init: picking default device: Apple M4
0.00.100.127 I ggml_metal_init: using embedded metal library
0.00.147.155 I ggml_metal_init: GPU name:   Apple M4
0.00.147.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.147.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.147.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.147.160 I ggml_metal_init: simdgroup reduction   = true
0.00.147.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.147.160 I ggml_metal_init: has bfloat            = true
0.00.147.160 I ggml_metal_init: use bfloat            = true
0.00.147.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.147.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.172.853 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.192.558 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.192.564 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.192.584 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.193.563 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.193.564 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.193.564 I llama_new_context_with_model: graph nodes  = 967
0.00.193.564 I llama_new_context_with_model: graph splits = 2
0.00.193.580 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.193.722 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.193.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.273.876 I main: llama threadpool init, n_threads = 4
0.00.273.915 I 
0.00.273.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.273.962 I 
0.00.274.042 I sampler seed: 1234
0.00.274.047 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.274.071 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.274.072 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.274.072 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.117.772 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.02.117.772 I llama_perf_context_print:        load time =     234.81 ms
0.02.117.773 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.85 tokens per second)
0.02.117.774 I llama_perf_context_print:        eval time =    1797.41 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.117.774 I llama_perf_context_print:       total time =    1843.90 ms /    70 tokens
0.02.118.043 I ggml_metal_free: deallocating

real	0m2.427s
user	0m0.133s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.587 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.164 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.450 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.619 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.113 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.114 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.115 I llama_model_loader: - type  f32:  194 tensors
0.00.050.115 I llama_model_loader: - type  f16:   98 tensors
0.00.078.490 I llm_load_vocab: special tokens cache size = 25
0.00.084.961 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.964 I llm_load_print_meta: arch             = gptneox
0.00.084.964 I llm_load_print_meta: vocab type       = BPE
0.00.084.964 I llm_load_print_meta: n_vocab          = 50304
0.00.084.964 I llm_load_print_meta: n_merges         = 50009
0.00.084.964 I llm_load_print_meta: vocab_only       = 0
0.00.084.965 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.965 I llm_load_print_meta: n_embd           = 2048
0.00.084.965 I llm_load_print_meta: n_layer          = 24
0.00.084.968 I llm_load_print_meta: n_head           = 16
0.00.084.969 I llm_load_print_meta: n_head_kv        = 16
0.00.084.969 I llm_load_print_meta: n_rot            = 32
0.00.084.969 I llm_load_print_meta: n_swa            = 0
0.00.084.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.972 I llm_load_print_meta: n_gqa            = 1
0.00.084.972 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.973 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.974 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.974 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.975 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.975 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.975 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.976 I llm_load_print_meta: n_ff             = 8192
0.00.084.976 I llm_load_print_meta: n_expert         = 0
0.00.084.976 I llm_load_print_meta: n_expert_used    = 0
0.00.084.976 I llm_load_print_meta: causal attn      = 1
0.00.084.977 I llm_load_print_meta: pooling type     = 0
0.00.084.977 I llm_load_print_meta: rope type        = 2
0.00.084.977 I llm_load_print_meta: rope scaling     = linear
0.00.084.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.977 I llm_load_print_meta: freq_scale_train = 1
0.00.084.978 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.978 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.979 I llm_load_print_meta: model type       = 1.4B
0.00.084.979 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.980 I llm_load_print_meta: model params     = 1.41 B
0.00.084.980 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.980 I llm_load_print_meta: general.name     = 1.4B
0.00.084.980 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.980 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.981 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.981 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.981 I llm_load_print_meta: LF token         = 128 ''
0.00.084.981 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.981 I llm_load_print_meta: max token length = 1024
0.00.087.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.482 I llm_load_tensors: offloading output layer to GPU
0.00.087.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.493 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.494 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.544 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.545 I llama_new_context_with_model: n_ctx         = 128
0.00.088.545 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.545 I llama_new_context_with_model: n_batch       = 128
0.00.088.545 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.545 I llama_new_context_with_model: flash_attn    = 0
0.00.088.546 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.546 I llama_new_context_with_model: freq_scale    = 1
0.00.088.546 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.547 I ggml_metal_init: allocating
0.00.088.550 I ggml_metal_init: found device: Apple M4
0.00.088.552 I ggml_metal_init: picking default device: Apple M4
0.00.089.131 I ggml_metal_init: using embedded metal library
0.00.091.667 I ggml_metal_init: GPU name:   Apple M4
0.00.091.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.670 I ggml_metal_init: simdgroup reduction   = true
0.00.091.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.670 I ggml_metal_init: has bfloat            = true
0.00.091.670 I ggml_metal_init: use bfloat            = true
0.00.091.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.129 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.146 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.015 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.016 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.016 I llama_new_context_with_model: graph nodes  = 967
0.00.103.017 I llama_new_context_with_model: graph splits = 2
0.00.103.029 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.351.132 I 
0.01.351.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.351.204 I perplexity: tokenizing the input ..
0.01.364.875 I perplexity: tokenization took 13.666 ms
0.01.364.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.484.918 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.486.341 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.486.348 I llama_perf_context_print:        load time =    1329.96 ms
0.01.486.352 I llama_perf_context_print: prompt eval time =     119.10 ms /   128 tokens (    0.93 ms per token,  1074.72 tokens per second)
0.01.486.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.486.353 I llama_perf_context_print:       total time =     135.22 ms /   129 tokens
0.01.486.694 I ggml_metal_free: deallocating

real	0m1.682s
user	0m0.115s
sys	0m0.233s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.464 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.147 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.149 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.149 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.151 I llama_model_loader: - type  f32:  194 tensors
0.00.040.151 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.021 I llm_load_vocab: special tokens cache size = 25
0.00.077.492 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.497 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.497 I llm_load_print_meta: arch             = gptneox
0.00.077.498 I llm_load_print_meta: vocab type       = BPE
0.00.077.498 I llm_load_print_meta: n_vocab          = 50304
0.00.077.498 I llm_load_print_meta: n_merges         = 50009
0.00.077.498 I llm_load_print_meta: vocab_only       = 0
0.00.077.499 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.499 I llm_load_print_meta: n_embd           = 2048
0.00.077.499 I llm_load_print_meta: n_layer          = 24
0.00.077.504 I llm_load_print_meta: n_head           = 16
0.00.077.505 I llm_load_print_meta: n_head_kv        = 16
0.00.077.505 I llm_load_print_meta: n_rot            = 32
0.00.077.505 I llm_load_print_meta: n_swa            = 0
0.00.077.506 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.507 I llm_load_print_meta: n_gqa            = 1
0.00.077.508 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.510 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.510 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.510 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.511 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.516 I llm_load_print_meta: n_ff             = 8192
0.00.077.516 I llm_load_print_meta: n_expert         = 0
0.00.077.516 I llm_load_print_meta: n_expert_used    = 0
0.00.077.516 I llm_load_print_meta: causal attn      = 1
0.00.077.517 I llm_load_print_meta: pooling type     = 0
0.00.077.517 I llm_load_print_meta: rope type        = 2
0.00.077.517 I llm_load_print_meta: rope scaling     = linear
0.00.077.518 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.518 I llm_load_print_meta: freq_scale_train = 1
0.00.077.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.521 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.521 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.521 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.522 I llm_load_print_meta: model type       = 1.4B
0.00.077.522 I llm_load_print_meta: model ftype      = Q8_0
0.00.077.523 I llm_load_print_meta: model params     = 1.41 B
0.00.077.532 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.077.533 I llm_load_print_meta: general.name     = 1.4B
0.00.077.533 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.534 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.534 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.534 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.535 I llm_load_print_meta: LF token         = 128 ''
0.00.077.535 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.535 I llm_load_print_meta: max token length = 1024
0.00.080.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.689 I llm_load_tensors: offloading output layer to GPU
0.00.080.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.702 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.080.704 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.082.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.124 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.124 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.125 I llama_new_context_with_model: n_batch       = 2048
0.00.082.125 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.125 I llama_new_context_with_model: flash_attn    = 0
0.00.082.126 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.126 I llama_new_context_with_model: freq_scale    = 1
0.00.082.127 I ggml_metal_init: allocating
0.00.082.133 I ggml_metal_init: found device: Apple M4
0.00.082.136 I ggml_metal_init: picking default device: Apple M4
0.00.083.122 I ggml_metal_init: using embedded metal library
0.00.086.819 I ggml_metal_init: GPU name:   Apple M4
0.00.086.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.822 I ggml_metal_init: simdgroup reduction   = true
0.00.086.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.823 I ggml_metal_init: has bfloat            = true
0.00.086.823 I ggml_metal_init: use bfloat            = true
0.00.086.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.591 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.927 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.936 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.963 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.980 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.983 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.983 I llama_new_context_with_model: graph nodes  = 967
0.00.126.983 I llama_new_context_with_model: graph splits = 2
0.00.127.003 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.367.229 I main: llama threadpool init, n_threads = 4
0.01.367.306 I 
0.01.367.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.367.382 I 
0.01.367.914 I sampler seed: 1234
0.01.367.920 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.367.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.367.952 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.367.952 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.463.629 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.02.463.629 I llama_perf_context_print:        load time =    1357.19 ms
0.02.463.630 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.99 tokens per second)
0.02.463.631 I llama_perf_context_print:        eval time =    1042.73 ms /    63 runs   (   16.55 ms per token,    60.42 tokens per second)
0.02.463.631 I llama_perf_context_print:       total time =    1096.41 ms /    70 tokens
0.02.463.858 I ggml_metal_free: deallocating

real	0m2.492s
user	0m0.135s
sys	0m0.271s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.443 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.834 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.851 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.851 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.945 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.769 I llama_model_loader: - type  f32:  194 tensors
0.00.024.769 I llama_model_loader: - type q8_0:   98 tensors
0.00.045.807 I llm_load_vocab: special tokens cache size = 25
0.00.052.002 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.007 I llm_load_print_meta: arch             = gptneox
0.00.052.007 I llm_load_print_meta: vocab type       = BPE
0.00.052.008 I llm_load_print_meta: n_vocab          = 50304
0.00.052.008 I llm_load_print_meta: n_merges         = 50009
0.00.052.008 I llm_load_print_meta: vocab_only       = 0
0.00.052.008 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.008 I llm_load_print_meta: n_embd           = 2048
0.00.052.009 I llm_load_print_meta: n_layer          = 24
0.00.052.012 I llm_load_print_meta: n_head           = 16
0.00.052.013 I llm_load_print_meta: n_head_kv        = 16
0.00.052.013 I llm_load_print_meta: n_rot            = 32
0.00.052.013 I llm_load_print_meta: n_swa            = 0
0.00.052.014 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.014 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.015 I llm_load_print_meta: n_gqa            = 1
0.00.052.016 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.016 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.017 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.017 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.018 I llm_load_print_meta: n_ff             = 8192
0.00.052.019 I llm_load_print_meta: n_expert         = 0
0.00.052.019 I llm_load_print_meta: n_expert_used    = 0
0.00.052.019 I llm_load_print_meta: causal attn      = 1
0.00.052.019 I llm_load_print_meta: pooling type     = 0
0.00.052.019 I llm_load_print_meta: rope type        = 2
0.00.052.019 I llm_load_print_meta: rope scaling     = linear
0.00.052.020 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.020 I llm_load_print_meta: freq_scale_train = 1
0.00.052.020 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.020 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.021 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.021 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.021 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.021 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.021 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.023 I llm_load_print_meta: model type       = 1.4B
0.00.052.023 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.024 I llm_load_print_meta: model params     = 1.41 B
0.00.052.024 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.024 I llm_load_print_meta: general.name     = 1.4B
0.00.052.024 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.025 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.025 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.026 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.026 I llm_load_print_meta: LF token         = 128 ''
0.00.052.027 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.027 I llm_load_print_meta: max token length = 1024
0.00.054.171 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.172 I llm_load_tensors: offloading output layer to GPU
0.00.054.172 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.183 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.184 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.221 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.222 I llama_new_context_with_model: n_ctx         = 128
0.00.055.222 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.222 I llama_new_context_with_model: n_batch       = 128
0.00.055.222 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.223 I llama_new_context_with_model: flash_attn    = 0
0.00.055.223 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.223 I llama_new_context_with_model: freq_scale    = 1
0.00.055.224 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.224 I ggml_metal_init: allocating
0.00.055.228 I ggml_metal_init: found device: Apple M4
0.00.055.230 I ggml_metal_init: picking default device: Apple M4
0.00.055.846 I ggml_metal_init: using embedded metal library
0.00.058.237 I ggml_metal_init: GPU name:   Apple M4
0.00.058.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.239 I ggml_metal_init: simdgroup reduction   = true
0.00.058.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.240 I ggml_metal_init: has bfloat            = true
0.00.058.240 I ggml_metal_init: use bfloat            = true
0.00.058.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.365 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.603 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.618 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.435 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.436 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.436 I llama_new_context_with_model: graph nodes  = 967
0.00.070.436 I llama_new_context_with_model: graph splits = 2
0.00.070.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.427 I 
0.00.935.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.509 I perplexity: tokenizing the input ..
0.00.943.672 I perplexity: tokenization took 8.16 ms
0.00.943.679 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.068.085 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.286 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.299 I llama_perf_context_print:        load time =     925.98 ms
0.01.069.301 I llama_perf_context_print: prompt eval time =     124.18 ms /   128 tokens (    0.97 ms per token,  1030.76 tokens per second)
0.01.069.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.304 I llama_perf_context_print:       total time =     133.88 ms /   129 tokens
0.01.069.726 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.081s
sys	0m0.170s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.052 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.008 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.015 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.016 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.016 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.016 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.017 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.018 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.018 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.019 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.019 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.953 I llama_model_loader: - type  f32:  194 tensors
0.00.026.953 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.305 I llm_load_vocab: special tokens cache size = 25
0.00.054.441 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.444 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.444 I llm_load_print_meta: arch             = gptneox
0.00.054.445 I llm_load_print_meta: vocab type       = BPE
0.00.054.445 I llm_load_print_meta: n_vocab          = 50304
0.00.054.445 I llm_load_print_meta: n_merges         = 50009
0.00.054.445 I llm_load_print_meta: vocab_only       = 0
0.00.054.445 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.446 I llm_load_print_meta: n_embd           = 2048
0.00.054.446 I llm_load_print_meta: n_layer          = 24
0.00.054.451 I llm_load_print_meta: n_head           = 16
0.00.054.451 I llm_load_print_meta: n_head_kv        = 16
0.00.054.453 I llm_load_print_meta: n_rot            = 32
0.00.054.453 I llm_load_print_meta: n_swa            = 0
0.00.054.455 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.456 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.456 I llm_load_print_meta: n_gqa            = 1
0.00.054.457 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.458 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.459 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.459 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.459 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.459 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.462 I llm_load_print_meta: n_ff             = 8192
0.00.054.462 I llm_load_print_meta: n_expert         = 0
0.00.054.462 I llm_load_print_meta: n_expert_used    = 0
0.00.054.462 I llm_load_print_meta: causal attn      = 1
0.00.054.462 I llm_load_print_meta: pooling type     = 0
0.00.054.463 I llm_load_print_meta: rope type        = 2
0.00.054.463 I llm_load_print_meta: rope scaling     = linear
0.00.054.463 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.463 I llm_load_print_meta: freq_scale_train = 1
0.00.054.464 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.464 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.464 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.464 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.464 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.464 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.465 I llm_load_print_meta: model type       = 1.4B
0.00.054.465 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.466 I llm_load_print_meta: model params     = 1.41 B
0.00.054.466 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.467 I llm_load_print_meta: general.name     = 1.4B
0.00.054.467 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.467 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.467 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.467 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: LF token         = 128 ''
0.00.054.468 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: max token length = 1024
0.00.056.841 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.841 I llm_load_tensors: offloading output layer to GPU
0.00.056.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.853 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.854 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.842 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.843 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.843 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.843 I llama_new_context_with_model: n_batch       = 2048
0.00.057.843 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.843 I llama_new_context_with_model: flash_attn    = 0
0.00.057.844 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.844 I llama_new_context_with_model: freq_scale    = 1
0.00.057.845 I ggml_metal_init: allocating
0.00.057.851 I ggml_metal_init: found device: Apple M4
0.00.057.853 I ggml_metal_init: picking default device: Apple M4
0.00.058.582 I ggml_metal_init: using embedded metal library
0.00.061.165 I ggml_metal_init: GPU name:   Apple M4
0.00.061.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.168 I ggml_metal_init: simdgroup reduction   = true
0.00.061.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.168 I ggml_metal_init: has bfloat            = true
0.00.061.168 I ggml_metal_init: use bfloat            = true
0.00.061.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.859 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.095.998 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.008 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.032 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.138 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.141 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.141 I llama_new_context_with_model: graph nodes  = 967
0.00.097.141 I llama_new_context_with_model: graph splits = 2
0.00.097.161 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.321 I main: llama threadpool init, n_threads = 4
0.00.692.370 I 
0.00.692.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.411 I 
0.00.692.639 I sampler seed: 1234
0.00.692.644 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.686 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.691 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.369.004 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.369.005 I llama_perf_context_print:        load time =     681.26 ms
0.01.369.006 I llama_perf_context_print: prompt eval time =      43.44 ms /     7 tokens (    6.21 ms per token,   161.13 tokens per second)
0.01.369.006 I llama_perf_context_print:        eval time =     629.85 ms /    63 runs   (   10.00 ms per token,   100.02 tokens per second)
0.01.369.008 I llama_perf_context_print:       total time =     676.69 ms /    70 tokens
0.01.369.239 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.115 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.964 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.965 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.993 I llama_model_loader: - type  f32:  194 tensors
0.00.024.993 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.993 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.134 I llm_load_vocab: special tokens cache size = 25
0.00.052.070 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.073 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.073 I llm_load_print_meta: arch             = gptneox
0.00.052.073 I llm_load_print_meta: vocab type       = BPE
0.00.052.073 I llm_load_print_meta: n_vocab          = 50304
0.00.052.074 I llm_load_print_meta: n_merges         = 50009
0.00.052.074 I llm_load_print_meta: vocab_only       = 0
0.00.052.074 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.074 I llm_load_print_meta: n_embd           = 2048
0.00.052.074 I llm_load_print_meta: n_layer          = 24
0.00.052.078 I llm_load_print_meta: n_head           = 16
0.00.052.078 I llm_load_print_meta: n_head_kv        = 16
0.00.052.078 I llm_load_print_meta: n_rot            = 32
0.00.052.079 I llm_load_print_meta: n_swa            = 0
0.00.052.079 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.079 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.080 I llm_load_print_meta: n_gqa            = 1
0.00.052.080 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.081 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.082 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.082 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.082 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.082 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.083 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.083 I llm_load_print_meta: n_ff             = 8192
0.00.052.083 I llm_load_print_meta: n_expert         = 0
0.00.052.083 I llm_load_print_meta: n_expert_used    = 0
0.00.052.084 I llm_load_print_meta: causal attn      = 1
0.00.052.084 I llm_load_print_meta: pooling type     = 0
0.00.052.084 I llm_load_print_meta: rope type        = 2
0.00.052.084 I llm_load_print_meta: rope scaling     = linear
0.00.052.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.094 I llm_load_print_meta: freq_scale_train = 1
0.00.052.094 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.094 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.095 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.096 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.096 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.096 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.096 I llm_load_print_meta: model type       = 1.4B
0.00.052.097 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.097 I llm_load_print_meta: model params     = 1.41 B
0.00.052.097 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.098 I llm_load_print_meta: general.name     = 1.4B
0.00.052.098 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.099 I llm_load_print_meta: LF token         = 128 ''
0.00.052.099 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.099 I llm_load_print_meta: max token length = 1024
0.00.054.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.016 I llm_load_tensors: offloading output layer to GPU
0.00.054.016 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.027 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.028 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.891 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.892 I llama_new_context_with_model: n_ctx         = 128
0.00.054.892 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.892 I llama_new_context_with_model: n_batch       = 128
0.00.054.892 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.892 I llama_new_context_with_model: flash_attn    = 0
0.00.054.893 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.893 I llama_new_context_with_model: freq_scale    = 1
0.00.054.893 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.893 I ggml_metal_init: allocating
0.00.054.896 I ggml_metal_init: found device: Apple M4
0.00.054.898 I ggml_metal_init: picking default device: Apple M4
0.00.055.482 I ggml_metal_init: using embedded metal library
0.00.057.848 I ggml_metal_init: GPU name:   Apple M4
0.00.057.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.850 I ggml_metal_init: simdgroup reduction   = true
0.00.057.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.850 I ggml_metal_init: has bfloat            = true
0.00.057.850 I ggml_metal_init: use bfloat            = true
0.00.057.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.656 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.859 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.863 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.876 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.758 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.760 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.760 I llama_new_context_with_model: graph nodes  = 967
0.00.069.760 I llama_new_context_with_model: graph splits = 2
0.00.069.772 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.204 I 
0.00.630.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.279 I perplexity: tokenizing the input ..
0.00.638.250 I perplexity: tokenization took 7.97 ms
0.00.638.257 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.094 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.762.252 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.762.270 I llama_perf_context_print:        load time =     620.08 ms
0.00.762.271 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.96 tokens per second)
0.00.762.272 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.272 I llama_perf_context_print:       total time =     132.07 ms /   129 tokens
0.00.762.684 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.079s
sys	0m0.106s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.139 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.444 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.445 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.161 I llama_model_loader: - type  f32:  194 tensors
0.00.024.162 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.162 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.490 I llm_load_vocab: special tokens cache size = 25
0.00.050.362 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.364 I llm_load_print_meta: arch             = gptneox
0.00.050.365 I llm_load_print_meta: vocab type       = BPE
0.00.050.365 I llm_load_print_meta: n_vocab          = 50304
0.00.050.365 I llm_load_print_meta: n_merges         = 50009
0.00.050.365 I llm_load_print_meta: vocab_only       = 0
0.00.050.366 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.366 I llm_load_print_meta: n_embd           = 2048
0.00.050.366 I llm_load_print_meta: n_layer          = 24
0.00.050.369 I llm_load_print_meta: n_head           = 16
0.00.050.369 I llm_load_print_meta: n_head_kv        = 16
0.00.050.371 I llm_load_print_meta: n_rot            = 32
0.00.050.372 I llm_load_print_meta: n_swa            = 0
0.00.050.372 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.372 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.373 I llm_load_print_meta: n_gqa            = 1
0.00.050.374 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.379 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.379 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.380 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.380 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.380 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.380 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.381 I llm_load_print_meta: n_ff             = 8192
0.00.050.381 I llm_load_print_meta: n_expert         = 0
0.00.050.382 I llm_load_print_meta: n_expert_used    = 0
0.00.050.382 I llm_load_print_meta: causal attn      = 1
0.00.050.382 I llm_load_print_meta: pooling type     = 0
0.00.050.382 I llm_load_print_meta: rope type        = 2
0.00.050.382 I llm_load_print_meta: rope scaling     = linear
0.00.050.383 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.383 I llm_load_print_meta: freq_scale_train = 1
0.00.050.383 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.384 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.384 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.384 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.385 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.385 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.385 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.386 I llm_load_print_meta: model type       = 1.4B
0.00.050.386 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.386 I llm_load_print_meta: model params     = 1.41 B
0.00.050.387 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.387 I llm_load_print_meta: general.name     = 1.4B
0.00.050.387 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.387 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.387 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: LF token         = 128 ''
0.00.050.388 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: max token length = 1024
0.00.052.249 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.250 I llm_load_tensors: offloading output layer to GPU
0.00.052.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.255 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.256 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.280 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.281 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.281 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.282 I llama_new_context_with_model: n_batch       = 2048
0.00.053.282 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.282 I llama_new_context_with_model: flash_attn    = 0
0.00.053.282 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.283 I llama_new_context_with_model: freq_scale    = 1
0.00.053.283 I ggml_metal_init: allocating
0.00.053.291 I ggml_metal_init: found device: Apple M4
0.00.053.293 I ggml_metal_init: picking default device: Apple M4
0.00.053.893 I ggml_metal_init: using embedded metal library
0.00.056.283 I ggml_metal_init: GPU name:   Apple M4
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.286 I ggml_metal_init: simdgroup reduction   = true
0.00.056.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.286 I ggml_metal_init: has bfloat            = true
0.00.056.286 I ggml_metal_init: use bfloat            = true
0.00.056.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.912 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.683 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.707 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.776 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.777 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.777 I llama_new_context_with_model: graph nodes  = 967
0.00.086.778 I llama_new_context_with_model: graph splits = 2
0.00.086.792 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.268 I main: llama threadpool init, n_threads = 4
0.00.721.318 I 
0.00.721.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.354 I 
0.00.721.571 I sampler seed: 1234
0.00.721.576 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.611 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.613 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.613 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.456.257 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.456.257 I llama_perf_context_print:        load time =     712.12 ms
0.01.456.259 I llama_perf_context_print: prompt eval time =      45.74 ms /     7 tokens (    6.53 ms per token,   153.05 tokens per second)
0.01.456.260 I llama_perf_context_print:        eval time =     686.21 ms /    63 runs   (   10.89 ms per token,    91.81 tokens per second)
0.01.456.260 I llama_perf_context_print:       total time =     734.99 ms /    70 tokens
0.01.456.498 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.108s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.846 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.369 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.370 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.372 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.260 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.171 I llama_model_loader: - type  f32:  194 tensors
0.00.023.172 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.172 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.363 I llm_load_vocab: special tokens cache size = 25
0.00.049.426 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.428 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.428 I llm_load_print_meta: arch             = gptneox
0.00.049.429 I llm_load_print_meta: vocab type       = BPE
0.00.049.429 I llm_load_print_meta: n_vocab          = 50304
0.00.049.429 I llm_load_print_meta: n_merges         = 50009
0.00.049.429 I llm_load_print_meta: vocab_only       = 0
0.00.049.429 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.430 I llm_load_print_meta: n_embd           = 2048
0.00.049.430 I llm_load_print_meta: n_layer          = 24
0.00.049.433 I llm_load_print_meta: n_head           = 16
0.00.049.433 I llm_load_print_meta: n_head_kv        = 16
0.00.049.434 I llm_load_print_meta: n_rot            = 32
0.00.049.434 I llm_load_print_meta: n_swa            = 0
0.00.049.434 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.434 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.435 I llm_load_print_meta: n_gqa            = 1
0.00.049.436 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.436 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.437 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.437 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.437 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.438 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.438 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.438 I llm_load_print_meta: n_ff             = 8192
0.00.049.439 I llm_load_print_meta: n_expert         = 0
0.00.049.439 I llm_load_print_meta: n_expert_used    = 0
0.00.049.439 I llm_load_print_meta: causal attn      = 1
0.00.049.439 I llm_load_print_meta: pooling type     = 0
0.00.049.439 I llm_load_print_meta: rope type        = 2
0.00.049.439 I llm_load_print_meta: rope scaling     = linear
0.00.049.440 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.440 I llm_load_print_meta: freq_scale_train = 1
0.00.049.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.441 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.441 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.441 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.441 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.441 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.441 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.442 I llm_load_print_meta: model type       = 1.4B
0.00.049.442 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.443 I llm_load_print_meta: model params     = 1.41 B
0.00.049.443 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.443 I llm_load_print_meta: general.name     = 1.4B
0.00.049.446 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: LF token         = 128 ''
0.00.049.447 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: max token length = 1024
0.00.051.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.405 I llm_load_tensors: offloading output layer to GPU
0.00.051.405 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.415 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.416 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.298 I llama_new_context_with_model: n_ctx         = 128
0.00.052.298 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.298 I llama_new_context_with_model: n_batch       = 128
0.00.052.298 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.299 I llama_new_context_with_model: flash_attn    = 0
0.00.052.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.299 I llama_new_context_with_model: freq_scale    = 1
0.00.052.300 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.300 I ggml_metal_init: allocating
0.00.052.303 I ggml_metal_init: found device: Apple M4
0.00.052.305 I ggml_metal_init: picking default device: Apple M4
0.00.052.866 I ggml_metal_init: using embedded metal library
0.00.055.209 I ggml_metal_init: GPU name:   Apple M4
0.00.055.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.212 I ggml_metal_init: simdgroup reduction   = true
0.00.055.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.212 I ggml_metal_init: has bfloat            = true
0.00.055.212 I ggml_metal_init: use bfloat            = true
0.00.055.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.733 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.961 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.965 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.980 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.879 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.880 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.880 I llama_new_context_with_model: graph nodes  = 967
0.00.066.880 I llama_new_context_with_model: graph splits = 2
0.00.066.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.668 I 
0.00.666.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.712 I perplexity: tokenizing the input ..
0.00.674.512 I perplexity: tokenization took 7.798 ms
0.00.674.516 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.278 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.798.553 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.798.573 I llama_perf_context_print:        load time =     657.82 ms
0.00.798.576 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.57 tokens per second)
0.00.798.577 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.577 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.00.799.055 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.109s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.014.996 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.029.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.730 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.633 I llama_model_loader: - type  f32:  194 tensors
0.00.039.633 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.634 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.141 I llm_load_vocab: special tokens cache size = 25
0.00.074.112 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.115 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.115 I llm_load_print_meta: arch             = gptneox
0.00.074.116 I llm_load_print_meta: vocab type       = BPE
0.00.074.116 I llm_load_print_meta: n_vocab          = 50304
0.00.074.116 I llm_load_print_meta: n_merges         = 50009
0.00.074.116 I llm_load_print_meta: vocab_only       = 0
0.00.074.116 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.116 I llm_load_print_meta: n_embd           = 2048
0.00.074.117 I llm_load_print_meta: n_layer          = 24
0.00.074.120 I llm_load_print_meta: n_head           = 16
0.00.074.120 I llm_load_print_meta: n_head_kv        = 16
0.00.074.121 I llm_load_print_meta: n_rot            = 32
0.00.074.121 I llm_load_print_meta: n_swa            = 0
0.00.074.121 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.121 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.122 I llm_load_print_meta: n_gqa            = 1
0.00.074.123 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.123 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.124 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.124 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.125 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.125 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.125 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.126 I llm_load_print_meta: n_ff             = 8192
0.00.074.126 I llm_load_print_meta: n_expert         = 0
0.00.074.126 I llm_load_print_meta: n_expert_used    = 0
0.00.074.128 I llm_load_print_meta: causal attn      = 1
0.00.074.128 I llm_load_print_meta: pooling type     = 0
0.00.074.128 I llm_load_print_meta: rope type        = 2
0.00.074.129 I llm_load_print_meta: rope scaling     = linear
0.00.074.129 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.130 I llm_load_print_meta: freq_scale_train = 1
0.00.074.130 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.130 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.130 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.130 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.131 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.131 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.131 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.131 I llm_load_print_meta: model type       = 1.4B
0.00.074.132 I llm_load_print_meta: model ftype      = Q5_0
0.00.074.132 I llm_load_print_meta: model params     = 1.41 B
0.00.074.133 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.074.133 I llm_load_print_meta: general.name     = 1.4B
0.00.074.133 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.133 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.133 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.134 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.134 I llm_load_print_meta: LF token         = 128 ''
0.00.074.134 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.134 I llm_load_print_meta: max token length = 1024
0.00.076.438 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.438 I llm_load_tensors: offloading output layer to GPU
0.00.076.439 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.449 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.076.451 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.077.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.552 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.553 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.553 I llama_new_context_with_model: n_batch       = 2048
0.00.077.553 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.553 I llama_new_context_with_model: flash_attn    = 0
0.00.077.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.554 I llama_new_context_with_model: freq_scale    = 1
0.00.077.555 I ggml_metal_init: allocating
0.00.077.558 I ggml_metal_init: found device: Apple M4
0.00.077.560 I ggml_metal_init: picking default device: Apple M4
0.00.078.262 I ggml_metal_init: using embedded metal library
0.00.081.308 I ggml_metal_init: GPU name:   Apple M4
0.00.081.311 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.311 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.312 I ggml_metal_init: simdgroup reduction   = true
0.00.081.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.312 I ggml_metal_init: has bfloat            = true
0.00.081.312 I ggml_metal_init: use bfloat            = true
0.00.081.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.501 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.114.661 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.666 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.687 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.706 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.707 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.707 I llama_new_context_with_model: graph nodes  = 967
0.00.115.708 I llama_new_context_with_model: graph splits = 2
0.00.115.723 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.643 I main: llama threadpool init, n_threads = 4
0.00.767.681 I 
0.00.767.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.708 I 
0.00.767.925 I sampler seed: 1234
0.00.767.929 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.990 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.556.055 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.556.056 I llama_perf_context_print:        load time =     752.64 ms
0.01.556.056 I llama_perf_context_print: prompt eval time =      43.37 ms /     7 tokens (    6.20 ms per token,   161.39 tokens per second)
0.01.556.057 I llama_perf_context_print:        eval time =     741.58 ms /    63 runs   (   11.77 ms per token,    84.95 tokens per second)
0.01.556.057 I llama_perf_context_print:       total time =     788.41 ms /    70 tokens
0.01.556.276 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.123s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.319 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.234 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.247 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.247 I llama_model_loader: - type  f32:  194 tensors
0.00.023.248 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.248 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.468 I llm_load_vocab: special tokens cache size = 25
0.00.049.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.253 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.253 I llm_load_print_meta: arch             = gptneox
0.00.049.253 I llm_load_print_meta: vocab type       = BPE
0.00.049.254 I llm_load_print_meta: n_vocab          = 50304
0.00.049.254 I llm_load_print_meta: n_merges         = 50009
0.00.049.254 I llm_load_print_meta: vocab_only       = 0
0.00.049.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.254 I llm_load_print_meta: n_embd           = 2048
0.00.049.254 I llm_load_print_meta: n_layer          = 24
0.00.049.257 I llm_load_print_meta: n_head           = 16
0.00.049.260 I llm_load_print_meta: n_head_kv        = 16
0.00.049.261 I llm_load_print_meta: n_rot            = 32
0.00.049.261 I llm_load_print_meta: n_swa            = 0
0.00.049.261 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.261 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.262 I llm_load_print_meta: n_gqa            = 1
0.00.049.263 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.263 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.264 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.264 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.264 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.265 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.269 I llm_load_print_meta: n_ff             = 8192
0.00.049.269 I llm_load_print_meta: n_expert         = 0
0.00.049.270 I llm_load_print_meta: n_expert_used    = 0
0.00.049.270 I llm_load_print_meta: causal attn      = 1
0.00.049.270 I llm_load_print_meta: pooling type     = 0
0.00.049.270 I llm_load_print_meta: rope type        = 2
0.00.049.270 I llm_load_print_meta: rope scaling     = linear
0.00.049.272 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.274 I llm_load_print_meta: freq_scale_train = 1
0.00.049.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.274 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.274 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.275 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.275 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.276 I llm_load_print_meta: model type       = 1.4B
0.00.049.277 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.277 I llm_load_print_meta: model params     = 1.41 B
0.00.049.277 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.278 I llm_load_print_meta: general.name     = 1.4B
0.00.049.278 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.278 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.278 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.278 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.279 I llm_load_print_meta: LF token         = 128 ''
0.00.049.279 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.279 I llm_load_print_meta: max token length = 1024
0.00.051.331 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.331 I llm_load_tensors: offloading output layer to GPU
0.00.051.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.342 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.343 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.315 I llama_new_context_with_model: n_ctx         = 128
0.00.052.315 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.315 I llama_new_context_with_model: n_batch       = 128
0.00.052.315 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.315 I llama_new_context_with_model: flash_attn    = 0
0.00.052.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.316 I llama_new_context_with_model: freq_scale    = 1
0.00.052.316 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.317 I ggml_metal_init: allocating
0.00.052.324 I ggml_metal_init: found device: Apple M4
0.00.052.326 I ggml_metal_init: picking default device: Apple M4
0.00.052.898 I ggml_metal_init: using embedded metal library
0.00.055.237 I ggml_metal_init: GPU name:   Apple M4
0.00.055.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.240 I ggml_metal_init: simdgroup reduction   = true
0.00.055.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.240 I ggml_metal_init: has bfloat            = true
0.00.055.240 I ggml_metal_init: use bfloat            = true
0.00.055.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.709 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.965 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.979 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.869 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.870 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.870 I llama_new_context_with_model: graph nodes  = 967
0.00.066.870 I llama_new_context_with_model: graph splits = 2
0.00.066.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.920 I 
0.00.712.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.964 I perplexity: tokenizing the input ..
0.00.721.082 I perplexity: tokenization took 8.116 ms
0.00.721.085 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.082 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.857.253 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.857.268 I llama_perf_context_print:        load time =     704.14 ms
0.00.857.269 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.77 tokens per second)
0.00.857.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.270 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.857.818 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.078s
sys	0m0.110s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.499 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.305 I llama_model_loader: - type  f32:  194 tensors
0.00.026.305 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.499 I llm_load_vocab: special tokens cache size = 25
0.00.052.632 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.635 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.635 I llm_load_print_meta: arch             = gptneox
0.00.052.635 I llm_load_print_meta: vocab type       = BPE
0.00.052.636 I llm_load_print_meta: n_vocab          = 50304
0.00.052.636 I llm_load_print_meta: n_merges         = 50009
0.00.052.636 I llm_load_print_meta: vocab_only       = 0
0.00.052.636 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.636 I llm_load_print_meta: n_embd           = 2048
0.00.052.636 I llm_load_print_meta: n_layer          = 24
0.00.052.639 I llm_load_print_meta: n_head           = 16
0.00.052.640 I llm_load_print_meta: n_head_kv        = 16
0.00.052.640 I llm_load_print_meta: n_rot            = 32
0.00.052.640 I llm_load_print_meta: n_swa            = 0
0.00.052.641 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.641 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.642 I llm_load_print_meta: n_gqa            = 1
0.00.052.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.643 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.644 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.644 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.644 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.645 I llm_load_print_meta: n_ff             = 8192
0.00.052.645 I llm_load_print_meta: n_expert         = 0
0.00.052.646 I llm_load_print_meta: n_expert_used    = 0
0.00.052.647 I llm_load_print_meta: causal attn      = 1
0.00.052.649 I llm_load_print_meta: pooling type     = 0
0.00.052.649 I llm_load_print_meta: rope type        = 2
0.00.052.649 I llm_load_print_meta: rope scaling     = linear
0.00.052.649 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.650 I llm_load_print_meta: freq_scale_train = 1
0.00.052.650 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.650 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.650 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.650 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.650 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.650 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.651 I llm_load_print_meta: model type       = 1.4B
0.00.052.651 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.652 I llm_load_print_meta: model params     = 1.41 B
0.00.052.652 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.652 I llm_load_print_meta: general.name     = 1.4B
0.00.052.653 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.653 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.653 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.653 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.653 I llm_load_print_meta: LF token         = 128 ''
0.00.052.654 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: max token length = 1024
0.00.054.701 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.701 I llm_load_tensors: offloading output layer to GPU
0.00.054.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.712 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.713 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.736 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.736 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.736 I llama_new_context_with_model: n_batch       = 2048
0.00.055.737 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.737 I llama_new_context_with_model: flash_attn    = 0
0.00.055.737 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.737 I llama_new_context_with_model: freq_scale    = 1
0.00.055.738 I ggml_metal_init: allocating
0.00.055.741 I ggml_metal_init: found device: Apple M4
0.00.055.743 I ggml_metal_init: picking default device: Apple M4
0.00.056.352 I ggml_metal_init: using embedded metal library
0.00.058.674 I ggml_metal_init: GPU name:   Apple M4
0.00.058.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.676 I ggml_metal_init: simdgroup reduction   = true
0.00.058.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.678 I ggml_metal_init: has bfloat            = true
0.00.058.678 I ggml_metal_init: use bfloat            = true
0.00.058.679 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.394 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.317 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.329 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.361 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.440 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.442 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.442 I llama_new_context_with_model: graph nodes  = 967
0.00.088.442 I llama_new_context_with_model: graph splits = 2
0.00.088.457 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.265 I main: llama threadpool init, n_threads = 4
0.00.704.301 I 
0.00.704.333 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.333 I 
0.00.704.564 I sampler seed: 1234
0.00.704.569 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.620 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.622 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.622 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.543.784 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.543.785 I llama_perf_context_print:        load time =     695.40 ms
0.01.543.786 I llama_perf_context_print: prompt eval time =      42.17 ms /     7 tokens (    6.02 ms per token,   165.98 tokens per second)
0.01.543.786 I llama_perf_context_print:        eval time =     793.99 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.543.787 I llama_perf_context_print:       total time =     839.52 ms /    70 tokens
0.01.544.037 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.108s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.978 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.812 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.774 I llama_model_loader: - type  f32:  194 tensors
0.00.024.774 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.892 I llm_load_vocab: special tokens cache size = 25
0.00.051.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.893 I llm_load_print_meta: arch             = gptneox
0.00.051.893 I llm_load_print_meta: vocab type       = BPE
0.00.051.893 I llm_load_print_meta: n_vocab          = 50304
0.00.051.893 I llm_load_print_meta: n_merges         = 50009
0.00.051.894 I llm_load_print_meta: vocab_only       = 0
0.00.051.894 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.894 I llm_load_print_meta: n_embd           = 2048
0.00.051.894 I llm_load_print_meta: n_layer          = 24
0.00.051.899 I llm_load_print_meta: n_head           = 16
0.00.051.899 I llm_load_print_meta: n_head_kv        = 16
0.00.051.900 I llm_load_print_meta: n_rot            = 32
0.00.051.900 I llm_load_print_meta: n_swa            = 0
0.00.051.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.901 I llm_load_print_meta: n_gqa            = 1
0.00.051.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.903 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.905 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.905 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.905 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.906 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.906 I llm_load_print_meta: n_ff             = 8192
0.00.051.906 I llm_load_print_meta: n_expert         = 0
0.00.051.907 I llm_load_print_meta: n_expert_used    = 0
0.00.051.907 I llm_load_print_meta: causal attn      = 1
0.00.051.907 I llm_load_print_meta: pooling type     = 0
0.00.051.907 I llm_load_print_meta: rope type        = 2
0.00.051.907 I llm_load_print_meta: rope scaling     = linear
0.00.051.908 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.908 I llm_load_print_meta: freq_scale_train = 1
0.00.051.908 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.909 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.909 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.909 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.909 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.909 I llm_load_print_meta: model type       = 1.4B
0.00.051.914 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.915 I llm_load_print_meta: model params     = 1.41 B
0.00.051.916 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.916 I llm_load_print_meta: general.name     = 1.4B
0.00.051.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.917 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.917 I llm_load_print_meta: LF token         = 128 ''
0.00.051.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.918 I llm_load_print_meta: max token length = 1024
0.00.053.910 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.910 I llm_load_tensors: offloading output layer to GPU
0.00.053.910 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.920 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.922 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.864 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.865 I llama_new_context_with_model: n_ctx         = 128
0.00.054.865 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.865 I llama_new_context_with_model: n_batch       = 128
0.00.054.865 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.865 I llama_new_context_with_model: flash_attn    = 0
0.00.054.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.866 I llama_new_context_with_model: freq_scale    = 1
0.00.054.867 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.867 I ggml_metal_init: allocating
0.00.054.871 I ggml_metal_init: found device: Apple M4
0.00.054.873 I ggml_metal_init: picking default device: Apple M4
0.00.055.437 I ggml_metal_init: using embedded metal library
0.00.057.800 I ggml_metal_init: GPU name:   Apple M4
0.00.057.802 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.802 I ggml_metal_init: simdgroup reduction   = true
0.00.057.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.803 I ggml_metal_init: has bfloat            = true
0.00.057.803 I ggml_metal_init: use bfloat            = true
0.00.057.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.648 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.912 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.916 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.932 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.816 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.817 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.817 I llama_new_context_with_model: graph nodes  = 967
0.00.069.817 I llama_new_context_with_model: graph splits = 2
0.00.069.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.035 I 
0.00.673.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.082 I perplexity: tokenizing the input ..
0.00.681.243 I perplexity: tokenization took 8.159 ms
0.00.681.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.427 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.817.612 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.817.639 I llama_perf_context_print:        load time =     663.05 ms
0.00.817.640 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.47 tokens per second)
0.00.817.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.641 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.818.124 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.128s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.231 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.527 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.529 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.529 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.407 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.408 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.409 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.410 I llama_model_loader: - type  f32:  194 tensors
0.00.024.410 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.411 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.819 I llm_load_vocab: special tokens cache size = 25
0.00.050.802 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.805 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.805 I llm_load_print_meta: arch             = gptneox
0.00.050.805 I llm_load_print_meta: vocab type       = BPE
0.00.050.805 I llm_load_print_meta: n_vocab          = 50304
0.00.050.806 I llm_load_print_meta: n_merges         = 50009
0.00.050.806 I llm_load_print_meta: vocab_only       = 0
0.00.050.806 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.806 I llm_load_print_meta: n_embd           = 2048
0.00.050.806 I llm_load_print_meta: n_layer          = 24
0.00.050.809 I llm_load_print_meta: n_head           = 16
0.00.050.812 I llm_load_print_meta: n_head_kv        = 16
0.00.050.812 I llm_load_print_meta: n_rot            = 32
0.00.050.812 I llm_load_print_meta: n_swa            = 0
0.00.050.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.813 I llm_load_print_meta: n_gqa            = 1
0.00.050.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.817 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.819 I llm_load_print_meta: n_ff             = 8192
0.00.050.819 I llm_load_print_meta: n_expert         = 0
0.00.050.819 I llm_load_print_meta: n_expert_used    = 0
0.00.050.819 I llm_load_print_meta: causal attn      = 1
0.00.050.819 I llm_load_print_meta: pooling type     = 0
0.00.050.819 I llm_load_print_meta: rope type        = 2
0.00.050.819 I llm_load_print_meta: rope scaling     = linear
0.00.050.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.820 I llm_load_print_meta: freq_scale_train = 1
0.00.050.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.823 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.823 I llm_load_print_meta: model type       = 1.4B
0.00.050.824 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.824 I llm_load_print_meta: model params     = 1.41 B
0.00.050.825 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.825 I llm_load_print_meta: general.name     = 1.4B
0.00.050.825 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.825 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.827 I llm_load_print_meta: LF token         = 128 ''
0.00.050.827 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.827 I llm_load_print_meta: max token length = 1024
0.00.052.761 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.761 I llm_load_tensors: offloading output layer to GPU
0.00.052.762 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.772 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.773 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.667 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.668 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.668 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.668 I llama_new_context_with_model: n_batch       = 2048
0.00.053.668 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.669 I llama_new_context_with_model: flash_attn    = 0
0.00.053.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.669 I llama_new_context_with_model: freq_scale    = 1
0.00.053.670 I ggml_metal_init: allocating
0.00.053.673 I ggml_metal_init: found device: Apple M4
0.00.053.675 I ggml_metal_init: picking default device: Apple M4
0.00.054.254 I ggml_metal_init: using embedded metal library
0.00.056.566 I ggml_metal_init: GPU name:   Apple M4
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.568 I ggml_metal_init: simdgroup reduction   = true
0.00.056.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.569 I ggml_metal_init: has bfloat            = true
0.00.056.569 I ggml_metal_init: use bfloat            = true
0.00.056.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.195 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.782 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.801 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.811 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.812 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.812 I llama_new_context_with_model: graph nodes  = 967
0.00.086.813 I llama_new_context_with_model: graph splits = 2
0.00.086.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.252 I main: llama threadpool init, n_threads = 4
0.00.435.285 I 
0.00.435.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.435.314 I 
0.00.435.452 I sampler seed: 1234
0.00.435.456 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.435.466 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.435.466 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.435.466 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.114.778 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.114.779 I llama_perf_context_print:        load time =     426.02 ms
0.01.114.780 I llama_perf_context_print: prompt eval time =      35.73 ms /     7 tokens (    5.10 ms per token,   195.92 tokens per second)
0.01.114.781 I llama_perf_context_print:        eval time =     640.52 ms /    63 runs   (   10.17 ms per token,    98.36 tokens per second)
0.01.114.781 I llama_perf_context_print:       total time =     679.53 ms /    70 tokens
0.01.115.030 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.108s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.582 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.583 I llama_model_loader: - type  f32:  194 tensors
0.00.023.583 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.583 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.584 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.866 I llm_load_vocab: special tokens cache size = 25
0.00.050.036 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.039 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.039 I llm_load_print_meta: arch             = gptneox
0.00.050.039 I llm_load_print_meta: vocab type       = BPE
0.00.050.040 I llm_load_print_meta: n_vocab          = 50304
0.00.050.040 I llm_load_print_meta: n_merges         = 50009
0.00.050.040 I llm_load_print_meta: vocab_only       = 0
0.00.050.040 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.040 I llm_load_print_meta: n_embd           = 2048
0.00.050.041 I llm_load_print_meta: n_layer          = 24
0.00.050.043 I llm_load_print_meta: n_head           = 16
0.00.050.044 I llm_load_print_meta: n_head_kv        = 16
0.00.050.045 I llm_load_print_meta: n_rot            = 32
0.00.050.045 I llm_load_print_meta: n_swa            = 0
0.00.050.045 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.045 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.046 I llm_load_print_meta: n_gqa            = 1
0.00.050.047 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.047 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.048 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.050 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.059 I llm_load_print_meta: n_ff             = 8192
0.00.050.060 I llm_load_print_meta: n_expert         = 0
0.00.050.060 I llm_load_print_meta: n_expert_used    = 0
0.00.050.060 I llm_load_print_meta: causal attn      = 1
0.00.050.060 I llm_load_print_meta: pooling type     = 0
0.00.050.061 I llm_load_print_meta: rope type        = 2
0.00.050.061 I llm_load_print_meta: rope scaling     = linear
0.00.050.061 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.062 I llm_load_print_meta: freq_scale_train = 1
0.00.050.062 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.062 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.062 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.062 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.062 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.063 I llm_load_print_meta: model type       = 1.4B
0.00.050.064 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.064 I llm_load_print_meta: model params     = 1.41 B
0.00.050.066 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.066 I llm_load_print_meta: general.name     = 1.4B
0.00.050.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.066 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.066 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: LF token         = 128 ''
0.00.050.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: max token length = 1024
0.00.051.957 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.957 I llm_load_tensors: offloading output layer to GPU
0.00.051.957 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.968 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.969 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.840 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.840 I llama_new_context_with_model: n_ctx         = 128
0.00.052.841 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.841 I llama_new_context_with_model: n_batch       = 128
0.00.052.841 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.841 I llama_new_context_with_model: flash_attn    = 0
0.00.052.842 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.842 I llama_new_context_with_model: freq_scale    = 1
0.00.052.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.843 I ggml_metal_init: allocating
0.00.052.848 I ggml_metal_init: found device: Apple M4
0.00.052.850 I ggml_metal_init: picking default device: Apple M4
0.00.053.396 I ggml_metal_init: using embedded metal library
0.00.055.791 I ggml_metal_init: GPU name:   Apple M4
0.00.055.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.793 I ggml_metal_init: simdgroup reduction   = true
0.00.055.793 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.793 I ggml_metal_init: has bfloat            = true
0.00.055.793 I ggml_metal_init: use bfloat            = true
0.00.055.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.300 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.689 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.691 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.715 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.498 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.499 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.499 I llama_new_context_with_model: graph nodes  = 967
0.00.067.499 I llama_new_context_with_model: graph splits = 2
0.00.067.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.888 I 
0.00.387.947 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.966 I perplexity: tokenizing the input ..
0.00.395.776 I perplexity: tokenization took 7.809 ms
0.00.395.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.527.282 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.718 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.528.735 I llama_perf_context_print:        load time =     379.06 ms
0.00.528.736 I llama_perf_context_print: prompt eval time =     131.27 ms /   128 tokens (    1.03 ms per token,   975.11 tokens per second)
0.00.528.737 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.528.738 I llama_perf_context_print:       total time =     140.85 ms /   129 tokens
0.00.529.200 I ggml_metal_free: deallocating

real	0m0.541s
user	0m0.077s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.038 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.496 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.363 I llama_model_loader: - type  f32:  194 tensors
0.00.023.363 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.363 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.363 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.364 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.348 I llm_load_vocab: special tokens cache size = 25
0.00.050.308 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.311 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.311 I llm_load_print_meta: arch             = gptneox
0.00.050.311 I llm_load_print_meta: vocab type       = BPE
0.00.050.312 I llm_load_print_meta: n_vocab          = 50304
0.00.050.312 I llm_load_print_meta: n_merges         = 50009
0.00.050.312 I llm_load_print_meta: vocab_only       = 0
0.00.050.312 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.312 I llm_load_print_meta: n_embd           = 2048
0.00.050.313 I llm_load_print_meta: n_layer          = 24
0.00.050.315 I llm_load_print_meta: n_head           = 16
0.00.050.316 I llm_load_print_meta: n_head_kv        = 16
0.00.050.316 I llm_load_print_meta: n_rot            = 32
0.00.050.317 I llm_load_print_meta: n_swa            = 0
0.00.050.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.318 I llm_load_print_meta: n_gqa            = 1
0.00.050.318 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.319 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.320 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.320 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.321 I llm_load_print_meta: n_ff             = 8192
0.00.050.321 I llm_load_print_meta: n_expert         = 0
0.00.050.321 I llm_load_print_meta: n_expert_used    = 0
0.00.050.321 I llm_load_print_meta: causal attn      = 1
0.00.050.322 I llm_load_print_meta: pooling type     = 0
0.00.050.322 I llm_load_print_meta: rope type        = 2
0.00.050.322 I llm_load_print_meta: rope scaling     = linear
0.00.050.322 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.323 I llm_load_print_meta: freq_scale_train = 1
0.00.050.323 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.323 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.323 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.323 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.326 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.326 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.327 I llm_load_print_meta: model type       = 1.4B
0.00.050.327 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.327 I llm_load_print_meta: model params     = 1.41 B
0.00.050.328 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.328 I llm_load_print_meta: general.name     = 1.4B
0.00.050.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.329 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.329 I llm_load_print_meta: LF token         = 128 ''
0.00.050.329 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.330 I llm_load_print_meta: max token length = 1024
0.00.052.192 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.192 I llm_load_tensors: offloading output layer to GPU
0.00.052.192 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.203 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.204 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.050 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.050 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.050 I llama_new_context_with_model: n_batch       = 2048
0.00.053.050 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.050 I llama_new_context_with_model: flash_attn    = 0
0.00.053.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.051 I llama_new_context_with_model: freq_scale    = 1
0.00.053.051 I ggml_metal_init: allocating
0.00.053.057 I ggml_metal_init: found device: Apple M4
0.00.053.064 I ggml_metal_init: picking default device: Apple M4
0.00.053.630 I ggml_metal_init: using embedded metal library
0.00.056.014 I ggml_metal_init: GPU name:   Apple M4
0.00.056.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.016 I ggml_metal_init: simdgroup reduction   = true
0.00.056.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.016 I ggml_metal_init: has bfloat            = true
0.00.056.016 I ggml_metal_init: use bfloat            = true
0.00.056.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.521 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.395 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.400 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.416 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.412 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.413 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.413 I llama_new_context_with_model: graph nodes  = 967
0.00.085.414 I llama_new_context_with_model: graph splits = 2
0.00.085.430 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.933 I main: llama threadpool init, n_threads = 4
0.00.541.974 I 
0.00.542.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.007 I 
0.00.542.234 I sampler seed: 1234
0.00.542.239 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.276 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.278 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.278.198 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47875.93 tokens per second)
0.01.278.200 I llama_perf_context_print:        load time =     532.89 ms
0.01.278.201 I llama_perf_context_print: prompt eval time =      40.25 ms /     7 tokens (    5.75 ms per token,   173.93 tokens per second)
0.01.278.202 I llama_perf_context_print:        eval time =     693.11 ms /    63 runs   (   11.00 ms per token,    90.89 tokens per second)
0.01.278.202 I llama_perf_context_print:       total time =     736.27 ms /    70 tokens
0.01.278.456 I ggml_metal_free: deallocating

real	0m1.294s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.592 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.252 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.259 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.260 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.262 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.280 I llama_model_loader: - type  f32:  194 tensors
0.00.023.281 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.281 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.281 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.304 I llm_load_vocab: special tokens cache size = 25
0.00.050.320 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.323 I llm_load_print_meta: arch             = gptneox
0.00.050.323 I llm_load_print_meta: vocab type       = BPE
0.00.050.323 I llm_load_print_meta: n_vocab          = 50304
0.00.050.323 I llm_load_print_meta: n_merges         = 50009
0.00.050.324 I llm_load_print_meta: vocab_only       = 0
0.00.050.324 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.324 I llm_load_print_meta: n_embd           = 2048
0.00.050.324 I llm_load_print_meta: n_layer          = 24
0.00.050.326 I llm_load_print_meta: n_head           = 16
0.00.050.327 I llm_load_print_meta: n_head_kv        = 16
0.00.050.329 I llm_load_print_meta: n_rot            = 32
0.00.050.329 I llm_load_print_meta: n_swa            = 0
0.00.050.329 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.329 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.330 I llm_load_print_meta: n_gqa            = 1
0.00.050.337 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.338 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.339 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.340 I llm_load_print_meta: n_ff             = 8192
0.00.050.340 I llm_load_print_meta: n_expert         = 0
0.00.050.340 I llm_load_print_meta: n_expert_used    = 0
0.00.050.341 I llm_load_print_meta: causal attn      = 1
0.00.050.341 I llm_load_print_meta: pooling type     = 0
0.00.050.342 I llm_load_print_meta: rope type        = 2
0.00.050.344 I llm_load_print_meta: rope scaling     = linear
0.00.050.344 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.344 I llm_load_print_meta: freq_scale_train = 1
0.00.050.345 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.345 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.348 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.348 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.348 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.348 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.348 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.350 I llm_load_print_meta: model type       = 1.4B
0.00.050.350 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.351 I llm_load_print_meta: model params     = 1.41 B
0.00.050.352 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.352 I llm_load_print_meta: general.name     = 1.4B
0.00.050.353 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.353 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.353 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.353 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.353 I llm_load_print_meta: LF token         = 128 ''
0.00.050.354 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.354 I llm_load_print_meta: max token length = 1024
0.00.052.370 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.370 I llm_load_tensors: offloading output layer to GPU
0.00.052.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.381 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.382 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.276 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.277 I llama_new_context_with_model: n_ctx         = 128
0.00.053.277 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.277 I llama_new_context_with_model: n_batch       = 128
0.00.053.277 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.278 I llama_new_context_with_model: flash_attn    = 0
0.00.053.278 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.278 I llama_new_context_with_model: freq_scale    = 1
0.00.053.279 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.279 I ggml_metal_init: allocating
0.00.053.282 I ggml_metal_init: found device: Apple M4
0.00.053.284 I ggml_metal_init: picking default device: Apple M4
0.00.053.883 I ggml_metal_init: using embedded metal library
0.00.056.244 I ggml_metal_init: GPU name:   Apple M4
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.248 I ggml_metal_init: simdgroup reduction   = true
0.00.056.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.249 I ggml_metal_init: has bfloat            = true
0.00.056.249 I ggml_metal_init: use bfloat            = true
0.00.056.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.226 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.228 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.177 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.178 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.179 I llama_new_context_with_model: graph nodes  = 967
0.00.068.179 I llama_new_context_with_model: graph splits = 2
0.00.068.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.896 I 
0.00.483.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.483.944 I perplexity: tokenizing the input ..
0.00.491.910 I perplexity: tokenization took 7.965 ms
0.00.491.913 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.623.998 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.625.155 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.625.179 I llama_perf_context_print:        load time =     475.30 ms
0.00.625.180 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.73 tokens per second)
0.00.625.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.181 I llama_perf_context_print:       total time =     141.28 ms /   129 tokens
0.00.625.705 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.079s
sys	0m0.087s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.882 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.381 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.381 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.382 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.977 I llama_model_loader: - type  f32:  194 tensors
0.00.022.977 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.978 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.978 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.401 I llm_load_vocab: special tokens cache size = 25
0.00.049.628 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.630 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.631 I llm_load_print_meta: arch             = gptneox
0.00.049.631 I llm_load_print_meta: vocab type       = BPE
0.00.049.631 I llm_load_print_meta: n_vocab          = 50304
0.00.049.631 I llm_load_print_meta: n_merges         = 50009
0.00.049.631 I llm_load_print_meta: vocab_only       = 0
0.00.049.632 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.632 I llm_load_print_meta: n_embd           = 2048
0.00.049.632 I llm_load_print_meta: n_layer          = 24
0.00.049.635 I llm_load_print_meta: n_head           = 16
0.00.049.638 I llm_load_print_meta: n_head_kv        = 16
0.00.049.638 I llm_load_print_meta: n_rot            = 32
0.00.049.638 I llm_load_print_meta: n_swa            = 0
0.00.049.638 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.638 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.639 I llm_load_print_meta: n_gqa            = 1
0.00.049.640 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.641 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.642 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.642 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.642 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.643 I llm_load_print_meta: n_ff             = 8192
0.00.049.643 I llm_load_print_meta: n_expert         = 0
0.00.049.645 I llm_load_print_meta: n_expert_used    = 0
0.00.049.646 I llm_load_print_meta: causal attn      = 1
0.00.049.647 I llm_load_print_meta: pooling type     = 0
0.00.049.647 I llm_load_print_meta: rope type        = 2
0.00.049.647 I llm_load_print_meta: rope scaling     = linear
0.00.049.647 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.648 I llm_load_print_meta: freq_scale_train = 1
0.00.049.648 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.648 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.648 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.655 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.657 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.657 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.657 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.658 I llm_load_print_meta: model type       = 1.4B
0.00.049.658 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.658 I llm_load_print_meta: model params     = 1.41 B
0.00.049.659 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.659 I llm_load_print_meta: general.name     = 1.4B
0.00.049.660 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.660 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.660 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.660 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.660 I llm_load_print_meta: LF token         = 128 ''
0.00.049.662 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.663 I llm_load_print_meta: max token length = 1024
0.00.051.619 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.620 I llm_load_tensors: offloading output layer to GPU
0.00.051.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.630 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.631 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.507 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.508 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.508 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.508 I llama_new_context_with_model: n_batch       = 2048
0.00.052.508 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.509 I llama_new_context_with_model: flash_attn    = 0
0.00.052.509 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.509 I llama_new_context_with_model: freq_scale    = 1
0.00.052.510 I ggml_metal_init: allocating
0.00.052.517 I ggml_metal_init: found device: Apple M4
0.00.052.519 I ggml_metal_init: picking default device: Apple M4
0.00.053.123 I ggml_metal_init: using embedded metal library
0.00.055.571 I ggml_metal_init: GPU name:   Apple M4
0.00.055.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.575 I ggml_metal_init: simdgroup reduction   = true
0.00.055.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.575 I ggml_metal_init: has bfloat            = true
0.00.055.575 I ggml_metal_init: use bfloat            = true
0.00.055.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.849 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.510 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.531 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.552 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.553 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.554 I llama_new_context_with_model: graph nodes  = 967
0.00.087.554 I llama_new_context_with_model: graph splits = 2
0.00.087.569 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.708 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.709 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.169 I main: llama threadpool init, n_threads = 4
0.00.617.206 I 
0.00.617.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.248 I 
0.00.617.481 I sampler seed: 1234
0.00.617.485 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.519 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.530 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.531 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.377.790 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.377.790 I llama_perf_context_print:        load time =     608.28 ms
0.01.377.791 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.377.792 I llama_perf_context_print:        eval time =     710.19 ms /    63 runs   (   11.27 ms per token,    88.71 tokens per second)
0.01.377.792 I llama_perf_context_print:       total time =     760.62 ms /    70 tokens
0.01.378.025 I ggml_metal_free: deallocating

real	0m1.396s
user	0m0.111s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.955 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.368 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.959 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.959 I llama_model_loader: - type  f32:  194 tensors
0.00.023.960 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.960 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.960 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.050 I llm_load_vocab: special tokens cache size = 25
0.00.050.050 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.053 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.053 I llm_load_print_meta: arch             = gptneox
0.00.050.053 I llm_load_print_meta: vocab type       = BPE
0.00.050.054 I llm_load_print_meta: n_vocab          = 50304
0.00.050.054 I llm_load_print_meta: n_merges         = 50009
0.00.050.054 I llm_load_print_meta: vocab_only       = 0
0.00.050.054 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.054 I llm_load_print_meta: n_embd           = 2048
0.00.050.055 I llm_load_print_meta: n_layer          = 24
0.00.050.057 I llm_load_print_meta: n_head           = 16
0.00.050.058 I llm_load_print_meta: n_head_kv        = 16
0.00.050.058 I llm_load_print_meta: n_rot            = 32
0.00.050.058 I llm_load_print_meta: n_swa            = 0
0.00.050.059 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.059 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.060 I llm_load_print_meta: n_gqa            = 1
0.00.050.060 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.061 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.062 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.062 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.062 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.062 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.062 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.063 I llm_load_print_meta: n_ff             = 8192
0.00.050.063 I llm_load_print_meta: n_expert         = 0
0.00.050.063 I llm_load_print_meta: n_expert_used    = 0
0.00.050.064 I llm_load_print_meta: causal attn      = 1
0.00.050.064 I llm_load_print_meta: pooling type     = 0
0.00.050.064 I llm_load_print_meta: rope type        = 2
0.00.050.064 I llm_load_print_meta: rope scaling     = linear
0.00.050.067 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.067 I llm_load_print_meta: freq_scale_train = 1
0.00.050.067 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.068 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.068 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.068 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.068 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.068 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.068 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.069 I llm_load_print_meta: model type       = 1.4B
0.00.050.069 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.069 I llm_load_print_meta: model params     = 1.41 B
0.00.050.070 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.070 I llm_load_print_meta: general.name     = 1.4B
0.00.050.074 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: LF token         = 128 ''
0.00.050.075 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: max token length = 1024
0.00.051.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.955 I llm_load_tensors: offloading output layer to GPU
0.00.051.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.965 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.967 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.884 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.885 I llama_new_context_with_model: n_ctx         = 128
0.00.052.885 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.885 I llama_new_context_with_model: n_batch       = 128
0.00.052.885 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.885 I llama_new_context_with_model: flash_attn    = 0
0.00.052.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.886 I llama_new_context_with_model: freq_scale    = 1
0.00.052.886 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.887 I ggml_metal_init: allocating
0.00.052.892 I ggml_metal_init: found device: Apple M4
0.00.052.894 I ggml_metal_init: picking default device: Apple M4
0.00.053.449 I ggml_metal_init: using embedded metal library
0.00.055.728 I ggml_metal_init: GPU name:   Apple M4
0.00.055.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.730 I ggml_metal_init: simdgroup reduction   = true
0.00.055.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.730 I ggml_metal_init: has bfloat            = true
0.00.055.731 I ggml_metal_init: use bfloat            = true
0.00.055.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.225 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.440 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.444 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.457 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.348 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.349 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.349 I llama_new_context_with_model: graph nodes  = 967
0.00.067.349 I llama_new_context_with_model: graph splits = 2
0.00.067.361 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.782 I 
0.00.553.820 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.831 I perplexity: tokenizing the input ..
0.00.561.550 I perplexity: tokenization took 7.717 ms
0.00.561.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.605 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.776 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.697.796 I llama_perf_context_print:        load time =     543.82 ms
0.00.697.798 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.38 tokens per second)
0.00.697.799 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.697.802 I llama_perf_context_print:       total time =     144.01 ms /   129 tokens
0.00.698.273 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.077s
sys	0m0.099s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.011.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.266 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.108 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.109 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.110 I llama_model_loader: - type  f32:  194 tensors
0.00.027.110 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.111 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.098 I llm_load_vocab: special tokens cache size = 25
0.00.054.158 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.160 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.160 I llm_load_print_meta: arch             = gptneox
0.00.054.161 I llm_load_print_meta: vocab type       = BPE
0.00.054.161 I llm_load_print_meta: n_vocab          = 50304
0.00.054.161 I llm_load_print_meta: n_merges         = 50009
0.00.054.161 I llm_load_print_meta: vocab_only       = 0
0.00.054.162 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.162 I llm_load_print_meta: n_embd           = 2048
0.00.054.162 I llm_load_print_meta: n_layer          = 24
0.00.054.165 I llm_load_print_meta: n_head           = 16
0.00.054.165 I llm_load_print_meta: n_head_kv        = 16
0.00.054.166 I llm_load_print_meta: n_rot            = 32
0.00.054.166 I llm_load_print_meta: n_swa            = 0
0.00.054.166 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.166 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.169 I llm_load_print_meta: n_gqa            = 1
0.00.054.170 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.171 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.173 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.173 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.173 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.173 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.174 I llm_load_print_meta: n_ff             = 8192
0.00.054.174 I llm_load_print_meta: n_expert         = 0
0.00.054.174 I llm_load_print_meta: n_expert_used    = 0
0.00.054.176 I llm_load_print_meta: causal attn      = 1
0.00.054.177 I llm_load_print_meta: pooling type     = 0
0.00.054.177 I llm_load_print_meta: rope type        = 2
0.00.054.178 I llm_load_print_meta: rope scaling     = linear
0.00.054.178 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.178 I llm_load_print_meta: freq_scale_train = 1
0.00.054.178 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.179 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.179 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.179 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.179 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.180 I llm_load_print_meta: model type       = 1.4B
0.00.054.180 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.054.181 I llm_load_print_meta: model params     = 1.41 B
0.00.054.181 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.054.181 I llm_load_print_meta: general.name     = 1.4B
0.00.054.182 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.182 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.182 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.182 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.182 I llm_load_print_meta: LF token         = 128 ''
0.00.054.183 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.183 I llm_load_print_meta: max token length = 1024
0.00.056.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.255 I llm_load_tensors: offloading output layer to GPU
0.00.056.255 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.265 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.056.267 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.057.177 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.178 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.178 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.178 I llama_new_context_with_model: n_batch       = 2048
0.00.057.179 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.179 I llama_new_context_with_model: flash_attn    = 0
0.00.057.179 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.180 I llama_new_context_with_model: freq_scale    = 1
0.00.057.180 I ggml_metal_init: allocating
0.00.057.183 I ggml_metal_init: found device: Apple M4
0.00.057.185 I ggml_metal_init: picking default device: Apple M4
0.00.057.791 I ggml_metal_init: using embedded metal library
0.00.060.121 I ggml_metal_init: GPU name:   Apple M4
0.00.060.123 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.125 I ggml_metal_init: simdgroup reduction   = true
0.00.060.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.125 I ggml_metal_init: has bfloat            = true
0.00.060.126 I ggml_metal_init: use bfloat            = true
0.00.060.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.021 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.019 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.027 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.057 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.098 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.099 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.100 I llama_new_context_with_model: graph nodes  = 967
0.00.091.100 I llama_new_context_with_model: graph splits = 2
0.00.091.115 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.178 I main: llama threadpool init, n_threads = 4
0.00.706.220 I 
0.00.706.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.254 I 
0.00.706.491 I sampler seed: 1234
0.00.706.495 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.546 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.548 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.548 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.552.090 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.552.090 I llama_perf_context_print:        load time =     694.32 ms
0.01.552.091 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.58 tokens per second)
0.01.552.092 I llama_perf_context_print:        eval time =     790.85 ms /    63 runs   (   12.55 ms per token,    79.66 tokens per second)
0.01.552.093 I llama_perf_context_print:       total time =     845.91 ms /    70 tokens
0.01.552.285 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.195 I llama_model_loader: - type  f32:  194 tensors
0.00.023.195 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.195 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.342 I llm_load_vocab: special tokens cache size = 25
0.00.049.432 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.435 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.435 I llm_load_print_meta: arch             = gptneox
0.00.049.435 I llm_load_print_meta: vocab type       = BPE
0.00.049.436 I llm_load_print_meta: n_vocab          = 50304
0.00.049.436 I llm_load_print_meta: n_merges         = 50009
0.00.049.436 I llm_load_print_meta: vocab_only       = 0
0.00.049.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.436 I llm_load_print_meta: n_embd           = 2048
0.00.049.436 I llm_load_print_meta: n_layer          = 24
0.00.049.439 I llm_load_print_meta: n_head           = 16
0.00.049.440 I llm_load_print_meta: n_head_kv        = 16
0.00.049.440 I llm_load_print_meta: n_rot            = 32
0.00.049.440 I llm_load_print_meta: n_swa            = 0
0.00.049.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.441 I llm_load_print_meta: n_gqa            = 1
0.00.049.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.443 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.445 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.445 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.445 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.446 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.446 I llm_load_print_meta: n_ff             = 8192
0.00.049.446 I llm_load_print_meta: n_expert         = 0
0.00.049.447 I llm_load_print_meta: n_expert_used    = 0
0.00.049.447 I llm_load_print_meta: causal attn      = 1
0.00.049.447 I llm_load_print_meta: pooling type     = 0
0.00.049.447 I llm_load_print_meta: rope type        = 2
0.00.049.447 I llm_load_print_meta: rope scaling     = linear
0.00.049.450 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.451 I llm_load_print_meta: freq_scale_train = 1
0.00.049.451 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.452 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.452 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.452 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.453 I llm_load_print_meta: model type       = 1.4B
0.00.049.453 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.457 I llm_load_print_meta: model params     = 1.41 B
0.00.049.458 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.459 I llm_load_print_meta: general.name     = 1.4B
0.00.049.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.460 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.460 I llm_load_print_meta: LF token         = 128 ''
0.00.049.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.460 I llm_load_print_meta: max token length = 1024
0.00.051.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.462 I llm_load_tensors: offloading output layer to GPU
0.00.051.462 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.472 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.474 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.372 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.373 I llama_new_context_with_model: n_ctx         = 128
0.00.052.373 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.373 I llama_new_context_with_model: n_batch       = 128
0.00.052.374 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.374 I llama_new_context_with_model: flash_attn    = 0
0.00.052.374 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.374 I llama_new_context_with_model: freq_scale    = 1
0.00.052.375 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.375 I ggml_metal_init: allocating
0.00.052.378 I ggml_metal_init: found device: Apple M4
0.00.052.380 I ggml_metal_init: picking default device: Apple M4
0.00.052.960 I ggml_metal_init: using embedded metal library
0.00.055.300 I ggml_metal_init: GPU name:   Apple M4
0.00.055.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.303 I ggml_metal_init: simdgroup reduction   = true
0.00.055.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.305 I ggml_metal_init: has bfloat            = true
0.00.055.305 I ggml_metal_init: use bfloat            = true
0.00.055.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.172 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.017 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.018 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.019 I llama_new_context_with_model: graph nodes  = 967
0.00.067.019 I llama_new_context_with_model: graph splits = 2
0.00.067.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.499 I 
0.00.621.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.550 I perplexity: tokenizing the input ..
0.00.629.336 I perplexity: tokenization took 7.784 ms
0.00.629.349 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.417 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.770.877 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.770.885 I llama_perf_context_print:        load time =     612.70 ms
0.00.770.886 I llama_perf_context_print: prompt eval time =     139.80 ms /   128 tokens (    1.09 ms per token,   915.57 tokens per second)
0.00.770.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.887 I llama_perf_context_print:       total time =     149.39 ms /   129 tokens
0.00.771.269 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.078s
sys	0m0.096s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.257 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.466 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.411 I llama_model_loader: - type  f32:  194 tensors
0.00.026.411 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.543 I llm_load_vocab: special tokens cache size = 25
0.00.053.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.628 I llm_load_print_meta: arch             = gptneox
0.00.053.628 I llm_load_print_meta: vocab type       = BPE
0.00.053.628 I llm_load_print_meta: n_vocab          = 50304
0.00.053.629 I llm_load_print_meta: n_merges         = 50009
0.00.053.629 I llm_load_print_meta: vocab_only       = 0
0.00.053.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.629 I llm_load_print_meta: n_embd           = 2048
0.00.053.629 I llm_load_print_meta: n_layer          = 24
0.00.053.632 I llm_load_print_meta: n_head           = 16
0.00.053.633 I llm_load_print_meta: n_head_kv        = 16
0.00.053.633 I llm_load_print_meta: n_rot            = 32
0.00.053.634 I llm_load_print_meta: n_swa            = 0
0.00.053.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.639 I llm_load_print_meta: n_gqa            = 1
0.00.053.640 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.640 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.641 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.641 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.641 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.643 I llm_load_print_meta: n_ff             = 8192
0.00.053.644 I llm_load_print_meta: n_expert         = 0
0.00.053.644 I llm_load_print_meta: n_expert_used    = 0
0.00.053.644 I llm_load_print_meta: causal attn      = 1
0.00.053.644 I llm_load_print_meta: pooling type     = 0
0.00.053.644 I llm_load_print_meta: rope type        = 2
0.00.053.644 I llm_load_print_meta: rope scaling     = linear
0.00.053.645 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.645 I llm_load_print_meta: freq_scale_train = 1
0.00.053.645 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.645 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.646 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.646 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.646 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.646 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.646 I llm_load_print_meta: model type       = 1.4B
0.00.053.647 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.647 I llm_load_print_meta: model params     = 1.41 B
0.00.053.652 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.652 I llm_load_print_meta: general.name     = 1.4B
0.00.053.653 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.654 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: LF token         = 128 ''
0.00.053.655 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.655 I llm_load_print_meta: max token length = 1024
0.00.055.750 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.750 I llm_load_tensors: offloading output layer to GPU
0.00.055.751 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.761 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.762 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.720 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.720 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.720 I llama_new_context_with_model: n_batch       = 2048
0.00.056.721 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.721 I llama_new_context_with_model: flash_attn    = 0
0.00.056.721 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.721 I llama_new_context_with_model: freq_scale    = 1
0.00.056.722 I ggml_metal_init: allocating
0.00.056.725 I ggml_metal_init: found device: Apple M4
0.00.056.727 I ggml_metal_init: picking default device: Apple M4
0.00.057.342 I ggml_metal_init: using embedded metal library
0.00.059.683 I ggml_metal_init: GPU name:   Apple M4
0.00.059.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.686 I ggml_metal_init: simdgroup reduction   = true
0.00.059.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.686 I ggml_metal_init: has bfloat            = true
0.00.059.686 I ggml_metal_init: use bfloat            = true
0.00.059.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.627 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.186 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.190 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.207 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.233 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.235 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.235 I llama_new_context_with_model: graph nodes  = 967
0.00.090.235 I llama_new_context_with_model: graph splits = 2
0.00.090.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.391 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.403 I main: llama threadpool init, n_threads = 4
0.00.749.442 I 
0.00.749.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.485 I 
0.00.749.706 I sampler seed: 1234
0.00.749.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.734 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.735 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.735 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.631.141 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.631.141 I llama_perf_context_print:        load time =     740.14 ms
0.01.631.142 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.631.143 I llama_perf_context_print:        eval time =     824.00 ms /    63 runs   (   13.08 ms per token,    76.46 tokens per second)
0.01.631.143 I llama_perf_context_print:       total time =     881.74 ms /    70 tokens
0.01.631.353 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.112s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4374 (e1ac3536) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.269 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.235 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.236 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.297 I llama_model_loader: - type  f32:  194 tensors
0.00.024.297 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.395 I llm_load_vocab: special tokens cache size = 25
0.00.052.509 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.513 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.513 I llm_load_print_meta: arch             = gptneox
0.00.052.514 I llm_load_print_meta: vocab type       = BPE
0.00.052.514 I llm_load_print_meta: n_vocab          = 50304
0.00.052.514 I llm_load_print_meta: n_merges         = 50009
0.00.052.514 I llm_load_print_meta: vocab_only       = 0
0.00.052.515 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.515 I llm_load_print_meta: n_embd           = 2048
0.00.052.515 I llm_load_print_meta: n_layer          = 24
0.00.052.519 I llm_load_print_meta: n_head           = 16
0.00.052.519 I llm_load_print_meta: n_head_kv        = 16
0.00.052.522 I llm_load_print_meta: n_rot            = 32
0.00.052.522 I llm_load_print_meta: n_swa            = 0
0.00.052.523 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.523 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.523 I llm_load_print_meta: n_gqa            = 1
0.00.052.524 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.525 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.525 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.526 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.526 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.526 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.526 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.527 I llm_load_print_meta: n_ff             = 8192
0.00.052.527 I llm_load_print_meta: n_expert         = 0
0.00.052.527 I llm_load_print_meta: n_expert_used    = 0
0.00.052.528 I llm_load_print_meta: causal attn      = 1
0.00.052.528 I llm_load_print_meta: pooling type     = 0
0.00.052.528 I llm_load_print_meta: rope type        = 2
0.00.052.529 I llm_load_print_meta: rope scaling     = linear
0.00.052.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.550 I llm_load_print_meta: freq_scale_train = 1
0.00.052.550 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.553 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.553 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.553 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.553 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.553 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.553 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.554 I llm_load_print_meta: model type       = 1.4B
0.00.052.554 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.555 I llm_load_print_meta: model params     = 1.41 B
0.00.052.555 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.556 I llm_load_print_meta: general.name     = 1.4B
0.00.052.558 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: LF token         = 128 ''
0.00.052.561 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.561 I llm_load_print_meta: max token length = 1024
0.00.054.548 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.549 I llm_load_tensors: offloading output layer to GPU
0.00.054.549 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.561 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.562 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.525 I llama_new_context_with_model: n_ctx         = 128
0.00.055.525 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.525 I llama_new_context_with_model: n_batch       = 128
0.00.055.525 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.525 I llama_new_context_with_model: flash_attn    = 0
0.00.055.526 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.526 I llama_new_context_with_model: freq_scale    = 1
0.00.055.526 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.527 I ggml_metal_init: allocating
0.00.055.531 I ggml_metal_init: found device: Apple M4
0.00.055.533 I ggml_metal_init: picking default device: Apple M4
0.00.056.104 I ggml_metal_init: using embedded metal library
0.00.058.562 I ggml_metal_init: GPU name:   Apple M4
0.00.058.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.564 I ggml_metal_init: simdgroup reduction   = true
0.00.058.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.564 I ggml_metal_init: has bfloat            = true
0.00.058.565 I ggml_metal_init: use bfloat            = true
0.00.058.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.751 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.177 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.183 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.201 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.045 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.046 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.046 I llama_new_context_with_model: graph nodes  = 967
0.00.071.047 I llama_new_context_with_model: graph splits = 2
0.00.071.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.969 I 
0.00.560.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.023 I perplexity: tokenizing the input ..
0.00.567.451 I perplexity: tokenization took 7.425 ms
0.00.567.454 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.889 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.707.119 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.707.142 I llama_perf_context_print:        load time =     550.69 ms
0.00.707.143 I llama_perf_context_print: prompt eval time =     138.21 ms /   128 tokens (    1.08 ms per token,   926.14 tokens per second)
0.00.707.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.144 I llama_perf_context_print:       total time =     147.18 ms /   129 tokens
0.00.707.582 I ggml_metal_free: deallocating

real	0m0.721s
user	0m0.081s
sys	0m0.086s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4374 (e1ac3536)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138f07590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138f07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138f08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138f08800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138f08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138f09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138f09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138f09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138f0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138f0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138f0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138f0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138f0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138f0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138f0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138f0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138f0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138f0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138f0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138f0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138f0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138f10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138f110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138f117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138f11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138f12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138f12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138f13240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138f13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138f139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138f13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138f144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138f14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138f14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138f15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138f15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138f15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138f15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138f16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138f168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138f16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138f171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138f17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138f17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138f17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138f18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138f18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138f194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138f19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138f1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138f1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138f1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138f1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138f1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138f1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138f1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138f1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138f1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138f1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138f1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138f1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138f1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138f1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138f1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138f1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138f1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138f20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138f20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138f20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138f21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138f219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138f21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138f22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138f22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138f23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138f239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138f23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138f24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138f24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138f24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138f25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138f25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138f26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138f26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138f26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138f27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138f27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138f27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138f28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138f28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138f28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138f18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138f29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138f29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138f2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138f2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138f2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138f2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138f2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138f2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138f2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138f2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138f2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138f2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138f2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138f2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138f2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138f2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138f2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138f2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138f2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138f2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138f2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138f304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138f30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138f30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138f312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138f31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138f31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138f32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138f32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138f329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138f32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138f33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138f337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138f33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138f340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138f34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138f34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138f34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138f35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138f35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138f35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138f36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138f365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138f36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138f36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138f373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138f37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138f37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138f381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138f38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138f38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138f38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138f39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138f398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138f39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138f3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138f3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138f3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138f3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138f3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138f3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138f3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138f3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138f3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138f3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138f3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138f3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138f3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138f3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138f3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138f3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138f3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138f3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138f3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138f3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138f3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138f40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138f407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138f40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138f41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138f415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138f41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138f41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138f42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138f42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138f43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138f43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138f43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138f43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138f443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138f44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138f44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138f451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138f45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138f45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138f46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138f469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138f46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138f475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138f47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138f48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138f48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138f49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138f49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138f49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138f4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138f4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138f4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138f4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138f4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138f4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138f4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138f4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138f4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138f4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138f4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138f4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138f4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138f4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138f4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138f4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138f4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138f4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138f504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138f509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138f50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138f51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138f519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138f52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138f529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138f52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138f53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138f539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138f53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138f54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138f549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138f54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138f55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138f559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138f55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138f56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138f56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138f56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138f57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138f57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138f57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138f58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138f58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138f58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138f59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138f59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138f59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138f5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138f5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138f5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138f5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138f5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138f5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138f5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138f5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138f5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138f5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138f5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138f5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138f5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138f5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138f5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138f5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138f5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138f5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138f5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138f60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138f60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138f60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138f61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138f615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138f61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138f61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138f623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138f62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138f63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138f63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138f63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138f645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138f64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138f65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138f65310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138f65920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138e0aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138e0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138e0b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138e0bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138e0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138e0c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138e0c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138e0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138e0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138e0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138e0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138e0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138e0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138e0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138e0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138e10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138e10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138e11250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138e11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138e120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138e127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138e12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138e13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138e13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138e14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138e14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138e149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138e14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138e152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138e15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138e15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138e160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138e16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138e167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138e16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138e170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138e17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138e179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138e17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138e186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138e18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138e18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138e19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138e198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138e19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138e1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138e1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138e1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138e1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138e1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138e1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138e1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138e1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138e1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138e1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138e1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138e1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138e1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138e1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138e1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138e1e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138e1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138e1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138e1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138e1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138e1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138e20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138e204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138e20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138e20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138e21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138e21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138e21af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138e21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138e223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138e22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138e22cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138e23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138e23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138e23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138e23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138e242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138e24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138e24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138e25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138e254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138e25910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138e25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138e261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138e26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138e26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138e26f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138e273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138e27820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138e27c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138e28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138e28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138e289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138e28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138e292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138e29730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138e29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138e2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138e2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138e2a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138e2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138e2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138e2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138e2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138e2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138e2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138e2cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138e2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138e2d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138e2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138e2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138e2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138e2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138e2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138e2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138e2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138e2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138e2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138e301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138e30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138e30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138e30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138e31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138e317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138e31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138e320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138e32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138e329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138e32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138e33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138e336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138e33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138e33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138e34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138e348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138e34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138e35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138e35600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138e35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138e35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138e36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138e367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138e36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138e370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138e37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138e37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138e38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138e386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138e38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138e38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138e39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138e39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138e39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138e3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138e3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138e3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138e3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138e3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138e3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138e3c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138e3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138e3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138e3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138e3d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138e3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138e3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138e3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138e3e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138e3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138e3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138e3f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138e3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138e3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138e3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138e40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138e40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138e40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138e41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138e414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138e41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138e42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138e42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138e42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138e42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138e433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138e43840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138e43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138e44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138e44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138e44a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138e44e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138e452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138e45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138e461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138e46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138e46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138e46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138e47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138e478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138e47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138e48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138e491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138e49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138e49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138e4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138e4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138e4ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138e4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138e4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138e4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138e4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138e4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138e4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138e4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138e4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138e4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138e4eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138e4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138e4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138e4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138e504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138e50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138e51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138e515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138e51ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138e52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138e52720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138e52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138e532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138e53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138e53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138e549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138e54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138e55520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138e55ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138e560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138e56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138e56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138e571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138e57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138e58320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138e588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138e58ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138e59460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138e59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138e59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138e5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138e5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138e5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138e5b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138e5bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138e5c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138e5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138e5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138e5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138e5d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138e5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138e5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138e5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138e5ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138e5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138e5f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138e5fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138e5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138e604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138e609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138e60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138e613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138e618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138e622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138e62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138e63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138e63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138e63b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138e64300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138e645c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138e64bd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cb046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cb04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cb04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cb05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cb058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cb05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cb06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cb065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cb06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cb06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cb07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cb079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cb084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cb08c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cb094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cb09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cb0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cb0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cb0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cb0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cb0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cb0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cb0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cb0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cb0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cb0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cb0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cb0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cb0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cb0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cb0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cb0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cb0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cb10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cb104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cb10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cb10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cb111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cb11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cb11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cb11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cb123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cb12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cb12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cb13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cb13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cb139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cb13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cb142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cb14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cb14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cb15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cb15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cb158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cb15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cb161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cb16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cb16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cb170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cb17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cb17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cb17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cb18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cb186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cb18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cb18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cb19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cb198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cb19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cb1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cb1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cb1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cb1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cb1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cb1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cb1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cb1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cb1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cb1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cb1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cb1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cb1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cb1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cb1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cb1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cb1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cb1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cb1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cb1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cb1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cb1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cb20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cb20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cb20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cb21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cb214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cb21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cb21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cb22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cb226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cb22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cb22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cb233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cb23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cb23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cb243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cb24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cb24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cb25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cb25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cb259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cb25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cb262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cb26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cb26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cb27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cb27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cb278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cb27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cb281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cb28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cb28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cb28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cb29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cb29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cb29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cb2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cb2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cb2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cb2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cb2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cb2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cb2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cb2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cb2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cb2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cb2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cb2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cb2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cb2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cb2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cb2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cb2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cb2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cb2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cb2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cb2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cb2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cb30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cb306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cb30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cb30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cb31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cb318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cb31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cb32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cb32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cb32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cb32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cb33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cb337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cb33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cb340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cb34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cb34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cb34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cb35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cb356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cb35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cb35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cb36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cb36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cb36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cb37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cb375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cb37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cb37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cb38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cb387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cb38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cb39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cb394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cb39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cb39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cb3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cb3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cb3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cb3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cb3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cb3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cb3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cb3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cb3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cb3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cb3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cb3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cb3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cb3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cb3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cb3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cb3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cb3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cb3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cb3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cb3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cb3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cb403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cb40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cb40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cb41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cb41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cb41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cb42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cb426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cb42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cb42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cb433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cb43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cb43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cb44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cb445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cb44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cb44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cb45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cb45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cb45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cb46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cb464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cb46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cb46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cb47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cb47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cb47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cb47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cb483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cb48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cb48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cb49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cb49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cb49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cb49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cb4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cb4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cb4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cb4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cb4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cb4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cb4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cb4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cb4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cb4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cb4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cb4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cb4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cb4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cb4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cb4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cb4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cb4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cb4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cb4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cb4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cb50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cb50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cb508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cb50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cb511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cb51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cb51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cb51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cb52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cb52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cb52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cb530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cb53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cb539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cb53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cb542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cb54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cb54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cb54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cb55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cb558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cb56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cb56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cb57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cb578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cb57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cb57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cb585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cb58be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.811s
user	0m0.297s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4374 (e1ac3536)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15560b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15560bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15560c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15560c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15560ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15560d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15560d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15560dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15560e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15560e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15560ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15560f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15560fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1556122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1556129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1556131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1556138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1556156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1556159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1556178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1556190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1556199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15561a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15561a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15561ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15561b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15561b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15561b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15561be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15561c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15561cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15561d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15561d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15561dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15561e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15561ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15561f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15561fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15561fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1556216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1556224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1556249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1556258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1556268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1556278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1556288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1556298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15562a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15562a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15562ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15562b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15562b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15562bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15562c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15562c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15562cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15561caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15562d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15562d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15562df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15562e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15562e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15562ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15562f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15562f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15562ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1556309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1556319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1556343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1556351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1556368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1556376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1556384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15563a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15563a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15563a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15563ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15563b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15563b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15563bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15563c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15563c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15563ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15563ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15563d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15563d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15563dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15563e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15563e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15563ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15563ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15563f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15563f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15563fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1556418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1556421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1556446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1556454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1556462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1556479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1556487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1556490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15564a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15564a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15564a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15564af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15564b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15564bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15564c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15564c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15564ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15564d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15564d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15564de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15564e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15564e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15564ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15564f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15564f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15564feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1556513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1556523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1556533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1556543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1556553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1556563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1556568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1556578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1556588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1556598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15565a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15565a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15565ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15565b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15565b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15565bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15565c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15565c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15565cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15565d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15565d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15565ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15565e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15565e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15565edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15565f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15565f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15565fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1556612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1556626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1556634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1556659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1556662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1556684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157204ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157204f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1572053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157205830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157205ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157206110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157206580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1572069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157206e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1572073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157207850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157207ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1572089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1572091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1572099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15720a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15720a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15720af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15720b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15720be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15720c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15720cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15720d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15720da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15720e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15720e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15720e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15720eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15720f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15720f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15720f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15720fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157210280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157210540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1572109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157210e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157211290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157211700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157211b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157211fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157212450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1572128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157212d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1572131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157213610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157213a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157213ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157214360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1572147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157214c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1572150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157215520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157215990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157215e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157216270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1572166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157216c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157217150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1572175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157217a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157217ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157218310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157218780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157219060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1572194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157219940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157219db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15721a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15721a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15721ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15721af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15721b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15721b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15721bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15721c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15721c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15721ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15721ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15721d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15721d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15721dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15721e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15721e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15721e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15721ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15721f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15721f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15721fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15721ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1572203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157220830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157220ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157221110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157221580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1572219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157221e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1572222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157222740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157222bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157223020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157223490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157223900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157223d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1572241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157224650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157224ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157224f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1572253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157225810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157225c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1572260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157226560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1572269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157226e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1572272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157227720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157227b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157228000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157228470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1572288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157228d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1572291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157229630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157229aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157229f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15722a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15722a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15722ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15722b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15722b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15722b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15722be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15722c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15722c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15722cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15722cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15722d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15722d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15722dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15722e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15722e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15722ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15722eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15722f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15722f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15722fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1572300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157230520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157230990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157230e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157231270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1572316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157231b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157231fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157232430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1572328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157232d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157233180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1572335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157233a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157233ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157234340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1572347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157234c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157235090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157235cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157235f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157236240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1572366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157236b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157236f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157237400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157237870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157237ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157238150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1572385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157238a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157238ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157239310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157239780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157239bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15723a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15723a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15723a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15723adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15723b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15723b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15723bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15723bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15723c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15723c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15723ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15723d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15723d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15723da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15723de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15723e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15723e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15723ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15723f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15723f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15723fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15723ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157240390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157240800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157240c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1572410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157241600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157241b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157242680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157242940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157242f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1572434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157243a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157244040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157244600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157244bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157245180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157245740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157245d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1572462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157246880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157246e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157247400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1572479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157247f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157248540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157248b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1572490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157249680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157249c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15724a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15724a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15724ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15724b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15724b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15724bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15724c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15724ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15724d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15724d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15724db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15724e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15724e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15724ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15724f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15724f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15724fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1572503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157250980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157250f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157251500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157251ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157252080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157252640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157252c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1572531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157253780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157253d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157254300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1572548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157254e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157255440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157255a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157255fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157256580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157256b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157257040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157257540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157257a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157257f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157258440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157258940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157258e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157259340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157259840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157259d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15725a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15725a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15725ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15725b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15725b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15725c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15725c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15725ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15725d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15725d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15725e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15725e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15725e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1556694f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15564b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15564abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15564b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15561e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15561e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1556208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15564d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155615c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15561c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15561d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15561d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15561bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15561dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155614c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155620ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15562d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155668a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155618100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15564d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15564bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155616270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1556167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155669ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155669f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15566a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15566a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15566a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15566aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15566ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15566afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15566b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15566b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15566b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15566bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15566bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15566c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15566c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15566c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15566c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15566cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15566ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15566d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15566d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15566d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15566d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15566dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15566dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15566e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15566e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15566e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15566e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15566ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15566ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15566f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15566f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15566f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15566fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15566fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15566ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155670260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155670520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1556707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155670aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155670d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155671020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1556712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1556715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155671860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155671b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155671de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1556720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155672360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155672620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1556728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155672ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155672e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155673120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1556733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1556736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155673960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155673c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155673ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1556741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155674460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155674720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1556749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155674ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155674f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155675220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1556754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1556757a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155675a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155675d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155675fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1556762a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155676560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155676820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155676ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155676da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155677060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155677320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1556775e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1556778a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155677b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155677e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1556780e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1556783a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155678660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155678920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155678be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155678ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155679160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155679420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1556796e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1556799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155679c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155679f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15567a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15567a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15567a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15567aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15567ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15567afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15567b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15567b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15567b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15567baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15567bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15567c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15567c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15567c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15567c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15567cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15567cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15567d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15567d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15567d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15567d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15567dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15567de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15567e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15567e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15567e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15567e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15567ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15567eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15567f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15567f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15567f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15567f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15567fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15567ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155680220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1556804e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1556807a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155680a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155680d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155680fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1556812a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155681560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155681820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155681ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155681da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155682060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155682320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1556825e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1556828a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155682b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155682e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1556830e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1556833a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155683660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155683920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155683be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155683ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155684160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155684420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1556846e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1556849a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155684c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155684f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1556851e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1556854a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155685760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155685a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155685ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155685fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155686260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155686520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1556867e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155686aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155686d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155687020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1556872e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1556875a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155687860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155687b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155687de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1556880a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155688360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155688620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1556888e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155688ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155688e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155689120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1556893e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1556896a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155689c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155689f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15568a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15568a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15568ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15568b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15568b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15568bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15568c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15568c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15568cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15568d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15568d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15568dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15568e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15568e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15568ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15568f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15568f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15568fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155690190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1556906e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155690c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155691180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1556916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155691c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155692170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1556926c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155692c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155693160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1556936b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155693c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155694150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1556946a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155694bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155695140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155695690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155695be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155696130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155696680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155696bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155697120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155697670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155697bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155698110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155698660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155698bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155699100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155699650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155699ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15569a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15569a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15569ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15569b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15569b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15569bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15569c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15569c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15569c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15569c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15569cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15569d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15569d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15569dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15569df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15569e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15569e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15569ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15569f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15569f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15569f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15569fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1556a02c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1556a0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1556a1420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1556a1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1556a2260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1556a2520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1556a2990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1556a2f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1556a35a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.242s
sys	0m0.139s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
