Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.45.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.16.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.2.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.0)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.14.0)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.3)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.4)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.25.1)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.31.0)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.5.10)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.0)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (8.9.2.26)
Requirement already satisfied: triton==2.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.0)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.3.1)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.11.0)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.0.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (11.4.5.107)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.19.3)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (10.3.2.106)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.12)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.3.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (11.0.2.54)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.4.127)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (4.66.4)
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (6.0.1)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.16.0-py3-none-any.whl size=3462 sha256=21a785bbda7f70574e9e564518770340ba410d02065476143fe78d1ebce56fe3
  Stored in directory: /tmp/pip-ephem-wheel-cache-3_9ymueo/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.16.0
    Uninstalling gguf-0.16.0:
      Successfully uninstalled gguf-0.16.0
Successfully installed gguf-0.16.0
+ gg_run_ctest_debug
+ tee /home/ggml/results/llama.cpp/8d/261091742b6766e55e062051ffb141806a407a/ggml-2-x86-cpu/ctest_debug.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/8d/261091742b6766e55e062051ffb141806a407a/ggml-2-x86-cpu/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5")
-- Found OpenMP_CXX: -fopenmp (found version "4.5")
-- Found OpenMP: TRUE (found version "4.5")
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.800s
user	0m0.577s
sys	0m0.228s
+ tee -a /home/ggml/results/llama.cpp/8d/261091742b6766e55e062051ffb141806a407a/ggml-2-x86-cpu/ctest_debug-make.log
++ nproc
+ make -j8
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
-- Found Git: /usr/bin/git (found version "2.34.1")
[  4%] Built target xxhash
[  4%] Built target sha256
[  4%] Built target sha1
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Linking CXX shared library ../../bin/libggml-base.so
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Built target ggml-cpu
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 11%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 11%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Linking CXX executable ../../bin/llama-gguf
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Built target llama-gguf
[ 20%] Built target llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
In file included from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:33:36: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   33 | using llama_graph_input_ptr = std::shared_ptr<llama_graph_input_i>;
      |                                    ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:5:1: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
    4 | #include <vector>
  +++ |+#include <memory>
    5 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:44:41: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                         ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:44:36: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                    ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:37: error: ‘unique_ptr’ in namespace ‘std’ does not name a template type
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                     ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:32: note: ‘std::unique_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:78:20: error: ‘llama_graph_input_ptr’ has not been declared
   78 |     void add_input(llama_graph_input_ptr && input) {
      |                    ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:87:17: error: ‘llama_graph_input_ptr’ was not declared in this scope; did you mean ‘llama_graph_input_i’?
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                 ^~~~~~~~~~~~~~~~~~~~~
      |                 llama_graph_input_i
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 1 is invalid
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                                      ^
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 2 is invalid
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘virtual void llama_graph_result::set_inputs(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘begin’ was not declared in this scope; did you mean ‘std::begin’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::begin
In file included from /usr/include/c++/11/vector:69,
                 from /home/ggml/work/llama.cpp/src/llama-graph.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/usr/include/c++/11/bits/range_access.h:108:37: note: ‘std::begin’ declared here
  108 |   template<typename _Tp> const _Tp* begin(const valarray<_Tp>&) noexcept;
      |                                     ^~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘end’ was not declared in this scope; did you mean ‘std::end’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::end
In file included from /usr/include/c++/11/vector:69,
                 from /home/ggml/work/llama.cpp/src/llama-graph.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/usr/include/c++/11/bits/range_access.h:110:37: note: ‘std::end’ declared here
  110 |   template<typename _Tp> const _Tp* end(const valarray<_Tp>&) noexcept;
      |                                     ^~~
In file included from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘void llama_graph_result::add_input(int&&)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:79:16: error: request for member ‘emplace_back’ in ‘((llama_graph_result*)this)->llama_graph_result::inputs’, which is of non-class type ‘int’
   79 |         inputs.emplace_back(std::move(input));
      |                ^~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: At global scope:
/home/ggml/work/llama.cpp/src/llama-graph.h:169:13: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  169 |     virtual llama_graph_input_attn_ptr build_attn_inp(
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |             llama_graph_input_attn_i
make[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-graph.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:4:
/home/ggml/work/llama.cpp/src/llama-graph.h:33:36: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   33 | using llama_graph_input_ptr = std::shared_ptr<llama_graph_input_i>;
      |                                    ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:1:1: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
  +++ |+#include <memory>
    1 | #pragma once
/home/ggml/work/llama.cpp/src/llama-graph.h:44:41: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                         ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:44:36: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                    ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:37: error: ‘unique_ptr’ in namespace ‘std’ does not name a template type
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                     ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:32: note: ‘std::unique_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:78:20: error: ‘llama_graph_input_ptr’ has not been declared
   78 |     void add_input(llama_graph_input_ptr && input) {
      |                    ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:87:17: error: ‘llama_graph_input_ptr’ was not declared in this scope; did you mean ‘llama_graph_input_i’?
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                 ^~~~~~~~~~~~~~~~~~~~~
      |                 llama_graph_input_i
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 1 is invalid
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                                      ^
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 2 is invalid
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘virtual void llama_graph_result::set_inputs(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘begin’ was not declared in this scope; did you mean ‘std::begin’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::begin
In file included from /usr/include/c++/11/string:54,
                 from /home/ggml/work/llama.cpp/src/llama-impl.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:3:
/usr/include/c++/11/bits/range_access.h:108:37: note: ‘std::begin’ declared here
  108 |   template<typename _Tp> const _Tp* begin(const valarray<_Tp>&) noexcept;
      |                                     ^~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:4:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘end’ was not declared in this scope; did you mean ‘std::end’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::end
In file included from /usr/include/c++/11/string:54,
                 from /home/ggml/work/llama.cpp/src/llama-impl.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:3:
/usr/include/c++/11/bits/range_access.h:110:37: note: ‘std::end’ declared here
  110 |   template<typename _Tp> const _Tp* end(const valarray<_Tp>&) noexcept;
      |                                     ^~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:4:
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘void llama_graph_result::add_input(int&&)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:79:16: error: request for member ‘emplace_back’ in ‘((llama_graph_result*)this)->llama_graph_result::inputs’, which is of non-class type ‘int’
   79 |         inputs.emplace_back(std::move(input));
      |                ^~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-quant.cpp:4:
/home/ggml/work/llama.cpp/src/llama-graph.h: At global scope:
/home/ggml/work/llama.cpp/src/llama-graph.h:169:13: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  169 |     virtual llama_graph_input_attn_ptr build_attn_inp(
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |             llama_graph_input_attn_i
In file included from /home/ggml/work/llama.cpp/src/llama-quant.cpp:4:
/home/ggml/work/llama.cpp/src/llama-model.h:369:5: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
  369 |     llama_graph_result_ptr build_graph(
      |     ^~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_result_i
make[2]: *** [src/CMakeFiles/llama.dir/build.make:286: src/CMakeFiles/llama.dir/llama-quant.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:33:36: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   33 | using llama_graph_input_ptr = std::shared_ptr<llama_graph_input_i>;
      |                                    ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:5:1: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
    4 | #include <vector>
  +++ |+#include <memory>
    5 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:44:41: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                         ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:44:36: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                    ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:37: error: ‘unique_ptr’ in namespace ‘std’ does not name a template type
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                     ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:32: note: ‘std::unique_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:78:20: error: ‘llama_graph_input_ptr’ has not been declared
   78 |     void add_input(llama_graph_input_ptr && input) {
      |                    ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:87:17: error: ‘llama_graph_input_ptr’ was not declared in this scope; did you mean ‘llama_graph_input_i’?
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                 ^~~~~~~~~~~~~~~~~~~~~
      |                 llama_graph_input_i
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 1 is invalid
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                                      ^
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 2 is invalid
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘virtual void llama_graph_result::set_inputs(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘begin’ was not declared in this scope; did you mean ‘std::begin’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::begin
In file included from /usr/include/c++/11/string:54,
                 from /home/ggml/work/llama.cpp/src/llama-arch.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/usr/include/c++/11/bits/range_access.h:108:37: note: ‘std::begin’ declared here
  108 |   template<typename _Tp> const _Tp* begin(const valarray<_Tp>&) noexcept;
      |                                     ^~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘end’ was not declared in this scope; did you mean ‘std::end’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::end
In file included from /usr/include/c++/11/string:54,
                 from /home/ggml/work/llama.cpp/src/llama-arch.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/usr/include/c++/11/bits/range_access.h:110:37: note: ‘std::end’ declared here
  110 |   template<typename _Tp> const _Tp* end(const valarray<_Tp>&) noexcept;
      |                                     ^~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘void llama_graph_result::add_input(int&&)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:79:16: error: request for member ‘emplace_back’ in ‘((llama_graph_result*)this)->llama_graph_result::inputs’, which is of non-class type ‘int’
   79 |         inputs.emplace_back(std::move(input));
      |                ^~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: At global scope:
/home/ggml/work/llama.cpp/src/llama-graph.h:169:13: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  169 |     virtual llama_graph_input_attn_ptr build_attn_inp(
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |             llama_graph_input_attn_i
In file included from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-model.h:369:5: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
  369 |     llama_graph_result_ptr build_graph(
      |     ^~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_result_i
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_llama(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:4501:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 4501 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_deci(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:4662:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 4662 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_baichuan(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:4818:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 4818 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_xverse(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:4934:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 4934 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_falcon(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5039:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5039 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_grok(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5162:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5162 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_dbrx(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5314:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5314 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_starcoder(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5436:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5436 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_refact(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5537:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5537 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_bert(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5651:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5651 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, false, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_bloom(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5796:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5796 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_mpt(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:5899:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 5899 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_stablelm(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6041:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6041 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_qwen(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6192:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6192 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_qwen2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6306:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6306 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_qwen2vl(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6419:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6419 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_qwen2moe(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6537:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6537 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_phi2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6684:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6684 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_phi3(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6807:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6807 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, true);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_plamo(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:6951:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 6951 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_gpt2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7057:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7057 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_codeshell(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7163:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7163 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_orion(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7274:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7274 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_internlm2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7393:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7393 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_minicpm3(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7521:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7521 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_gemma(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7722:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7722 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_gemma2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7830:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7830 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, true);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_starcoder2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:7960:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 7960 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_command_r(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8135:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8135 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_cohere2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8283:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8283 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, true);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_olmo(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8418:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8418 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_olmo2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8538:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8538 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_olmoe(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8662:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8662 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_openelm(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8783:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8783 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_gptneox(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:8911:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 8911 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_arctic(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9055:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9055 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_deepseek(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9185:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9185 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_deepseek2(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9348:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9348 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_bitnet(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9566:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9566 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_t5_enc(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9717:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9717 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, false, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_t5_dec(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9820:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9820 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_jais(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:9983:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
 9983 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_chatglm(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:10077:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
10077 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_nemotron(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:10207:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
10207 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_exaone(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:10328:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
10328 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: In member function ‘void llm_build_context::build_chameleon(ggml_cgraph*)’:
/home/ggml/work/llama.cpp/src/llama-model.cpp:10633:30: error: ‘class llama_graph_i’ has no member named ‘build_attn_inp’; did you mean ‘build_attn’?
10633 |         auto inp_attn = lgf->build_attn_inp(res.get(), ctx0, n_tokens, true, false);
      |                              ^~~~~~~~~~~~~~
      |                              build_attn
/home/ggml/work/llama.cpp/src/llama-model.cpp: At global scope:
/home/ggml/work/llama.cpp/src/llama-model.cpp:10939:1: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
10939 | llama_graph_result_ptr llama_model::build_graph(
      | ^~~~~~~~~~~~~~~~~~~~~~
      | llama_graph_result_i
make[2]: *** [src/CMakeFiles/llama.dir/build.make:272: src/CMakeFiles/llama.dir/llama-model.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:33:36: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   33 | using llama_graph_input_ptr = std::shared_ptr<llama_graph_input_i>;
      |                                    ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:1:1: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
  +++ |+#include <memory>
    1 | #pragma once
/home/ggml/work/llama.cpp/src/llama-graph.h:44:41: error: ‘shared_ptr’ in namespace ‘std’ does not name a template type
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                         ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:44:36: note: ‘std::shared_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   44 | using llama_graph_input_attn_ptr = std::shared_ptr<llama_graph_input_attn_i>;
      |                                    ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:37: error: ‘unique_ptr’ in namespace ‘std’ does not name a template type
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                     ^~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:61:32: note: ‘std::unique_ptr’ is defined in header ‘<memory>’; did you forget to ‘#include <memory>’?
   61 | using llama_graph_result_ptr = std::unique_ptr<llama_graph_result_i>;
      |                                ^~~
/home/ggml/work/llama.cpp/src/llama-graph.h:78:20: error: ‘llama_graph_input_ptr’ has not been declared
   78 |     void add_input(llama_graph_input_ptr && input) {
      |                    ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:87:17: error: ‘llama_graph_input_ptr’ was not declared in this scope; did you mean ‘llama_graph_input_i’?
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                 ^~~~~~~~~~~~~~~~~~~~~
      |                 llama_graph_input_i
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 1 is invalid
   87 |     std::vector<llama_graph_input_ptr> inputs;
      |                                      ^
/home/ggml/work/llama.cpp/src/llama-graph.h:87:38: error: template argument 2 is invalid
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘virtual void llama_graph_result::set_inputs(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘begin’ was not declared in this scope; did you mean ‘std::begin’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::begin
In file included from /usr/include/c++/11/array:41,
                 from /home/ggml/work/llama.cpp/src/llama-batch.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-context.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/usr/include/c++/11/bits/range_access.h:108:37: note: ‘std::begin’ declared here
  108 |   template<typename _Tp> const _Tp* begin(const valarray<_Tp>&) noexcept;
      |                                     ^~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:73:29: error: ‘end’ was not declared in this scope; did you mean ‘std::end’?
   73 |         for (auto & input : inputs) {
      |                             ^~~~~~
      |                             std::end
In file included from /usr/include/c++/11/array:41,
                 from /home/ggml/work/llama.cpp/src/llama-batch.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-context.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/usr/include/c++/11/bits/range_access.h:110:37: note: ‘std::end’ declared here
  110 |   template<typename _Tp> const _Tp* end(const valarray<_Tp>&) noexcept;
      |                                     ^~~
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: In member function ‘void llama_graph_result::add_input(int&&)’:
/home/ggml/work/llama.cpp/src/llama-graph.h:79:16: error: request for member ‘emplace_back’ in ‘((llama_graph_result*)this)->llama_graph_result::inputs’, which is of non-class type ‘int’
   79 |         inputs.emplace_back(std::move(input));
      |                ^~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h: At global scope:
/home/ggml/work/llama.cpp/src/llama-graph.h:169:13: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  169 |     virtual llama_graph_input_attn_ptr build_attn_inp(
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |             llama_graph_input_attn_i
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:7,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-model.h:369:5: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
  369 |     llama_graph_result_ptr build_graph(
      |     ^~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_result_i
In file included from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-context.h:291:13: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
  291 |     virtual llama_graph_result_ptr graph_build(
      |             ^~~~~~~~~~~~~~~~~~~~~~
      |             llama_graph_result_i
/home/ggml/work/llama.cpp/src/llama-context.h:367:5: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  367 |     llama_graph_input_attn_ptr build_attn_inp(
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_input_attn_i
/home/ggml/work/llama.cpp/src/llama-context.h:584:5: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  584 |     llama_graph_input_attn_ptr build_attn_inp(
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_input_attn_i
/home/ggml/work/llama.cpp/src/llama-context.h:798:5: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
  798 |     llama_graph_input_attn_ptr build_attn_inp(
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |     llama_graph_input_attn_i
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_context_base::reserve()’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:492:9: error: ‘graph_build’ was not declared in this scope
  492 |         graph_build(ctx_compute.get(), gf, ubatch_pp);
      |         ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:506:9: error: ‘graph_build’ was not declared in this scope
  506 |         graph_build(ctx_compute.get(), gf, ubatch_tg);
      |         ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:519:9: error: ‘graph_build’ was not declared in this scope
  519 |         graph_build(ctx_compute.get(), gf, ubatch_pp);
      |         ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_base::encode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:844:16: error: ‘graph_build’ was not declared in this scope
  844 |     auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_base::decode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:999:16: error: ‘graph_build’ was not declared in this scope
  999 |     auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: At global scope:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1439:1: error: ‘llama_graph_result_ptr’ does not name a type; did you mean ‘llama_graph_result_i’?
 1439 | llama_graph_result_ptr llama_context_base::graph_build(
      | ^~~~~~~~~~~~~~~~~~~~~~
      | llama_graph_result_i
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual ggml_tensor* llama_context_base::build_inp_embd(llama_graph_result*, ggml_context*, ggml_tensor*, const llama_ubatch&) const’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1689:29: error: cannot convert ‘std::remove_reference<std::shared_ptr<llama_graph_input_embd>&>::type’ {aka ‘std::shared_ptr<llama_graph_input_embd>’} to ‘int&&’
 1689 |     res->add_input(std::move(inp));
      |                    ~~~~~~~~~^~~~~
      |                             |
      |                             std::remove_reference<std::shared_ptr<llama_graph_input_embd>&>::type {aka std::shared_ptr<llama_graph_input_embd>}
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:78:45: note:   initializing argument 1 of ‘void llama_graph_result::add_input(int&&)’
   78 |     void add_input(llama_graph_input_ptr && input) {
      |                    ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: At global scope:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1742:1: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
 1742 | llama_graph_input_attn_ptr llama_context_base::build_attn_inp(
      | ^~~~~~~~~~~~~~~~~~~~~~~~~~
      | llama_graph_input_attn_i
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_context_kv_self::kv_self_update()’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:2721:9: error: ‘graph_build’ was not declared in this scope
 2721 |         graph_build(ctx_compute.get(), gf, ubatch);
      |         ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_kv_self::encode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:2790:16: error: ‘graph_build’ was not declared in this scope
 2790 |     auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_kv_self::decode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:3027:20: error: ‘graph_build’ was not declared in this scope
 3027 |         auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                    ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: At global scope:
/home/ggml/work/llama.cpp/src/llama-context.cpp:3255:1: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
 3255 | llama_graph_input_attn_ptr llama_context_kv_self::build_attn_inp(
      | ^~~~~~~~~~~~~~~~~~~~~~~~~~
      | llama_graph_input_attn_i
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_recurrent::decode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:3981:20: error: ‘graph_build’ was not declared in this scope
 3981 |         auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                    ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual int llama_context_enc::encode(llama_batch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:4691:16: error: ‘graph_build’ was not declared in this scope
 4691 |     auto res = graph_build(ctx_compute.get(), gf, ubatch);
      |                ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: At global scope:
/home/ggml/work/llama.cpp/src/llama-context.cpp:4883:1: error: ‘llama_graph_input_attn_ptr’ does not name a type; did you mean ‘llama_graph_input_attn_i’?
 4883 | llama_graph_input_attn_ptr llama_context_dec::build_attn_inp(
      | ^~~~~~~~~~~~~~~~~~~~~~~~~~
      | llama_graph_input_attn_i
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1771: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m1.839s
user	0m7.773s
sys	0m0.964s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/8d/261091742b6766e55e062051ffb141806a407a/ggml-2-x86-cpu/ctest_debug-ctest.log: No such file or directory
