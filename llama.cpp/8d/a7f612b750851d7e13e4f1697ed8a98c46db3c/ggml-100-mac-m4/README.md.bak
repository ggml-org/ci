### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.14 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.69 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.09 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.63 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.94 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.01 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.92 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.42 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.34 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.61 sec*proc (29 tests)

Total Test time (real) = 255.63 sec

real	4m15.781s
user	8m35.019s
sys	0m7.322s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.79 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.43 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.84 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.38 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.12 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.74 sec*proc (29 tests)

Total Test time (real) =  54.75 sec

real	0m54.763s
user	1m16.956s
sys	0m6.332s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.113 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.572 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.310 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.318 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.323 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.324 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.324 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.324 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.325 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.326 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.326 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.326 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.327 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.329 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.330 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.330 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.330 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.331 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.331 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.331 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.067 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.068 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.068 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.068 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.069 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.069 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.070 I llama_model_loader: - type  f32:  124 tensors
0.00.024.070 I llama_model_loader: - type  f16:   73 tensors
0.00.024.070 I print_info: file format = GGUF V3 (latest)
0.00.024.071 I print_info: file type   = F16
0.00.024.073 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.026.397 I load: special tokens cache size = 5
0.00.027.620 I load: token to piece cache size = 0.2032 MB
0.00.027.634 I print_info: arch             = bert
0.00.027.636 I print_info: vocab_only       = 0
0.00.027.636 I print_info: n_ctx_train      = 512
0.00.027.636 I print_info: n_embd           = 384
0.00.027.637 I print_info: n_layer          = 12
0.00.027.640 I print_info: n_head           = 12
0.00.027.641 I print_info: n_head_kv        = 12
0.00.027.641 I print_info: n_rot            = 32
0.00.027.641 I print_info: n_swa            = 0
0.00.027.641 I print_info: n_embd_head_k    = 32
0.00.027.645 I print_info: n_embd_head_v    = 32
0.00.027.646 I print_info: n_gqa            = 1
0.00.027.646 I print_info: n_embd_k_gqa     = 384
0.00.027.647 I print_info: n_embd_v_gqa     = 384
0.00.027.648 I print_info: f_norm_eps       = 1.0e-12
0.00.027.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.027.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.027.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.027.650 I print_info: f_logit_scale    = 0.0e+00
0.00.027.650 I print_info: n_ff             = 1536
0.00.027.652 I print_info: n_expert         = 0
0.00.027.652 I print_info: n_expert_used    = 0
0.00.027.652 I print_info: causal attn      = 0
0.00.027.652 I print_info: pooling type     = 2
0.00.027.652 I print_info: rope type        = 2
0.00.027.655 I print_info: rope scaling     = linear
0.00.027.656 I print_info: freq_base_train  = 10000.0
0.00.027.656 I print_info: freq_scale_train = 1
0.00.027.657 I print_info: n_ctx_orig_yarn  = 512
0.00.027.657 I print_info: rope_finetuned   = unknown
0.00.027.657 I print_info: ssm_d_conv       = 0
0.00.027.658 I print_info: ssm_d_inner      = 0
0.00.027.658 I print_info: ssm_d_state      = 0
0.00.027.658 I print_info: ssm_dt_rank      = 0
0.00.027.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.027.658 I print_info: model type       = 33M
0.00.027.658 I print_info: model params     = 33.21 M
0.00.027.658 I print_info: general.name     = Bge Small
0.00.027.659 I print_info: vocab type       = WPM
0.00.027.659 I print_info: n_vocab          = 30522
0.00.027.659 I print_info: n_merges         = 0
0.00.027.660 I print_info: BOS token        = 101 '[CLS]'
0.00.027.662 I print_info: UNK token        = 100 '[UNK]'
0.00.027.662 I print_info: SEP token        = 102 '[SEP]'
0.00.027.662 I print_info: PAD token        = 0 '[PAD]'
0.00.027.663 I print_info: MASK token       = 103 '[MASK]'
0.00.027.664 I print_info: LF token         = 0 '[PAD]'
0.00.027.664 I print_info: max token length = 21
0.00.027.665 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.029.702 I load_tensors: offloading 12 repeating layers to GPU
0.00.029.702 I load_tensors: offloading output layer to GPU
0.00.029.703 I load_tensors: offloaded 13/13 layers to GPU
0.00.029.723 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.029.724 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.029.914 I llama_context_unified: n_seq_max     = 1
0.00.029.915 I llama_context_unified: n_ctx         = 512
0.00.029.915 I llama_context_unified: n_ctx_per_seq = 512
0.00.029.915 I llama_context_unified: n_batch       = 2048
0.00.029.915 I llama_context_unified: n_ubatch      = 2048
0.00.029.915 I llama_context_unified: flash_attn    = 0
0.00.029.916 I llama_context_unified: freq_base     = 10000.0
0.00.029.916 I llama_context_unified: freq_scale    = 1
0.00.029.917 I ggml_metal_init: allocating
0.00.029.920 I ggml_metal_init: found device: Apple M4
0.00.029.924 I ggml_metal_init: picking default device: Apple M4
0.00.030.464 I ggml_metal_init: using embedded metal library
0.00.032.951 I ggml_metal_init: GPU name:   Apple M4
0.00.032.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.954 I ggml_metal_init: simdgroup reduction   = true
0.00.032.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.954 I ggml_metal_init: has residency sets    = true
0.00.032.954 I ggml_metal_init: has bfloat            = true
0.00.032.955 I ggml_metal_init: use bfloat            = true
0.00.032.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.043.124 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.709 I init:      Metal KV buffer size =     9.00 MiB
0.00.043.711 I llama_context_unified: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.712 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.044.775 I llama_context_unified:      Metal compute buffer size =    16.00 MiB
0.00.044.776 I llama_context_unified:        CPU compute buffer size =     2.51 MiB
0.00.044.776 I llama_context_unified: graph nodes  = 429
0.00.044.776 I llama_context_unified: graph splits = 2
0.00.044.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.044.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.049.372 I 
0.00.049.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.918 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.054.267 I llama_perf_context_print:        load time =      32.79 ms
0.00.054.268 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2134.72 tokens per second)
0.00.054.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.054.269 I llama_perf_context_print:       total time =       4.90 ms /    10 tokens
0.00.054.541 I ggml_metal_free: deallocating

real	0m0.225s
user	0m0.037s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.339 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.343 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.346 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.347 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.347 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.348 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.349 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.349 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.349 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.350 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.350 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.353 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.353 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.354 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.354 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.354 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.354 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.486 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.087 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.088 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.089 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.089 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.089 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.090 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.090 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.090 I llama_model_loader: - type  f32:  124 tensors
0.00.014.091 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.091 I print_info: file format = GGUF V3 (latest)
0.00.014.092 I print_info: file type   = Q8_0
0.00.014.093 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.430 I load: special tokens cache size = 5
0.00.017.678 I load: token to piece cache size = 0.2032 MB
0.00.017.687 I print_info: arch             = bert
0.00.017.688 I print_info: vocab_only       = 0
0.00.017.689 I print_info: n_ctx_train      = 512
0.00.017.689 I print_info: n_embd           = 384
0.00.017.689 I print_info: n_layer          = 12
0.00.017.692 I print_info: n_head           = 12
0.00.017.692 I print_info: n_head_kv        = 12
0.00.017.692 I print_info: n_rot            = 32
0.00.017.692 I print_info: n_swa            = 0
0.00.017.692 I print_info: n_embd_head_k    = 32
0.00.017.692 I print_info: n_embd_head_v    = 32
0.00.017.693 I print_info: n_gqa            = 1
0.00.017.693 I print_info: n_embd_k_gqa     = 384
0.00.017.694 I print_info: n_embd_v_gqa     = 384
0.00.017.694 I print_info: f_norm_eps       = 1.0e-12
0.00.017.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.695 I print_info: f_logit_scale    = 0.0e+00
0.00.017.696 I print_info: n_ff             = 1536
0.00.017.696 I print_info: n_expert         = 0
0.00.017.696 I print_info: n_expert_used    = 0
0.00.017.696 I print_info: causal attn      = 0
0.00.017.696 I print_info: pooling type     = 2
0.00.017.697 I print_info: rope type        = 2
0.00.017.697 I print_info: rope scaling     = linear
0.00.017.697 I print_info: freq_base_train  = 10000.0
0.00.017.697 I print_info: freq_scale_train = 1
0.00.017.697 I print_info: n_ctx_orig_yarn  = 512
0.00.017.698 I print_info: rope_finetuned   = unknown
0.00.017.698 I print_info: ssm_d_conv       = 0
0.00.017.698 I print_info: ssm_d_inner      = 0
0.00.017.698 I print_info: ssm_d_state      = 0
0.00.017.698 I print_info: ssm_dt_rank      = 0
0.00.017.698 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.698 I print_info: model type       = 33M
0.00.017.699 I print_info: model params     = 33.21 M
0.00.017.699 I print_info: general.name     = Bge Small
0.00.017.699 I print_info: vocab type       = WPM
0.00.017.699 I print_info: n_vocab          = 30522
0.00.017.699 I print_info: n_merges         = 0
0.00.017.700 I print_info: BOS token        = 101 '[CLS]'
0.00.017.700 I print_info: UNK token        = 100 '[UNK]'
0.00.017.700 I print_info: SEP token        = 102 '[SEP]'
0.00.017.700 I print_info: PAD token        = 0 '[PAD]'
0.00.017.700 I print_info: MASK token       = 103 '[MASK]'
0.00.017.700 I print_info: LF token         = 0 '[PAD]'
0.00.017.701 I print_info: max token length = 21
0.00.017.701 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.374 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.375 I load_tensors: offloading output layer to GPU
0.00.019.375 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.381 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.382 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.555 I llama_context_unified: n_seq_max     = 1
0.00.019.556 I llama_context_unified: n_ctx         = 512
0.00.019.556 I llama_context_unified: n_ctx_per_seq = 512
0.00.019.556 I llama_context_unified: n_batch       = 2048
0.00.019.556 I llama_context_unified: n_ubatch      = 2048
0.00.019.557 I llama_context_unified: flash_attn    = 0
0.00.019.557 I llama_context_unified: freq_base     = 10000.0
0.00.019.557 I llama_context_unified: freq_scale    = 1
0.00.019.558 I ggml_metal_init: allocating
0.00.019.561 I ggml_metal_init: found device: Apple M4
0.00.019.566 I ggml_metal_init: picking default device: Apple M4
0.00.020.058 I ggml_metal_init: using embedded metal library
0.00.022.400 I ggml_metal_init: GPU name:   Apple M4
0.00.022.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.402 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.403 I ggml_metal_init: simdgroup reduction   = true
0.00.022.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.403 I ggml_metal_init: has residency sets    = true
0.00.022.403 I ggml_metal_init: has bfloat            = true
0.00.022.403 I ggml_metal_init: use bfloat            = true
0.00.022.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.765 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.373 I init:      Metal KV buffer size =     9.00 MiB
0.00.033.375 I llama_context_unified: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.378 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.034.328 I llama_context_unified:      Metal compute buffer size =    16.00 MiB
0.00.034.329 I llama_context_unified:        CPU compute buffer size =     2.51 MiB
0.00.034.329 I llama_context_unified: graph nodes  = 429
0.00.034.329 I llama_context_unified: graph splits = 2
0.00.034.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.343 I 
0.00.038.370 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.871 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.289 I llama_perf_context_print:        load time =      29.40 ms
0.00.043.291 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2090.59 tokens per second)
0.00.043.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.292 I llama_perf_context_print:       total time =       4.95 ms /    10 tokens
0.00.043.515 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.323 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.605 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.212 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.217 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.219 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.221 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.222 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.222 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.223 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.224 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.225 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.226 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.226 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.229 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.229 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.230 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.180 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.560 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.560 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.560 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.561 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.561 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.562 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.562 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.562 I llama_model_loader: - type  f32:   40 tensors
0.00.047.563 I llama_model_loader: - type  f16:   30 tensors
0.00.047.568 I print_info: file format = GGUF V3 (latest)
0.00.047.569 I print_info: file type   = F16
0.00.047.570 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.051.713 W load: empty token at index 5
0.00.056.688 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.190 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.224 I load: special tokens cache size = 5
0.00.321.930 I load: token to piece cache size = 1.5060 MB
0.00.321.938 I print_info: arch             = jina-bert-v2
0.00.321.938 I print_info: vocab_only       = 0
0.00.321.938 I print_info: n_ctx_train      = 8192
0.00.321.939 I print_info: n_embd           = 384
0.00.321.939 I print_info: n_layer          = 4
0.00.321.947 I print_info: n_head           = 12
0.00.321.950 I print_info: n_head_kv        = 12
0.00.321.951 I print_info: n_rot            = 32
0.00.321.951 I print_info: n_swa            = 0
0.00.321.951 I print_info: n_embd_head_k    = 32
0.00.321.951 I print_info: n_embd_head_v    = 32
0.00.321.952 I print_info: n_gqa            = 1
0.00.321.952 I print_info: n_embd_k_gqa     = 384
0.00.321.954 I print_info: n_embd_v_gqa     = 384
0.00.321.955 I print_info: f_norm_eps       = 1.0e-12
0.00.321.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.321.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.321.956 I print_info: f_max_alibi_bias = 8.0e+00
0.00.321.956 I print_info: f_logit_scale    = 0.0e+00
0.00.321.957 I print_info: n_ff             = 1536
0.00.321.957 I print_info: n_expert         = 0
0.00.321.957 I print_info: n_expert_used    = 0
0.00.321.957 I print_info: causal attn      = 0
0.00.321.958 I print_info: pooling type     = -1
0.00.321.958 I print_info: rope type        = -1
0.00.321.958 I print_info: rope scaling     = linear
0.00.321.958 I print_info: freq_base_train  = 10000.0
0.00.321.959 I print_info: freq_scale_train = 1
0.00.321.959 I print_info: n_ctx_orig_yarn  = 8192
0.00.321.959 I print_info: rope_finetuned   = unknown
0.00.321.959 I print_info: ssm_d_conv       = 0
0.00.321.959 I print_info: ssm_d_inner      = 0
0.00.321.961 I print_info: ssm_d_state      = 0
0.00.321.961 I print_info: ssm_dt_rank      = 0
0.00.321.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.321.961 I print_info: model type       = 33M
0.00.321.962 I print_info: model params     = 32.90 M
0.00.321.962 I print_info: general.name     = Jina Bert Implementation
0.00.321.964 I print_info: vocab type       = BPE
0.00.321.964 I print_info: n_vocab          = 61056
0.00.321.964 I print_info: n_merges         = 39382
0.00.321.964 I print_info: BOS token        = 0 '<s>'
0.00.321.966 I print_info: EOS token        = 2 '</s>'
0.00.321.966 I print_info: UNK token        = 3 '<unk>'
0.00.321.966 I print_info: SEP token        = 2 '</s>'
0.00.321.966 I print_info: PAD token        = 1 '<pad>'
0.00.321.966 I print_info: MASK token       = 4 '<mask>'
0.00.321.967 I print_info: EOG token        = 2 '</s>'
0.00.321.967 I print_info: max token length = 45
0.00.321.968 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.370 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.371 I load_tensors: offloading output layer to GPU
0.00.324.371 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.400 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.401 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.666 I llama_context_unified: n_seq_max     = 1
0.00.324.667 I llama_context_unified: n_ctx         = 8192
0.00.324.667 I llama_context_unified: n_ctx_per_seq = 8192
0.00.324.667 I llama_context_unified: n_batch       = 2048
0.00.324.667 I llama_context_unified: n_ubatch      = 2048
0.00.324.668 I llama_context_unified: flash_attn    = 0
0.00.324.668 I llama_context_unified: freq_base     = 10000.0
0.00.324.668 I llama_context_unified: freq_scale    = 1
0.00.324.669 I ggml_metal_init: allocating
0.00.324.673 I ggml_metal_init: found device: Apple M4
0.00.324.676 I ggml_metal_init: picking default device: Apple M4
0.00.325.376 I ggml_metal_init: using embedded metal library
0.00.328.252 I ggml_metal_init: GPU name:   Apple M4
0.00.328.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.254 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.254 I ggml_metal_init: simdgroup reduction   = true
0.00.328.255 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.255 I ggml_metal_init: has residency sets    = true
0.00.328.255 I ggml_metal_init: has bfloat            = true
0.00.328.255 I ggml_metal_init: use bfloat            = true
0.00.328.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.256 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.904 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.087 I init:      Metal KV buffer size =    48.00 MiB
0.00.341.089 I llama_context_unified: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.091 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.349.177 I llama_context_unified:      Metal compute buffer size =   220.01 MiB
0.00.349.179 I llama_context_unified:        CPU compute buffer size =    22.02 MiB
0.00.349.179 I llama_context_unified: graph nodes  = 154
0.00.349.179 I llama_context_unified: graph splits = 2
0.00.349.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.414 I 
0.00.357.451 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.560 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.561 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.564 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.564 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.569 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.569 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.135 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.659 I llama_perf_context_print:        load time =     335.80 ms
0.00.361.661 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17638.69 tokens per second)
0.00.361.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.662 I llama_perf_context_print:       total time =       4.25 ms /    63 tokens
0.00.362.182 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.330s
sys	0m0.052s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.266 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.461 I main: llama backend init
0.00.000.472 I main: load the model and apply lora adapter, if any
0.00.064.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.077.046 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.077.058 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.077.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.077.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.077.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.077.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.077.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.077.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.077.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.077.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.077.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.077.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.077.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.077.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.077.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.077.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.077.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.083.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.086.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.095.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.755 I llama_model_loader: - type  f32:  194 tensors
0.00.095.755 I llama_model_loader: - type  f16:   98 tensors
0.00.095.760 I print_info: file format = GGUF V3 (latest)
0.00.095.761 I print_info: file type   = all F32 (guessed)
0.00.095.764 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.112.149 I load: special tokens cache size = 25
0.00.121.491 I load: token to piece cache size = 0.2984 MB
0.00.121.495 I print_info: arch             = gptneox
0.00.121.495 I print_info: vocab_only       = 0
0.00.121.496 I print_info: n_ctx_train      = 2048
0.00.121.496 I print_info: n_embd           = 2048
0.00.121.496 I print_info: n_layer          = 24
0.00.121.503 I print_info: n_head           = 16
0.00.121.504 I print_info: n_head_kv        = 16
0.00.121.504 I print_info: n_rot            = 32
0.00.121.504 I print_info: n_swa            = 0
0.00.121.504 I print_info: n_embd_head_k    = 128
0.00.121.505 I print_info: n_embd_head_v    = 128
0.00.121.505 I print_info: n_gqa            = 1
0.00.121.506 I print_info: n_embd_k_gqa     = 2048
0.00.121.507 I print_info: n_embd_v_gqa     = 2048
0.00.121.508 I print_info: f_norm_eps       = 1.0e-05
0.00.121.509 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.509 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.510 I print_info: f_logit_scale    = 0.0e+00
0.00.121.511 I print_info: n_ff             = 8192
0.00.121.511 I print_info: n_expert         = 0
0.00.121.511 I print_info: n_expert_used    = 0
0.00.121.511 I print_info: causal attn      = 1
0.00.121.511 I print_info: pooling type     = 0
0.00.121.512 I print_info: rope type        = 2
0.00.121.512 I print_info: rope scaling     = linear
0.00.121.515 I print_info: freq_base_train  = 10000.0
0.00.121.515 I print_info: freq_scale_train = 1
0.00.121.516 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.516 I print_info: rope_finetuned   = unknown
0.00.121.516 I print_info: ssm_d_conv       = 0
0.00.121.516 I print_info: ssm_d_inner      = 0
0.00.121.516 I print_info: ssm_d_state      = 0
0.00.121.517 I print_info: ssm_dt_rank      = 0
0.00.121.517 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.517 I print_info: model type       = 1.4B
0.00.121.517 I print_info: model params     = 1.41 B
0.00.121.518 I print_info: general.name     = 1.4B
0.00.121.519 I print_info: vocab type       = BPE
0.00.121.519 I print_info: n_vocab          = 50304
0.00.121.519 I print_info: n_merges         = 50009
0.00.121.519 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.520 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.520 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.520 I print_info: LF token         = 187 ''
0.00.121.521 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.521 I print_info: max token length = 1024
0.00.121.521 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.177.011 I load_tensors: offloading 24 repeating layers to GPU
0.00.177.015 I load_tensors: offloading output layer to GPU
0.00.177.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.177.039 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.177.041 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.177.530 I llama_context_unified: n_seq_max     = 1
0.00.177.531 I llama_context_unified: n_ctx         = 2048
0.00.177.531 I llama_context_unified: n_ctx_per_seq = 2048
0.00.177.531 I llama_context_unified: n_batch       = 2048
0.00.177.532 I llama_context_unified: n_ubatch      = 512
0.00.177.532 I llama_context_unified: flash_attn    = 0
0.00.177.532 I llama_context_unified: freq_base     = 10000.0
0.00.177.533 I llama_context_unified: freq_scale    = 1
0.00.177.533 I ggml_metal_init: allocating
0.00.177.579 I ggml_metal_init: found device: Apple M4
0.00.177.585 I ggml_metal_init: picking default device: Apple M4
0.00.178.264 I ggml_metal_init: using embedded metal library
0.00.187.451 I ggml_metal_init: GPU name:   Apple M4
0.00.187.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.187.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.187.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.187.454 I ggml_metal_init: simdgroup reduction   = true
0.00.187.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.187.455 I ggml_metal_init: has residency sets    = true
0.00.187.455 I ggml_metal_init: has bfloat            = true
0.00.187.455 I ggml_metal_init: use bfloat            = true
0.00.187.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.187.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.214.631 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.244.190 I init:      Metal KV buffer size =   384.00 MiB
0.00.244.196 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.244.217 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.248.259 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.248.260 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.248.261 I llama_context_unified: graph nodes  = 967
0.00.248.261 I llama_context_unified: graph splits = 2
0.00.248.269 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.248.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.248.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.314.511 I main: llama threadpool init, n_threads = 4
0.00.314.548 I 
0.00.314.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.314.577 I 
0.00.314.752 I sampler seed: 1234
0.00.314.756 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.314.780 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.314.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.314.781 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.145.445 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.02.145.446 I llama_perf_context_print:        load time =     248.92 ms
0.02.145.448 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.60 tokens per second)
0.02.145.448 I llama_perf_context_print:        eval time =    1784.23 ms /    63 runs   (   28.32 ms per token,    35.31 tokens per second)
0.02.145.449 I llama_perf_context_print:       total time =    1831.78 ms /    70 tokens
0.02.149.487 I ggml_metal_free: deallocating

real	0m2.461s
user	0m0.135s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.543 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.680 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.482 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.651 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.892 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.893 I llama_model_loader: - type  f32:  194 tensors
0.00.054.894 I llama_model_loader: - type  f16:   98 tensors
0.00.054.894 I print_info: file format = GGUF V3 (latest)
0.00.054.895 I print_info: file type   = all F32 (guessed)
0.00.054.897 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.234 I load: special tokens cache size = 25
0.00.076.717 I load: token to piece cache size = 0.2984 MB
0.00.076.721 I print_info: arch             = gptneox
0.00.076.721 I print_info: vocab_only       = 0
0.00.076.721 I print_info: n_ctx_train      = 2048
0.00.076.721 I print_info: n_embd           = 2048
0.00.076.722 I print_info: n_layer          = 24
0.00.076.725 I print_info: n_head           = 16
0.00.076.726 I print_info: n_head_kv        = 16
0.00.076.726 I print_info: n_rot            = 32
0.00.076.726 I print_info: n_swa            = 0
0.00.076.726 I print_info: n_embd_head_k    = 128
0.00.076.728 I print_info: n_embd_head_v    = 128
0.00.076.729 I print_info: n_gqa            = 1
0.00.076.730 I print_info: n_embd_k_gqa     = 2048
0.00.076.731 I print_info: n_embd_v_gqa     = 2048
0.00.076.731 I print_info: f_norm_eps       = 1.0e-05
0.00.076.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.732 I print_info: f_logit_scale    = 0.0e+00
0.00.076.733 I print_info: n_ff             = 8192
0.00.076.733 I print_info: n_expert         = 0
0.00.076.733 I print_info: n_expert_used    = 0
0.00.076.733 I print_info: causal attn      = 1
0.00.076.733 I print_info: pooling type     = 0
0.00.076.734 I print_info: rope type        = 2
0.00.076.735 I print_info: rope scaling     = linear
0.00.076.736 I print_info: freq_base_train  = 10000.0
0.00.076.736 I print_info: freq_scale_train = 1
0.00.076.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.736 I print_info: rope_finetuned   = unknown
0.00.076.736 I print_info: ssm_d_conv       = 0
0.00.076.737 I print_info: ssm_d_inner      = 0
0.00.076.737 I print_info: ssm_d_state      = 0
0.00.076.739 I print_info: ssm_dt_rank      = 0
0.00.076.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.739 I print_info: model type       = 1.4B
0.00.076.739 I print_info: model params     = 1.41 B
0.00.076.740 I print_info: general.name     = 1.4B
0.00.076.740 I print_info: vocab type       = BPE
0.00.076.741 I print_info: n_vocab          = 50304
0.00.076.741 I print_info: n_merges         = 50009
0.00.076.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.741 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.742 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.742 I print_info: LF token         = 187 ''
0.00.076.742 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.742 I print_info: max token length = 1024
0.00.076.743 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.447.091 I load_tensors: offloading 24 repeating layers to GPU
0.01.447.096 I load_tensors: offloading output layer to GPU
0.01.447.097 I load_tensors: offloaded 25/25 layers to GPU
0.01.447.120 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.447.122 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.447.941 I llama_context_unified: n_seq_max     = 1
0.01.447.942 I llama_context_unified: n_ctx         = 128
0.01.447.942 I llama_context_unified: n_ctx_per_seq = 128
0.01.447.943 I llama_context_unified: n_batch       = 128
0.01.447.943 I llama_context_unified: n_ubatch      = 128
0.01.447.943 I llama_context_unified: flash_attn    = 0
0.01.447.944 I llama_context_unified: freq_base     = 10000.0
0.01.447.944 I llama_context_unified: freq_scale    = 1
0.01.447.944 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.447.945 I ggml_metal_init: allocating
0.01.447.989 I ggml_metal_init: found device: Apple M4
0.01.448.000 I ggml_metal_init: picking default device: Apple M4
0.01.449.089 I ggml_metal_init: using embedded metal library
0.01.453.009 I ggml_metal_init: GPU name:   Apple M4
0.01.453.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.453.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.453.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.453.013 I ggml_metal_init: simdgroup reduction   = true
0.01.453.013 I ggml_metal_init: simdgroup matrix mul. = true
0.01.453.013 I ggml_metal_init: has residency sets    = true
0.01.453.013 I ggml_metal_init: has bfloat            = true
0.01.453.014 I ggml_metal_init: use bfloat            = true
0.01.453.014 I ggml_metal_init: hasUnifiedMemory      = true
0.01.453.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.463.731 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.465.488 I init:      Metal KV buffer size =    24.00 MiB
0.01.465.490 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.465.503 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.467.190 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.467.192 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.467.192 I llama_context_unified: graph nodes  = 967
0.01.467.192 I llama_context_unified: graph splits = 2
0.01.467.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.467.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.502.508 I 
0.01.502.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.502.571 I perplexity: tokenizing the input ..
0.01.508.028 I perplexity: tokenization took 5.455 ms
0.01.508.049 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.626.517 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.627.855 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.627.869 I llama_perf_context_print:        load time =    1478.82 ms
0.01.627.870 I llama_perf_context_print: prompt eval time =     118.16 ms /   128 tokens (    0.92 ms per token,  1083.28 tokens per second)
0.01.627.871 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.627.871 I llama_perf_context_print:       total time =     125.36 ms /   129 tokens
0.01.628.484 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.099s
sys	0m0.267s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.943 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.764 I llama_model_loader: - type  f32:  194 tensors
0.00.034.764 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.765 I print_info: file format = GGUF V3 (latest)
0.00.034.766 I print_info: file type   = Q8_0
0.00.034.767 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.701 I load: special tokens cache size = 25
0.00.049.681 I load: token to piece cache size = 0.2984 MB
0.00.049.684 I print_info: arch             = gptneox
0.00.049.684 I print_info: vocab_only       = 0
0.00.049.685 I print_info: n_ctx_train      = 2048
0.00.049.685 I print_info: n_embd           = 2048
0.00.049.685 I print_info: n_layer          = 24
0.00.049.689 I print_info: n_head           = 16
0.00.049.690 I print_info: n_head_kv        = 16
0.00.049.691 I print_info: n_rot            = 32
0.00.049.691 I print_info: n_swa            = 0
0.00.049.691 I print_info: n_embd_head_k    = 128
0.00.049.691 I print_info: n_embd_head_v    = 128
0.00.049.692 I print_info: n_gqa            = 1
0.00.049.692 I print_info: n_embd_k_gqa     = 2048
0.00.049.693 I print_info: n_embd_v_gqa     = 2048
0.00.049.694 I print_info: f_norm_eps       = 1.0e-05
0.00.049.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.695 I print_info: f_logit_scale    = 0.0e+00
0.00.049.696 I print_info: n_ff             = 8192
0.00.049.696 I print_info: n_expert         = 0
0.00.049.696 I print_info: n_expert_used    = 0
0.00.049.696 I print_info: causal attn      = 1
0.00.049.696 I print_info: pooling type     = 0
0.00.049.696 I print_info: rope type        = 2
0.00.049.697 I print_info: rope scaling     = linear
0.00.049.700 I print_info: freq_base_train  = 10000.0
0.00.049.700 I print_info: freq_scale_train = 1
0.00.049.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.701 I print_info: rope_finetuned   = unknown
0.00.049.701 I print_info: ssm_d_conv       = 0
0.00.049.701 I print_info: ssm_d_inner      = 0
0.00.049.701 I print_info: ssm_d_state      = 0
0.00.049.701 I print_info: ssm_dt_rank      = 0
0.00.049.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.701 I print_info: model type       = 1.4B
0.00.049.702 I print_info: model params     = 1.41 B
0.00.049.702 I print_info: general.name     = 1.4B
0.00.049.702 I print_info: vocab type       = BPE
0.00.049.703 I print_info: n_vocab          = 50304
0.00.049.703 I print_info: n_merges         = 50009
0.00.049.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.711 I print_info: LF token         = 187 ''
0.00.049.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.711 I print_info: max token length = 1024
0.00.049.711 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.135.314 I load_tensors: offloading 24 repeating layers to GPU
0.01.135.319 I load_tensors: offloading output layer to GPU
0.01.135.320 I load_tensors: offloaded 25/25 layers to GPU
0.01.135.343 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.135.344 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.136.714 I llama_context_unified: n_seq_max     = 1
0.01.136.716 I llama_context_unified: n_ctx         = 2048
0.01.136.716 I llama_context_unified: n_ctx_per_seq = 2048
0.01.136.717 I llama_context_unified: n_batch       = 2048
0.01.136.717 I llama_context_unified: n_ubatch      = 512
0.01.136.718 I llama_context_unified: flash_attn    = 0
0.01.136.718 I llama_context_unified: freq_base     = 10000.0
0.01.136.719 I llama_context_unified: freq_scale    = 1
0.01.136.720 I ggml_metal_init: allocating
0.01.136.729 I ggml_metal_init: found device: Apple M4
0.01.136.736 I ggml_metal_init: picking default device: Apple M4
0.01.138.007 I ggml_metal_init: using embedded metal library
0.01.143.329 I ggml_metal_init: GPU name:   Apple M4
0.01.143.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.143.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.143.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.143.334 I ggml_metal_init: simdgroup reduction   = true
0.01.143.334 I ggml_metal_init: simdgroup matrix mul. = true
0.01.143.334 I ggml_metal_init: has residency sets    = true
0.01.143.334 I ggml_metal_init: has bfloat            = true
0.01.143.335 I ggml_metal_init: use bfloat            = true
0.01.143.335 I ggml_metal_init: hasUnifiedMemory      = true
0.01.143.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.159.218 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.216.041 I init:      Metal KV buffer size =   384.00 MiB
0.01.216.046 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.216.066 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.220.628 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.01.220.630 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.01.220.630 I llama_context_unified: graph nodes  = 967
0.01.220.631 I llama_context_unified: graph splits = 2
0.01.220.637 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.220.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.220.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.269.888 I main: llama threadpool init, n_threads = 4
0.01.269.928 I 
0.01.269.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.269.959 I 
0.01.270.162 I sampler seed: 1234
0.01.270.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.270.183 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.270.184 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.270.184 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.361.216 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.02.361.217 I llama_perf_context_print:        load time =    1259.27 ms
0.02.361.218 I llama_perf_context_print: prompt eval time =      45.16 ms /     7 tokens (    6.45 ms per token,   154.99 tokens per second)
0.02.361.218 I llama_perf_context_print:        eval time =    1043.02 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.361.219 I llama_perf_context_print:       total time =    1092.04 ms /    70 tokens
0.02.364.960 I ggml_metal_free: deallocating

real	0m2.383s
user	0m0.108s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.030 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.039 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.710 I llama_model_loader: - type  f32:  194 tensors
0.00.025.711 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.711 I print_info: file format = GGUF V3 (latest)
0.00.025.712 I print_info: file type   = Q8_0
0.00.025.713 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.823 I load: special tokens cache size = 25
0.00.039.974 I load: token to piece cache size = 0.2984 MB
0.00.039.978 I print_info: arch             = gptneox
0.00.039.979 I print_info: vocab_only       = 0
0.00.039.979 I print_info: n_ctx_train      = 2048
0.00.039.979 I print_info: n_embd           = 2048
0.00.039.979 I print_info: n_layer          = 24
0.00.039.984 I print_info: n_head           = 16
0.00.039.984 I print_info: n_head_kv        = 16
0.00.039.985 I print_info: n_rot            = 32
0.00.039.985 I print_info: n_swa            = 0
0.00.039.985 I print_info: n_embd_head_k    = 128
0.00.039.985 I print_info: n_embd_head_v    = 128
0.00.039.986 I print_info: n_gqa            = 1
0.00.039.987 I print_info: n_embd_k_gqa     = 2048
0.00.039.988 I print_info: n_embd_v_gqa     = 2048
0.00.039.990 I print_info: f_norm_eps       = 1.0e-05
0.00.039.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.991 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.991 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.991 I print_info: f_logit_scale    = 0.0e+00
0.00.039.992 I print_info: n_ff             = 8192
0.00.039.992 I print_info: n_expert         = 0
0.00.039.992 I print_info: n_expert_used    = 0
0.00.039.992 I print_info: causal attn      = 1
0.00.039.992 I print_info: pooling type     = 0
0.00.039.993 I print_info: rope type        = 2
0.00.039.994 I print_info: rope scaling     = linear
0.00.039.994 I print_info: freq_base_train  = 10000.0
0.00.039.994 I print_info: freq_scale_train = 1
0.00.039.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.995 I print_info: rope_finetuned   = unknown
0.00.039.995 I print_info: ssm_d_conv       = 0
0.00.039.995 I print_info: ssm_d_inner      = 0
0.00.039.995 I print_info: ssm_d_state      = 0
0.00.039.995 I print_info: ssm_dt_rank      = 0
0.00.039.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.997 I print_info: model type       = 1.4B
0.00.039.997 I print_info: model params     = 1.41 B
0.00.039.997 I print_info: general.name     = 1.4B
0.00.039.998 I print_info: vocab type       = BPE
0.00.039.998 I print_info: n_vocab          = 50304
0.00.039.998 I print_info: n_merges         = 50009
0.00.039.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: LF token         = 187 ''
0.00.039.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.000 I print_info: max token length = 1024
0.00.040.000 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.762.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.762.802 I load_tensors: offloading output layer to GPU
0.00.762.803 I load_tensors: offloaded 25/25 layers to GPU
0.00.762.830 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.762.832 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.764.290 I llama_context_unified: n_seq_max     = 1
0.00.764.291 I llama_context_unified: n_ctx         = 128
0.00.764.292 I llama_context_unified: n_ctx_per_seq = 128
0.00.764.292 I llama_context_unified: n_batch       = 128
0.00.764.292 I llama_context_unified: n_ubatch      = 128
0.00.764.292 I llama_context_unified: flash_attn    = 0
0.00.764.293 I llama_context_unified: freq_base     = 10000.0
0.00.764.294 I llama_context_unified: freq_scale    = 1
0.00.764.294 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.764.296 I ggml_metal_init: allocating
0.00.764.371 I ggml_metal_init: found device: Apple M4
0.00.764.381 I ggml_metal_init: picking default device: Apple M4
0.00.765.742 I ggml_metal_init: using embedded metal library
0.00.770.964 I ggml_metal_init: GPU name:   Apple M4
0.00.770.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.770.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.770.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.770.969 I ggml_metal_init: simdgroup reduction   = true
0.00.770.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.770.970 I ggml_metal_init: has residency sets    = true
0.00.770.970 I ggml_metal_init: has bfloat            = true
0.00.770.970 I ggml_metal_init: use bfloat            = true
0.00.770.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.770.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.787.069 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.790.439 I init:      Metal KV buffer size =    24.00 MiB
0.00.790.443 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.790.473 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.793.473 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.793.475 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.793.475 I llama_context_unified: graph nodes  = 967
0.00.793.475 I llama_context_unified: graph splits = 2
0.00.793.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.793.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.438 I 
0.00.818.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.549 I perplexity: tokenizing the input ..
0.00.825.493 I perplexity: tokenization took 6.941 ms
0.00.825.519 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.950.866 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.952.193 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.952.208 I llama_perf_context_print:        load time =     808.40 ms
0.00.952.209 I llama_perf_context_print: prompt eval time =     124.38 ms /   128 tokens (    0.97 ms per token,  1029.13 tokens per second)
0.00.952.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.952.210 I llama_perf_context_print:       total time =     133.78 ms /   129 tokens
0.00.952.754 I ggml_metal_free: deallocating

real	0m0.968s
user	0m0.077s
sys	0m0.160s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.717 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.400 I llama_model_loader: - type  f32:  194 tensors
0.00.027.401 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.401 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.402 I print_info: file format = GGUF V3 (latest)
0.00.027.402 I print_info: file type   = Q4_0
0.00.027.403 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.902 I load: special tokens cache size = 25
0.00.041.693 I load: token to piece cache size = 0.2984 MB
0.00.041.696 I print_info: arch             = gptneox
0.00.041.696 I print_info: vocab_only       = 0
0.00.041.696 I print_info: n_ctx_train      = 2048
0.00.041.697 I print_info: n_embd           = 2048
0.00.041.697 I print_info: n_layer          = 24
0.00.041.700 I print_info: n_head           = 16
0.00.041.701 I print_info: n_head_kv        = 16
0.00.041.702 I print_info: n_rot            = 32
0.00.041.702 I print_info: n_swa            = 0
0.00.041.702 I print_info: n_embd_head_k    = 128
0.00.041.702 I print_info: n_embd_head_v    = 128
0.00.041.703 I print_info: n_gqa            = 1
0.00.041.704 I print_info: n_embd_k_gqa     = 2048
0.00.041.705 I print_info: n_embd_v_gqa     = 2048
0.00.041.705 I print_info: f_norm_eps       = 1.0e-05
0.00.041.706 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.706 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.707 I print_info: f_logit_scale    = 0.0e+00
0.00.041.707 I print_info: n_ff             = 8192
0.00.041.707 I print_info: n_expert         = 0
0.00.041.708 I print_info: n_expert_used    = 0
0.00.041.708 I print_info: causal attn      = 1
0.00.041.708 I print_info: pooling type     = 0
0.00.041.708 I print_info: rope type        = 2
0.00.041.708 I print_info: rope scaling     = linear
0.00.041.709 I print_info: freq_base_train  = 10000.0
0.00.041.709 I print_info: freq_scale_train = 1
0.00.041.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.710 I print_info: rope_finetuned   = unknown
0.00.041.710 I print_info: ssm_d_conv       = 0
0.00.041.710 I print_info: ssm_d_inner      = 0
0.00.041.710 I print_info: ssm_d_state      = 0
0.00.041.710 I print_info: ssm_dt_rank      = 0
0.00.041.711 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.711 I print_info: model type       = 1.4B
0.00.041.711 I print_info: model params     = 1.41 B
0.00.041.712 I print_info: general.name     = 1.4B
0.00.041.712 I print_info: vocab type       = BPE
0.00.041.712 I print_info: n_vocab          = 50304
0.00.041.713 I print_info: n_merges         = 50009
0.00.041.713 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.713 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.714 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.714 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.716 I print_info: LF token         = 187 ''
0.00.041.717 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.717 I print_info: max token length = 1024
0.00.041.717 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.748 I load_tensors: offloading output layer to GPU
0.00.595.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.784 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.595.785 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.597.367 I llama_context_unified: n_seq_max     = 1
0.00.597.370 I llama_context_unified: n_ctx         = 2048
0.00.597.370 I llama_context_unified: n_ctx_per_seq = 2048
0.00.597.371 I llama_context_unified: n_batch       = 2048
0.00.597.371 I llama_context_unified: n_ubatch      = 512
0.00.597.372 I llama_context_unified: flash_attn    = 0
0.00.597.375 I llama_context_unified: freq_base     = 10000.0
0.00.597.375 I llama_context_unified: freq_scale    = 1
0.00.597.390 I ggml_metal_init: allocating
0.00.597.476 I ggml_metal_init: found device: Apple M4
0.00.597.501 I ggml_metal_init: picking default device: Apple M4
0.00.599.417 I ggml_metal_init: using embedded metal library
0.00.606.527 I ggml_metal_init: GPU name:   Apple M4
0.00.606.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.536 I ggml_metal_init: simdgroup reduction   = true
0.00.606.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.536 I ggml_metal_init: has residency sets    = true
0.00.606.537 I ggml_metal_init: has bfloat            = true
0.00.606.537 I ggml_metal_init: use bfloat            = true
0.00.606.538 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.956 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.788 I init:      Metal KV buffer size =   384.00 MiB
0.00.677.795 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.819 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.683.118 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.683.120 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.683.120 I llama_context_unified: graph nodes  = 967
0.00.683.120 I llama_context_unified: graph splits = 2
0.00.683.130 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.677 I main: llama threadpool init, n_threads = 4
0.00.735.715 I 
0.00.735.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.734 I 
0.00.735.908 I sampler seed: 1234
0.00.735.912 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.923 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.923 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.924 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.413.902 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48763.74 tokens per second)
0.01.413.903 I llama_perf_context_print:        load time =     723.97 ms
0.01.413.904 I llama_perf_context_print: prompt eval time =      39.39 ms /     7 tokens (    5.63 ms per token,   177.70 tokens per second)
0.01.413.905 I llama_perf_context_print:        eval time =     635.70 ms /    63 runs   (   10.09 ms per token,    99.10 tokens per second)
0.01.413.905 I llama_perf_context_print:       total time =     678.98 ms /    70 tokens
0.01.417.711 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.110s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.763 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.550 I llama_model_loader: - type  f32:  194 tensors
0.00.025.550 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.550 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.551 I print_info: file format = GGUF V3 (latest)
0.00.025.552 I print_info: file type   = Q4_0
0.00.025.553 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.725 I load: special tokens cache size = 25
0.00.039.927 I load: token to piece cache size = 0.2984 MB
0.00.039.932 I print_info: arch             = gptneox
0.00.039.932 I print_info: vocab_only       = 0
0.00.039.932 I print_info: n_ctx_train      = 2048
0.00.039.932 I print_info: n_embd           = 2048
0.00.039.933 I print_info: n_layer          = 24
0.00.039.936 I print_info: n_head           = 16
0.00.039.937 I print_info: n_head_kv        = 16
0.00.039.937 I print_info: n_rot            = 32
0.00.039.937 I print_info: n_swa            = 0
0.00.039.938 I print_info: n_embd_head_k    = 128
0.00.039.938 I print_info: n_embd_head_v    = 128
0.00.039.939 I print_info: n_gqa            = 1
0.00.039.939 I print_info: n_embd_k_gqa     = 2048
0.00.039.940 I print_info: n_embd_v_gqa     = 2048
0.00.039.941 I print_info: f_norm_eps       = 1.0e-05
0.00.039.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.942 I print_info: f_logit_scale    = 0.0e+00
0.00.039.942 I print_info: n_ff             = 8192
0.00.039.942 I print_info: n_expert         = 0
0.00.039.942 I print_info: n_expert_used    = 0
0.00.039.943 I print_info: causal attn      = 1
0.00.039.943 I print_info: pooling type     = 0
0.00.039.943 I print_info: rope type        = 2
0.00.039.943 I print_info: rope scaling     = linear
0.00.039.944 I print_info: freq_base_train  = 10000.0
0.00.039.944 I print_info: freq_scale_train = 1
0.00.039.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.944 I print_info: rope_finetuned   = unknown
0.00.039.944 I print_info: ssm_d_conv       = 0
0.00.039.945 I print_info: ssm_d_inner      = 0
0.00.039.945 I print_info: ssm_d_state      = 0
0.00.039.945 I print_info: ssm_dt_rank      = 0
0.00.039.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.945 I print_info: model type       = 1.4B
0.00.039.945 I print_info: model params     = 1.41 B
0.00.039.946 I print_info: general.name     = 1.4B
0.00.039.946 I print_info: vocab type       = BPE
0.00.039.946 I print_info: n_vocab          = 50304
0.00.039.947 I print_info: n_merges         = 50009
0.00.039.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.948 I print_info: LF token         = 187 ''
0.00.039.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.948 I print_info: max token length = 1024
0.00.039.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.838 I load_tensors: offloading output layer to GPU
0.00.596.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.872 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.596.874 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.598.632 I llama_context_unified: n_seq_max     = 1
0.00.598.637 I llama_context_unified: n_ctx         = 128
0.00.598.637 I llama_context_unified: n_ctx_per_seq = 128
0.00.598.638 I llama_context_unified: n_batch       = 128
0.00.598.638 I llama_context_unified: n_ubatch      = 128
0.00.598.638 I llama_context_unified: flash_attn    = 0
0.00.598.639 I llama_context_unified: freq_base     = 10000.0
0.00.598.640 I llama_context_unified: freq_scale    = 1
0.00.598.640 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.642 I ggml_metal_init: allocating
0.00.598.746 I ggml_metal_init: found device: Apple M4
0.00.598.759 I ggml_metal_init: picking default device: Apple M4
0.00.601.259 I ggml_metal_init: using embedded metal library
0.00.608.355 I ggml_metal_init: GPU name:   Apple M4
0.00.608.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.366 I ggml_metal_init: simdgroup reduction   = true
0.00.608.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.367 I ggml_metal_init: has residency sets    = true
0.00.608.367 I ggml_metal_init: has bfloat            = true
0.00.608.367 I ggml_metal_init: use bfloat            = true
0.00.608.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.464 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.981 I init:      Metal KV buffer size =    24.00 MiB
0.00.629.987 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.032 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.633.216 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.633.218 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.633.219 I llama_context_unified: graph nodes  = 967
0.00.633.219 I llama_context_unified: graph splits = 2
0.00.633.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.769 I 
0.00.661.839 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.860 I perplexity: tokenizing the input ..
0.00.668.907 I perplexity: tokenization took 7.044 ms
0.00.668.925 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.455 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.806.978 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.806.995 I llama_perf_context_print:        load time =     652.00 ms
0.00.806.995 I llama_perf_context_print: prompt eval time =     135.55 ms /   128 tokens (    1.06 ms per token,   944.31 tokens per second)
0.00.806.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.996 I llama_perf_context_print:       total time =     145.23 ms /   129 tokens
0.00.807.585 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.130s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.576 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.484 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.493 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.133 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.135 I llama_model_loader: - type  f32:  194 tensors
0.00.025.135 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.135 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.136 I print_info: file format = GGUF V3 (latest)
0.00.025.136 I print_info: file type   = Q4_1
0.00.025.141 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.986 I load: special tokens cache size = 25
0.00.039.012 I load: token to piece cache size = 0.2984 MB
0.00.039.015 I print_info: arch             = gptneox
0.00.039.016 I print_info: vocab_only       = 0
0.00.039.016 I print_info: n_ctx_train      = 2048
0.00.039.016 I print_info: n_embd           = 2048
0.00.039.016 I print_info: n_layer          = 24
0.00.039.019 I print_info: n_head           = 16
0.00.039.020 I print_info: n_head_kv        = 16
0.00.039.020 I print_info: n_rot            = 32
0.00.039.020 I print_info: n_swa            = 0
0.00.039.021 I print_info: n_embd_head_k    = 128
0.00.039.021 I print_info: n_embd_head_v    = 128
0.00.039.022 I print_info: n_gqa            = 1
0.00.039.022 I print_info: n_embd_k_gqa     = 2048
0.00.039.023 I print_info: n_embd_v_gqa     = 2048
0.00.039.025 I print_info: f_norm_eps       = 1.0e-05
0.00.039.025 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.025 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.026 I print_info: f_logit_scale    = 0.0e+00
0.00.039.028 I print_info: n_ff             = 8192
0.00.039.028 I print_info: n_expert         = 0
0.00.039.028 I print_info: n_expert_used    = 0
0.00.039.028 I print_info: causal attn      = 1
0.00.039.028 I print_info: pooling type     = 0
0.00.039.030 I print_info: rope type        = 2
0.00.039.031 I print_info: rope scaling     = linear
0.00.039.032 I print_info: freq_base_train  = 10000.0
0.00.039.032 I print_info: freq_scale_train = 1
0.00.039.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.033 I print_info: rope_finetuned   = unknown
0.00.039.033 I print_info: ssm_d_conv       = 0
0.00.039.033 I print_info: ssm_d_inner      = 0
0.00.039.033 I print_info: ssm_d_state      = 0
0.00.039.033 I print_info: ssm_dt_rank      = 0
0.00.039.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.033 I print_info: model type       = 1.4B
0.00.039.037 I print_info: model params     = 1.41 B
0.00.039.037 I print_info: general.name     = 1.4B
0.00.039.038 I print_info: vocab type       = BPE
0.00.039.038 I print_info: n_vocab          = 50304
0.00.039.038 I print_info: n_merges         = 50009
0.00.039.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: LF token         = 187 ''
0.00.039.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.040 I print_info: max token length = 1024
0.00.039.040 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.562 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.575 I load_tensors: offloading output layer to GPU
0.00.594.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.624 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.594.629 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.596.358 I llama_context_unified: n_seq_max     = 1
0.00.596.361 I llama_context_unified: n_ctx         = 2048
0.00.596.362 I llama_context_unified: n_ctx_per_seq = 2048
0.00.596.363 I llama_context_unified: n_batch       = 2048
0.00.596.363 I llama_context_unified: n_ubatch      = 512
0.00.596.364 I llama_context_unified: flash_attn    = 0
0.00.596.367 I llama_context_unified: freq_base     = 10000.0
0.00.596.367 I llama_context_unified: freq_scale    = 1
0.00.596.370 I ggml_metal_init: allocating
0.00.596.449 I ggml_metal_init: found device: Apple M4
0.00.596.462 I ggml_metal_init: picking default device: Apple M4
0.00.598.398 I ggml_metal_init: using embedded metal library
0.00.604.890 I ggml_metal_init: GPU name:   Apple M4
0.00.604.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.895 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.895 I ggml_metal_init: simdgroup reduction   = true
0.00.604.896 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.896 I ggml_metal_init: has residency sets    = true
0.00.604.896 I ggml_metal_init: has bfloat            = true
0.00.604.896 I ggml_metal_init: use bfloat            = true
0.00.604.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.465 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.999 I init:      Metal KV buffer size =   384.00 MiB
0.00.678.006 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.029 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.682.346 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.682.348 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.682.348 I llama_context_unified: graph nodes  = 967
0.00.682.348 I llama_context_unified: graph splits = 2
0.00.682.354 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.483 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.932 I main: llama threadpool init, n_threads = 4
0.00.739.972 I 
0.00.739.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.997 I 
0.00.740.166 I sampler seed: 1234
0.00.740.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.190 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.190 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.191 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.476.488 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.476.488 I llama_perf_context_print:        load time =     730.61 ms
0.01.476.489 I llama_perf_context_print: prompt eval time =      48.12 ms /     7 tokens (    6.87 ms per token,   145.48 tokens per second)
0.01.476.490 I llama_perf_context_print:        eval time =     685.42 ms /    63 runs   (   10.88 ms per token,    91.91 tokens per second)
0.01.476.490 I llama_perf_context_print:       total time =     737.30 ms /    70 tokens
0.01.480.557 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.409 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.316 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.187 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.187 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.188 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.188 I print_info: file format = GGUF V3 (latest)
0.00.025.189 I print_info: file type   = Q4_1
0.00.025.190 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.369 I load: special tokens cache size = 25
0.00.039.685 I load: token to piece cache size = 0.2984 MB
0.00.039.690 I print_info: arch             = gptneox
0.00.039.690 I print_info: vocab_only       = 0
0.00.039.690 I print_info: n_ctx_train      = 2048
0.00.039.690 I print_info: n_embd           = 2048
0.00.039.691 I print_info: n_layer          = 24
0.00.039.695 I print_info: n_head           = 16
0.00.039.696 I print_info: n_head_kv        = 16
0.00.039.696 I print_info: n_rot            = 32
0.00.039.696 I print_info: n_swa            = 0
0.00.039.696 I print_info: n_embd_head_k    = 128
0.00.039.696 I print_info: n_embd_head_v    = 128
0.00.039.697 I print_info: n_gqa            = 1
0.00.039.698 I print_info: n_embd_k_gqa     = 2048
0.00.039.698 I print_info: n_embd_v_gqa     = 2048
0.00.039.699 I print_info: f_norm_eps       = 1.0e-05
0.00.039.699 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.700 I print_info: f_logit_scale    = 0.0e+00
0.00.039.701 I print_info: n_ff             = 8192
0.00.039.702 I print_info: n_expert         = 0
0.00.039.702 I print_info: n_expert_used    = 0
0.00.039.702 I print_info: causal attn      = 1
0.00.039.703 I print_info: pooling type     = 0
0.00.039.703 I print_info: rope type        = 2
0.00.039.704 I print_info: rope scaling     = linear
0.00.039.704 I print_info: freq_base_train  = 10000.0
0.00.039.705 I print_info: freq_scale_train = 1
0.00.039.705 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.705 I print_info: rope_finetuned   = unknown
0.00.039.705 I print_info: ssm_d_conv       = 0
0.00.039.706 I print_info: ssm_d_inner      = 0
0.00.039.706 I print_info: ssm_d_state      = 0
0.00.039.706 I print_info: ssm_dt_rank      = 0
0.00.039.706 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.706 I print_info: model type       = 1.4B
0.00.039.707 I print_info: model params     = 1.41 B
0.00.039.707 I print_info: general.name     = 1.4B
0.00.039.707 I print_info: vocab type       = BPE
0.00.039.707 I print_info: n_vocab          = 50304
0.00.039.708 I print_info: n_merges         = 50009
0.00.039.709 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.709 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.709 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.710 I print_info: LF token         = 187 ''
0.00.039.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.711 I print_info: max token length = 1024
0.00.039.711 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.180 I load_tensors: offloading output layer to GPU
0.00.595.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.213 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.595.215 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.596.752 I llama_context_unified: n_seq_max     = 1
0.00.596.757 I llama_context_unified: n_ctx         = 128
0.00.596.757 I llama_context_unified: n_ctx_per_seq = 128
0.00.596.758 I llama_context_unified: n_batch       = 128
0.00.596.758 I llama_context_unified: n_ubatch      = 128
0.00.596.758 I llama_context_unified: flash_attn    = 0
0.00.596.761 I llama_context_unified: freq_base     = 10000.0
0.00.596.761 I llama_context_unified: freq_scale    = 1
0.00.596.762 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.764 I ggml_metal_init: allocating
0.00.596.835 I ggml_metal_init: found device: Apple M4
0.00.596.848 I ggml_metal_init: picking default device: Apple M4
0.00.598.671 I ggml_metal_init: using embedded metal library
0.00.605.263 I ggml_metal_init: GPU name:   Apple M4
0.00.605.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.270 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.271 I ggml_metal_init: simdgroup reduction   = true
0.00.605.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.272 I ggml_metal_init: has residency sets    = true
0.00.605.272 I ggml_metal_init: has bfloat            = true
0.00.605.272 I ggml_metal_init: use bfloat            = true
0.00.605.273 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.603 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.168 I init:      Metal KV buffer size =    24.00 MiB
0.00.627.172 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.197 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.630.559 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.630.561 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.630.561 I llama_context_unified: graph nodes  = 967
0.00.630.562 I llama_context_unified: graph splits = 2
0.00.630.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.774 I 
0.00.656.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.874 I perplexity: tokenizing the input ..
0.00.664.479 I perplexity: tokenization took 7.601 ms
0.00.664.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.272 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.800.617 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.800.636 I llama_perf_context_print:        load time =     647.84 ms
0.00.800.637 I llama_perf_context_print: prompt eval time =     133.82 ms /   128 tokens (    1.05 ms per token,   956.49 tokens per second)
0.00.800.637 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.639 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.801.178 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.081s
sys	0m0.128s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.294 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.104 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.104 I llama_model_loader: - type  f32:  194 tensors
0.00.026.105 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.105 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.105 I print_info: file format = GGUF V3 (latest)
0.00.026.106 I print_info: file type   = Q5_0
0.00.026.107 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.979 I load: special tokens cache size = 25
0.00.040.067 I load: token to piece cache size = 0.2984 MB
0.00.040.070 I print_info: arch             = gptneox
0.00.040.070 I print_info: vocab_only       = 0
0.00.040.070 I print_info: n_ctx_train      = 2048
0.00.040.070 I print_info: n_embd           = 2048
0.00.040.070 I print_info: n_layer          = 24
0.00.040.073 I print_info: n_head           = 16
0.00.040.074 I print_info: n_head_kv        = 16
0.00.040.074 I print_info: n_rot            = 32
0.00.040.074 I print_info: n_swa            = 0
0.00.040.074 I print_info: n_embd_head_k    = 128
0.00.040.074 I print_info: n_embd_head_v    = 128
0.00.040.075 I print_info: n_gqa            = 1
0.00.040.076 I print_info: n_embd_k_gqa     = 2048
0.00.040.076 I print_info: n_embd_v_gqa     = 2048
0.00.040.077 I print_info: f_norm_eps       = 1.0e-05
0.00.040.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.078 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.078 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.078 I print_info: f_logit_scale    = 0.0e+00
0.00.040.079 I print_info: n_ff             = 8192
0.00.040.079 I print_info: n_expert         = 0
0.00.040.079 I print_info: n_expert_used    = 0
0.00.040.079 I print_info: causal attn      = 1
0.00.040.079 I print_info: pooling type     = 0
0.00.040.081 I print_info: rope type        = 2
0.00.040.083 I print_info: rope scaling     = linear
0.00.040.083 I print_info: freq_base_train  = 10000.0
0.00.040.084 I print_info: freq_scale_train = 1
0.00.040.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.084 I print_info: rope_finetuned   = unknown
0.00.040.084 I print_info: ssm_d_conv       = 0
0.00.040.084 I print_info: ssm_d_inner      = 0
0.00.040.086 I print_info: ssm_d_state      = 0
0.00.040.086 I print_info: ssm_dt_rank      = 0
0.00.040.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.086 I print_info: model type       = 1.4B
0.00.040.087 I print_info: model params     = 1.41 B
0.00.040.087 I print_info: general.name     = 1.4B
0.00.040.087 I print_info: vocab type       = BPE
0.00.040.088 I print_info: n_vocab          = 50304
0.00.040.088 I print_info: n_merges         = 50009
0.00.040.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.093 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: LF token         = 187 ''
0.00.040.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: max token length = 1024
0.00.040.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.676.199 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.214 I load_tensors: offloading output layer to GPU
0.00.676.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.249 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.676.254 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.678.020 I llama_context_unified: n_seq_max     = 1
0.00.678.024 I llama_context_unified: n_ctx         = 2048
0.00.678.025 I llama_context_unified: n_ctx_per_seq = 2048
0.00.678.025 I llama_context_unified: n_batch       = 2048
0.00.678.026 I llama_context_unified: n_ubatch      = 512
0.00.678.026 I llama_context_unified: flash_attn    = 0
0.00.678.028 I llama_context_unified: freq_base     = 10000.0
0.00.678.028 I llama_context_unified: freq_scale    = 1
0.00.678.030 I ggml_metal_init: allocating
0.00.678.108 I ggml_metal_init: found device: Apple M4
0.00.678.121 I ggml_metal_init: picking default device: Apple M4
0.00.679.768 I ggml_metal_init: using embedded metal library
0.00.686.223 I ggml_metal_init: GPU name:   Apple M4
0.00.686.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.229 I ggml_metal_init: simdgroup reduction   = true
0.00.686.229 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.230 I ggml_metal_init: has residency sets    = true
0.00.686.230 I ggml_metal_init: has bfloat            = true
0.00.686.230 I ggml_metal_init: use bfloat            = true
0.00.686.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.703.476 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.037 I init:      Metal KV buffer size =   384.00 MiB
0.00.758.045 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.072 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.762.307 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.762.310 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.762.311 I llama_context_unified: graph nodes  = 967
0.00.762.311 I llama_context_unified: graph splits = 2
0.00.762.317 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.059 I main: llama threadpool init, n_threads = 4
0.00.819.103 I 
0.00.819.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.127 I 
0.00.819.281 I sampler seed: 1234
0.00.819.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.305 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.607.985 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51824.82 tokens per second)
0.01.607.985 I llama_perf_context_print:        load time =     808.59 ms
0.01.607.986 I llama_perf_context_print: prompt eval time =      42.79 ms /     7 tokens (    6.11 ms per token,   163.59 tokens per second)
0.01.607.987 I llama_perf_context_print:        eval time =     743.01 ms /    63 runs   (   11.79 ms per token,    84.79 tokens per second)
0.01.607.987 I llama_perf_context_print:       total time =     789.62 ms /    70 tokens
0.01.611.789 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.761 I llama_model_loader: - type  f32:  194 tensors
0.00.025.762 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.763 I print_info: file format = GGUF V3 (latest)
0.00.025.763 I print_info: file type   = Q5_0
0.00.025.764 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.062 I load: special tokens cache size = 25
0.00.040.438 I load: token to piece cache size = 0.2984 MB
0.00.040.442 I print_info: arch             = gptneox
0.00.040.442 I print_info: vocab_only       = 0
0.00.040.442 I print_info: n_ctx_train      = 2048
0.00.040.442 I print_info: n_embd           = 2048
0.00.040.443 I print_info: n_layer          = 24
0.00.040.447 I print_info: n_head           = 16
0.00.040.447 I print_info: n_head_kv        = 16
0.00.040.448 I print_info: n_rot            = 32
0.00.040.448 I print_info: n_swa            = 0
0.00.040.448 I print_info: n_embd_head_k    = 128
0.00.040.448 I print_info: n_embd_head_v    = 128
0.00.040.449 I print_info: n_gqa            = 1
0.00.040.450 I print_info: n_embd_k_gqa     = 2048
0.00.040.450 I print_info: n_embd_v_gqa     = 2048
0.00.040.451 I print_info: f_norm_eps       = 1.0e-05
0.00.040.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.452 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.452 I print_info: f_logit_scale    = 0.0e+00
0.00.040.455 I print_info: n_ff             = 8192
0.00.040.455 I print_info: n_expert         = 0
0.00.040.455 I print_info: n_expert_used    = 0
0.00.040.455 I print_info: causal attn      = 1
0.00.040.455 I print_info: pooling type     = 0
0.00.040.455 I print_info: rope type        = 2
0.00.040.456 I print_info: rope scaling     = linear
0.00.040.457 I print_info: freq_base_train  = 10000.0
0.00.040.457 I print_info: freq_scale_train = 1
0.00.040.457 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.457 I print_info: rope_finetuned   = unknown
0.00.040.458 I print_info: ssm_d_conv       = 0
0.00.040.459 I print_info: ssm_d_inner      = 0
0.00.040.459 I print_info: ssm_d_state      = 0
0.00.040.459 I print_info: ssm_dt_rank      = 0
0.00.040.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.459 I print_info: model type       = 1.4B
0.00.040.460 I print_info: model params     = 1.41 B
0.00.040.461 I print_info: general.name     = 1.4B
0.00.040.461 I print_info: vocab type       = BPE
0.00.040.461 I print_info: n_vocab          = 50304
0.00.040.462 I print_info: n_merges         = 50009
0.00.040.462 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.463 I print_info: LF token         = 187 ''
0.00.040.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.463 I print_info: max token length = 1024
0.00.040.463 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.635 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.651 I load_tensors: offloading output layer to GPU
0.00.670.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.686 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.670.687 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.672.342 I llama_context_unified: n_seq_max     = 1
0.00.672.345 I llama_context_unified: n_ctx         = 128
0.00.672.345 I llama_context_unified: n_ctx_per_seq = 128
0.00.672.346 I llama_context_unified: n_batch       = 128
0.00.672.346 I llama_context_unified: n_ubatch      = 128
0.00.672.347 I llama_context_unified: flash_attn    = 0
0.00.672.349 I llama_context_unified: freq_base     = 10000.0
0.00.672.350 I llama_context_unified: freq_scale    = 1
0.00.672.350 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.672.352 I ggml_metal_init: allocating
0.00.672.427 I ggml_metal_init: found device: Apple M4
0.00.672.440 I ggml_metal_init: picking default device: Apple M4
0.00.674.278 I ggml_metal_init: using embedded metal library
0.00.680.937 I ggml_metal_init: GPU name:   Apple M4
0.00.680.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.680.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.680.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.680.944 I ggml_metal_init: simdgroup reduction   = true
0.00.680.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.680.945 I ggml_metal_init: has residency sets    = true
0.00.680.945 I ggml_metal_init: has bfloat            = true
0.00.680.945 I ggml_metal_init: use bfloat            = true
0.00.680.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.680.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.703 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.254 I init:      Metal KV buffer size =    24.00 MiB
0.00.702.258 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.702.282 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.705.564 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.705.566 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.705.567 I llama_context_unified: graph nodes  = 967
0.00.705.567 I llama_context_unified: graph splits = 2
0.00.705.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.705.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.322 I 
0.00.737.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.416 I perplexity: tokenizing the input ..
0.00.744.289 I perplexity: tokenization took 6.87 ms
0.00.744.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.327 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.888.670 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.888.683 I llama_perf_context_print:        load time =     727.43 ms
0.00.888.684 I llama_perf_context_print: prompt eval time =     142.06 ms /   128 tokens (    1.11 ms per token,   901.02 tokens per second)
0.00.888.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.685 I llama_perf_context_print:       total time =     151.37 ms /   129 tokens
0.00.889.222 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.080s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.558 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.149 I llama_model_loader: - type  f32:  194 tensors
0.00.025.149 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.150 I print_info: file format = GGUF V3 (latest)
0.00.025.150 I print_info: file type   = Q5_1
0.00.025.151 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.991 I load: special tokens cache size = 25
0.00.039.041 I load: token to piece cache size = 0.2984 MB
0.00.039.043 I print_info: arch             = gptneox
0.00.039.043 I print_info: vocab_only       = 0
0.00.039.044 I print_info: n_ctx_train      = 2048
0.00.039.044 I print_info: n_embd           = 2048
0.00.039.044 I print_info: n_layer          = 24
0.00.039.047 I print_info: n_head           = 16
0.00.039.048 I print_info: n_head_kv        = 16
0.00.039.048 I print_info: n_rot            = 32
0.00.039.048 I print_info: n_swa            = 0
0.00.039.049 I print_info: n_embd_head_k    = 128
0.00.039.050 I print_info: n_embd_head_v    = 128
0.00.039.051 I print_info: n_gqa            = 1
0.00.039.052 I print_info: n_embd_k_gqa     = 2048
0.00.039.053 I print_info: n_embd_v_gqa     = 2048
0.00.039.057 I print_info: f_norm_eps       = 1.0e-05
0.00.039.058 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.058 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.058 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.059 I print_info: f_logit_scale    = 0.0e+00
0.00.039.059 I print_info: n_ff             = 8192
0.00.039.061 I print_info: n_expert         = 0
0.00.039.061 I print_info: n_expert_used    = 0
0.00.039.061 I print_info: causal attn      = 1
0.00.039.061 I print_info: pooling type     = 0
0.00.039.061 I print_info: rope type        = 2
0.00.039.062 I print_info: rope scaling     = linear
0.00.039.062 I print_info: freq_base_train  = 10000.0
0.00.039.062 I print_info: freq_scale_train = 1
0.00.039.062 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.063 I print_info: rope_finetuned   = unknown
0.00.039.066 I print_info: ssm_d_conv       = 0
0.00.039.067 I print_info: ssm_d_inner      = 0
0.00.039.067 I print_info: ssm_d_state      = 0
0.00.039.067 I print_info: ssm_dt_rank      = 0
0.00.039.067 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.067 I print_info: model type       = 1.4B
0.00.039.068 I print_info: model params     = 1.41 B
0.00.039.068 I print_info: general.name     = 1.4B
0.00.039.068 I print_info: vocab type       = BPE
0.00.039.069 I print_info: n_vocab          = 50304
0.00.039.069 I print_info: n_merges         = 50009
0.00.039.069 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.070 I print_info: LF token         = 187 ''
0.00.039.070 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.070 I print_info: max token length = 1024
0.00.039.070 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.883 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.901 I load_tensors: offloading output layer to GPU
0.00.728.902 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.933 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.728.935 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.729.991 I llama_context_unified: n_seq_max     = 1
0.00.729.994 I llama_context_unified: n_ctx         = 2048
0.00.729.994 I llama_context_unified: n_ctx_per_seq = 2048
0.00.729.994 I llama_context_unified: n_batch       = 2048
0.00.729.995 I llama_context_unified: n_ubatch      = 512
0.00.729.995 I llama_context_unified: flash_attn    = 0
0.00.729.996 I llama_context_unified: freq_base     = 10000.0
0.00.729.997 I llama_context_unified: freq_scale    = 1
0.00.729.998 I ggml_metal_init: allocating
0.00.730.033 I ggml_metal_init: found device: Apple M4
0.00.730.043 I ggml_metal_init: picking default device: Apple M4
0.00.731.097 I ggml_metal_init: using embedded metal library
0.00.736.522 I ggml_metal_init: GPU name:   Apple M4
0.00.736.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.528 I ggml_metal_init: simdgroup reduction   = true
0.00.736.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.528 I ggml_metal_init: has residency sets    = true
0.00.736.528 I ggml_metal_init: has bfloat            = true
0.00.736.528 I ggml_metal_init: use bfloat            = true
0.00.736.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.752.437 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.272 I init:      Metal KV buffer size =   384.00 MiB
0.00.797.278 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.304 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.801.281 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.801.283 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.801.283 I llama_context_unified: graph nodes  = 967
0.00.801.283 I llama_context_unified: graph splits = 2
0.00.801.293 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.706 I main: llama threadpool init, n_threads = 4
0.00.857.750 I 
0.00.857.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.777 I 
0.00.857.932 I sampler seed: 1234
0.00.857.936 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.857.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.857.958 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.857.958 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.696.478 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.696.479 I llama_perf_context_print:        load time =     848.23 ms
0.01.696.480 I llama_perf_context_print: prompt eval time =      41.97 ms /     7 tokens (    6.00 ms per token,   166.77 tokens per second)
0.01.696.480 I llama_perf_context_print:        eval time =     793.58 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.696.480 I llama_perf_context_print:       total time =     839.47 ms /    70 tokens
0.01.700.351 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.106s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.903 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.523 I llama_model_loader: - type  f32:  194 tensors
0.00.024.523 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.524 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.524 I print_info: file format = GGUF V3 (latest)
0.00.024.525 I print_info: file type   = Q5_1
0.00.024.526 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.497 I load: special tokens cache size = 25
0.00.038.297 I load: token to piece cache size = 0.2984 MB
0.00.038.300 I print_info: arch             = gptneox
0.00.038.300 I print_info: vocab_only       = 0
0.00.038.300 I print_info: n_ctx_train      = 2048
0.00.038.300 I print_info: n_embd           = 2048
0.00.038.301 I print_info: n_layer          = 24
0.00.038.305 I print_info: n_head           = 16
0.00.038.306 I print_info: n_head_kv        = 16
0.00.038.306 I print_info: n_rot            = 32
0.00.038.306 I print_info: n_swa            = 0
0.00.038.306 I print_info: n_embd_head_k    = 128
0.00.038.307 I print_info: n_embd_head_v    = 128
0.00.038.307 I print_info: n_gqa            = 1
0.00.038.308 I print_info: n_embd_k_gqa     = 2048
0.00.038.309 I print_info: n_embd_v_gqa     = 2048
0.00.038.309 I print_info: f_norm_eps       = 1.0e-05
0.00.038.310 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.310 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.310 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.310 I print_info: f_logit_scale    = 0.0e+00
0.00.038.314 I print_info: n_ff             = 8192
0.00.038.314 I print_info: n_expert         = 0
0.00.038.314 I print_info: n_expert_used    = 0
0.00.038.314 I print_info: causal attn      = 1
0.00.038.314 I print_info: pooling type     = 0
0.00.038.315 I print_info: rope type        = 2
0.00.038.315 I print_info: rope scaling     = linear
0.00.038.315 I print_info: freq_base_train  = 10000.0
0.00.038.316 I print_info: freq_scale_train = 1
0.00.038.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.316 I print_info: rope_finetuned   = unknown
0.00.038.316 I print_info: ssm_d_conv       = 0
0.00.038.316 I print_info: ssm_d_inner      = 0
0.00.038.316 I print_info: ssm_d_state      = 0
0.00.038.318 I print_info: ssm_dt_rank      = 0
0.00.038.318 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.319 I print_info: model type       = 1.4B
0.00.038.319 I print_info: model params     = 1.41 B
0.00.038.319 I print_info: general.name     = 1.4B
0.00.038.320 I print_info: vocab type       = BPE
0.00.038.320 I print_info: n_vocab          = 50304
0.00.038.320 I print_info: n_merges         = 50009
0.00.038.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.321 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.321 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.321 I print_info: LF token         = 187 ''
0.00.038.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: max token length = 1024
0.00.038.322 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.743.932 I load_tensors: offloading 24 repeating layers to GPU
0.00.743.947 I load_tensors: offloading output layer to GPU
0.00.743.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.743.980 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.743.982 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.745.668 I llama_context_unified: n_seq_max     = 1
0.00.745.671 I llama_context_unified: n_ctx         = 128
0.00.745.672 I llama_context_unified: n_ctx_per_seq = 128
0.00.745.672 I llama_context_unified: n_batch       = 128
0.00.745.673 I llama_context_unified: n_ubatch      = 128
0.00.745.673 I llama_context_unified: flash_attn    = 0
0.00.745.675 I llama_context_unified: freq_base     = 10000.0
0.00.745.676 I llama_context_unified: freq_scale    = 1
0.00.745.676 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.745.678 I ggml_metal_init: allocating
0.00.745.698 I ggml_metal_init: found device: Apple M4
0.00.745.708 I ggml_metal_init: picking default device: Apple M4
0.00.747.087 I ggml_metal_init: using embedded metal library
0.00.753.399 I ggml_metal_init: GPU name:   Apple M4
0.00.753.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.753.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.753.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.753.406 I ggml_metal_init: simdgroup reduction   = true
0.00.753.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.753.406 I ggml_metal_init: has residency sets    = true
0.00.753.406 I ggml_metal_init: has bfloat            = true
0.00.753.407 I ggml_metal_init: use bfloat            = true
0.00.753.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.753.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.771.382 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.774.877 I init:      Metal KV buffer size =    24.00 MiB
0.00.774.885 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.774.920 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.778.014 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.778.016 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.778.017 I llama_context_unified: graph nodes  = 967
0.00.778.017 I llama_context_unified: graph splits = 2
0.00.778.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.778.023 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.314 I 
0.00.807.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.420 I perplexity: tokenizing the input ..
0.00.814.720 I perplexity: tokenization took 7.298 ms
0.00.814.750 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.950.044 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.951.376 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.951.390 I llama_perf_context_print:        load time =     798.46 ms
0.00.951.391 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.91 tokens per second)
0.00.951.392 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.951.393 I llama_perf_context_print:       total time =     144.08 ms /   129 tokens
0.00.951.994 I ggml_metal_free: deallocating

real	0m0.965s
user	0m0.079s
sys	0m0.161s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.336 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.006 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.009 I llama_model_loader: - type  f32:  194 tensors
0.00.025.009 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.009 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.010 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.010 I print_info: file format = GGUF V3 (latest)
0.00.025.011 I print_info: file type   = Q2_K - Medium
0.00.025.012 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.847 I load: special tokens cache size = 25
0.00.038.882 I load: token to piece cache size = 0.2984 MB
0.00.038.885 I print_info: arch             = gptneox
0.00.038.885 I print_info: vocab_only       = 0
0.00.038.886 I print_info: n_ctx_train      = 2048
0.00.038.886 I print_info: n_embd           = 2048
0.00.038.886 I print_info: n_layer          = 24
0.00.038.889 I print_info: n_head           = 16
0.00.038.890 I print_info: n_head_kv        = 16
0.00.038.890 I print_info: n_rot            = 32
0.00.038.890 I print_info: n_swa            = 0
0.00.038.890 I print_info: n_embd_head_k    = 128
0.00.038.890 I print_info: n_embd_head_v    = 128
0.00.038.891 I print_info: n_gqa            = 1
0.00.038.892 I print_info: n_embd_k_gqa     = 2048
0.00.038.894 I print_info: n_embd_v_gqa     = 2048
0.00.038.894 I print_info: f_norm_eps       = 1.0e-05
0.00.038.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.897 I print_info: f_logit_scale    = 0.0e+00
0.00.038.897 I print_info: n_ff             = 8192
0.00.038.898 I print_info: n_expert         = 0
0.00.038.898 I print_info: n_expert_used    = 0
0.00.038.898 I print_info: causal attn      = 1
0.00.038.898 I print_info: pooling type     = 0
0.00.038.898 I print_info: rope type        = 2
0.00.038.899 I print_info: rope scaling     = linear
0.00.038.899 I print_info: freq_base_train  = 10000.0
0.00.038.899 I print_info: freq_scale_train = 1
0.00.038.900 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.900 I print_info: rope_finetuned   = unknown
0.00.038.900 I print_info: ssm_d_conv       = 0
0.00.038.900 I print_info: ssm_d_inner      = 0
0.00.038.900 I print_info: ssm_d_state      = 0
0.00.038.900 I print_info: ssm_dt_rank      = 0
0.00.038.901 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.902 I print_info: model type       = 1.4B
0.00.038.903 I print_info: model params     = 1.41 B
0.00.038.903 I print_info: general.name     = 1.4B
0.00.038.903 I print_info: vocab type       = BPE
0.00.038.903 I print_info: n_vocab          = 50304
0.00.038.904 I print_info: n_merges         = 50009
0.00.038.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: LF token         = 187 ''
0.00.038.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.906 I print_info: max token length = 1024
0.00.038.907 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.386.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.386.748 I load_tensors: offloading output layer to GPU
0.00.386.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.386.783 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.386.785 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.388.538 I llama_context_unified: n_seq_max     = 1
0.00.388.543 I llama_context_unified: n_ctx         = 2048
0.00.388.543 I llama_context_unified: n_ctx_per_seq = 2048
0.00.388.543 I llama_context_unified: n_batch       = 2048
0.00.388.544 I llama_context_unified: n_ubatch      = 512
0.00.388.544 I llama_context_unified: flash_attn    = 0
0.00.388.546 I llama_context_unified: freq_base     = 10000.0
0.00.388.547 I llama_context_unified: freq_scale    = 1
0.00.388.549 I ggml_metal_init: allocating
0.00.388.651 I ggml_metal_init: found device: Apple M4
0.00.388.666 I ggml_metal_init: picking default device: Apple M4
0.00.390.637 I ggml_metal_init: using embedded metal library
0.00.396.014 I ggml_metal_init: GPU name:   Apple M4
0.00.396.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.396.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.396.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.396.036 I ggml_metal_init: simdgroup reduction   = true
0.00.396.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.396.037 I ggml_metal_init: has residency sets    = true
0.00.396.037 I ggml_metal_init: has bfloat            = true
0.00.396.038 I ggml_metal_init: use bfloat            = true
0.00.396.040 I ggml_metal_init: hasUnifiedMemory      = true
0.00.396.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.418.577 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.527 I init:      Metal KV buffer size =   384.00 MiB
0.00.472.541 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.472.568 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.477.382 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.477.384 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.477.384 I llama_context_unified: graph nodes  = 967
0.00.477.384 I llama_context_unified: graph splits = 2
0.00.477.389 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.477.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.477.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.059 I main: llama threadpool init, n_threads = 4
0.00.536.102 I 
0.00.536.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.130 I 
0.00.536.300 I sampler seed: 1234
0.00.536.304 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.536.315 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.536.315 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.536.315 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.214.185 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.214.185 I llama_perf_context_print:        load time =     525.52 ms
0.01.214.186 I llama_perf_context_print: prompt eval time =      42.80 ms /     7 tokens (    6.11 ms per token,   163.57 tokens per second)
0.01.214.188 I llama_perf_context_print:        eval time =     632.11 ms /    63 runs   (   10.03 ms per token,    99.67 tokens per second)
0.01.214.188 I llama_perf_context_print:       total time =     678.82 ms /    70 tokens
0.01.217.452 I ggml_metal_free: deallocating

real	0m1.234s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.086 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.087 I llama_model_loader: - type  f32:  194 tensors
0.00.025.087 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.087 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.088 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.088 I print_info: file format = GGUF V3 (latest)
0.00.025.089 I print_info: file type   = Q2_K - Medium
0.00.025.090 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.520 I load: special tokens cache size = 25
0.00.039.491 I load: token to piece cache size = 0.2984 MB
0.00.039.494 I print_info: arch             = gptneox
0.00.039.495 I print_info: vocab_only       = 0
0.00.039.495 I print_info: n_ctx_train      = 2048
0.00.039.495 I print_info: n_embd           = 2048
0.00.039.495 I print_info: n_layer          = 24
0.00.039.499 I print_info: n_head           = 16
0.00.039.500 I print_info: n_head_kv        = 16
0.00.039.500 I print_info: n_rot            = 32
0.00.039.501 I print_info: n_swa            = 0
0.00.039.501 I print_info: n_embd_head_k    = 128
0.00.039.504 I print_info: n_embd_head_v    = 128
0.00.039.504 I print_info: n_gqa            = 1
0.00.039.505 I print_info: n_embd_k_gqa     = 2048
0.00.039.506 I print_info: n_embd_v_gqa     = 2048
0.00.039.506 I print_info: f_norm_eps       = 1.0e-05
0.00.039.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.507 I print_info: f_logit_scale    = 0.0e+00
0.00.039.508 I print_info: n_ff             = 8192
0.00.039.508 I print_info: n_expert         = 0
0.00.039.508 I print_info: n_expert_used    = 0
0.00.039.508 I print_info: causal attn      = 1
0.00.039.509 I print_info: pooling type     = 0
0.00.039.509 I print_info: rope type        = 2
0.00.039.510 I print_info: rope scaling     = linear
0.00.039.511 I print_info: freq_base_train  = 10000.0
0.00.039.512 I print_info: freq_scale_train = 1
0.00.039.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.512 I print_info: rope_finetuned   = unknown
0.00.039.512 I print_info: ssm_d_conv       = 0
0.00.039.513 I print_info: ssm_d_inner      = 0
0.00.039.513 I print_info: ssm_d_state      = 0
0.00.039.513 I print_info: ssm_dt_rank      = 0
0.00.039.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.513 I print_info: model type       = 1.4B
0.00.039.514 I print_info: model params     = 1.41 B
0.00.039.514 I print_info: general.name     = 1.4B
0.00.039.515 I print_info: vocab type       = BPE
0.00.039.515 I print_info: n_vocab          = 50304
0.00.039.515 I print_info: n_merges         = 50009
0.00.039.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.516 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.516 I print_info: LF token         = 187 ''
0.00.039.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.516 I print_info: max token length = 1024
0.00.039.517 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.385.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.385.523 I load_tensors: offloading output layer to GPU
0.00.385.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.385.554 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.385.556 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.387.256 I llama_context_unified: n_seq_max     = 1
0.00.387.259 I llama_context_unified: n_ctx         = 128
0.00.387.260 I llama_context_unified: n_ctx_per_seq = 128
0.00.387.260 I llama_context_unified: n_batch       = 128
0.00.387.261 I llama_context_unified: n_ubatch      = 128
0.00.387.261 I llama_context_unified: flash_attn    = 0
0.00.387.263 I llama_context_unified: freq_base     = 10000.0
0.00.387.264 I llama_context_unified: freq_scale    = 1
0.00.387.265 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.387.267 I ggml_metal_init: allocating
0.00.387.340 I ggml_metal_init: found device: Apple M4
0.00.387.353 I ggml_metal_init: picking default device: Apple M4
0.00.389.103 I ggml_metal_init: using embedded metal library
0.00.394.481 I ggml_metal_init: GPU name:   Apple M4
0.00.394.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.394.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.394.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.394.500 I ggml_metal_init: simdgroup reduction   = true
0.00.394.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.394.501 I ggml_metal_init: has residency sets    = true
0.00.394.501 I ggml_metal_init: has bfloat            = true
0.00.394.501 I ggml_metal_init: use bfloat            = true
0.00.394.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.394.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.415.529 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.419.215 I init:      Metal KV buffer size =    24.00 MiB
0.00.419.228 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.419.276 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.422.601 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.422.603 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.422.603 I llama_context_unified: graph nodes  = 967
0.00.422.604 I llama_context_unified: graph splits = 2
0.00.422.608 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.422.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.452.936 I 
0.00.453.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.039 I perplexity: tokenizing the input ..
0.00.459.604 I perplexity: tokenization took 6.562 ms
0.00.459.624 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.299 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.600.634 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.600.650 I llama_perf_context_print:        load time =     443.06 ms
0.00.600.651 I llama_perf_context_print: prompt eval time =     138.81 ms /   128 tokens (    1.08 ms per token,   922.13 tokens per second)
0.00.600.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.657 I llama_perf_context_print:       total time =     147.72 ms /   129 tokens
0.00.601.208 I ggml_metal_free: deallocating

real	0m0.618s
user	0m0.080s
sys	0m0.095s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.508 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.516 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.989 I llama_model_loader: - type  f32:  194 tensors
0.00.024.990 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.990 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.990 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.991 I print_info: file format = GGUF V3 (latest)
0.00.024.992 I print_info: file type   = Q3_K - Medium
0.00.024.993 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.240 I load: special tokens cache size = 25
0.00.039.190 I load: token to piece cache size = 0.2984 MB
0.00.039.193 I print_info: arch             = gptneox
0.00.039.193 I print_info: vocab_only       = 0
0.00.039.193 I print_info: n_ctx_train      = 2048
0.00.039.193 I print_info: n_embd           = 2048
0.00.039.193 I print_info: n_layer          = 24
0.00.039.196 I print_info: n_head           = 16
0.00.039.197 I print_info: n_head_kv        = 16
0.00.039.197 I print_info: n_rot            = 32
0.00.039.197 I print_info: n_swa            = 0
0.00.039.197 I print_info: n_embd_head_k    = 128
0.00.039.197 I print_info: n_embd_head_v    = 128
0.00.039.198 I print_info: n_gqa            = 1
0.00.039.199 I print_info: n_embd_k_gqa     = 2048
0.00.039.202 I print_info: n_embd_v_gqa     = 2048
0.00.039.202 I print_info: f_norm_eps       = 1.0e-05
0.00.039.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.203 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.203 I print_info: f_logit_scale    = 0.0e+00
0.00.039.204 I print_info: n_ff             = 8192
0.00.039.204 I print_info: n_expert         = 0
0.00.039.204 I print_info: n_expert_used    = 0
0.00.039.206 I print_info: causal attn      = 1
0.00.039.207 I print_info: pooling type     = 0
0.00.039.207 I print_info: rope type        = 2
0.00.039.207 I print_info: rope scaling     = linear
0.00.039.208 I print_info: freq_base_train  = 10000.0
0.00.039.208 I print_info: freq_scale_train = 1
0.00.039.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.208 I print_info: rope_finetuned   = unknown
0.00.039.209 I print_info: ssm_d_conv       = 0
0.00.039.209 I print_info: ssm_d_inner      = 0
0.00.039.209 I print_info: ssm_d_state      = 0
0.00.039.209 I print_info: ssm_dt_rank      = 0
0.00.039.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.209 I print_info: model type       = 1.4B
0.00.039.210 I print_info: model params     = 1.41 B
0.00.039.210 I print_info: general.name     = 1.4B
0.00.039.210 I print_info: vocab type       = BPE
0.00.039.215 I print_info: n_vocab          = 50304
0.00.039.215 I print_info: n_merges         = 50009
0.00.039.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.217 I print_info: LF token         = 187 ''
0.00.039.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: max token length = 1024
0.00.039.218 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.460.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.460.779 I load_tensors: offloading output layer to GPU
0.00.460.780 I load_tensors: offloaded 25/25 layers to GPU
0.00.460.811 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.460.816 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.462.495 I llama_context_unified: n_seq_max     = 1
0.00.462.498 I llama_context_unified: n_ctx         = 2048
0.00.462.498 I llama_context_unified: n_ctx_per_seq = 2048
0.00.462.499 I llama_context_unified: n_batch       = 2048
0.00.462.499 I llama_context_unified: n_ubatch      = 512
0.00.462.500 I llama_context_unified: flash_attn    = 0
0.00.462.503 I llama_context_unified: freq_base     = 10000.0
0.00.462.503 I llama_context_unified: freq_scale    = 1
0.00.462.505 I ggml_metal_init: allocating
0.00.462.588 I ggml_metal_init: found device: Apple M4
0.00.462.605 I ggml_metal_init: picking default device: Apple M4
0.00.464.558 I ggml_metal_init: using embedded metal library
0.00.470.364 I ggml_metal_init: GPU name:   Apple M4
0.00.470.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.470.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.470.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.470.382 I ggml_metal_init: simdgroup reduction   = true
0.00.470.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.470.383 I ggml_metal_init: has residency sets    = true
0.00.470.383 I ggml_metal_init: has bfloat            = true
0.00.470.384 I ggml_metal_init: use bfloat            = true
0.00.470.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.470.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.122 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.791 I init:      Metal KV buffer size =   384.00 MiB
0.00.549.799 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.549.824 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.554.159 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.554.162 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.554.162 I llama_context_unified: graph nodes  = 967
0.00.554.162 I llama_context_unified: graph splits = 2
0.00.554.168 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.554.290 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.554.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.331 I main: llama threadpool init, n_threads = 4
0.00.613.374 I 
0.00.613.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.396 I 
0.00.613.576 I sampler seed: 1234
0.00.613.580 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.591 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.591 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.357.582 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.357.583 I llama_perf_context_print:        load time =     603.85 ms
0.01.357.584 I llama_perf_context_print: prompt eval time =      49.58 ms /     7 tokens (    7.08 ms per token,   141.19 tokens per second)
0.01.357.585 I llama_perf_context_print:        eval time =     691.57 ms /    63 runs   (   10.98 ms per token,    91.10 tokens per second)
0.01.357.586 I llama_perf_context_print:       total time =     744.96 ms /    70 tokens
0.01.361.412 I ggml_metal_free: deallocating

real	0m1.377s
user	0m0.110s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.876 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.562 I llama_model_loader: - type  f32:  194 tensors
0.00.024.562 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.563 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.563 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.563 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.564 I print_info: file format = GGUF V3 (latest)
0.00.024.567 I print_info: file type   = Q3_K - Medium
0.00.024.568 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.011 I load: special tokens cache size = 25
0.00.039.030 I load: token to piece cache size = 0.2984 MB
0.00.039.033 I print_info: arch             = gptneox
0.00.039.033 I print_info: vocab_only       = 0
0.00.039.033 I print_info: n_ctx_train      = 2048
0.00.039.033 I print_info: n_embd           = 2048
0.00.039.033 I print_info: n_layer          = 24
0.00.039.037 I print_info: n_head           = 16
0.00.039.038 I print_info: n_head_kv        = 16
0.00.039.038 I print_info: n_rot            = 32
0.00.039.039 I print_info: n_swa            = 0
0.00.039.039 I print_info: n_embd_head_k    = 128
0.00.039.039 I print_info: n_embd_head_v    = 128
0.00.039.040 I print_info: n_gqa            = 1
0.00.039.040 I print_info: n_embd_k_gqa     = 2048
0.00.039.042 I print_info: n_embd_v_gqa     = 2048
0.00.039.044 I print_info: f_norm_eps       = 1.0e-05
0.00.039.044 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.044 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.044 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.046 I print_info: f_logit_scale    = 0.0e+00
0.00.039.047 I print_info: n_ff             = 8192
0.00.039.047 I print_info: n_expert         = 0
0.00.039.047 I print_info: n_expert_used    = 0
0.00.039.047 I print_info: causal attn      = 1
0.00.039.047 I print_info: pooling type     = 0
0.00.039.047 I print_info: rope type        = 2
0.00.039.048 I print_info: rope scaling     = linear
0.00.039.048 I print_info: freq_base_train  = 10000.0
0.00.039.048 I print_info: freq_scale_train = 1
0.00.039.048 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.049 I print_info: rope_finetuned   = unknown
0.00.039.049 I print_info: ssm_d_conv       = 0
0.00.039.049 I print_info: ssm_d_inner      = 0
0.00.039.049 I print_info: ssm_d_state      = 0
0.00.039.049 I print_info: ssm_dt_rank      = 0
0.00.039.053 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.054 I print_info: model type       = 1.4B
0.00.039.054 I print_info: model params     = 1.41 B
0.00.039.054 I print_info: general.name     = 1.4B
0.00.039.055 I print_info: vocab type       = BPE
0.00.039.055 I print_info: n_vocab          = 50304
0.00.039.055 I print_info: n_merges         = 50009
0.00.039.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: LF token         = 187 ''
0.00.039.057 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.057 I print_info: max token length = 1024
0.00.039.058 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.943 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.955 I load_tensors: offloading output layer to GPU
0.00.442.956 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.993 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.994 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.672 I llama_context_unified: n_seq_max     = 1
0.00.444.678 I llama_context_unified: n_ctx         = 128
0.00.444.678 I llama_context_unified: n_ctx_per_seq = 128
0.00.444.679 I llama_context_unified: n_batch       = 128
0.00.444.679 I llama_context_unified: n_ubatch      = 128
0.00.444.679 I llama_context_unified: flash_attn    = 0
0.00.444.681 I llama_context_unified: freq_base     = 10000.0
0.00.444.682 I llama_context_unified: freq_scale    = 1
0.00.444.682 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.444.690 I ggml_metal_init: allocating
0.00.444.809 I ggml_metal_init: found device: Apple M4
0.00.444.822 I ggml_metal_init: picking default device: Apple M4
0.00.446.798 I ggml_metal_init: using embedded metal library
0.00.452.137 I ggml_metal_init: GPU name:   Apple M4
0.00.452.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.157 I ggml_metal_init: simdgroup reduction   = true
0.00.452.158 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.158 I ggml_metal_init: has residency sets    = true
0.00.452.158 I ggml_metal_init: has bfloat            = true
0.00.452.158 I ggml_metal_init: use bfloat            = true
0.00.452.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.986 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.475.533 I init:      Metal KV buffer size =    24.00 MiB
0.00.475.536 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.475.568 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.478.699 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.478.701 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.478.701 I llama_context_unified: graph nodes  = 967
0.00.478.702 I llama_context_unified: graph splits = 2
0.00.478.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.106 I 
0.00.505.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.209 I perplexity: tokenizing the input ..
0.00.512.085 I perplexity: tokenization took 6.871 ms
0.00.512.108 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.644.422 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.645.753 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.645.769 I llama_perf_context_print:        load time =     496.37 ms
0.00.645.770 I llama_perf_context_print: prompt eval time =     131.38 ms /   128 tokens (    1.03 ms per token,   974.24 tokens per second)
0.00.645.770 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.771 I llama_perf_context_print:       total time =     140.67 ms /   129 tokens
0.00.646.282 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.080s
sys	0m0.108s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.646 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.482 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.484 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.988 I llama_model_loader: - type  f32:  194 tensors
0.00.024.989 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.989 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.989 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.990 I print_info: file format = GGUF V3 (latest)
0.00.024.990 I print_info: file type   = Q4_K - Medium
0.00.024.995 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.146 I load: special tokens cache size = 25
0.00.039.218 I load: token to piece cache size = 0.2984 MB
0.00.039.221 I print_info: arch             = gptneox
0.00.039.221 I print_info: vocab_only       = 0
0.00.039.221 I print_info: n_ctx_train      = 2048
0.00.039.222 I print_info: n_embd           = 2048
0.00.039.222 I print_info: n_layer          = 24
0.00.039.224 I print_info: n_head           = 16
0.00.039.225 I print_info: n_head_kv        = 16
0.00.039.225 I print_info: n_rot            = 32
0.00.039.226 I print_info: n_swa            = 0
0.00.039.226 I print_info: n_embd_head_k    = 128
0.00.039.227 I print_info: n_embd_head_v    = 128
0.00.039.228 I print_info: n_gqa            = 1
0.00.039.228 I print_info: n_embd_k_gqa     = 2048
0.00.039.229 I print_info: n_embd_v_gqa     = 2048
0.00.039.229 I print_info: f_norm_eps       = 1.0e-05
0.00.039.230 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.230 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.230 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.230 I print_info: f_logit_scale    = 0.0e+00
0.00.039.231 I print_info: n_ff             = 8192
0.00.039.231 I print_info: n_expert         = 0
0.00.039.231 I print_info: n_expert_used    = 0
0.00.039.232 I print_info: causal attn      = 1
0.00.039.232 I print_info: pooling type     = 0
0.00.039.232 I print_info: rope type        = 2
0.00.039.232 I print_info: rope scaling     = linear
0.00.039.233 I print_info: freq_base_train  = 10000.0
0.00.039.233 I print_info: freq_scale_train = 1
0.00.039.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.233 I print_info: rope_finetuned   = unknown
0.00.039.235 I print_info: ssm_d_conv       = 0
0.00.039.236 I print_info: ssm_d_inner      = 0
0.00.039.236 I print_info: ssm_d_state      = 0
0.00.039.236 I print_info: ssm_dt_rank      = 0
0.00.039.236 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.236 I print_info: model type       = 1.4B
0.00.039.237 I print_info: model params     = 1.41 B
0.00.039.237 I print_info: general.name     = 1.4B
0.00.039.237 I print_info: vocab type       = BPE
0.00.039.237 I print_info: n_vocab          = 50304
0.00.039.238 I print_info: n_merges         = 50009
0.00.039.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.239 I print_info: LF token         = 187 ''
0.00.039.239 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.239 I print_info: max token length = 1024
0.00.039.240 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.549.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.523 I load_tensors: offloading output layer to GPU
0.00.549.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.553 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.549.555 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.551.028 I llama_context_unified: n_seq_max     = 1
0.00.551.033 I llama_context_unified: n_ctx         = 2048
0.00.551.033 I llama_context_unified: n_ctx_per_seq = 2048
0.00.551.034 I llama_context_unified: n_batch       = 2048
0.00.551.034 I llama_context_unified: n_ubatch      = 512
0.00.551.035 I llama_context_unified: flash_attn    = 0
0.00.551.036 I llama_context_unified: freq_base     = 10000.0
0.00.551.036 I llama_context_unified: freq_scale    = 1
0.00.551.038 I ggml_metal_init: allocating
0.00.551.090 I ggml_metal_init: found device: Apple M4
0.00.551.103 I ggml_metal_init: picking default device: Apple M4
0.00.552.830 I ggml_metal_init: using embedded metal library
0.00.559.673 I ggml_metal_init: GPU name:   Apple M4
0.00.559.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.686 I ggml_metal_init: simdgroup reduction   = true
0.00.559.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.686 I ggml_metal_init: has residency sets    = true
0.00.559.686 I ggml_metal_init: has bfloat            = true
0.00.559.687 I ggml_metal_init: use bfloat            = true
0.00.559.688 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.577.236 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.084 I init:      Metal KV buffer size =   384.00 MiB
0.00.635.089 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.635.112 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.640.229 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.640.231 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.640.231 I llama_context_unified: graph nodes  = 967
0.00.640.231 I llama_context_unified: graph splits = 2
0.00.640.237 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.640.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.749 I main: llama threadpool init, n_threads = 4
0.00.696.792 I 
0.00.696.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.811 I 
0.00.696.963 I sampler seed: 1234
0.00.696.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.978 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.982 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.982 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.449.998 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47082.23 tokens per second)
0.01.449.999 I llama_perf_context_print:        load time =     687.40 ms
0.01.449.999 I llama_perf_context_print: prompt eval time =      46.81 ms /     7 tokens (    6.69 ms per token,   149.55 tokens per second)
0.01.450.000 I llama_perf_context_print:        eval time =     703.64 ms /    63 runs   (   11.17 ms per token,    89.53 tokens per second)
0.01.450.000 I llama_perf_context_print:       total time =     753.95 ms /    70 tokens
0.01.452.989 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.111s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.909 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.910 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.910 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.911 I print_info: file format = GGUF V3 (latest)
0.00.025.912 I print_info: file type   = Q4_K - Medium
0.00.025.913 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.537 I load: special tokens cache size = 25
0.00.040.315 I load: token to piece cache size = 0.2984 MB
0.00.040.320 I print_info: arch             = gptneox
0.00.040.320 I print_info: vocab_only       = 0
0.00.040.321 I print_info: n_ctx_train      = 2048
0.00.040.321 I print_info: n_embd           = 2048
0.00.040.321 I print_info: n_layer          = 24
0.00.040.325 I print_info: n_head           = 16
0.00.040.326 I print_info: n_head_kv        = 16
0.00.040.326 I print_info: n_rot            = 32
0.00.040.326 I print_info: n_swa            = 0
0.00.040.326 I print_info: n_embd_head_k    = 128
0.00.040.326 I print_info: n_embd_head_v    = 128
0.00.040.327 I print_info: n_gqa            = 1
0.00.040.328 I print_info: n_embd_k_gqa     = 2048
0.00.040.328 I print_info: n_embd_v_gqa     = 2048
0.00.040.329 I print_info: f_norm_eps       = 1.0e-05
0.00.040.330 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.335 I print_info: f_logit_scale    = 0.0e+00
0.00.040.336 I print_info: n_ff             = 8192
0.00.040.336 I print_info: n_expert         = 0
0.00.040.336 I print_info: n_expert_used    = 0
0.00.040.336 I print_info: causal attn      = 1
0.00.040.338 I print_info: pooling type     = 0
0.00.040.338 I print_info: rope type        = 2
0.00.040.338 I print_info: rope scaling     = linear
0.00.040.339 I print_info: freq_base_train  = 10000.0
0.00.040.339 I print_info: freq_scale_train = 1
0.00.040.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.339 I print_info: rope_finetuned   = unknown
0.00.040.339 I print_info: ssm_d_conv       = 0
0.00.040.340 I print_info: ssm_d_inner      = 0
0.00.040.340 I print_info: ssm_d_state      = 0
0.00.040.340 I print_info: ssm_dt_rank      = 0
0.00.040.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.340 I print_info: model type       = 1.4B
0.00.040.341 I print_info: model params     = 1.41 B
0.00.040.341 I print_info: general.name     = 1.4B
0.00.040.341 I print_info: vocab type       = BPE
0.00.040.344 I print_info: n_vocab          = 50304
0.00.040.344 I print_info: n_merges         = 50009
0.00.040.344 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.344 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.345 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.345 I print_info: LF token         = 187 ''
0.00.040.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.346 I print_info: max token length = 1024
0.00.040.346 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.545.336 I load_tensors: offloading 24 repeating layers to GPU
0.00.545.352 I load_tensors: offloading output layer to GPU
0.00.545.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.545.389 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.545.390 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.546.780 I llama_context_unified: n_seq_max     = 1
0.00.546.783 I llama_context_unified: n_ctx         = 128
0.00.546.783 I llama_context_unified: n_ctx_per_seq = 128
0.00.546.784 I llama_context_unified: n_batch       = 128
0.00.546.784 I llama_context_unified: n_ubatch      = 128
0.00.546.785 I llama_context_unified: flash_attn    = 0
0.00.546.787 I llama_context_unified: freq_base     = 10000.0
0.00.546.787 I llama_context_unified: freq_scale    = 1
0.00.546.788 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.546.790 I ggml_metal_init: allocating
0.00.546.887 I ggml_metal_init: found device: Apple M4
0.00.546.902 I ggml_metal_init: picking default device: Apple M4
0.00.548.810 I ggml_metal_init: using embedded metal library
0.00.555.123 I ggml_metal_init: GPU name:   Apple M4
0.00.555.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.130 I ggml_metal_init: simdgroup reduction   = true
0.00.555.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.131 I ggml_metal_init: has residency sets    = true
0.00.555.131 I ggml_metal_init: has bfloat            = true
0.00.555.131 I ggml_metal_init: use bfloat            = true
0.00.555.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.981 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.577.573 I init:      Metal KV buffer size =    24.00 MiB
0.00.577.577 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.577.607 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.580.656 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.580.658 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.580.659 I llama_context_unified: graph nodes  = 967
0.00.580.659 I llama_context_unified: graph splits = 2
0.00.580.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.580.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.658 I 
0.00.610.728 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.748 I perplexity: tokenizing the input ..
0.00.618.277 I perplexity: tokenization took 7.526 ms
0.00.618.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.060 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.765.512 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.765.532 I llama_perf_context_print:        load time =     600.80 ms
0.00.765.533 I llama_perf_context_print: prompt eval time =     144.85 ms /   128 tokens (    1.13 ms per token,   883.67 tokens per second)
0.00.765.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.534 I llama_perf_context_print:       total time =     154.87 ms /   129 tokens
0.00.766.094 I ggml_metal_free: deallocating

real	0m0.779s
user	0m0.080s
sys	0m0.145s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.860 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.862 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.863 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.726 I llama_model_loader: - type  f32:  194 tensors
0.00.026.726 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.726 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.727 I print_info: file format = GGUF V3 (latest)
0.00.026.727 I print_info: file type   = Q5_K - Medium
0.00.026.729 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.979 I load: special tokens cache size = 25
0.00.040.997 I load: token to piece cache size = 0.2984 MB
0.00.041.004 I print_info: arch             = gptneox
0.00.041.004 I print_info: vocab_only       = 0
0.00.041.005 I print_info: n_ctx_train      = 2048
0.00.041.005 I print_info: n_embd           = 2048
0.00.041.005 I print_info: n_layer          = 24
0.00.041.009 I print_info: n_head           = 16
0.00.041.010 I print_info: n_head_kv        = 16
0.00.041.010 I print_info: n_rot            = 32
0.00.041.010 I print_info: n_swa            = 0
0.00.041.010 I print_info: n_embd_head_k    = 128
0.00.041.010 I print_info: n_embd_head_v    = 128
0.00.041.011 I print_info: n_gqa            = 1
0.00.041.012 I print_info: n_embd_k_gqa     = 2048
0.00.041.012 I print_info: n_embd_v_gqa     = 2048
0.00.041.013 I print_info: f_norm_eps       = 1.0e-05
0.00.041.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.013 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.013 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.013 I print_info: f_logit_scale    = 0.0e+00
0.00.041.014 I print_info: n_ff             = 8192
0.00.041.014 I print_info: n_expert         = 0
0.00.041.014 I print_info: n_expert_used    = 0
0.00.041.017 I print_info: causal attn      = 1
0.00.041.017 I print_info: pooling type     = 0
0.00.041.017 I print_info: rope type        = 2
0.00.041.017 I print_info: rope scaling     = linear
0.00.041.018 I print_info: freq_base_train  = 10000.0
0.00.041.019 I print_info: freq_scale_train = 1
0.00.041.019 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.019 I print_info: rope_finetuned   = unknown
0.00.041.020 I print_info: ssm_d_conv       = 0
0.00.041.020 I print_info: ssm_d_inner      = 0
0.00.041.021 I print_info: ssm_d_state      = 0
0.00.041.021 I print_info: ssm_dt_rank      = 0
0.00.041.021 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.021 I print_info: model type       = 1.4B
0.00.041.021 I print_info: model params     = 1.41 B
0.00.041.022 I print_info: general.name     = 1.4B
0.00.041.023 I print_info: vocab type       = BPE
0.00.041.023 I print_info: n_vocab          = 50304
0.00.041.023 I print_info: n_merges         = 50009
0.00.041.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.024 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.024 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.024 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.024 I print_info: LF token         = 187 ''
0.00.041.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.025 I print_info: max token length = 1024
0.00.041.025 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.970 I load_tensors: offloading output layer to GPU
0.00.580.970 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.007 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.581.008 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.582.230 I llama_context_unified: n_seq_max     = 1
0.00.582.239 I llama_context_unified: n_ctx         = 2048
0.00.582.240 I llama_context_unified: n_ctx_per_seq = 2048
0.00.582.240 I llama_context_unified: n_batch       = 2048
0.00.582.241 I llama_context_unified: n_ubatch      = 512
0.00.582.241 I llama_context_unified: flash_attn    = 0
0.00.582.243 I llama_context_unified: freq_base     = 10000.0
0.00.582.243 I llama_context_unified: freq_scale    = 1
0.00.582.246 I ggml_metal_init: allocating
0.00.582.340 I ggml_metal_init: found device: Apple M4
0.00.582.360 I ggml_metal_init: picking default device: Apple M4
0.00.584.117 I ggml_metal_init: using embedded metal library
0.00.590.584 I ggml_metal_init: GPU name:   Apple M4
0.00.590.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.591 I ggml_metal_init: simdgroup reduction   = true
0.00.590.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.591 I ggml_metal_init: has residency sets    = true
0.00.590.592 I ggml_metal_init: has bfloat            = true
0.00.590.592 I ggml_metal_init: use bfloat            = true
0.00.590.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.607.430 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.027 I init:      Metal KV buffer size =   384.00 MiB
0.00.665.033 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.665.053 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.669.331 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.669.333 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.669.333 I llama_context_unified: graph nodes  = 967
0.00.669.333 I llama_context_unified: graph splits = 2
0.00.669.340 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.669.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.669.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.671 I main: llama threadpool init, n_threads = 4
0.00.730.715 I 
0.00.730.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.735 I 
0.00.730.920 I sampler seed: 1234
0.00.730.924 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.942 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.942 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.576.819 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.576.819 I llama_perf_context_print:        load time =     719.95 ms
0.01.576.820 I llama_perf_context_print: prompt eval time =      51.28 ms /     7 tokens (    7.33 ms per token,   136.51 tokens per second)
0.01.576.821 I llama_perf_context_print:        eval time =     791.86 ms /    63 runs   (   12.57 ms per token,    79.56 tokens per second)
0.01.576.821 I llama_perf_context_print:       total time =     846.87 ms /    70 tokens
0.01.580.735 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.110s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.021.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.306 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.030.329 I llama_model_loader: - type  f32:  194 tensors
0.00.030.329 I llama_model_loader: - type q5_K:   61 tensors
0.00.030.329 I llama_model_loader: - type q6_K:   37 tensors
0.00.030.330 I print_info: file format = GGUF V3 (latest)
0.00.030.330 I print_info: file type   = Q5_K - Medium
0.00.030.331 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.038.390 I load: special tokens cache size = 25
0.00.044.686 I load: token to piece cache size = 0.2984 MB
0.00.044.689 I print_info: arch             = gptneox
0.00.044.690 I print_info: vocab_only       = 0
0.00.044.690 I print_info: n_ctx_train      = 2048
0.00.044.690 I print_info: n_embd           = 2048
0.00.044.690 I print_info: n_layer          = 24
0.00.044.693 I print_info: n_head           = 16
0.00.044.694 I print_info: n_head_kv        = 16
0.00.044.695 I print_info: n_rot            = 32
0.00.044.695 I print_info: n_swa            = 0
0.00.044.695 I print_info: n_embd_head_k    = 128
0.00.044.695 I print_info: n_embd_head_v    = 128
0.00.044.696 I print_info: n_gqa            = 1
0.00.044.697 I print_info: n_embd_k_gqa     = 2048
0.00.044.697 I print_info: n_embd_v_gqa     = 2048
0.00.044.698 I print_info: f_norm_eps       = 1.0e-05
0.00.044.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.699 I print_info: f_logit_scale    = 0.0e+00
0.00.044.699 I print_info: n_ff             = 8192
0.00.044.700 I print_info: n_expert         = 0
0.00.044.700 I print_info: n_expert_used    = 0
0.00.044.700 I print_info: causal attn      = 1
0.00.044.700 I print_info: pooling type     = 0
0.00.044.700 I print_info: rope type        = 2
0.00.044.702 I print_info: rope scaling     = linear
0.00.044.704 I print_info: freq_base_train  = 10000.0
0.00.044.704 I print_info: freq_scale_train = 1
0.00.044.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.704 I print_info: rope_finetuned   = unknown
0.00.044.704 I print_info: ssm_d_conv       = 0
0.00.044.705 I print_info: ssm_d_inner      = 0
0.00.044.705 I print_info: ssm_d_state      = 0
0.00.044.705 I print_info: ssm_dt_rank      = 0
0.00.044.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.705 I print_info: model type       = 1.4B
0.00.044.705 I print_info: model params     = 1.41 B
0.00.044.705 I print_info: general.name     = 1.4B
0.00.044.706 I print_info: vocab type       = BPE
0.00.044.707 I print_info: n_vocab          = 50304
0.00.044.708 I print_info: n_merges         = 50009
0.00.044.708 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.708 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.709 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.709 I print_info: LF token         = 187 ''
0.00.044.709 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.709 I print_info: max token length = 1024
0.00.044.713 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.249 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.257 I load_tensors: offloading output layer to GPU
0.00.585.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.275 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.585.276 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.586.001 I llama_context_unified: n_seq_max     = 1
0.00.586.004 I llama_context_unified: n_ctx         = 128
0.00.586.005 I llama_context_unified: n_ctx_per_seq = 128
0.00.586.005 I llama_context_unified: n_batch       = 128
0.00.586.005 I llama_context_unified: n_ubatch      = 128
0.00.586.006 I llama_context_unified: flash_attn    = 0
0.00.586.007 I llama_context_unified: freq_base     = 10000.0
0.00.586.007 I llama_context_unified: freq_scale    = 1
0.00.586.008 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.009 I ggml_metal_init: allocating
0.00.586.052 I ggml_metal_init: found device: Apple M4
0.00.586.062 I ggml_metal_init: picking default device: Apple M4
0.00.587.079 I ggml_metal_init: using embedded metal library
0.00.591.487 I ggml_metal_init: GPU name:   Apple M4
0.00.591.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.591.495 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.591.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.591.496 I ggml_metal_init: simdgroup reduction   = true
0.00.591.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.591.497 I ggml_metal_init: has residency sets    = true
0.00.591.497 I ggml_metal_init: has bfloat            = true
0.00.591.497 I ggml_metal_init: use bfloat            = true
0.00.591.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.591.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.348 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.177 I init:      Metal KV buffer size =    24.00 MiB
0.00.607.182 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.607.207 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.608.917 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.608.918 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.608.919 I llama_context_unified: graph nodes  = 967
0.00.608.919 I llama_context_unified: graph splits = 2
0.00.608.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.608.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.522 I 
0.00.635.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.565 I perplexity: tokenizing the input ..
0.00.639.448 I perplexity: tokenization took 3.881 ms
0.00.639.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.142 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.607 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.619 I llama_perf_context_print:        load time =     621.75 ms
0.00.780.620 I llama_perf_context_print: prompt eval time =     139.44 ms /   128 tokens (    1.09 ms per token,   917.94 tokens per second)
0.00.780.620 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.621 I llama_perf_context_print:       total time =     145.10 ms /   129 tokens
0.00.781.170 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.069s
sys	0m0.098s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.714 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.662 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.270 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.273 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.273 I llama_model_loader: - type  f32:  194 tensors
0.00.025.274 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.274 I print_info: file format = GGUF V3 (latest)
0.00.025.275 I print_info: file type   = Q6_K
0.00.025.275 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.454 I load: special tokens cache size = 25
0.00.039.404 I load: token to piece cache size = 0.2984 MB
0.00.039.407 I print_info: arch             = gptneox
0.00.039.407 I print_info: vocab_only       = 0
0.00.039.407 I print_info: n_ctx_train      = 2048
0.00.039.407 I print_info: n_embd           = 2048
0.00.039.408 I print_info: n_layer          = 24
0.00.039.411 I print_info: n_head           = 16
0.00.039.411 I print_info: n_head_kv        = 16
0.00.039.412 I print_info: n_rot            = 32
0.00.039.412 I print_info: n_swa            = 0
0.00.039.412 I print_info: n_embd_head_k    = 128
0.00.039.412 I print_info: n_embd_head_v    = 128
0.00.039.413 I print_info: n_gqa            = 1
0.00.039.414 I print_info: n_embd_k_gqa     = 2048
0.00.039.415 I print_info: n_embd_v_gqa     = 2048
0.00.039.416 I print_info: f_norm_eps       = 1.0e-05
0.00.039.416 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.416 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.417 I print_info: f_logit_scale    = 0.0e+00
0.00.039.417 I print_info: n_ff             = 8192
0.00.039.419 I print_info: n_expert         = 0
0.00.039.419 I print_info: n_expert_used    = 0
0.00.039.420 I print_info: causal attn      = 1
0.00.039.420 I print_info: pooling type     = 0
0.00.039.420 I print_info: rope type        = 2
0.00.039.421 I print_info: rope scaling     = linear
0.00.039.422 I print_info: freq_base_train  = 10000.0
0.00.039.422 I print_info: freq_scale_train = 1
0.00.039.422 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.422 I print_info: rope_finetuned   = unknown
0.00.039.423 I print_info: ssm_d_conv       = 0
0.00.039.423 I print_info: ssm_d_inner      = 0
0.00.039.423 I print_info: ssm_d_state      = 0
0.00.039.423 I print_info: ssm_dt_rank      = 0
0.00.039.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.423 I print_info: model type       = 1.4B
0.00.039.424 I print_info: model params     = 1.41 B
0.00.039.424 I print_info: general.name     = 1.4B
0.00.039.424 I print_info: vocab type       = BPE
0.00.039.425 I print_info: n_vocab          = 50304
0.00.039.425 I print_info: n_merges         = 50009
0.00.039.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: LF token         = 187 ''
0.00.039.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: max token length = 1024
0.00.039.432 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.815 I load_tensors: offloading output layer to GPU
0.00.647.816 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.840 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.647.841 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.648.906 I llama_context_unified: n_seq_max     = 1
0.00.648.908 I llama_context_unified: n_ctx         = 2048
0.00.648.908 I llama_context_unified: n_ctx_per_seq = 2048
0.00.648.908 I llama_context_unified: n_batch       = 2048
0.00.648.909 I llama_context_unified: n_ubatch      = 512
0.00.648.909 I llama_context_unified: flash_attn    = 0
0.00.648.910 I llama_context_unified: freq_base     = 10000.0
0.00.648.911 I llama_context_unified: freq_scale    = 1
0.00.648.912 I ggml_metal_init: allocating
0.00.648.929 I ggml_metal_init: found device: Apple M4
0.00.648.937 I ggml_metal_init: picking default device: Apple M4
0.00.650.250 I ggml_metal_init: using embedded metal library
0.00.656.187 I ggml_metal_init: GPU name:   Apple M4
0.00.656.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.193 I ggml_metal_init: simdgroup reduction   = true
0.00.656.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.193 I ggml_metal_init: has residency sets    = true
0.00.656.194 I ggml_metal_init: has bfloat            = true
0.00.656.194 I ggml_metal_init: use bfloat            = true
0.00.656.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.368 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.573 I init:      Metal KV buffer size =   384.00 MiB
0.00.729.579 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.645 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.734.209 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.734.211 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.734.211 I llama_context_unified: graph nodes  = 967
0.00.734.211 I llama_context_unified: graph splits = 2
0.00.734.218 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.570 I main: llama threadpool init, n_threads = 4
0.00.801.611 I 
0.00.801.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.633 I 
0.00.801.801 I sampler seed: 1234
0.00.801.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.816 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.819 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.819 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.676.801 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.676.801 I llama_perf_context_print:        load time =     792.16 ms
0.01.676.802 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.36 tokens per second)
0.01.676.803 I llama_perf_context_print:        eval time =     817.95 ms /    63 runs   (   12.98 ms per token,    77.02 tokens per second)
0.01.676.804 I llama_perf_context_print:       total time =     875.93 ms /    70 tokens
0.01.680.826 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.106s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4719 (8da7f612) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.073 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.087 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.087 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.088 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.121 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.122 I print_info: file format = GGUF V3 (latest)
0.00.025.123 I print_info: file type   = Q6_K
0.00.025.124 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.115 I load: special tokens cache size = 25
0.00.038.845 I load: token to piece cache size = 0.2984 MB
0.00.038.848 I print_info: arch             = gptneox
0.00.038.848 I print_info: vocab_only       = 0
0.00.038.848 I print_info: n_ctx_train      = 2048
0.00.038.849 I print_info: n_embd           = 2048
0.00.038.849 I print_info: n_layer          = 24
0.00.038.853 I print_info: n_head           = 16
0.00.038.854 I print_info: n_head_kv        = 16
0.00.038.854 I print_info: n_rot            = 32
0.00.038.854 I print_info: n_swa            = 0
0.00.038.854 I print_info: n_embd_head_k    = 128
0.00.038.854 I print_info: n_embd_head_v    = 128
0.00.038.855 I print_info: n_gqa            = 1
0.00.038.856 I print_info: n_embd_k_gqa     = 2048
0.00.038.857 I print_info: n_embd_v_gqa     = 2048
0.00.038.857 I print_info: f_norm_eps       = 1.0e-05
0.00.038.858 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.858 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.858 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.858 I print_info: f_logit_scale    = 0.0e+00
0.00.038.859 I print_info: n_ff             = 8192
0.00.038.861 I print_info: n_expert         = 0
0.00.038.863 I print_info: n_expert_used    = 0
0.00.038.863 I print_info: causal attn      = 1
0.00.038.863 I print_info: pooling type     = 0
0.00.038.863 I print_info: rope type        = 2
0.00.038.864 I print_info: rope scaling     = linear
0.00.038.864 I print_info: freq_base_train  = 10000.0
0.00.038.864 I print_info: freq_scale_train = 1
0.00.038.864 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.865 I print_info: rope_finetuned   = unknown
0.00.038.865 I print_info: ssm_d_conv       = 0
0.00.038.865 I print_info: ssm_d_inner      = 0
0.00.038.865 I print_info: ssm_d_state      = 0
0.00.038.865 I print_info: ssm_dt_rank      = 0
0.00.038.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.866 I print_info: model type       = 1.4B
0.00.038.866 I print_info: model params     = 1.41 B
0.00.038.868 I print_info: general.name     = 1.4B
0.00.038.868 I print_info: vocab type       = BPE
0.00.038.868 I print_info: n_vocab          = 50304
0.00.038.869 I print_info: n_merges         = 50009
0.00.038.869 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.869 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: LF token         = 187 ''
0.00.038.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: max token length = 1024
0.00.038.875 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.587 I load_tensors: offloading output layer to GPU
0.00.580.587 I load_tensors: offloaded 25/25 layers to GPU
0.00.580.604 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.580.605 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.581.335 I llama_context_unified: n_seq_max     = 1
0.00.581.338 I llama_context_unified: n_ctx         = 128
0.00.581.339 I llama_context_unified: n_ctx_per_seq = 128
0.00.581.339 I llama_context_unified: n_batch       = 128
0.00.581.339 I llama_context_unified: n_ubatch      = 128
0.00.581.340 I llama_context_unified: flash_attn    = 0
0.00.581.341 I llama_context_unified: freq_base     = 10000.0
0.00.581.342 I llama_context_unified: freq_scale    = 1
0.00.581.342 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.581.343 I ggml_metal_init: allocating
0.00.581.379 I ggml_metal_init: found device: Apple M4
0.00.581.390 I ggml_metal_init: picking default device: Apple M4
0.00.582.426 I ggml_metal_init: using embedded metal library
0.00.586.692 I ggml_metal_init: GPU name:   Apple M4
0.00.586.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.701 I ggml_metal_init: simdgroup reduction   = true
0.00.586.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.701 I ggml_metal_init: has residency sets    = true
0.00.586.702 I ggml_metal_init: has bfloat            = true
0.00.586.702 I ggml_metal_init: use bfloat            = true
0.00.586.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.706 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.598.948 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.565 I init:      Metal KV buffer size =    24.00 MiB
0.00.600.569 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.600.591 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.602.140 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.602.141 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.602.141 I llama_context_unified: graph nodes  = 967
0.00.602.141 I llama_context_unified: graph splits = 2
0.00.602.143 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.602.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.721 I 
0.00.635.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.765 I perplexity: tokenizing the input ..
0.00.639.584 I perplexity: tokenization took 3.818 ms
0.00.639.596 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.968 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.780.374 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.780.389 I llama_perf_context_print:        load time =     626.80 ms
0.00.780.390 I llama_perf_context_print: prompt eval time =     139.12 ms /   128 tokens (    1.09 ms per token,   920.04 tokens per second)
0.00.780.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.391 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.780.928 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.067s
sys	0m0.120s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4719 (8da7f612)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103c07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103c085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103c08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x103c09150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x103c09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x103c09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x103c0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x103c0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103c0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x103c0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x103c0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x103c0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x103c0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x103c0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x103c0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x103c0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x103c0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x103c0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x103c0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x103c0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x103c10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x103c10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x103c11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103c119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103c12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103c123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103c129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103c13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103c13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103c142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103c145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x103c14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x103c15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103c15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103c15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103c15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103c16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103c168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x103c16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x103c17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103c176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103c17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103c17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x103c182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103c188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103c18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x103c197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103c19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103c1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x103c1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x103c1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x103c1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x103c1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x103c1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x103c1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x103c1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x103c1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x103c1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x103c1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x103c1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x103c1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x103c1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x103c1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x103c1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x103c1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x103c1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x103c20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x103c205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103c20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x103c20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x103c213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x103c21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x103c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x103c22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103c22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103c22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103c23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103c23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103c23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103c242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103c24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103c24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103c252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103c25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103c25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103c262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103c26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103c26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103c272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103c27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x103c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103c282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103c28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103c28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103c292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103c297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103c194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103c29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103c2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103c2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103c2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103c2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103c2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x103c2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x103c2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x103c2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x103c2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x103c2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x103c2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x103c2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x103c2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x103c2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103c2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x103c2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x103c2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x103c2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x103c30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x103c304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103c30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x103c30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103c312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x103c31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x103c31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x103c320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103c32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103c329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x103c32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x103c33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103c337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x103c33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x103c34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x103c345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x103c34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103c34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x103c35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103c35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103c35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103c36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103c36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103c36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103c36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103c373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103c37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x103c37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x103c381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x103c38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x103c38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x103c38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x103c39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x103c398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x103c39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x103c3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x103c3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x103c3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x103c3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x103c3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x103c3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x103c3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x103c3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x103c3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x103c3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x103c3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x103c3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x103c3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103c3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x103c3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x103c3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x103c3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x103c3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x103c3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x103c3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x103c3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x103c40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x103c407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x103c40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x103c41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x103c415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103c41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x103c41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x103c423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103c42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103c42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103c43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x103c43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103c43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x103c43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103c44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103c448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x103c44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103c451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103c45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x103c45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103c46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103c465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103c46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103c47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103c47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x103c47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x103c47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103c48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x103c48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103c491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103c494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103c49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103c4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103c4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103c4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x103c4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103c4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x103c4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x103c4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x103c4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103c4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x103c4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x103c4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x103c4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x103c4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x103c4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x103c4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x103c4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x103c4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x103c4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x103c50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x103c508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x103c50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x103c51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x103c51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x103c51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x103c52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x103c52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103c52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x103c53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103c53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103c53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103c54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103c54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103c54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103c55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103c55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103c55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103c562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103c56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103c56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103c572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103c57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103c57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x103c582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103c58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103c58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103c592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103c59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103c59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103c5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x103c5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x103c5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x103c5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x103c5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x103c5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103c5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x103c5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x103c5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x103c5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x103c5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x103c5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x103c5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x103c5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x103c5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x103c5f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x103c5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x103c5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x103c5fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x103c60380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x103c60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x103c60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x103c61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x103c61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x103c61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x103c61f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x103c623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x103c62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x103c62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x103c63270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x103c63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x103c640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103c647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x103c64ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x103c651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x103c659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x103c65c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x103c66270 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
0.00.722.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ff04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ff04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ff05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ff05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ff05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ff06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ff065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ff06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ff06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ff07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ff07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ff07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ff08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ff09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ff09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ff0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ff0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ff0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ff0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ff0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ff0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ff0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ff0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ff0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ff0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ff0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ff0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ff0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ff0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ff0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ff0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ff0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ff10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ff10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ff108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ff10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ff11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ff11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ff11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ff11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ff12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ff127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ff12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ff130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ff13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ff13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ff13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ff14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ff146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ff14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ff14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ff15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ff15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ff15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ff16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ff165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ff16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ff17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ff174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ff17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ff17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ff18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ff18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ff18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ff18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ff193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ff19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ff19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ff1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ff1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ff1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ff1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ff1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ff1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ff1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ff1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ff1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ff1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ff1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ff1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ff1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ff1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ff1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ff1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ff1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ff1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ff1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ff1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ff1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ff1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ff202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ff20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ff20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ff21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ff21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ff218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ff21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ff221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ff22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ff22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ff22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ff23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ff23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ff23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ff240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ff24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ff249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ff24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ff252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ff25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ff25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ff25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ff26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ff268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ff26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ff271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ff27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ff27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ff27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ff28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ff287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ff28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ff290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ff29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ff299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ff29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ff2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ff2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ff2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ff2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ff2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ff2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ff2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ff2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ff2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ff2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ff2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ff2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ff2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ff2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ff2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ff2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ff2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ff2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ff2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ff2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ff2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ff2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ff30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ff30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ff30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ff31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ff315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ff31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ff31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ff32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ff327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ff32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ff33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ff334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ff33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ff33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ff34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ff346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ff34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ff34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ff35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ff35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ff36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ff365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ff36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ff36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ff37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ff37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ff37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ff38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ff384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ff38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ff38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ff39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ff39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ff39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ff39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ff3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ff3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ff3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ff3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ff3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ff3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ff3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ff3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ff3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ff3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ff3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ff3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ff3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ff3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ff3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ff3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ff3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ff3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ff3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ff3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ff3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ff40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ff40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ff40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ff40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ff41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ff41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ff42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ff42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ff42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ff433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ff43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ff43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ff44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ff44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ff45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ff45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ff45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ff461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ff46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ff46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ff47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ff478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ff47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ff48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ff48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ff48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ff49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ff49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ff4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ff4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ff4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ff4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ff4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ff4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ff4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ff4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ff4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ff4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ff4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ff4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ff4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ff4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ff4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ff4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ff4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ff502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ff50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ff50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ff51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ff519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ff51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ff52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ff52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ff530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ff53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ff53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ff54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ff547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ff54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ff55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ff55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ff55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ff56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ff56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ff56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ff57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ff57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ff57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ff58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ff58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ff58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ff59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ff59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ff59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ff5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ff5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ff5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ff5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ff5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ff5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ff5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ff5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ff5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ff5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ff5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ff5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ff5e830 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103c65f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103c49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103c475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x103c48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x103c1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x103c1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x103c1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x103c49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103c12690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x103c19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x103c19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x103c1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x103c18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x103c1a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x103c11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x103c1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x103c29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x103c65470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x103c14870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x103c14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x103c4a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x103c48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x103c12ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103c12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103c13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103c666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103c66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103c66c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103c66f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103c671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103c67490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103c67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x103c67a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x103c67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103c67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103c68250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103c68510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103c687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103c68a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x103c68d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x103c69010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103c692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103c69590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103c69850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x103c69b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103c69dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103c6a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x103c6a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103c6a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103c6a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x103c6ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x103c6ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x103c6b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x103c6b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x103c6b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x103c6b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x103c6bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x103c6bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x103c6c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x103c6c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x103c6c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x103c6c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x103c6cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x103c6cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x103c6d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x103c6d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x103c6d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x103c6da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x103c6dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103c6dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x103c6e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x103c6e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x103c6e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x103c6ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x103c6ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103c6f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103c6f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103c6f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103c6f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103c6fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103c6fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103c700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103c70390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103c70650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103c70910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103c70bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103c70e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103c71150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103c71410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103c716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103c71990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x103c71c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103c71f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103c721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103c72490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103c72750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103c72a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103c72cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103c72f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103c73250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103c73510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103c737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103c73a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103c73d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x103c74010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x103c742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x103c74590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x103c74850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x103c74b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x103c74dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x103c75090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x103c75350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x103c75610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103c758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x103c75b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x103c75e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x103c76110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x103c763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x103c76690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103c76950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x103c76c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103c76ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x103c77190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x103c77450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x103c77710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103c779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103c77c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x103c77f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x103c78210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103c784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x103c78790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x103c78a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x103c78d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x103c78fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103c79290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x103c79550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103c79810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103c79ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103c79d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103c7a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103c7a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103c7a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103c7a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103c7ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x103c7ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x103c7b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x103c7b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x103c7b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x103c7b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x103c7bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x103c7be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x103c7c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x103c7c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x103c7c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x103c7c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x103c7cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x103c7cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x103c7d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x103c7d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x103c7d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x103c7da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x103c7dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x103c7df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x103c7e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x103c7e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103c7e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x103c7ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x103c7ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x103c7f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x103c7f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x103c7f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x103c7f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x103c7fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x103c7fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x103c80090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x103c80350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x103c80610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x103c808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103c80b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x103c80e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x103c81110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103c813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103c81690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103c81950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x103c81c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103c81ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x103c82190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103c82450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103c82710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x103c829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103c82c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103c82f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x103c83210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103c834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103c83790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103c83a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103c83d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103c83fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x103c84290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x103c84550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103c84810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x103c84ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103c84d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103c85050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103c85310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103c855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103c85890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103c85b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x103c85f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103c863f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x103c86ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x103c86e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x103c87120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103c87590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x103c87a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x103c87e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x103c882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x103c88750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x103c88bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x103c89030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x103c894a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x103c89910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x103c89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x103c8a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x103c8a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x103c8aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x103c8af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x103c8b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x103c8b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x103c8bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x103c8c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103c8c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x103c8c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103c8ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103c8d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103c8d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103c8dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103c8e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103c8e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103c8e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103c8ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103c8f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103c8f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103c8fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103c8ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103c90390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103c90800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x103c90c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103c910e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103c91550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103c919c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103c91e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103c922a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103c92710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x103c92b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x103c92ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x103c93460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x103c938d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x103c93d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103c941b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x103c94620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x103c94a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x103c94f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x103c95370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x103c957e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x103c95c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x103c960c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x103c96530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x103c969a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x103c96e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x103c97280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x103c976f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x103c97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x103c97fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x103c98440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x103c988b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x103c98d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x103c99190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x103c99600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x103c99a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x103c99ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x103c9a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x103c9a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x103c9b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x103c9b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103c9c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x103c9c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x103c9ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x103c9d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x103c9d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x103c9db10 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.782s
user	0m0.280s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4719 (8da7f612)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f008a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f009120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f0096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f009c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f00a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f00a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f00ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f00b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f00b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f00bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de09250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de0a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de0aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de0c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de0c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de0d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de10dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de13080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de13340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de13c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de14970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de18de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de19540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de1ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de1d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de1eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de20b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de22d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de25500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de27650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de27c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de28d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de2de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de2e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de31540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de34740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de37940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de37e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de3a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de3bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de3d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de3f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de40040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de44b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de45040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de45ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de48120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de48880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de4bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de4d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de4dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de51c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de5e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de5e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de5f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de5fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de61c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de62100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de62650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de62d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de63bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de64590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de65040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de65650 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
0.00.101.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f009f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f009990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f00c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f00c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f00c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f00caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f00cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f00d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f00da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f00df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f00e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f00e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f00f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f00fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f010460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f010b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f0112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f0119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f0120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f0128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f012fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f0136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f013e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f014530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f014c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f014f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f015520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f015b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f016140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f016930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f016dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f017090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f017920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f017e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f018120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f0185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f018a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f018f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f0193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f019840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f019ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f01a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f01a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f01aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f01ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f01b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f01b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f01bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f01c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f01cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f01d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f01d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f01de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f01e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f01ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f01f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f01f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f01f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f01fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f020600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f020aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f020f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f0213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f021880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f021d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f0221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f022660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f022fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f0238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f023d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f024220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f024770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f024cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f025210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f026200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f026750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f026ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f0271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f027c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f0281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f028730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f028c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f0291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f029720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f029c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f02a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f02a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f02ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f02b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f02b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f02bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f02c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f02c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f02cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f02d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f02d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f02dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f02e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f02e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f02ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f02f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f02f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f02fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f030160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f0306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f030c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f031150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f0316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f032480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f032920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f033260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f033700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f033ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f034040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f0344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f034980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f0352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f035c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f0360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f036540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f0369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f036e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f0377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f037c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f038100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f0385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f038a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f039380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f039820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f039cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f03a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f03a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f03aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f03af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f03b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f03b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f03bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f03c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f03c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f03cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f03cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f03d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f03d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f03dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f03e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f03e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f03eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f03f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f03f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f03f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f03fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f040280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f040720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f041060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f041500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f0419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f041e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f0422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f042780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f042c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f0430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f043560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f043a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f043ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f0447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f044c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f045120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f045a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f045f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f0463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f046840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f046ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f047180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f047620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f047ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f047f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f048400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f0488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f049de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f04a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f04a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f04acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f04b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f04bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f04bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f04c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f04c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f04ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f04d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f04dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f04df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f04e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f04ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f04f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f04fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f050100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f050650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f050ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f0510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f051640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f051b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f0520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f052630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f052b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f0530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f053620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f053b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f0540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f054610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f054b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f0550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f055600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f0560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f0565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f056b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f057090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f0575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f057b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f058080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f0585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f058b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f059070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f0595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f059b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f05a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f05a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f05ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f05b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f05b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f05baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f05c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f05c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f05cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f05d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f05d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f05dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f05e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f05e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f05eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f05f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f05f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f05fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f060000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f060550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f060aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f060ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f061540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f0619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f061e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f062320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f0627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f062c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f063100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f0635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f063a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f063ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f064380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f064820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f064cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f065160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f065600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f065aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f065ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f066710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f066e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f067550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f067c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f067f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f068720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f0689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f068ff0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1052046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105204b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105204fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105205430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1052058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105205d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105206180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1052065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105206a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105206ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105207340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1052079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105208500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105208cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1052094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105209be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10520a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10520aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10520b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10520b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10520c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10520c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10520ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10520d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10520dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10520df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10520e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10520e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10520eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10520ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10520f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10520f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10520fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105210050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1052104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105210930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105210da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105211210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105211680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105211af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105211f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1052123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105212840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105212cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105213120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105213590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105213a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105213e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1052142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105214750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105214bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105215030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1052154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105215910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105215d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1052161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105216760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105216c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1052170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105217540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1052179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105217e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105218290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105218700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105218b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105218fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105219450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1052198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105219d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10521a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10521a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10521aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10521aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10521b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10521b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10521bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10521c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10521c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10521c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10521ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10521d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10521d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10521db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10521dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10521e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10521e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10521ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10521f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10521f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10521fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10521fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105220340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1052207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105220c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105221090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105221500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105221970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105221de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105222250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1052226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105222b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105222fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105223410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105223ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105223f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1052243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105224840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105224cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105225120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105225590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105225a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105225e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1052262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105226750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105226bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105227030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1052274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105227910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105227d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1052281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105228660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105228ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105228f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1052293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105229820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105229c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10522a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10522a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10522a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10522ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10522b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10522b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10522bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10522c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10522c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10522c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10522cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10522d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10522d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10522dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10522df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10522e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10522e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10522ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10522f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10522f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10522f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10522fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1052302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105230710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105230b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105230ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105231460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1052318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105231d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1052321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105232620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105232a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105232f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105233370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1052337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105233c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1052340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105234530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1052349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105234e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105235280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1052356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105235b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105235fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105236440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1052368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105236d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105237190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105237600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105237a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105237ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105238350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1052387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105238c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1052390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105239510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105239980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105239df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10523a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10523a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10523ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10523afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10523b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10523b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10523bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10523c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10523c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10523ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10523cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10523d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10523d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10523dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10523e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10523e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10523e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10523edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10523f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10523f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10523fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10523ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105240400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105240870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105240ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105241150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105241cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105241f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105242250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1052426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105242b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105242fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105243410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105243880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105243cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105244160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1052445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105244a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105244eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105245320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105245790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105245c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105246070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1052464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105246950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105246dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105247230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1052476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105247b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105247f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1052483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105248860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105248cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105249140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1052495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105249a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105249e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10524a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10524a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10524abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10524b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10524b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10524b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10524bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10524c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10524c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10524caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10524cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10524d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10524d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10524dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10524e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10524e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10524ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10524ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10524f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10524f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10524fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105250030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1052504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105250910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105250d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1052511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105251660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105251ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105251f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1052523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105252820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105252c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105253100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105253570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1052539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105253e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1052542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105254730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105254ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105255010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105255480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1052558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105256360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105256a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1052571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1052578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105257b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105257ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1052585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105258c00 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.229s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.54 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.97 sec*proc (2 tests)

Total Test time (real) =   1.98 sec
        2.01 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.56 sec*proc (2 tests)

Total Test time (real) =   0.57 sec
        0.57 real         0.13 user         0.08 sys
```
