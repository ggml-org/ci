### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.59 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.18 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.32 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.21 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.35 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.15 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.41 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.96 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.86 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.13 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.93 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.95 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.57 sec*proc (29 tests)

Total Test time (real) = 254.58 sec

real	4m14.664s
user	8m40.658s
sys	0m7.289s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.16 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.20 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.30 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.38 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.87 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.39 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.22 sec*proc (29 tests)

Total Test time (real) =  55.23 sec

real	0m55.247s
user	1m17.017s
sys	0m6.483s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.196 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.685 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.151 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.160 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.161 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.162 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.163 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.164 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.165 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.165 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.166 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.166 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.169 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.170 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.171 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.171 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.172 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.172 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.173 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.776 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.778 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.779 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.779 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.780 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.780 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.781 I llama_model_loader: - type  f32:  124 tensors
0.00.033.781 I llama_model_loader: - type  f16:   73 tensors
0.00.033.782 I print_info: file format = GGUF V3 (latest)
0.00.033.783 I print_info: file type   = F16
0.00.033.784 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.038.191 I load: special tokens cache size = 5
0.00.040.307 I load: token to piece cache size = 0.2032 MB
0.00.040.311 I print_info: arch             = bert
0.00.040.312 I print_info: vocab_only       = 0
0.00.040.312 I print_info: n_ctx_train      = 512
0.00.040.312 I print_info: n_embd           = 384
0.00.040.313 I print_info: n_layer          = 12
0.00.040.316 I print_info: n_head           = 12
0.00.040.317 I print_info: n_head_kv        = 12
0.00.040.317 I print_info: n_rot            = 32
0.00.040.318 I print_info: n_swa            = 0
0.00.040.318 I print_info: n_embd_head_k    = 32
0.00.040.321 I print_info: n_embd_head_v    = 32
0.00.040.322 I print_info: n_gqa            = 1
0.00.040.323 I print_info: n_embd_k_gqa     = 384
0.00.040.324 I print_info: n_embd_v_gqa     = 384
0.00.040.324 I print_info: f_norm_eps       = 1.0e-12
0.00.040.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.328 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.328 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.328 I print_info: f_logit_scale    = 0.0e+00
0.00.040.329 I print_info: n_ff             = 1536
0.00.040.329 I print_info: n_expert         = 0
0.00.040.329 I print_info: n_expert_used    = 0
0.00.040.330 I print_info: causal attn      = 0
0.00.040.330 I print_info: pooling type     = 2
0.00.040.330 I print_info: rope type        = 2
0.00.040.330 I print_info: rope scaling     = linear
0.00.040.331 I print_info: freq_base_train  = 10000.0
0.00.040.331 I print_info: freq_scale_train = 1
0.00.040.332 I print_info: n_ctx_orig_yarn  = 512
0.00.040.332 I print_info: rope_finetuned   = unknown
0.00.040.332 I print_info: ssm_d_conv       = 0
0.00.040.332 I print_info: ssm_d_inner      = 0
0.00.040.332 I print_info: ssm_d_state      = 0
0.00.040.333 I print_info: ssm_dt_rank      = 0
0.00.040.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.333 I print_info: model type       = 33M
0.00.040.339 I print_info: model params     = 33.21 M
0.00.040.340 I print_info: general.name     = Bge Small
0.00.040.340 I print_info: vocab type       = WPM
0.00.040.341 I print_info: n_vocab          = 30522
0.00.040.341 I print_info: n_merges         = 0
0.00.040.341 I print_info: BOS token        = 101 '[CLS]'
0.00.040.344 I print_info: UNK token        = 100 '[UNK]'
0.00.040.344 I print_info: SEP token        = 102 '[SEP]'
0.00.040.344 I print_info: PAD token        = 0 '[PAD]'
0.00.040.344 I print_info: MASK token       = 103 '[MASK]'
0.00.040.345 I print_info: LF token         = 0 '[PAD]'
0.00.040.345 I print_info: max token length = 21
0.00.043.238 I load_tensors: offloading 12 repeating layers to GPU
0.00.043.240 I load_tensors: offloading output layer to GPU
0.00.043.241 I load_tensors: offloaded 13/13 layers to GPU
0.00.043.264 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.266 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.043.500 I llama_init_from_model: n_seq_max     = 1
0.00.043.502 I llama_init_from_model: n_ctx         = 512
0.00.043.502 I llama_init_from_model: n_ctx_per_seq = 512
0.00.043.502 I llama_init_from_model: n_batch       = 2048
0.00.043.503 I llama_init_from_model: n_ubatch      = 2048
0.00.043.503 I llama_init_from_model: flash_attn    = 0
0.00.043.503 I llama_init_from_model: freq_base     = 10000.0
0.00.043.504 I llama_init_from_model: freq_scale    = 1
0.00.043.504 I ggml_metal_init: allocating
0.00.043.509 I ggml_metal_init: found device: Apple M4
0.00.043.514 I ggml_metal_init: picking default device: Apple M4
0.00.044.184 I ggml_metal_init: using embedded metal library
0.00.048.128 I ggml_metal_init: GPU name:   Apple M4
0.00.048.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.132 I ggml_metal_init: simdgroup reduction   = true
0.00.048.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.133 I ggml_metal_init: has residency sets    = true
0.00.048.133 I ggml_metal_init: has bfloat            = true
0.00.048.133 I ggml_metal_init: use bfloat            = true
0.00.048.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.243 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.060.911 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.914 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.935 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.062.056 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.062.057 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.062.058 I llama_init_from_model: graph nodes  = 429
0.00.062.058 I llama_init_from_model: graph splits = 2
0.00.062.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.062.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.586 I 
0.00.067.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.286 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.402 I llama_perf_context_print:        load time =      44.89 ms
0.00.073.403 I llama_perf_context_print: prompt eval time =       4.96 ms /     9 tokens (    0.55 ms per token,  1813.42 tokens per second)
0.00.073.404 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.404 I llama_perf_context_print:       total time =       5.82 ms /    10 tokens
0.00.073.550 I ggml_metal_free: deallocating

real	0m0.296s
user	0m0.050s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.461 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.135 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.141 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.141 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.142 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.142 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.143 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.143 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.143 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.144 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.144 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.146 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.146 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.147 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.147 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.147 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.148 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.460 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.103 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.105 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.105 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.106 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.106 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.106 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.106 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.107 I llama_model_loader: - type  f32:  124 tensors
0.00.015.107 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.108 I print_info: file format = GGUF V3 (latest)
0.00.015.108 I print_info: file type   = Q8_0
0.00.015.109 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.596 I load: special tokens cache size = 5
0.00.018.867 I load: token to piece cache size = 0.2032 MB
0.00.018.869 I print_info: arch             = bert
0.00.018.870 I print_info: vocab_only       = 0
0.00.018.870 I print_info: n_ctx_train      = 512
0.00.018.870 I print_info: n_embd           = 384
0.00.018.870 I print_info: n_layer          = 12
0.00.018.873 I print_info: n_head           = 12
0.00.018.874 I print_info: n_head_kv        = 12
0.00.018.874 I print_info: n_rot            = 32
0.00.018.874 I print_info: n_swa            = 0
0.00.018.875 I print_info: n_embd_head_k    = 32
0.00.018.875 I print_info: n_embd_head_v    = 32
0.00.018.875 I print_info: n_gqa            = 1
0.00.018.876 I print_info: n_embd_k_gqa     = 384
0.00.018.877 I print_info: n_embd_v_gqa     = 384
0.00.018.877 I print_info: f_norm_eps       = 1.0e-12
0.00.018.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.880 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.880 I print_info: f_logit_scale    = 0.0e+00
0.00.018.881 I print_info: n_ff             = 1536
0.00.018.881 I print_info: n_expert         = 0
0.00.018.881 I print_info: n_expert_used    = 0
0.00.018.881 I print_info: causal attn      = 0
0.00.018.881 I print_info: pooling type     = 2
0.00.018.881 I print_info: rope type        = 2
0.00.018.882 I print_info: rope scaling     = linear
0.00.018.882 I print_info: freq_base_train  = 10000.0
0.00.018.882 I print_info: freq_scale_train = 1
0.00.018.882 I print_info: n_ctx_orig_yarn  = 512
0.00.018.884 I print_info: rope_finetuned   = unknown
0.00.018.884 I print_info: ssm_d_conv       = 0
0.00.018.884 I print_info: ssm_d_inner      = 0
0.00.018.884 I print_info: ssm_d_state      = 0
0.00.018.884 I print_info: ssm_dt_rank      = 0
0.00.018.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.885 I print_info: model type       = 33M
0.00.018.885 I print_info: model params     = 33.21 M
0.00.018.885 I print_info: general.name     = Bge Small
0.00.018.886 I print_info: vocab type       = WPM
0.00.018.886 I print_info: n_vocab          = 30522
0.00.018.886 I print_info: n_merges         = 0
0.00.018.886 I print_info: BOS token        = 101 '[CLS]'
0.00.018.886 I print_info: UNK token        = 100 '[UNK]'
0.00.018.887 I print_info: SEP token        = 102 '[SEP]'
0.00.018.887 I print_info: PAD token        = 0 '[PAD]'
0.00.018.887 I print_info: MASK token       = 103 '[MASK]'
0.00.018.887 I print_info: LF token         = 0 '[PAD]'
0.00.018.887 I print_info: max token length = 21
0.00.020.628 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.629 I load_tensors: offloading output layer to GPU
0.00.020.629 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.635 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.635 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.786 I llama_init_from_model: n_seq_max     = 1
0.00.020.787 I llama_init_from_model: n_ctx         = 512
0.00.020.787 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.787 I llama_init_from_model: n_batch       = 2048
0.00.020.787 I llama_init_from_model: n_ubatch      = 2048
0.00.020.787 I llama_init_from_model: flash_attn    = 0
0.00.020.788 I llama_init_from_model: freq_base     = 10000.0
0.00.020.788 I llama_init_from_model: freq_scale    = 1
0.00.020.788 I ggml_metal_init: allocating
0.00.020.792 I ggml_metal_init: found device: Apple M4
0.00.020.797 I ggml_metal_init: picking default device: Apple M4
0.00.021.305 I ggml_metal_init: using embedded metal library
0.00.023.842 I ggml_metal_init: GPU name:   Apple M4
0.00.023.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.845 I ggml_metal_init: simdgroup reduction   = true
0.00.023.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.846 I ggml_metal_init: has residency sets    = true
0.00.023.846 I ggml_metal_init: has bfloat            = true
0.00.023.846 I ggml_metal_init: use bfloat            = true
0.00.023.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.057 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.649 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.650 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.664 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.614 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.615 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.616 I llama_init_from_model: graph nodes  = 429
0.00.035.616 I llama_init_from_model: graph splits = 2
0.00.035.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.656 I 
0.00.039.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.197 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.629 I llama_perf_context_print:        load time =      30.19 ms
0.00.044.630 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2094.48 tokens per second)
0.00.044.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.631 I llama_perf_context_print:       total time =       4.97 ms /    10 tokens
0.00.044.843 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.189 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.121 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.887 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.892 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.023.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.894 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.023.894 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.023.894 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.023.901 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.023.902 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.023.902 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.023.903 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.023.903 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.023.906 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.906 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.907 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.023.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.027.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.028.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.031.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.031.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.031.742 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.031.742 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.031.742 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.031.743 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.031.743 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.031.743 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.031.744 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.031.744 I llama_model_loader: - type  f32:   40 tensors
0.00.031.744 I llama_model_loader: - type  f16:   30 tensors
0.00.031.745 I print_info: file format = GGUF V3 (latest)
0.00.031.746 I print_info: file type   = F16
0.00.031.747 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.034.658 W load: empty token at index 5
0.00.038.425 W load: model vocab missing newline token, using special_pad_id instead
0.00.039.614 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.039.648 I load: special tokens cache size = 5
0.00.305.939 I load: token to piece cache size = 1.5060 MB
0.00.305.948 I print_info: arch             = jina-bert-v2
0.00.305.948 I print_info: vocab_only       = 0
0.00.305.948 I print_info: n_ctx_train      = 8192
0.00.305.948 I print_info: n_embd           = 384
0.00.305.949 I print_info: n_layer          = 4
0.00.305.953 I print_info: n_head           = 12
0.00.305.954 I print_info: n_head_kv        = 12
0.00.305.954 I print_info: n_rot            = 32
0.00.305.954 I print_info: n_swa            = 0
0.00.305.954 I print_info: n_embd_head_k    = 32
0.00.305.954 I print_info: n_embd_head_v    = 32
0.00.305.955 I print_info: n_gqa            = 1
0.00.305.955 I print_info: n_embd_k_gqa     = 384
0.00.305.956 I print_info: n_embd_v_gqa     = 384
0.00.305.956 I print_info: f_norm_eps       = 1.0e-12
0.00.305.957 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.305.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.305.957 I print_info: f_max_alibi_bias = 8.0e+00
0.00.305.957 I print_info: f_logit_scale    = 0.0e+00
0.00.305.957 I print_info: n_ff             = 1536
0.00.305.958 I print_info: n_expert         = 0
0.00.305.958 I print_info: n_expert_used    = 0
0.00.305.958 I print_info: causal attn      = 0
0.00.305.958 I print_info: pooling type     = -1
0.00.305.958 I print_info: rope type        = -1
0.00.305.958 I print_info: rope scaling     = linear
0.00.305.959 I print_info: freq_base_train  = 10000.0
0.00.305.959 I print_info: freq_scale_train = 1
0.00.305.959 I print_info: n_ctx_orig_yarn  = 8192
0.00.305.959 I print_info: rope_finetuned   = unknown
0.00.305.959 I print_info: ssm_d_conv       = 0
0.00.305.959 I print_info: ssm_d_inner      = 0
0.00.305.963 I print_info: ssm_d_state      = 0
0.00.305.963 I print_info: ssm_dt_rank      = 0
0.00.305.963 I print_info: ssm_dt_b_c_rms   = 0
0.00.305.963 I print_info: model type       = 33M
0.00.305.963 I print_info: model params     = 32.90 M
0.00.305.965 I print_info: general.name     = Jina Bert Implementation
0.00.305.965 I print_info: vocab type       = BPE
0.00.305.965 I print_info: n_vocab          = 61056
0.00.305.967 I print_info: n_merges         = 39382
0.00.305.967 I print_info: BOS token        = 0 '<s>'
0.00.305.967 I print_info: EOS token        = 2 '</s>'
0.00.305.968 I print_info: UNK token        = 3 '<unk>'
0.00.305.968 I print_info: SEP token        = 2 '</s>'
0.00.305.968 I print_info: PAD token        = 1 '<pad>'
0.00.305.968 I print_info: MASK token       = 4 '<mask>'
0.00.305.968 I print_info: EOG token        = 2 '</s>'
0.00.305.970 I print_info: max token length = 45
0.00.307.195 I load_tensors: offloading 4 repeating layers to GPU
0.00.307.196 I load_tensors: offloading output layer to GPU
0.00.307.196 I load_tensors: offloaded 5/5 layers to GPU
0.00.307.217 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.307.218 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.307.384 I llama_init_from_model: n_seq_max     = 1
0.00.307.385 I llama_init_from_model: n_ctx         = 8192
0.00.307.385 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.307.385 I llama_init_from_model: n_batch       = 2048
0.00.307.385 I llama_init_from_model: n_ubatch      = 2048
0.00.307.386 I llama_init_from_model: flash_attn    = 0
0.00.307.386 I llama_init_from_model: freq_base     = 10000.0
0.00.307.386 I llama_init_from_model: freq_scale    = 1
0.00.307.387 I ggml_metal_init: allocating
0.00.307.391 I ggml_metal_init: found device: Apple M4
0.00.307.394 I ggml_metal_init: picking default device: Apple M4
0.00.307.954 I ggml_metal_init: using embedded metal library
0.00.310.489 I ggml_metal_init: GPU name:   Apple M4
0.00.310.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.310.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.310.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.310.492 I ggml_metal_init: simdgroup reduction   = true
0.00.310.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.310.492 I ggml_metal_init: has residency sets    = true
0.00.310.492 I ggml_metal_init: has bfloat            = true
0.00.310.492 I ggml_metal_init: use bfloat            = true
0.00.310.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.310.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.320.613 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.323.816 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.323.818 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.323.840 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.330.920 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.330.922 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.330.923 I llama_init_from_model: graph nodes  = 154
0.00.330.923 I llama_init_from_model: graph splits = 2
0.00.330.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.330.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.312 I 
0.00.339.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.339.440 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.339.441 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.339.445 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.339.446 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.339.449 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.339.449 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.339.945 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.343.484 I llama_perf_context_print:        load time =     321.19 ms
0.00.343.485 I llama_perf_context_print: prompt eval time =       3.53 ms /    62 tokens (    0.06 ms per token, 17563.74 tokens per second)
0.00.343.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.343.486 I llama_perf_context_print:       total time =       4.17 ms /    63 tokens
0.00.343.737 I ggml_metal_free: deallocating

real	0m1.152s
user	0m0.316s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.182 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.322 I main: llama backend init
0.00.000.330 I main: load the model and apply lora adapter, if any
0.00.044.202 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.056.652 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.672 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.072.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.072.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.072.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.072.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.072.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.072.783 I llama_model_loader: - type  f32:  194 tensors
0.00.072.783 I llama_model_loader: - type  f16:   98 tensors
0.00.072.784 I print_info: file format = GGUF V3 (latest)
0.00.072.785 I print_info: file type   = all F32 (guessed)
0.00.072.787 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.453 I load: special tokens cache size = 25
0.00.089.545 I load: token to piece cache size = 0.2984 MB
0.00.089.551 I print_info: arch             = gptneox
0.00.089.551 I print_info: vocab_only       = 0
0.00.089.551 I print_info: n_ctx_train      = 2048
0.00.089.554 I print_info: n_embd           = 2048
0.00.089.554 I print_info: n_layer          = 24
0.00.089.559 I print_info: n_head           = 16
0.00.089.560 I print_info: n_head_kv        = 16
0.00.089.560 I print_info: n_rot            = 32
0.00.089.560 I print_info: n_swa            = 0
0.00.089.564 I print_info: n_embd_head_k    = 128
0.00.089.565 I print_info: n_embd_head_v    = 128
0.00.089.566 I print_info: n_gqa            = 1
0.00.089.566 I print_info: n_embd_k_gqa     = 2048
0.00.089.567 I print_info: n_embd_v_gqa     = 2048
0.00.089.568 I print_info: f_norm_eps       = 1.0e-05
0.00.089.568 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.568 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.569 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.570 I print_info: f_logit_scale    = 0.0e+00
0.00.089.570 I print_info: n_ff             = 8192
0.00.089.570 I print_info: n_expert         = 0
0.00.089.571 I print_info: n_expert_used    = 0
0.00.089.571 I print_info: causal attn      = 1
0.00.089.571 I print_info: pooling type     = 0
0.00.089.571 I print_info: rope type        = 2
0.00.089.571 I print_info: rope scaling     = linear
0.00.089.572 I print_info: freq_base_train  = 10000.0
0.00.089.572 I print_info: freq_scale_train = 1
0.00.089.572 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.572 I print_info: rope_finetuned   = unknown
0.00.089.573 I print_info: ssm_d_conv       = 0
0.00.089.573 I print_info: ssm_d_inner      = 0
0.00.089.573 I print_info: ssm_d_state      = 0
0.00.089.573 I print_info: ssm_dt_rank      = 0
0.00.089.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.575 I print_info: model type       = 1.4B
0.00.089.575 I print_info: model params     = 1.41 B
0.00.089.575 I print_info: general.name     = 1.4B
0.00.089.576 I print_info: vocab type       = BPE
0.00.089.576 I print_info: n_vocab          = 50304
0.00.089.576 I print_info: n_merges         = 50009
0.00.089.576 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.577 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.577 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.577 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.577 I print_info: LF token         = 187 'Ċ'
0.00.089.577 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.578 I print_info: max token length = 1024
0.00.154.187 I load_tensors: offloading 24 repeating layers to GPU
0.00.154.190 I load_tensors: offloading output layer to GPU
0.00.154.190 I load_tensors: offloaded 25/25 layers to GPU
0.00.154.216 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.154.218 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.154.783 I llama_init_from_model: n_seq_max     = 1
0.00.154.784 I llama_init_from_model: n_ctx         = 2048
0.00.154.784 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.154.785 I llama_init_from_model: n_batch       = 2048
0.00.154.785 I llama_init_from_model: n_ubatch      = 512
0.00.154.785 I llama_init_from_model: flash_attn    = 0
0.00.154.786 I llama_init_from_model: freq_base     = 10000.0
0.00.154.786 I llama_init_from_model: freq_scale    = 1
0.00.154.787 I ggml_metal_init: allocating
0.00.154.822 I ggml_metal_init: found device: Apple M4
0.00.154.827 I ggml_metal_init: picking default device: Apple M4
0.00.155.549 I ggml_metal_init: using embedded metal library
0.00.167.095 I ggml_metal_init: GPU name:   Apple M4
0.00.167.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.167.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.167.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.167.101 I ggml_metal_init: simdgroup reduction   = true
0.00.167.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.167.101 I ggml_metal_init: has residency sets    = true
0.00.167.102 I ggml_metal_init: has bfloat            = true
0.00.167.102 I ggml_metal_init: use bfloat            = true
0.00.167.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.167.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.211.107 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.243.680 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.243.689 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.243.735 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.248.152 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.248.154 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.248.155 I llama_init_from_model: graph nodes  = 967
0.00.248.155 I llama_init_from_model: graph splits = 2
0.00.248.161 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.248.277 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.248.278 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.309.577 I main: llama threadpool init, n_threads = 4
0.00.309.620 I 
0.00.309.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.309.651 I 
0.00.309.759 I sampler seed: 1234
0.00.309.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.309.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.309.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.309.788 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.256.386 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.02.256.387 I llama_perf_context_print:        load time =     264.50 ms
0.02.256.388 I llama_perf_context_print: prompt eval time =      44.01 ms /     7 tokens (    6.29 ms per token,   159.04 tokens per second)
0.02.256.388 I llama_perf_context_print:        eval time =    1899.59 ms /    63 runs   (   30.15 ms per token,    33.17 tokens per second)
0.02.256.389 I llama_perf_context_print:       total time =    1947.68 ms /    70 tokens
0.02.256.675 I ggml_metal_free: deallocating

real	0m2.639s
user	0m0.122s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.603 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.424 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.155 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.187 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.309 I llama_model_loader: - type  f32:  194 tensors
0.00.056.309 I llama_model_loader: - type  f16:   98 tensors
0.00.056.310 I print_info: file format = GGUF V3 (latest)
0.00.056.311 I print_info: file type   = all F32 (guessed)
0.00.056.313 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.004 I load: special tokens cache size = 25
0.00.077.169 I load: token to piece cache size = 0.2984 MB
0.00.077.172 I print_info: arch             = gptneox
0.00.077.172 I print_info: vocab_only       = 0
0.00.077.172 I print_info: n_ctx_train      = 2048
0.00.077.172 I print_info: n_embd           = 2048
0.00.077.172 I print_info: n_layer          = 24
0.00.077.175 I print_info: n_head           = 16
0.00.077.176 I print_info: n_head_kv        = 16
0.00.077.177 I print_info: n_rot            = 32
0.00.077.179 I print_info: n_swa            = 0
0.00.077.179 I print_info: n_embd_head_k    = 128
0.00.077.179 I print_info: n_embd_head_v    = 128
0.00.077.180 I print_info: n_gqa            = 1
0.00.077.181 I print_info: n_embd_k_gqa     = 2048
0.00.077.182 I print_info: n_embd_v_gqa     = 2048
0.00.077.183 I print_info: f_norm_eps       = 1.0e-05
0.00.077.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.184 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.184 I print_info: f_logit_scale    = 0.0e+00
0.00.077.184 I print_info: n_ff             = 8192
0.00.077.185 I print_info: n_expert         = 0
0.00.077.185 I print_info: n_expert_used    = 0
0.00.077.185 I print_info: causal attn      = 1
0.00.077.185 I print_info: pooling type     = 0
0.00.077.185 I print_info: rope type        = 2
0.00.077.185 I print_info: rope scaling     = linear
0.00.077.186 I print_info: freq_base_train  = 10000.0
0.00.077.186 I print_info: freq_scale_train = 1
0.00.077.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.187 I print_info: rope_finetuned   = unknown
0.00.077.187 I print_info: ssm_d_conv       = 0
0.00.077.187 I print_info: ssm_d_inner      = 0
0.00.077.187 I print_info: ssm_d_state      = 0
0.00.077.187 I print_info: ssm_dt_rank      = 0
0.00.077.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.187 I print_info: model type       = 1.4B
0.00.077.188 I print_info: model params     = 1.41 B
0.00.077.188 I print_info: general.name     = 1.4B
0.00.077.189 I print_info: vocab type       = BPE
0.00.077.193 I print_info: n_vocab          = 50304
0.00.077.193 I print_info: n_merges         = 50009
0.00.077.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.194 I print_info: LF token         = 187 'Ċ'
0.00.077.195 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.195 I print_info: max token length = 1024
0.01.491.994 I load_tensors: offloading 24 repeating layers to GPU
0.01.491.999 I load_tensors: offloading output layer to GPU
0.01.491.999 I load_tensors: offloaded 25/25 layers to GPU
0.01.492.022 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.492.024 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.492.691 I llama_init_from_model: n_seq_max     = 1
0.01.492.692 I llama_init_from_model: n_ctx         = 128
0.01.492.692 I llama_init_from_model: n_ctx_per_seq = 128
0.01.492.693 I llama_init_from_model: n_batch       = 128
0.01.492.693 I llama_init_from_model: n_ubatch      = 128
0.01.492.693 I llama_init_from_model: flash_attn    = 0
0.01.492.693 I llama_init_from_model: freq_base     = 10000.0
0.01.492.694 I llama_init_from_model: freq_scale    = 1
0.01.492.694 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.492.695 I ggml_metal_init: allocating
0.01.492.736 I ggml_metal_init: found device: Apple M4
0.01.492.741 I ggml_metal_init: picking default device: Apple M4
0.01.493.775 I ggml_metal_init: using embedded metal library
0.01.497.636 I ggml_metal_init: GPU name:   Apple M4
0.01.497.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.497.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.497.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.497.639 I ggml_metal_init: simdgroup reduction   = true
0.01.497.639 I ggml_metal_init: simdgroup matrix mul. = true
0.01.497.639 I ggml_metal_init: has residency sets    = true
0.01.497.639 I ggml_metal_init: has bfloat            = true
0.01.497.639 I ggml_metal_init: use bfloat            = true
0.01.497.640 I ggml_metal_init: hasUnifiedMemory      = true
0.01.497.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.508.101 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.509.899 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.509.901 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.509.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.511.588 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.511.589 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.511.590 I llama_init_from_model: graph nodes  = 967
0.01.511.590 I llama_init_from_model: graph splits = 2
0.01.511.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.511.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.547.385 I 
0.01.547.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.547.429 I perplexity: tokenizing the input ..
0.01.552.633 I perplexity: tokenization took 5.202 ms
0.01.552.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.671.039 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.672.573 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.672.605 I llama_perf_context_print:        load time =    1522.95 ms
0.01.672.608 I llama_perf_context_print: prompt eval time =     118.09 ms /   128 tokens (    0.92 ms per token,  1083.87 tokens per second)
0.01.672.609 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.672.609 I llama_perf_context_print:       total time =     125.22 ms /   129 tokens
0.01.672.992 I ggml_metal_free: deallocating

real	0m1.904s
user	0m0.099s
sys	0m0.282s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.100 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.010.231 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.874 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.280 I llama_model_loader: - type  f32:  194 tensors
0.00.039.280 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.281 I print_info: file format = GGUF V3 (latest)
0.00.039.282 I print_info: file type   = Q8_0
0.00.039.283 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.682 I load: special tokens cache size = 25
0.00.056.577 I load: token to piece cache size = 0.2984 MB
0.00.056.581 I print_info: arch             = gptneox
0.00.056.581 I print_info: vocab_only       = 0
0.00.056.582 I print_info: n_ctx_train      = 2048
0.00.056.582 I print_info: n_embd           = 2048
0.00.056.582 I print_info: n_layer          = 24
0.00.056.588 I print_info: n_head           = 16
0.00.056.589 I print_info: n_head_kv        = 16
0.00.056.592 I print_info: n_rot            = 32
0.00.056.592 I print_info: n_swa            = 0
0.00.056.592 I print_info: n_embd_head_k    = 128
0.00.056.592 I print_info: n_embd_head_v    = 128
0.00.056.593 I print_info: n_gqa            = 1
0.00.056.594 I print_info: n_embd_k_gqa     = 2048
0.00.056.595 I print_info: n_embd_v_gqa     = 2048
0.00.056.596 I print_info: f_norm_eps       = 1.0e-05
0.00.056.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.599 I print_info: f_logit_scale    = 0.0e+00
0.00.056.600 I print_info: n_ff             = 8192
0.00.056.600 I print_info: n_expert         = 0
0.00.056.600 I print_info: n_expert_used    = 0
0.00.056.601 I print_info: causal attn      = 1
0.00.056.601 I print_info: pooling type     = 0
0.00.056.601 I print_info: rope type        = 2
0.00.056.601 I print_info: rope scaling     = linear
0.00.056.602 I print_info: freq_base_train  = 10000.0
0.00.056.602 I print_info: freq_scale_train = 1
0.00.056.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.602 I print_info: rope_finetuned   = unknown
0.00.056.603 I print_info: ssm_d_conv       = 0
0.00.056.603 I print_info: ssm_d_inner      = 0
0.00.056.603 I print_info: ssm_d_state      = 0
0.00.056.603 I print_info: ssm_dt_rank      = 0
0.00.056.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.603 I print_info: model type       = 1.4B
0.00.056.604 I print_info: model params     = 1.41 B
0.00.056.605 I print_info: general.name     = 1.4B
0.00.056.606 I print_info: vocab type       = BPE
0.00.056.606 I print_info: n_vocab          = 50304
0.00.056.606 I print_info: n_merges         = 50009
0.00.056.606 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.606 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.607 I print_info: LF token         = 187 'Ċ'
0.00.056.607 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.607 I print_info: max token length = 1024
0.01.304.555 I load_tensors: offloading 24 repeating layers to GPU
0.01.304.559 I load_tensors: offloading output layer to GPU
0.01.304.560 I load_tensors: offloaded 25/25 layers to GPU
0.01.304.584 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.304.586 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.305.437 I llama_init_from_model: n_seq_max     = 1
0.01.305.439 I llama_init_from_model: n_ctx         = 2048
0.01.305.439 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.305.439 I llama_init_from_model: n_batch       = 2048
0.01.305.440 I llama_init_from_model: n_ubatch      = 512
0.01.305.440 I llama_init_from_model: flash_attn    = 0
0.01.305.441 I llama_init_from_model: freq_base     = 10000.0
0.01.305.441 I llama_init_from_model: freq_scale    = 1
0.01.305.442 I ggml_metal_init: allocating
0.01.305.453 I ggml_metal_init: found device: Apple M4
0.01.305.460 I ggml_metal_init: picking default device: Apple M4
0.01.306.735 I ggml_metal_init: using embedded metal library
0.01.311.914 I ggml_metal_init: GPU name:   Apple M4
0.01.311.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.311.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.311.918 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.311.919 I ggml_metal_init: simdgroup reduction   = true
0.01.311.919 I ggml_metal_init: simdgroup matrix mul. = true
0.01.311.919 I ggml_metal_init: has residency sets    = true
0.01.311.920 I ggml_metal_init: has bfloat            = true
0.01.311.920 I ggml_metal_init: use bfloat            = true
0.01.311.920 I ggml_metal_init: hasUnifiedMemory      = true
0.01.311.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.329.371 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.378.608 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.378.616 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.378.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.383.213 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.383.215 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.383.216 I llama_init_from_model: graph nodes  = 967
0.01.383.216 I llama_init_from_model: graph splits = 2
0.01.383.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.383.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.383.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.437.173 I main: llama threadpool init, n_threads = 4
0.01.437.218 I 
0.01.437.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.437.242 I 
0.01.437.408 I sampler seed: 1234
0.01.437.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.437.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.437.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.437.462 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.532.818 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.02.532.819 I llama_perf_context_print:        load time =    1426.26 ms
0.02.532.821 I llama_perf_context_print: prompt eval time =      49.42 ms /     7 tokens (    7.06 ms per token,   141.64 tokens per second)
0.02.532.822 I llama_perf_context_print:        eval time =    1043.04 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.532.822 I llama_perf_context_print:       total time =    1096.33 ms /    70 tokens
0.02.533.050 I ggml_metal_free: deallocating

real	0m2.554s
user	0m0.112s
sys	0m0.309s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.243 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.217 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.107 I llama_model_loader: - type  f32:  194 tensors
0.00.026.107 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.108 I print_info: file format = GGUF V3 (latest)
0.00.026.108 I print_info: file type   = Q8_0
0.00.026.109 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.535 I load: special tokens cache size = 25
0.00.040.638 I load: token to piece cache size = 0.2984 MB
0.00.040.642 I print_info: arch             = gptneox
0.00.040.642 I print_info: vocab_only       = 0
0.00.040.642 I print_info: n_ctx_train      = 2048
0.00.040.643 I print_info: n_embd           = 2048
0.00.040.643 I print_info: n_layer          = 24
0.00.040.647 I print_info: n_head           = 16
0.00.040.648 I print_info: n_head_kv        = 16
0.00.040.648 I print_info: n_rot            = 32
0.00.040.648 I print_info: n_swa            = 0
0.00.040.649 I print_info: n_embd_head_k    = 128
0.00.040.649 I print_info: n_embd_head_v    = 128
0.00.040.650 I print_info: n_gqa            = 1
0.00.040.650 I print_info: n_embd_k_gqa     = 2048
0.00.040.651 I print_info: n_embd_v_gqa     = 2048
0.00.040.652 I print_info: f_norm_eps       = 1.0e-05
0.00.040.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.652 I print_info: f_logit_scale    = 0.0e+00
0.00.040.653 I print_info: n_ff             = 8192
0.00.040.653 I print_info: n_expert         = 0
0.00.040.653 I print_info: n_expert_used    = 0
0.00.040.653 I print_info: causal attn      = 1
0.00.040.653 I print_info: pooling type     = 0
0.00.040.654 I print_info: rope type        = 2
0.00.040.654 I print_info: rope scaling     = linear
0.00.040.654 I print_info: freq_base_train  = 10000.0
0.00.040.654 I print_info: freq_scale_train = 1
0.00.040.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.655 I print_info: rope_finetuned   = unknown
0.00.040.655 I print_info: ssm_d_conv       = 0
0.00.040.655 I print_info: ssm_d_inner      = 0
0.00.040.655 I print_info: ssm_d_state      = 0
0.00.040.655 I print_info: ssm_dt_rank      = 0
0.00.040.655 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.658 I print_info: model type       = 1.4B
0.00.040.659 I print_info: model params     = 1.41 B
0.00.040.659 I print_info: general.name     = 1.4B
0.00.040.659 I print_info: vocab type       = BPE
0.00.040.659 I print_info: n_vocab          = 50304
0.00.040.660 I print_info: n_merges         = 50009
0.00.040.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.660 I print_info: LF token         = 187 'Ċ'
0.00.040.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.661 I print_info: max token length = 1024
0.00.815.730 I load_tensors: offloading 24 repeating layers to GPU
0.00.815.736 I load_tensors: offloading output layer to GPU
0.00.815.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.815.764 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.815.767 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.816.982 I llama_init_from_model: n_seq_max     = 1
0.00.816.984 I llama_init_from_model: n_ctx         = 128
0.00.816.984 I llama_init_from_model: n_ctx_per_seq = 128
0.00.816.984 I llama_init_from_model: n_batch       = 128
0.00.816.985 I llama_init_from_model: n_ubatch      = 128
0.00.816.985 I llama_init_from_model: flash_attn    = 0
0.00.816.986 I llama_init_from_model: freq_base     = 10000.0
0.00.816.986 I llama_init_from_model: freq_scale    = 1
0.00.816.987 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.816.988 I ggml_metal_init: allocating
0.00.817.048 I ggml_metal_init: found device: Apple M4
0.00.817.057 I ggml_metal_init: picking default device: Apple M4
0.00.818.271 I ggml_metal_init: using embedded metal library
0.00.823.466 I ggml_metal_init: GPU name:   Apple M4
0.00.823.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.823.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.823.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.823.474 I ggml_metal_init: simdgroup reduction   = true
0.00.823.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.823.476 I ggml_metal_init: has residency sets    = true
0.00.823.477 I ggml_metal_init: has bfloat            = true
0.00.823.478 I ggml_metal_init: use bfloat            = true
0.00.823.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.823.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.838.287 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.841.673 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.841.676 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.841.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.844.835 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.844.837 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.844.837 I llama_init_from_model: graph nodes  = 967
0.00.844.837 I llama_init_from_model: graph splits = 2
0.00.844.841 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.844.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.609 I 
0.00.870.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.870.681 I perplexity: tokenizing the input ..
0.00.877.767 I perplexity: tokenization took 7.083 ms
0.00.877.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.015.172 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.016.721 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.016.747 I llama_perf_context_print:        load time =     860.62 ms
0.01.016.748 I llama_perf_context_print: prompt eval time =     136.54 ms /   128 tokens (    1.07 ms per token,   937.45 tokens per second)
0.01.016.749 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.016.749 I llama_perf_context_print:       total time =     146.14 ms /   129 tokens
0.01.017.151 I ggml_metal_free: deallocating

real	0m1.034s
user	0m0.078s
sys	0m0.168s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.017.094 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.509 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.833 I llama_model_loader: - type  f32:  194 tensors
0.00.047.834 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.834 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.835 I print_info: file format = GGUF V3 (latest)
0.00.047.835 I print_info: file type   = Q4_0
0.00.047.837 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.059.970 I load: special tokens cache size = 25
0.00.070.292 I load: token to piece cache size = 0.2984 MB
0.00.070.298 I print_info: arch             = gptneox
0.00.070.299 I print_info: vocab_only       = 0
0.00.070.299 I print_info: n_ctx_train      = 2048
0.00.070.299 I print_info: n_embd           = 2048
0.00.070.300 I print_info: n_layer          = 24
0.00.070.306 I print_info: n_head           = 16
0.00.070.308 I print_info: n_head_kv        = 16
0.00.070.308 I print_info: n_rot            = 32
0.00.070.311 I print_info: n_swa            = 0
0.00.070.311 I print_info: n_embd_head_k    = 128
0.00.070.312 I print_info: n_embd_head_v    = 128
0.00.070.313 I print_info: n_gqa            = 1
0.00.070.315 I print_info: n_embd_k_gqa     = 2048
0.00.070.317 I print_info: n_embd_v_gqa     = 2048
0.00.070.318 I print_info: f_norm_eps       = 1.0e-05
0.00.070.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.319 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.320 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.320 I print_info: f_logit_scale    = 0.0e+00
0.00.070.322 I print_info: n_ff             = 8192
0.00.070.322 I print_info: n_expert         = 0
0.00.070.322 I print_info: n_expert_used    = 0
0.00.070.323 I print_info: causal attn      = 1
0.00.070.323 I print_info: pooling type     = 0
0.00.070.323 I print_info: rope type        = 2
0.00.070.324 I print_info: rope scaling     = linear
0.00.070.325 I print_info: freq_base_train  = 10000.0
0.00.070.325 I print_info: freq_scale_train = 1
0.00.070.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.326 I print_info: rope_finetuned   = unknown
0.00.070.326 I print_info: ssm_d_conv       = 0
0.00.070.328 I print_info: ssm_d_inner      = 0
0.00.070.328 I print_info: ssm_d_state      = 0
0.00.070.332 I print_info: ssm_dt_rank      = 0
0.00.070.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.333 I print_info: model type       = 1.4B
0.00.070.334 I print_info: model params     = 1.41 B
0.00.070.334 I print_info: general.name     = 1.4B
0.00.070.335 I print_info: vocab type       = BPE
0.00.070.336 I print_info: n_vocab          = 50304
0.00.070.336 I print_info: n_merges         = 50009
0.00.070.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.338 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.341 I print_info: LF token         = 187 'Ċ'
0.00.070.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.342 I print_info: max token length = 1024
0.00.645.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.879 I load_tensors: offloading output layer to GPU
0.00.645.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.912 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.645.913 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.647.366 I llama_init_from_model: n_seq_max     = 1
0.00.647.374 I llama_init_from_model: n_ctx         = 2048
0.00.647.374 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.375 I llama_init_from_model: n_batch       = 2048
0.00.647.375 I llama_init_from_model: n_ubatch      = 512
0.00.647.375 I llama_init_from_model: flash_attn    = 0
0.00.647.377 I llama_init_from_model: freq_base     = 10000.0
0.00.647.377 I llama_init_from_model: freq_scale    = 1
0.00.647.380 I ggml_metal_init: allocating
0.00.647.427 I ggml_metal_init: found device: Apple M4
0.00.647.443 I ggml_metal_init: picking default device: Apple M4
0.00.649.602 I ggml_metal_init: using embedded metal library
0.00.656.026 I ggml_metal_init: GPU name:   Apple M4
0.00.656.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.038 I ggml_metal_init: simdgroup reduction   = true
0.00.656.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.038 I ggml_metal_init: has residency sets    = true
0.00.656.039 I ggml_metal_init: has bfloat            = true
0.00.656.039 I ggml_metal_init: use bfloat            = true
0.00.656.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.047 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.201 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.630 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.641 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.675 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.467 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.744.469 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.744.469 I llama_init_from_model: graph nodes  = 967
0.00.744.470 I llama_init_from_model: graph splits = 2
0.00.744.474 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.605 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.400 I main: llama threadpool init, n_threads = 4
0.00.802.439 I 
0.00.802.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.459 I 
0.00.802.637 I sampler seed: 1234
0.00.802.641 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.651 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.652 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.652 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.484.909 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.01.484.910 I llama_perf_context_print:        load time =     784.62 ms
0.01.484.911 I llama_perf_context_print: prompt eval time =      49.47 ms /     7 tokens (    7.07 ms per token,   141.51 tokens per second)
0.01.484.912 I llama_perf_context_print:        eval time =     630.33 ms /    63 runs   (   10.01 ms per token,    99.95 tokens per second)
0.01.484.912 I llama_perf_context_print:       total time =     683.19 ms /    70 tokens
0.01.485.134 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.125s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.874 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.874 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.874 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.875 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.876 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.876 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.877 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.879 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.674 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.675 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.677 I llama_model_loader: - type  f32:  194 tensors
0.00.025.678 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.679 I print_info: file format = GGUF V3 (latest)
0.00.025.679 I print_info: file type   = Q4_0
0.00.025.680 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.672 I load: special tokens cache size = 25
0.00.039.681 I load: token to piece cache size = 0.2984 MB
0.00.039.684 I print_info: arch             = gptneox
0.00.039.685 I print_info: vocab_only       = 0
0.00.039.685 I print_info: n_ctx_train      = 2048
0.00.039.685 I print_info: n_embd           = 2048
0.00.039.685 I print_info: n_layer          = 24
0.00.039.689 I print_info: n_head           = 16
0.00.039.690 I print_info: n_head_kv        = 16
0.00.039.690 I print_info: n_rot            = 32
0.00.039.693 I print_info: n_swa            = 0
0.00.039.693 I print_info: n_embd_head_k    = 128
0.00.039.693 I print_info: n_embd_head_v    = 128
0.00.039.694 I print_info: n_gqa            = 1
0.00.039.695 I print_info: n_embd_k_gqa     = 2048
0.00.039.695 I print_info: n_embd_v_gqa     = 2048
0.00.039.696 I print_info: f_norm_eps       = 1.0e-05
0.00.039.696 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.697 I print_info: f_logit_scale    = 0.0e+00
0.00.039.697 I print_info: n_ff             = 8192
0.00.039.697 I print_info: n_expert         = 0
0.00.039.698 I print_info: n_expert_used    = 0
0.00.039.698 I print_info: causal attn      = 1
0.00.039.698 I print_info: pooling type     = 0
0.00.039.698 I print_info: rope type        = 2
0.00.039.698 I print_info: rope scaling     = linear
0.00.039.699 I print_info: freq_base_train  = 10000.0
0.00.039.699 I print_info: freq_scale_train = 1
0.00.039.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.699 I print_info: rope_finetuned   = unknown
0.00.039.700 I print_info: ssm_d_conv       = 0
0.00.039.700 I print_info: ssm_d_inner      = 0
0.00.039.700 I print_info: ssm_d_state      = 0
0.00.039.700 I print_info: ssm_dt_rank      = 0
0.00.039.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.702 I print_info: model type       = 1.4B
0.00.039.703 I print_info: model params     = 1.41 B
0.00.039.703 I print_info: general.name     = 1.4B
0.00.039.703 I print_info: vocab type       = BPE
0.00.039.704 I print_info: n_vocab          = 50304
0.00.039.704 I print_info: n_merges         = 50009
0.00.039.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.705 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.705 I print_info: LF token         = 187 'Ċ'
0.00.039.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.705 I print_info: max token length = 1024
0.00.597.827 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.842 I load_tensors: offloading output layer to GPU
0.00.597.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.871 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.597.873 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.599.116 I llama_init_from_model: n_seq_max     = 1
0.00.599.122 I llama_init_from_model: n_ctx         = 128
0.00.599.123 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.128 I llama_init_from_model: n_batch       = 128
0.00.599.129 I llama_init_from_model: n_ubatch      = 128
0.00.599.129 I llama_init_from_model: flash_attn    = 0
0.00.599.146 I llama_init_from_model: freq_base     = 10000.0
0.00.599.147 I llama_init_from_model: freq_scale    = 1
0.00.599.148 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.150 I ggml_metal_init: allocating
0.00.599.217 I ggml_metal_init: found device: Apple M4
0.00.599.229 I ggml_metal_init: picking default device: Apple M4
0.00.601.016 I ggml_metal_init: using embedded metal library
0.00.606.438 I ggml_metal_init: GPU name:   Apple M4
0.00.606.443 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.444 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.446 I ggml_metal_init: simdgroup reduction   = true
0.00.606.446 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.447 I ggml_metal_init: has residency sets    = true
0.00.606.447 I ggml_metal_init: has bfloat            = true
0.00.606.447 I ggml_metal_init: use bfloat            = true
0.00.606.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.908 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.336 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.380 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.464 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.466 I llama_init_from_model: graph nodes  = 967
0.00.632.466 I llama_init_from_model: graph splits = 2
0.00.632.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.546 I 
0.00.662.625 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.632 I perplexity: tokenizing the input ..
0.00.669.873 I perplexity: tokenization took 7.239 ms
0.00.669.884 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.707 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.808.316 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.808.340 I llama_perf_context_print:        load time =     652.79 ms
0.00.808.341 I llama_perf_context_print: prompt eval time =     135.78 ms /   128 tokens (    1.06 ms per token,   942.73 tokens per second)
0.00.808.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.342 I llama_perf_context_print:       total time =     145.80 ms /   129 tokens
0.00.808.686 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.081s
sys	0m0.122s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.104 I main: llama backend init
0.00.000.106 I main: load the model and apply lora adapter, if any
0.00.009.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.031.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.081 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.084 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.880 I llama_model_loader: - type  f32:  194 tensors
0.00.039.880 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.881 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.881 I print_info: file format = GGUF V3 (latest)
0.00.039.883 I print_info: file type   = Q4_1
0.00.039.884 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.970 I load: special tokens cache size = 25
0.00.054.006 I load: token to piece cache size = 0.2984 MB
0.00.054.011 I print_info: arch             = gptneox
0.00.054.011 I print_info: vocab_only       = 0
0.00.054.011 I print_info: n_ctx_train      = 2048
0.00.054.012 I print_info: n_embd           = 2048
0.00.054.012 I print_info: n_layer          = 24
0.00.054.016 I print_info: n_head           = 16
0.00.054.016 I print_info: n_head_kv        = 16
0.00.054.016 I print_info: n_rot            = 32
0.00.054.016 I print_info: n_swa            = 0
0.00.054.019 I print_info: n_embd_head_k    = 128
0.00.054.019 I print_info: n_embd_head_v    = 128
0.00.054.019 I print_info: n_gqa            = 1
0.00.054.020 I print_info: n_embd_k_gqa     = 2048
0.00.054.021 I print_info: n_embd_v_gqa     = 2048
0.00.054.021 I print_info: f_norm_eps       = 1.0e-05
0.00.054.022 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.022 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.022 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.023 I print_info: f_logit_scale    = 0.0e+00
0.00.054.023 I print_info: n_ff             = 8192
0.00.054.024 I print_info: n_expert         = 0
0.00.054.024 I print_info: n_expert_used    = 0
0.00.054.024 I print_info: causal attn      = 1
0.00.054.025 I print_info: pooling type     = 0
0.00.054.025 I print_info: rope type        = 2
0.00.054.025 I print_info: rope scaling     = linear
0.00.054.025 I print_info: freq_base_train  = 10000.0
0.00.054.026 I print_info: freq_scale_train = 1
0.00.054.026 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.026 I print_info: rope_finetuned   = unknown
0.00.054.026 I print_info: ssm_d_conv       = 0
0.00.054.026 I print_info: ssm_d_inner      = 0
0.00.054.026 I print_info: ssm_d_state      = 0
0.00.054.027 I print_info: ssm_dt_rank      = 0
0.00.054.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.029 I print_info: model type       = 1.4B
0.00.054.029 I print_info: model params     = 1.41 B
0.00.054.029 I print_info: general.name     = 1.4B
0.00.054.030 I print_info: vocab type       = BPE
0.00.054.031 I print_info: n_vocab          = 50304
0.00.054.031 I print_info: n_merges         = 50009
0.00.054.031 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.032 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.033 I print_info: LF token         = 187 'Ċ'
0.00.054.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.033 I print_info: max token length = 1024
0.01.432.975 I load_tensors: offloading 24 repeating layers to GPU
0.01.432.981 I load_tensors: offloading output layer to GPU
0.01.432.981 I load_tensors: offloaded 25/25 layers to GPU
0.01.432.999 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.01.433.000 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.01.433.669 I llama_init_from_model: n_seq_max     = 1
0.01.433.673 I llama_init_from_model: n_ctx         = 2048
0.01.433.673 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.433.674 I llama_init_from_model: n_batch       = 2048
0.01.433.674 I llama_init_from_model: n_ubatch      = 512
0.01.433.674 I llama_init_from_model: flash_attn    = 0
0.01.433.676 I llama_init_from_model: freq_base     = 10000.0
0.01.433.676 I llama_init_from_model: freq_scale    = 1
0.01.433.677 I ggml_metal_init: allocating
0.01.433.726 I ggml_metal_init: found device: Apple M4
0.01.433.736 I ggml_metal_init: picking default device: Apple M4
0.01.434.872 I ggml_metal_init: using embedded metal library
0.01.443.470 I ggml_metal_init: GPU name:   Apple M4
0.01.443.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.443.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.443.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.443.478 I ggml_metal_init: simdgroup reduction   = true
0.01.443.478 I ggml_metal_init: simdgroup matrix mul. = true
0.01.443.478 I ggml_metal_init: has residency sets    = true
0.01.443.479 I ggml_metal_init: has bfloat            = true
0.01.443.479 I ggml_metal_init: use bfloat            = true
0.01.443.480 I ggml_metal_init: hasUnifiedMemory      = true
0.01.443.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.463.484 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.498.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.498.149 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.498.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.502.871 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.502.874 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.502.874 I llama_init_from_model: graph nodes  = 967
0.01.502.875 I llama_init_from_model: graph splits = 2
0.01.502.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.503.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.503.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.557.888 I main: llama threadpool init, n_threads = 4
0.01.557.936 I 
0.01.557.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.557.961 I 
0.01.558.135 I sampler seed: 1234
0.01.558.140 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.558.161 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.558.162 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.558.162 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.290.604 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.02.290.605 I llama_perf_context_print:        load time =    1547.72 ms
0.02.290.606 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.13 tokens per second)
0.02.290.607 I llama_perf_context_print:        eval time =     680.85 ms /    63 runs   (   10.81 ms per token,    92.53 tokens per second)
0.02.290.607 I llama_perf_context_print:       total time =     733.40 ms /    70 tokens
0.02.290.837 I ggml_metal_free: deallocating

real	0m2.323s
user	0m0.106s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.186 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.189 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.061 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.062 I print_info: file format = GGUF V3 (latest)
0.00.025.062 I print_info: file type   = Q4_1
0.00.025.064 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.972 I load: special tokens cache size = 25
0.00.038.780 I load: token to piece cache size = 0.2984 MB
0.00.038.783 I print_info: arch             = gptneox
0.00.038.783 I print_info: vocab_only       = 0
0.00.038.784 I print_info: n_ctx_train      = 2048
0.00.038.784 I print_info: n_embd           = 2048
0.00.038.784 I print_info: n_layer          = 24
0.00.038.787 I print_info: n_head           = 16
0.00.038.787 I print_info: n_head_kv        = 16
0.00.038.788 I print_info: n_rot            = 32
0.00.038.788 I print_info: n_swa            = 0
0.00.038.788 I print_info: n_embd_head_k    = 128
0.00.038.788 I print_info: n_embd_head_v    = 128
0.00.038.789 I print_info: n_gqa            = 1
0.00.038.790 I print_info: n_embd_k_gqa     = 2048
0.00.038.790 I print_info: n_embd_v_gqa     = 2048
0.00.038.791 I print_info: f_norm_eps       = 1.0e-05
0.00.038.791 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.792 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.792 I print_info: f_logit_scale    = 0.0e+00
0.00.038.793 I print_info: n_ff             = 8192
0.00.038.793 I print_info: n_expert         = 0
0.00.038.793 I print_info: n_expert_used    = 0
0.00.038.793 I print_info: causal attn      = 1
0.00.038.793 I print_info: pooling type     = 0
0.00.038.793 I print_info: rope type        = 2
0.00.038.794 I print_info: rope scaling     = linear
0.00.038.794 I print_info: freq_base_train  = 10000.0
0.00.038.794 I print_info: freq_scale_train = 1
0.00.038.795 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.795 I print_info: rope_finetuned   = unknown
0.00.038.795 I print_info: ssm_d_conv       = 0
0.00.038.795 I print_info: ssm_d_inner      = 0
0.00.038.795 I print_info: ssm_d_state      = 0
0.00.038.795 I print_info: ssm_dt_rank      = 0
0.00.038.796 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.796 I print_info: model type       = 1.4B
0.00.038.798 I print_info: model params     = 1.41 B
0.00.038.798 I print_info: general.name     = 1.4B
0.00.038.799 I print_info: vocab type       = BPE
0.00.038.799 I print_info: n_vocab          = 50304
0.00.038.799 I print_info: n_merges         = 50009
0.00.038.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.800 I print_info: LF token         = 187 'Ċ'
0.00.038.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.805 I print_info: max token length = 1024
0.00.653.862 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.877 I load_tensors: offloading output layer to GPU
0.00.653.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.917 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.653.919 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.655.355 I llama_init_from_model: n_seq_max     = 1
0.00.655.359 I llama_init_from_model: n_ctx         = 128
0.00.655.360 I llama_init_from_model: n_ctx_per_seq = 128
0.00.655.361 I llama_init_from_model: n_batch       = 128
0.00.655.361 I llama_init_from_model: n_ubatch      = 128
0.00.655.362 I llama_init_from_model: flash_attn    = 0
0.00.655.363 I llama_init_from_model: freq_base     = 10000.0
0.00.655.364 I llama_init_from_model: freq_scale    = 1
0.00.655.364 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.655.366 I ggml_metal_init: allocating
0.00.655.443 I ggml_metal_init: found device: Apple M4
0.00.655.458 I ggml_metal_init: picking default device: Apple M4
0.00.657.192 I ggml_metal_init: using embedded metal library
0.00.663.832 I ggml_metal_init: GPU name:   Apple M4
0.00.663.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.840 I ggml_metal_init: simdgroup reduction   = true
0.00.663.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.841 I ggml_metal_init: has residency sets    = true
0.00.663.841 I ggml_metal_init: has bfloat            = true
0.00.663.841 I ggml_metal_init: use bfloat            = true
0.00.663.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.275 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.709 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.685.712 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.685.752 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.011 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.013 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.013 I llama_init_from_model: graph nodes  = 967
0.00.689.014 I llama_init_from_model: graph splits = 2
0.00.689.017 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.741 I 
0.00.713.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.834 I perplexity: tokenizing the input ..
0.00.721.260 I perplexity: tokenization took 7.423 ms
0.00.721.268 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.077 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.859.614 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.859.644 I llama_perf_context_print:        load time =     704.82 ms
0.00.859.646 I llama_perf_context_print: prompt eval time =     135.84 ms /   128 tokens (    1.06 ms per token,   942.25 tokens per second)
0.00.859.647 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.647 I llama_perf_context_print:       total time =     145.91 ms /   129 tokens
0.00.860.016 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.081s
sys	0m0.131s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.085 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.118 I main: llama backend init
0.00.000.120 I main: load the model and apply lora adapter, if any
0.00.012.135 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.108 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.108 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.111 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.112 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.115 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.115 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.689 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.365 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.366 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.366 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.366 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.367 I llama_model_loader: - type  f32:  194 tensors
0.00.034.367 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.368 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.368 I print_info: file format = GGUF V3 (latest)
0.00.034.369 I print_info: file type   = Q5_0
0.00.034.371 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.009 I load: special tokens cache size = 25
0.00.059.345 I load: token to piece cache size = 0.2984 MB
0.00.059.349 I print_info: arch             = gptneox
0.00.059.349 I print_info: vocab_only       = 0
0.00.059.350 I print_info: n_ctx_train      = 2048
0.00.059.350 I print_info: n_embd           = 2048
0.00.059.351 I print_info: n_layer          = 24
0.00.059.354 I print_info: n_head           = 16
0.00.059.355 I print_info: n_head_kv        = 16
0.00.059.356 I print_info: n_rot            = 32
0.00.059.359 I print_info: n_swa            = 0
0.00.059.359 I print_info: n_embd_head_k    = 128
0.00.059.359 I print_info: n_embd_head_v    = 128
0.00.059.360 I print_info: n_gqa            = 1
0.00.059.362 I print_info: n_embd_k_gqa     = 2048
0.00.059.363 I print_info: n_embd_v_gqa     = 2048
0.00.059.366 I print_info: f_norm_eps       = 1.0e-05
0.00.059.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.367 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.367 I print_info: f_logit_scale    = 0.0e+00
0.00.059.368 I print_info: n_ff             = 8192
0.00.059.368 I print_info: n_expert         = 0
0.00.059.369 I print_info: n_expert_used    = 0
0.00.059.369 I print_info: causal attn      = 1
0.00.059.369 I print_info: pooling type     = 0
0.00.059.371 I print_info: rope type        = 2
0.00.059.373 I print_info: rope scaling     = linear
0.00.059.374 I print_info: freq_base_train  = 10000.0
0.00.059.374 I print_info: freq_scale_train = 1
0.00.059.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.375 I print_info: rope_finetuned   = unknown
0.00.059.375 I print_info: ssm_d_conv       = 0
0.00.059.375 I print_info: ssm_d_inner      = 0
0.00.059.376 I print_info: ssm_d_state      = 0
0.00.059.377 I print_info: ssm_dt_rank      = 0
0.00.059.377 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.378 I print_info: model type       = 1.4B
0.00.059.378 I print_info: model params     = 1.41 B
0.00.059.379 I print_info: general.name     = 1.4B
0.00.059.379 I print_info: vocab type       = BPE
0.00.059.380 I print_info: n_vocab          = 50304
0.00.059.380 I print_info: n_merges         = 50009
0.00.059.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.382 I print_info: LF token         = 187 'Ċ'
0.00.059.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.383 I print_info: max token length = 1024
0.00.742.230 I load_tensors: offloading 24 repeating layers to GPU
0.00.742.246 I load_tensors: offloading output layer to GPU
0.00.742.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.742.287 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.742.289 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.743.556 I llama_init_from_model: n_seq_max     = 1
0.00.743.561 I llama_init_from_model: n_ctx         = 2048
0.00.743.561 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.743.562 I llama_init_from_model: n_batch       = 2048
0.00.743.562 I llama_init_from_model: n_ubatch      = 512
0.00.743.563 I llama_init_from_model: flash_attn    = 0
0.00.743.564 I llama_init_from_model: freq_base     = 10000.0
0.00.743.564 I llama_init_from_model: freq_scale    = 1
0.00.743.570 I ggml_metal_init: allocating
0.00.743.652 I ggml_metal_init: found device: Apple M4
0.00.743.666 I ggml_metal_init: picking default device: Apple M4
0.00.745.621 I ggml_metal_init: using embedded metal library
0.00.752.258 I ggml_metal_init: GPU name:   Apple M4
0.00.752.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.752.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.752.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.752.266 I ggml_metal_init: simdgroup reduction   = true
0.00.752.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.752.266 I ggml_metal_init: has residency sets    = true
0.00.752.266 I ggml_metal_init: has bfloat            = true
0.00.752.267 I ggml_metal_init: use bfloat            = true
0.00.752.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.752.276 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.769.750 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.825.960 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.825.966 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.826.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.831.254 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.831.257 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.831.257 I llama_init_from_model: graph nodes  = 967
0.00.831.257 I llama_init_from_model: graph splits = 2
0.00.831.262 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.831.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.831.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.888.925 I main: llama threadpool init, n_threads = 4
0.00.888.967 I 
0.00.888.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.888.992 I 
0.00.889.143 I sampler seed: 1234
0.00.889.148 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.889.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.889.159 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.889.159 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.672.192 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.672.193 I llama_perf_context_print:        load time =     876.08 ms
0.01.672.194 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.52 tokens per second)
0.01.672.194 I llama_perf_context_print:        eval time =     737.44 ms /    63 runs   (   11.71 ms per token,    85.43 tokens per second)
0.01.672.195 I llama_perf_context_print:       total time =     783.97 ms /    70 tokens
0.01.672.433 I ggml_metal_free: deallocating

real	0m1.718s
user	0m0.127s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.964 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.497 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.312 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.313 I llama_model_loader: - type  f32:  194 tensors
0.00.026.313 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.314 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.314 I print_info: file format = GGUF V3 (latest)
0.00.026.315 I print_info: file type   = Q5_0
0.00.026.316 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.475 I load: special tokens cache size = 25
0.00.040.357 I load: token to piece cache size = 0.2984 MB
0.00.040.360 I print_info: arch             = gptneox
0.00.040.360 I print_info: vocab_only       = 0
0.00.040.360 I print_info: n_ctx_train      = 2048
0.00.040.361 I print_info: n_embd           = 2048
0.00.040.361 I print_info: n_layer          = 24
0.00.040.364 I print_info: n_head           = 16
0.00.040.365 I print_info: n_head_kv        = 16
0.00.040.365 I print_info: n_rot            = 32
0.00.040.365 I print_info: n_swa            = 0
0.00.040.365 I print_info: n_embd_head_k    = 128
0.00.040.366 I print_info: n_embd_head_v    = 128
0.00.040.369 I print_info: n_gqa            = 1
0.00.040.369 I print_info: n_embd_k_gqa     = 2048
0.00.040.370 I print_info: n_embd_v_gqa     = 2048
0.00.040.371 I print_info: f_norm_eps       = 1.0e-05
0.00.040.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.372 I print_info: f_logit_scale    = 0.0e+00
0.00.040.373 I print_info: n_ff             = 8192
0.00.040.373 I print_info: n_expert         = 0
0.00.040.373 I print_info: n_expert_used    = 0
0.00.040.373 I print_info: causal attn      = 1
0.00.040.373 I print_info: pooling type     = 0
0.00.040.373 I print_info: rope type        = 2
0.00.040.374 I print_info: rope scaling     = linear
0.00.040.374 I print_info: freq_base_train  = 10000.0
0.00.040.374 I print_info: freq_scale_train = 1
0.00.040.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.375 I print_info: rope_finetuned   = unknown
0.00.040.375 I print_info: ssm_d_conv       = 0
0.00.040.375 I print_info: ssm_d_inner      = 0
0.00.040.375 I print_info: ssm_d_state      = 0
0.00.040.375 I print_info: ssm_dt_rank      = 0
0.00.040.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.377 I print_info: model type       = 1.4B
0.00.040.377 I print_info: model params     = 1.41 B
0.00.040.377 I print_info: general.name     = 1.4B
0.00.040.378 I print_info: vocab type       = BPE
0.00.040.378 I print_info: n_vocab          = 50304
0.00.040.378 I print_info: n_merges         = 50009
0.00.040.378 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.379 I print_info: LF token         = 187 'Ċ'
0.00.040.380 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.380 I print_info: max token length = 1024
0.00.700.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.698 I load_tensors: offloading output layer to GPU
0.00.700.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.732 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.700.733 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.702.191 I llama_init_from_model: n_seq_max     = 1
0.00.702.196 I llama_init_from_model: n_ctx         = 128
0.00.702.196 I llama_init_from_model: n_ctx_per_seq = 128
0.00.702.197 I llama_init_from_model: n_batch       = 128
0.00.702.197 I llama_init_from_model: n_ubatch      = 128
0.00.702.198 I llama_init_from_model: flash_attn    = 0
0.00.702.200 I llama_init_from_model: freq_base     = 10000.0
0.00.702.201 I llama_init_from_model: freq_scale    = 1
0.00.702.201 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.702.209 I ggml_metal_init: allocating
0.00.702.294 I ggml_metal_init: found device: Apple M4
0.00.702.308 I ggml_metal_init: picking default device: Apple M4
0.00.704.000 I ggml_metal_init: using embedded metal library
0.00.710.415 I ggml_metal_init: GPU name:   Apple M4
0.00.710.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.710.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.710.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.710.421 I ggml_metal_init: simdgroup reduction   = true
0.00.710.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.710.422 I ggml_metal_init: has residency sets    = true
0.00.710.422 I ggml_metal_init: has bfloat            = true
0.00.710.422 I ggml_metal_init: use bfloat            = true
0.00.710.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.710.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.221 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.730.774 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.730.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.733.952 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.733.954 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.733.955 I llama_init_from_model: graph nodes  = 967
0.00.733.955 I llama_init_from_model: graph splits = 2
0.00.733.958 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.733.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.591 I 
0.00.765.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.681 I perplexity: tokenizing the input ..
0.00.772.986 I perplexity: tokenization took 7.301 ms
0.00.772.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.919.146 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.920.694 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.920.717 I llama_perf_context_print:        load time =     755.62 ms
0.00.920.717 I llama_perf_context_print: prompt eval time =     145.26 ms /   128 tokens (    1.13 ms per token,   881.15 tokens per second)
0.00.920.718 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.920.719 I llama_perf_context_print:       total time =     155.13 ms /   129 tokens
0.00.921.127 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.080s
sys	0m0.141s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.014.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.029.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.245 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.296 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.226 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.227 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.227 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.038.228 I llama_model_loader: - type  f32:  194 tensors
0.00.038.228 I llama_model_loader: - type q5_1:   97 tensors
0.00.038.228 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.229 I print_info: file format = GGUF V3 (latest)
0.00.038.229 I print_info: file type   = Q5_1
0.00.038.233 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.945 I load: special tokens cache size = 25
0.00.053.851 I load: token to piece cache size = 0.2984 MB
0.00.053.854 I print_info: arch             = gptneox
0.00.053.854 I print_info: vocab_only       = 0
0.00.053.855 I print_info: n_ctx_train      = 2048
0.00.053.855 I print_info: n_embd           = 2048
0.00.053.855 I print_info: n_layer          = 24
0.00.053.858 I print_info: n_head           = 16
0.00.053.859 I print_info: n_head_kv        = 16
0.00.053.859 I print_info: n_rot            = 32
0.00.053.859 I print_info: n_swa            = 0
0.00.053.860 I print_info: n_embd_head_k    = 128
0.00.053.860 I print_info: n_embd_head_v    = 128
0.00.053.861 I print_info: n_gqa            = 1
0.00.053.861 I print_info: n_embd_k_gqa     = 2048
0.00.053.862 I print_info: n_embd_v_gqa     = 2048
0.00.053.862 I print_info: f_norm_eps       = 1.0e-05
0.00.053.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.863 I print_info: f_logit_scale    = 0.0e+00
0.00.053.864 I print_info: n_ff             = 8192
0.00.053.864 I print_info: n_expert         = 0
0.00.053.864 I print_info: n_expert_used    = 0
0.00.053.864 I print_info: causal attn      = 1
0.00.053.864 I print_info: pooling type     = 0
0.00.053.864 I print_info: rope type        = 2
0.00.053.865 I print_info: rope scaling     = linear
0.00.053.865 I print_info: freq_base_train  = 10000.0
0.00.053.867 I print_info: freq_scale_train = 1
0.00.053.867 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.868 I print_info: rope_finetuned   = unknown
0.00.053.868 I print_info: ssm_d_conv       = 0
0.00.053.868 I print_info: ssm_d_inner      = 0
0.00.053.869 I print_info: ssm_d_state      = 0
0.00.053.869 I print_info: ssm_dt_rank      = 0
0.00.053.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.870 I print_info: model type       = 1.4B
0.00.053.870 I print_info: model params     = 1.41 B
0.00.053.870 I print_info: general.name     = 1.4B
0.00.053.871 I print_info: vocab type       = BPE
0.00.053.871 I print_info: n_vocab          = 50304
0.00.053.871 I print_info: n_merges         = 50009
0.00.053.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.872 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.872 I print_info: LF token         = 187 'Ċ'
0.00.053.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.876 I print_info: max token length = 1024
0.00.774.747 I load_tensors: offloading 24 repeating layers to GPU
0.00.774.764 I load_tensors: offloading output layer to GPU
0.00.774.764 I load_tensors: offloaded 25/25 layers to GPU
0.00.774.803 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.774.805 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.776.173 I llama_init_from_model: n_seq_max     = 1
0.00.776.177 I llama_init_from_model: n_ctx         = 2048
0.00.776.178 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.776.178 I llama_init_from_model: n_batch       = 2048
0.00.776.178 I llama_init_from_model: n_ubatch      = 512
0.00.776.179 I llama_init_from_model: flash_attn    = 0
0.00.776.180 I llama_init_from_model: freq_base     = 10000.0
0.00.776.181 I llama_init_from_model: freq_scale    = 1
0.00.776.183 I ggml_metal_init: allocating
0.00.776.262 I ggml_metal_init: found device: Apple M4
0.00.776.275 I ggml_metal_init: picking default device: Apple M4
0.00.778.122 I ggml_metal_init: using embedded metal library
0.00.784.852 I ggml_metal_init: GPU name:   Apple M4
0.00.784.856 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.784.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.784.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.784.863 I ggml_metal_init: simdgroup reduction   = true
0.00.784.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.784.863 I ggml_metal_init: has residency sets    = true
0.00.784.864 I ggml_metal_init: has bfloat            = true
0.00.784.864 I ggml_metal_init: use bfloat            = true
0.00.784.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.784.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.802.500 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.863.959 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.863.967 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.864.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.868.262 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.868.264 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.868.265 I llama_init_from_model: graph nodes  = 967
0.00.868.265 I llama_init_from_model: graph splits = 2
0.00.868.270 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.868.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.868.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.603 I main: llama threadpool init, n_threads = 4
0.00.928.649 I 
0.00.928.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.674 I 
0.00.928.828 I sampler seed: 1234
0.00.928.832 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.928.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.928.879 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.928.880 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.770.324 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.770.325 I llama_perf_context_print:        load time =     913.80 ms
0.01.770.326 I llama_perf_context_print: prompt eval time =      52.33 ms /     7 tokens (    7.48 ms per token,   133.78 tokens per second)
0.01.770.326 I llama_perf_context_print:        eval time =     786.19 ms /    63 runs   (   12.48 ms per token,    80.13 tokens per second)
0.01.770.327 I llama_perf_context_print:       total time =     842.42 ms /    70 tokens
0.01.770.561 I ggml_metal_free: deallocating

real	0m1.791s
user	0m0.111s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.847 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.850 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.850 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.852 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.852 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.853 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.854 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.351 I llama_model_loader: - type  f32:  194 tensors
0.00.024.351 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.351 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.352 I print_info: file format = GGUF V3 (latest)
0.00.024.352 I print_info: file type   = Q5_1
0.00.024.353 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.350 I load: special tokens cache size = 25
0.00.038.310 I load: token to piece cache size = 0.2984 MB
0.00.038.313 I print_info: arch             = gptneox
0.00.038.313 I print_info: vocab_only       = 0
0.00.038.313 I print_info: n_ctx_train      = 2048
0.00.038.313 I print_info: n_embd           = 2048
0.00.038.314 I print_info: n_layer          = 24
0.00.038.317 I print_info: n_head           = 16
0.00.038.318 I print_info: n_head_kv        = 16
0.00.038.318 I print_info: n_rot            = 32
0.00.038.318 I print_info: n_swa            = 0
0.00.038.318 I print_info: n_embd_head_k    = 128
0.00.038.319 I print_info: n_embd_head_v    = 128
0.00.038.319 I print_info: n_gqa            = 1
0.00.038.320 I print_info: n_embd_k_gqa     = 2048
0.00.038.321 I print_info: n_embd_v_gqa     = 2048
0.00.038.321 I print_info: f_norm_eps       = 1.0e-05
0.00.038.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.324 I print_info: f_logit_scale    = 0.0e+00
0.00.038.325 I print_info: n_ff             = 8192
0.00.038.325 I print_info: n_expert         = 0
0.00.038.325 I print_info: n_expert_used    = 0
0.00.038.325 I print_info: causal attn      = 1
0.00.038.325 I print_info: pooling type     = 0
0.00.038.326 I print_info: rope type        = 2
0.00.038.326 I print_info: rope scaling     = linear
0.00.038.326 I print_info: freq_base_train  = 10000.0
0.00.038.326 I print_info: freq_scale_train = 1
0.00.038.327 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.328 I print_info: rope_finetuned   = unknown
0.00.038.328 I print_info: ssm_d_conv       = 0
0.00.038.329 I print_info: ssm_d_inner      = 0
0.00.038.329 I print_info: ssm_d_state      = 0
0.00.038.329 I print_info: ssm_dt_rank      = 0
0.00.038.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.329 I print_info: model type       = 1.4B
0.00.038.330 I print_info: model params     = 1.41 B
0.00.038.330 I print_info: general.name     = 1.4B
0.00.038.330 I print_info: vocab type       = BPE
0.00.038.330 I print_info: n_vocab          = 50304
0.00.038.331 I print_info: n_merges         = 50009
0.00.038.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.336 I print_info: LF token         = 187 'Ċ'
0.00.038.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.336 I print_info: max token length = 1024
0.00.675.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.672 I load_tensors: offloading output layer to GPU
0.00.675.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.699 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.675.701 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.677.114 I llama_init_from_model: n_seq_max     = 1
0.00.677.116 I llama_init_from_model: n_ctx         = 128
0.00.677.116 I llama_init_from_model: n_ctx_per_seq = 128
0.00.677.117 I llama_init_from_model: n_batch       = 128
0.00.677.117 I llama_init_from_model: n_ubatch      = 128
0.00.677.118 I llama_init_from_model: flash_attn    = 0
0.00.677.119 I llama_init_from_model: freq_base     = 10000.0
0.00.677.119 I llama_init_from_model: freq_scale    = 1
0.00.677.120 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.677.121 I ggml_metal_init: allocating
0.00.677.135 I ggml_metal_init: found device: Apple M4
0.00.677.145 I ggml_metal_init: picking default device: Apple M4
0.00.678.477 I ggml_metal_init: using embedded metal library
0.00.684.497 I ggml_metal_init: GPU name:   Apple M4
0.00.684.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.502 I ggml_metal_init: simdgroup reduction   = true
0.00.684.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.503 I ggml_metal_init: has residency sets    = true
0.00.684.503 I ggml_metal_init: has bfloat            = true
0.00.684.503 I ggml_metal_init: use bfloat            = true
0.00.684.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.829 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.704.333 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.704.375 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.670 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.707.672 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.707.673 I llama_init_from_model: graph nodes  = 967
0.00.707.673 I llama_init_from_model: graph splits = 2
0.00.707.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.768 I 
0.00.736.850 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.861 I perplexity: tokenizing the input ..
0.00.743.870 I perplexity: tokenization took 7.009 ms
0.00.743.879 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.926 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.881.467 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.881.486 I llama_perf_context_print:        load time =     727.97 ms
0.00.881.487 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.09 tokens per second)
0.00.881.488 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.881.488 I llama_perf_context_print:       total time =     144.72 ms /   129 tokens
0.00.881.871 I ggml_metal_free: deallocating

real	0m0.896s
user	0m0.079s
sys	0m0.152s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.644 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.502 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.027 I llama_model_loader: - type  f32:  194 tensors
0.00.026.027 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.028 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.029 I print_info: file format = GGUF V3 (latest)
0.00.026.029 I print_info: file type   = Q2_K - Medium
0.00.026.030 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.835 I load: special tokens cache size = 25
0.00.039.893 I load: token to piece cache size = 0.2984 MB
0.00.039.896 I print_info: arch             = gptneox
0.00.039.896 I print_info: vocab_only       = 0
0.00.039.896 I print_info: n_ctx_train      = 2048
0.00.039.896 I print_info: n_embd           = 2048
0.00.039.896 I print_info: n_layer          = 24
0.00.039.899 I print_info: n_head           = 16
0.00.039.900 I print_info: n_head_kv        = 16
0.00.039.900 I print_info: n_rot            = 32
0.00.039.900 I print_info: n_swa            = 0
0.00.039.900 I print_info: n_embd_head_k    = 128
0.00.039.900 I print_info: n_embd_head_v    = 128
0.00.039.901 I print_info: n_gqa            = 1
0.00.039.902 I print_info: n_embd_k_gqa     = 2048
0.00.039.903 I print_info: n_embd_v_gqa     = 2048
0.00.039.903 I print_info: f_norm_eps       = 1.0e-05
0.00.039.904 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.904 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.904 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.904 I print_info: f_logit_scale    = 0.0e+00
0.00.039.905 I print_info: n_ff             = 8192
0.00.039.905 I print_info: n_expert         = 0
0.00.039.905 I print_info: n_expert_used    = 0
0.00.039.905 I print_info: causal attn      = 1
0.00.039.905 I print_info: pooling type     = 0
0.00.039.905 I print_info: rope type        = 2
0.00.039.907 I print_info: rope scaling     = linear
0.00.039.909 I print_info: freq_base_train  = 10000.0
0.00.039.909 I print_info: freq_scale_train = 1
0.00.039.909 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.909 I print_info: rope_finetuned   = unknown
0.00.039.910 I print_info: ssm_d_conv       = 0
0.00.039.910 I print_info: ssm_d_inner      = 0
0.00.039.910 I print_info: ssm_d_state      = 0
0.00.039.910 I print_info: ssm_dt_rank      = 0
0.00.039.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.910 I print_info: model type       = 1.4B
0.00.039.911 I print_info: model params     = 1.41 B
0.00.039.911 I print_info: general.name     = 1.4B
0.00.039.911 I print_info: vocab type       = BPE
0.00.039.912 I print_info: n_vocab          = 50304
0.00.039.912 I print_info: n_merges         = 50009
0.00.039.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.914 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.914 I print_info: LF token         = 187 'Ċ'
0.00.039.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.915 I print_info: max token length = 1024
0.00.354.740 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.757 I load_tensors: offloading output layer to GPU
0.00.354.757 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.796 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.797 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.356.221 I llama_init_from_model: n_seq_max     = 1
0.00.356.227 I llama_init_from_model: n_ctx         = 2048
0.00.356.227 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.356.228 I llama_init_from_model: n_batch       = 2048
0.00.356.228 I llama_init_from_model: n_ubatch      = 512
0.00.356.228 I llama_init_from_model: flash_attn    = 0
0.00.356.230 I llama_init_from_model: freq_base     = 10000.0
0.00.356.233 I llama_init_from_model: freq_scale    = 1
0.00.356.241 I ggml_metal_init: allocating
0.00.356.345 I ggml_metal_init: found device: Apple M4
0.00.356.360 I ggml_metal_init: picking default device: Apple M4
0.00.358.299 I ggml_metal_init: using embedded metal library
0.00.363.800 I ggml_metal_init: GPU name:   Apple M4
0.00.363.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.817 I ggml_metal_init: simdgroup reduction   = true
0.00.363.817 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.818 I ggml_metal_init: has residency sets    = true
0.00.363.818 I ggml_metal_init: has bfloat            = true
0.00.363.818 I ggml_metal_init: use bfloat            = true
0.00.363.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.213 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.914 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.444.923 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.444.978 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.449.174 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.449.176 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.449.176 I llama_init_from_model: graph nodes  = 967
0.00.449.177 I llama_init_from_model: graph splits = 2
0.00.449.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.449.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.449.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.828 I main: llama threadpool init, n_threads = 4
0.00.506.876 I 
0.00.506.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.899 I 
0.00.507.070 I sampler seed: 1234
0.00.507.075 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.507.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.507.153 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.507.153 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.178.640 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.178.641 I llama_perf_context_print:        load time =     495.48 ms
0.01.178.642 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.53 tokens per second)
0.01.178.643 I llama_perf_context_print:        eval time =     632.94 ms /    63 runs   (   10.05 ms per token,    99.54 tokens per second)
0.01.178.643 I llama_perf_context_print:       total time =     672.52 ms /    70 tokens
0.01.178.875 I ggml_metal_free: deallocating

real	0m1.199s
user	0m0.112s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.995 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.004 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.006 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.798 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.649 I llama_model_loader: - type  f32:  194 tensors
0.00.025.649 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.649 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.649 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.650 I print_info: file format = GGUF V3 (latest)
0.00.025.651 I print_info: file type   = Q2_K - Medium
0.00.025.652 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.346 I load: special tokens cache size = 25
0.00.039.396 I load: token to piece cache size = 0.2984 MB
0.00.039.399 I print_info: arch             = gptneox
0.00.039.399 I print_info: vocab_only       = 0
0.00.039.400 I print_info: n_ctx_train      = 2048
0.00.039.400 I print_info: n_embd           = 2048
0.00.039.400 I print_info: n_layer          = 24
0.00.039.403 I print_info: n_head           = 16
0.00.039.404 I print_info: n_head_kv        = 16
0.00.039.404 I print_info: n_rot            = 32
0.00.039.404 I print_info: n_swa            = 0
0.00.039.405 I print_info: n_embd_head_k    = 128
0.00.039.405 I print_info: n_embd_head_v    = 128
0.00.039.405 I print_info: n_gqa            = 1
0.00.039.406 I print_info: n_embd_k_gqa     = 2048
0.00.039.407 I print_info: n_embd_v_gqa     = 2048
0.00.039.408 I print_info: f_norm_eps       = 1.0e-05
0.00.039.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.409 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.409 I print_info: f_logit_scale    = 0.0e+00
0.00.039.409 I print_info: n_ff             = 8192
0.00.039.410 I print_info: n_expert         = 0
0.00.039.410 I print_info: n_expert_used    = 0
0.00.039.410 I print_info: causal attn      = 1
0.00.039.410 I print_info: pooling type     = 0
0.00.039.410 I print_info: rope type        = 2
0.00.039.411 I print_info: rope scaling     = linear
0.00.039.411 I print_info: freq_base_train  = 10000.0
0.00.039.411 I print_info: freq_scale_train = 1
0.00.039.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.412 I print_info: rope_finetuned   = unknown
0.00.039.412 I print_info: ssm_d_conv       = 0
0.00.039.412 I print_info: ssm_d_inner      = 0
0.00.039.412 I print_info: ssm_d_state      = 0
0.00.039.412 I print_info: ssm_dt_rank      = 0
0.00.039.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.415 I print_info: model type       = 1.4B
0.00.039.416 I print_info: model params     = 1.41 B
0.00.039.416 I print_info: general.name     = 1.4B
0.00.039.416 I print_info: vocab type       = BPE
0.00.039.417 I print_info: n_vocab          = 50304
0.00.039.417 I print_info: n_merges         = 50009
0.00.039.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.417 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.422 I print_info: LF token         = 187 'Ċ'
0.00.039.422 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.422 I print_info: max token length = 1024
0.00.342.678 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.695 I load_tensors: offloading output layer to GPU
0.00.342.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.731 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.733 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.873 I llama_init_from_model: n_seq_max     = 1
0.00.343.881 I llama_init_from_model: n_ctx         = 128
0.00.343.885 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.886 I llama_init_from_model: n_batch       = 128
0.00.343.886 I llama_init_from_model: n_ubatch      = 128
0.00.343.887 I llama_init_from_model: flash_attn    = 0
0.00.343.888 I llama_init_from_model: freq_base     = 10000.0
0.00.343.889 I llama_init_from_model: freq_scale    = 1
0.00.343.889 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.891 I ggml_metal_init: allocating
0.00.343.995 I ggml_metal_init: found device: Apple M4
0.00.344.009 I ggml_metal_init: picking default device: Apple M4
0.00.345.867 I ggml_metal_init: using embedded metal library
0.00.351.287 I ggml_metal_init: GPU name:   Apple M4
0.00.351.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.310 I ggml_metal_init: simdgroup reduction   = true
0.00.351.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.310 I ggml_metal_init: has residency sets    = true
0.00.351.310 I ggml_metal_init: has bfloat            = true
0.00.351.311 I ggml_metal_init: use bfloat            = true
0.00.351.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.319 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.648 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.302 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.376.314 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.376.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.379.734 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.379.736 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.379.737 I llama_init_from_model: graph nodes  = 967
0.00.379.738 I llama_init_from_model: graph splits = 2
0.00.379.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.379.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.026 I 
0.00.413.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.121 I perplexity: tokenizing the input ..
0.00.419.769 I perplexity: tokenization took 6.647 ms
0.00.419.774 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.561.837 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.356 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.378 I llama_perf_context_print:        load time =     403.02 ms
0.00.563.378 I llama_perf_context_print: prompt eval time =     141.83 ms /   128 tokens (    1.11 ms per token,   902.46 tokens per second)
0.00.563.379 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.380 I llama_perf_context_print:       total time =     150.36 ms /   129 tokens
0.00.563.745 I ggml_metal_free: deallocating

real	0m0.580s
user	0m0.080s
sys	0m0.091s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.757 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.456 I llama_model_loader: - type  f32:  194 tensors
0.00.025.456 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.457 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.457 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.458 I print_info: file type   = Q3_K - Medium
0.00.025.459 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.666 I load: special tokens cache size = 25
0.00.039.607 I load: token to piece cache size = 0.2984 MB
0.00.039.610 I print_info: arch             = gptneox
0.00.039.610 I print_info: vocab_only       = 0
0.00.039.610 I print_info: n_ctx_train      = 2048
0.00.039.611 I print_info: n_embd           = 2048
0.00.039.611 I print_info: n_layer          = 24
0.00.039.614 I print_info: n_head           = 16
0.00.039.614 I print_info: n_head_kv        = 16
0.00.039.615 I print_info: n_rot            = 32
0.00.039.615 I print_info: n_swa            = 0
0.00.039.615 I print_info: n_embd_head_k    = 128
0.00.039.615 I print_info: n_embd_head_v    = 128
0.00.039.616 I print_info: n_gqa            = 1
0.00.039.617 I print_info: n_embd_k_gqa     = 2048
0.00.039.618 I print_info: n_embd_v_gqa     = 2048
0.00.039.618 I print_info: f_norm_eps       = 1.0e-05
0.00.039.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.619 I print_info: f_logit_scale    = 0.0e+00
0.00.039.620 I print_info: n_ff             = 8192
0.00.039.620 I print_info: n_expert         = 0
0.00.039.620 I print_info: n_expert_used    = 0
0.00.039.620 I print_info: causal attn      = 1
0.00.039.621 I print_info: pooling type     = 0
0.00.039.621 I print_info: rope type        = 2
0.00.039.621 I print_info: rope scaling     = linear
0.00.039.621 I print_info: freq_base_train  = 10000.0
0.00.039.622 I print_info: freq_scale_train = 1
0.00.039.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.622 I print_info: rope_finetuned   = unknown
0.00.039.622 I print_info: ssm_d_conv       = 0
0.00.039.622 I print_info: ssm_d_inner      = 0
0.00.039.623 I print_info: ssm_d_state      = 0
0.00.039.623 I print_info: ssm_dt_rank      = 0
0.00.039.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.623 I print_info: model type       = 1.4B
0.00.039.626 I print_info: model params     = 1.41 B
0.00.039.626 I print_info: general.name     = 1.4B
0.00.039.626 I print_info: vocab type       = BPE
0.00.039.627 I print_info: n_vocab          = 50304
0.00.039.627 I print_info: n_merges         = 50009
0.00.039.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: LF token         = 187 'Ċ'
0.00.039.628 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: max token length = 1024
0.00.450.586 I load_tensors: offloading 24 repeating layers to GPU
0.00.450.599 I load_tensors: offloading output layer to GPU
0.00.450.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.627 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.629 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.451.951 I llama_init_from_model: n_seq_max     = 1
0.00.451.959 I llama_init_from_model: n_ctx         = 2048
0.00.451.959 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.451.960 I llama_init_from_model: n_batch       = 2048
0.00.451.960 I llama_init_from_model: n_ubatch      = 512
0.00.451.961 I llama_init_from_model: flash_attn    = 0
0.00.451.962 I llama_init_from_model: freq_base     = 10000.0
0.00.451.962 I llama_init_from_model: freq_scale    = 1
0.00.451.965 I ggml_metal_init: allocating
0.00.452.016 I ggml_metal_init: found device: Apple M4
0.00.452.025 I ggml_metal_init: picking default device: Apple M4
0.00.453.857 I ggml_metal_init: using embedded metal library
0.00.459.567 I ggml_metal_init: GPU name:   Apple M4
0.00.459.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.459.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.459.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.459.575 I ggml_metal_init: simdgroup reduction   = true
0.00.459.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.459.576 I ggml_metal_init: has residency sets    = true
0.00.459.576 I ggml_metal_init: has bfloat            = true
0.00.459.576 I ggml_metal_init: use bfloat            = true
0.00.459.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.459.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.444 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.536.427 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.536.433 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.536.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.540.610 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.540.612 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.540.612 I llama_init_from_model: graph nodes  = 967
0.00.540.612 I llama_init_from_model: graph splits = 2
0.00.540.618 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.540.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.540.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.907 I main: llama threadpool init, n_threads = 4
0.00.600.948 I 
0.00.600.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.971 I 
0.00.601.125 I sampler seed: 1234
0.00.601.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.160 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.163 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.347.240 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.01.347.241 I llama_perf_context_print:        load time =     591.31 ms
0.01.347.242 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.81 tokens per second)
0.01.347.243 I llama_perf_context_print:        eval time =     693.11 ms /    63 runs   (   11.00 ms per token,    90.90 tokens per second)
0.01.347.244 I llama_perf_context_print:       total time =     747.02 ms /    70 tokens
0.01.347.442 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.990 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.235 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.235 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.236 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.236 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.243 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.998 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.656 I llama_model_loader: - type  f32:  194 tensors
0.00.024.656 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.656 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.657 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.657 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.658 I print_info: file format = GGUF V3 (latest)
0.00.024.659 I print_info: file type   = Q3_K - Medium
0.00.024.660 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.733 I load: special tokens cache size = 25
0.00.038.718 I load: token to piece cache size = 0.2984 MB
0.00.038.720 I print_info: arch             = gptneox
0.00.038.721 I print_info: vocab_only       = 0
0.00.038.721 I print_info: n_ctx_train      = 2048
0.00.038.721 I print_info: n_embd           = 2048
0.00.038.721 I print_info: n_layer          = 24
0.00.038.725 I print_info: n_head           = 16
0.00.038.725 I print_info: n_head_kv        = 16
0.00.038.725 I print_info: n_rot            = 32
0.00.038.726 I print_info: n_swa            = 0
0.00.038.726 I print_info: n_embd_head_k    = 128
0.00.038.726 I print_info: n_embd_head_v    = 128
0.00.038.728 I print_info: n_gqa            = 1
0.00.038.729 I print_info: n_embd_k_gqa     = 2048
0.00.038.730 I print_info: n_embd_v_gqa     = 2048
0.00.038.730 I print_info: f_norm_eps       = 1.0e-05
0.00.038.736 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.740 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.740 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.741 I print_info: f_logit_scale    = 0.0e+00
0.00.038.743 I print_info: n_ff             = 8192
0.00.038.743 I print_info: n_expert         = 0
0.00.038.743 I print_info: n_expert_used    = 0
0.00.038.743 I print_info: causal attn      = 1
0.00.038.743 I print_info: pooling type     = 0
0.00.038.743 I print_info: rope type        = 2
0.00.038.744 I print_info: rope scaling     = linear
0.00.038.745 I print_info: freq_base_train  = 10000.0
0.00.038.745 I print_info: freq_scale_train = 1
0.00.038.745 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.745 I print_info: rope_finetuned   = unknown
0.00.038.745 I print_info: ssm_d_conv       = 0
0.00.038.746 I print_info: ssm_d_inner      = 0
0.00.038.746 I print_info: ssm_d_state      = 0
0.00.038.746 I print_info: ssm_dt_rank      = 0
0.00.038.746 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.746 I print_info: model type       = 1.4B
0.00.038.747 I print_info: model params     = 1.41 B
0.00.038.747 I print_info: general.name     = 1.4B
0.00.038.748 I print_info: vocab type       = BPE
0.00.038.748 I print_info: n_vocab          = 50304
0.00.038.748 I print_info: n_merges         = 50009
0.00.038.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: LF token         = 187 'Ċ'
0.00.038.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: max token length = 1024
0.00.435.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.873 I load_tensors: offloading output layer to GPU
0.00.435.873 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.908 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.909 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.437.299 I llama_init_from_model: n_seq_max     = 1
0.00.437.304 I llama_init_from_model: n_ctx         = 128
0.00.437.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.305 I llama_init_from_model: n_batch       = 128
0.00.437.305 I llama_init_from_model: n_ubatch      = 128
0.00.437.306 I llama_init_from_model: flash_attn    = 0
0.00.437.308 I llama_init_from_model: freq_base     = 10000.0
0.00.437.308 I llama_init_from_model: freq_scale    = 1
0.00.437.309 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.310 I ggml_metal_init: allocating
0.00.437.392 I ggml_metal_init: found device: Apple M4
0.00.437.407 I ggml_metal_init: picking default device: Apple M4
0.00.439.191 I ggml_metal_init: using embedded metal library
0.00.444.669 I ggml_metal_init: GPU name:   Apple M4
0.00.444.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.688 I ggml_metal_init: simdgroup reduction   = true
0.00.444.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.688 I ggml_metal_init: has residency sets    = true
0.00.444.688 I ggml_metal_init: has bfloat            = true
0.00.444.689 I ggml_metal_init: use bfloat            = true
0.00.444.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.619 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.142 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.469.146 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.186 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.482 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.484 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.484 I llama_init_from_model: graph nodes  = 967
0.00.472.485 I llama_init_from_model: graph splits = 2
0.00.472.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.996 I 
0.00.502.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.092 I perplexity: tokenizing the input ..
0.00.508.538 I perplexity: tokenization took 6.445 ms
0.00.508.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.647.892 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.614 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.637 I llama_perf_context_print:        load time =     493.00 ms
0.00.649.638 I llama_perf_context_print: prompt eval time =     139.06 ms /   128 tokens (    1.09 ms per token,   920.48 tokens per second)
0.00.649.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.639 I llama_perf_context_print:       total time =     147.65 ms /   129 tokens
0.00.649.977 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.080s
sys	0m0.102s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.053 I llama_model_loader: - type  f32:  194 tensors
0.00.025.054 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.054 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.054 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.055 I print_info: file format = GGUF V3 (latest)
0.00.025.055 I print_info: file type   = Q4_K - Medium
0.00.025.056 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.784 I load: special tokens cache size = 25
0.00.038.840 I load: token to piece cache size = 0.2984 MB
0.00.038.842 I print_info: arch             = gptneox
0.00.038.843 I print_info: vocab_only       = 0
0.00.038.843 I print_info: n_ctx_train      = 2048
0.00.038.843 I print_info: n_embd           = 2048
0.00.038.843 I print_info: n_layer          = 24
0.00.038.846 I print_info: n_head           = 16
0.00.038.847 I print_info: n_head_kv        = 16
0.00.038.847 I print_info: n_rot            = 32
0.00.038.847 I print_info: n_swa            = 0
0.00.038.847 I print_info: n_embd_head_k    = 128
0.00.038.848 I print_info: n_embd_head_v    = 128
0.00.038.849 I print_info: n_gqa            = 1
0.00.038.850 I print_info: n_embd_k_gqa     = 2048
0.00.038.850 I print_info: n_embd_v_gqa     = 2048
0.00.038.851 I print_info: f_norm_eps       = 1.0e-05
0.00.038.852 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.852 I print_info: f_logit_scale    = 0.0e+00
0.00.038.853 I print_info: n_ff             = 8192
0.00.038.853 I print_info: n_expert         = 0
0.00.038.854 I print_info: n_expert_used    = 0
0.00.038.854 I print_info: causal attn      = 1
0.00.038.854 I print_info: pooling type     = 0
0.00.038.854 I print_info: rope type        = 2
0.00.038.856 I print_info: rope scaling     = linear
0.00.038.856 I print_info: freq_base_train  = 10000.0
0.00.038.856 I print_info: freq_scale_train = 1
0.00.038.857 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.857 I print_info: rope_finetuned   = unknown
0.00.038.857 I print_info: ssm_d_conv       = 0
0.00.038.857 I print_info: ssm_d_inner      = 0
0.00.038.857 I print_info: ssm_d_state      = 0
0.00.038.857 I print_info: ssm_dt_rank      = 0
0.00.038.857 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.858 I print_info: model type       = 1.4B
0.00.038.858 I print_info: model params     = 1.41 B
0.00.038.858 I print_info: general.name     = 1.4B
0.00.038.859 I print_info: vocab type       = BPE
0.00.038.859 I print_info: n_vocab          = 50304
0.00.038.859 I print_info: n_merges         = 50009
0.00.038.859 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.860 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.860 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.860 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.860 I print_info: LF token         = 187 'Ċ'
0.00.038.860 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.861 I print_info: max token length = 1024
0.00.530.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.530.930 I load_tensors: offloading output layer to GPU
0.00.530.931 I load_tensors: offloaded 25/25 layers to GPU
0.00.530.967 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.530.968 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.532.365 I llama_init_from_model: n_seq_max     = 1
0.00.532.370 I llama_init_from_model: n_ctx         = 2048
0.00.532.370 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.532.371 I llama_init_from_model: n_batch       = 2048
0.00.532.371 I llama_init_from_model: n_ubatch      = 512
0.00.532.372 I llama_init_from_model: flash_attn    = 0
0.00.532.374 I llama_init_from_model: freq_base     = 10000.0
0.00.532.374 I llama_init_from_model: freq_scale    = 1
0.00.532.377 I ggml_metal_init: allocating
0.00.532.421 I ggml_metal_init: found device: Apple M4
0.00.532.434 I ggml_metal_init: picking default device: Apple M4
0.00.534.243 I ggml_metal_init: using embedded metal library
0.00.540.759 I ggml_metal_init: GPU name:   Apple M4
0.00.540.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.540.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.540.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.540.767 I ggml_metal_init: simdgroup reduction   = true
0.00.540.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.540.767 I ggml_metal_init: has residency sets    = true
0.00.540.768 I ggml_metal_init: has bfloat            = true
0.00.540.768 I ggml_metal_init: use bfloat            = true
0.00.540.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.540.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.342 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.613.306 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.613.312 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.613.357 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.617.842 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.617.844 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.617.844 I llama_init_from_model: graph nodes  = 967
0.00.617.844 I llama_init_from_model: graph splits = 2
0.00.617.851 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.617.974 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.617.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.757 I main: llama threadpool init, n_threads = 4
0.00.675.811 I 
0.00.675.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.835 I 
0.00.675.991 I sampler seed: 1234
0.00.675.995 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.014 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.015 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.015 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.444.030 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49511.85 tokens per second)
0.01.444.031 I llama_perf_context_print:        load time =     666.25 ms
0.01.444.032 I llama_perf_context_print: prompt eval time =      58.33 ms /     7 tokens (    8.33 ms per token,   120.01 tokens per second)
0.01.444.032 I llama_perf_context_print:        eval time =     706.61 ms /    63 runs   (   11.22 ms per token,    89.16 tokens per second)
0.01.444.034 I llama_perf_context_print:       total time =     768.97 ms /    70 tokens
0.01.444.257 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.735 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.476 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.264 I llama_model_loader: - type  f32:  194 tensors
0.00.024.264 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.264 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.265 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.265 I print_info: file format = GGUF V3 (latest)
0.00.024.266 I print_info: file type   = Q4_K - Medium
0.00.024.269 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.020 I load: special tokens cache size = 25
0.00.037.966 I load: token to piece cache size = 0.2984 MB
0.00.037.969 I print_info: arch             = gptneox
0.00.037.969 I print_info: vocab_only       = 0
0.00.037.969 I print_info: n_ctx_train      = 2048
0.00.037.970 I print_info: n_embd           = 2048
0.00.037.970 I print_info: n_layer          = 24
0.00.037.973 I print_info: n_head           = 16
0.00.037.974 I print_info: n_head_kv        = 16
0.00.037.974 I print_info: n_rot            = 32
0.00.037.974 I print_info: n_swa            = 0
0.00.037.974 I print_info: n_embd_head_k    = 128
0.00.037.974 I print_info: n_embd_head_v    = 128
0.00.037.975 I print_info: n_gqa            = 1
0.00.037.976 I print_info: n_embd_k_gqa     = 2048
0.00.037.978 I print_info: n_embd_v_gqa     = 2048
0.00.037.979 I print_info: f_norm_eps       = 1.0e-05
0.00.037.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.979 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.979 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.979 I print_info: f_logit_scale    = 0.0e+00
0.00.037.980 I print_info: n_ff             = 8192
0.00.037.980 I print_info: n_expert         = 0
0.00.037.980 I print_info: n_expert_used    = 0
0.00.037.981 I print_info: causal attn      = 1
0.00.037.981 I print_info: pooling type     = 0
0.00.037.981 I print_info: rope type        = 2
0.00.037.981 I print_info: rope scaling     = linear
0.00.037.982 I print_info: freq_base_train  = 10000.0
0.00.037.982 I print_info: freq_scale_train = 1
0.00.037.982 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.982 I print_info: rope_finetuned   = unknown
0.00.037.982 I print_info: ssm_d_conv       = 0
0.00.037.983 I print_info: ssm_d_inner      = 0
0.00.037.984 I print_info: ssm_d_state      = 0
0.00.037.984 I print_info: ssm_dt_rank      = 0
0.00.037.984 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.984 I print_info: model type       = 1.4B
0.00.037.985 I print_info: model params     = 1.41 B
0.00.037.985 I print_info: general.name     = 1.4B
0.00.037.985 I print_info: vocab type       = BPE
0.00.037.986 I print_info: n_vocab          = 50304
0.00.037.986 I print_info: n_merges         = 50009
0.00.037.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.990 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.990 I print_info: LF token         = 187 'Ċ'
0.00.037.991 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.991 I print_info: max token length = 1024
0.00.542.089 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.106 I load_tensors: offloading output layer to GPU
0.00.542.107 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.141 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.142 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.543.782 I llama_init_from_model: n_seq_max     = 1
0.00.543.787 I llama_init_from_model: n_ctx         = 128
0.00.543.787 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.788 I llama_init_from_model: n_batch       = 128
0.00.543.788 I llama_init_from_model: n_ubatch      = 128
0.00.543.789 I llama_init_from_model: flash_attn    = 0
0.00.543.791 I llama_init_from_model: freq_base     = 10000.0
0.00.543.791 I llama_init_from_model: freq_scale    = 1
0.00.543.792 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.794 I ggml_metal_init: allocating
0.00.543.875 I ggml_metal_init: found device: Apple M4
0.00.543.889 I ggml_metal_init: picking default device: Apple M4
0.00.545.669 I ggml_metal_init: using embedded metal library
0.00.552.434 I ggml_metal_init: GPU name:   Apple M4
0.00.552.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.441 I ggml_metal_init: simdgroup reduction   = true
0.00.552.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.441 I ggml_metal_init: has residency sets    = true
0.00.552.442 I ggml_metal_init: has bfloat            = true
0.00.552.442 I ggml_metal_init: use bfloat            = true
0.00.552.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.363 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.573.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.573.971 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.574.019 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.296 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.298 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.298 I llama_init_from_model: graph nodes  = 967
0.00.577.299 I llama_init_from_model: graph splits = 2
0.00.577.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.841 I 
0.00.606.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.931 I perplexity: tokenizing the input ..
0.00.613.959 I perplexity: tokenization took 7.025 ms
0.00.613.965 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.591 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.760.135 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.760.165 I llama_perf_context_print:        load time =     597.92 ms
0.00.760.167 I llama_perf_context_print: prompt eval time =     143.66 ms /   128 tokens (    1.12 ms per token,   890.96 tokens per second)
0.00.760.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.168 I llama_perf_context_print:       total time =     153.33 ms /   129 tokens
0.00.760.569 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.081s
sys	0m0.139s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.776 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.603 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.458 I llama_model_loader: - type  f32:  194 tensors
0.00.027.458 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.458 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.459 I print_info: file format = GGUF V3 (latest)
0.00.027.460 I print_info: file type   = Q5_K - Medium
0.00.027.460 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.530 I load: special tokens cache size = 25
0.00.041.689 I load: token to piece cache size = 0.2984 MB
0.00.041.692 I print_info: arch             = gptneox
0.00.041.692 I print_info: vocab_only       = 0
0.00.041.692 I print_info: n_ctx_train      = 2048
0.00.041.692 I print_info: n_embd           = 2048
0.00.041.693 I print_info: n_layer          = 24
0.00.041.695 I print_info: n_head           = 16
0.00.041.696 I print_info: n_head_kv        = 16
0.00.041.696 I print_info: n_rot            = 32
0.00.041.698 I print_info: n_swa            = 0
0.00.041.698 I print_info: n_embd_head_k    = 128
0.00.041.698 I print_info: n_embd_head_v    = 128
0.00.041.699 I print_info: n_gqa            = 1
0.00.041.700 I print_info: n_embd_k_gqa     = 2048
0.00.041.700 I print_info: n_embd_v_gqa     = 2048
0.00.041.701 I print_info: f_norm_eps       = 1.0e-05
0.00.041.701 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.702 I print_info: f_logit_scale    = 0.0e+00
0.00.041.702 I print_info: n_ff             = 8192
0.00.041.703 I print_info: n_expert         = 0
0.00.041.703 I print_info: n_expert_used    = 0
0.00.041.703 I print_info: causal attn      = 1
0.00.041.703 I print_info: pooling type     = 0
0.00.041.705 I print_info: rope type        = 2
0.00.041.706 I print_info: rope scaling     = linear
0.00.041.706 I print_info: freq_base_train  = 10000.0
0.00.041.707 I print_info: freq_scale_train = 1
0.00.041.707 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.707 I print_info: rope_finetuned   = unknown
0.00.041.707 I print_info: ssm_d_conv       = 0
0.00.041.707 I print_info: ssm_d_inner      = 0
0.00.041.708 I print_info: ssm_d_state      = 0
0.00.041.708 I print_info: ssm_dt_rank      = 0
0.00.041.708 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.708 I print_info: model type       = 1.4B
0.00.041.708 I print_info: model params     = 1.41 B
0.00.041.708 I print_info: general.name     = 1.4B
0.00.041.709 I print_info: vocab type       = BPE
0.00.041.709 I print_info: n_vocab          = 50304
0.00.041.709 I print_info: n_merges         = 50009
0.00.041.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.711 I print_info: LF token         = 187 'Ċ'
0.00.041.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.711 I print_info: max token length = 1024
0.00.585.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.349 I load_tensors: offloading output layer to GPU
0.00.585.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.383 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.585.385 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.586.906 I llama_init_from_model: n_seq_max     = 1
0.00.586.909 I llama_init_from_model: n_ctx         = 2048
0.00.586.910 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.910 I llama_init_from_model: n_batch       = 2048
0.00.586.910 I llama_init_from_model: n_ubatch      = 512
0.00.586.911 I llama_init_from_model: flash_attn    = 0
0.00.586.912 I llama_init_from_model: freq_base     = 10000.0
0.00.586.913 I llama_init_from_model: freq_scale    = 1
0.00.586.917 I ggml_metal_init: allocating
0.00.586.940 I ggml_metal_init: found device: Apple M4
0.00.586.954 I ggml_metal_init: picking default device: Apple M4
0.00.588.472 I ggml_metal_init: using embedded metal library
0.00.594.593 I ggml_metal_init: GPU name:   Apple M4
0.00.594.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.600 I ggml_metal_init: simdgroup reduction   = true
0.00.594.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.600 I ggml_metal_init: has residency sets    = true
0.00.594.601 I ggml_metal_init: has bfloat            = true
0.00.594.601 I ggml_metal_init: use bfloat            = true
0.00.594.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.605 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.124 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.664.652 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.664.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.748 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.668.750 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.668.750 I llama_init_from_model: graph nodes  = 967
0.00.668.750 I llama_init_from_model: graph splits = 2
0.00.668.755 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.668.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.668.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.618 I main: llama threadpool init, n_threads = 4
0.00.732.659 I 
0.00.732.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.682 I 
0.00.732.851 I sampler seed: 1234
0.00.732.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.899 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.903 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.584.785 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48398.09 tokens per second)
0.01.584.786 I llama_perf_context_print:        load time =     720.96 ms
0.01.584.787 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.72 tokens per second)
0.01.584.787 I llama_perf_context_print:        eval time =     797.82 ms /    63 runs   (   12.66 ms per token,    78.97 tokens per second)
0.01.584.788 I llama_perf_context_print:       total time =     852.91 ms /    70 tokens
0.01.585.094 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.215 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.215 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.216 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.876 I llama_model_loader: - type  f32:  194 tensors
0.00.025.876 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.876 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.877 I print_info: file format = GGUF V3 (latest)
0.00.025.877 I print_info: file type   = Q5_K - Medium
0.00.025.878 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.013 I load: special tokens cache size = 25
0.00.040.071 I load: token to piece cache size = 0.2984 MB
0.00.040.074 I print_info: arch             = gptneox
0.00.040.075 I print_info: vocab_only       = 0
0.00.040.075 I print_info: n_ctx_train      = 2048
0.00.040.075 I print_info: n_embd           = 2048
0.00.040.075 I print_info: n_layer          = 24
0.00.040.079 I print_info: n_head           = 16
0.00.040.080 I print_info: n_head_kv        = 16
0.00.040.080 I print_info: n_rot            = 32
0.00.040.080 I print_info: n_swa            = 0
0.00.040.080 I print_info: n_embd_head_k    = 128
0.00.040.080 I print_info: n_embd_head_v    = 128
0.00.040.081 I print_info: n_gqa            = 1
0.00.040.082 I print_info: n_embd_k_gqa     = 2048
0.00.040.082 I print_info: n_embd_v_gqa     = 2048
0.00.040.083 I print_info: f_norm_eps       = 1.0e-05
0.00.040.083 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.083 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.084 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.084 I print_info: f_logit_scale    = 0.0e+00
0.00.040.084 I print_info: n_ff             = 8192
0.00.040.085 I print_info: n_expert         = 0
0.00.040.085 I print_info: n_expert_used    = 0
0.00.040.085 I print_info: causal attn      = 1
0.00.040.085 I print_info: pooling type     = 0
0.00.040.085 I print_info: rope type        = 2
0.00.040.086 I print_info: rope scaling     = linear
0.00.040.086 I print_info: freq_base_train  = 10000.0
0.00.040.086 I print_info: freq_scale_train = 1
0.00.040.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.087 I print_info: rope_finetuned   = unknown
0.00.040.087 I print_info: ssm_d_conv       = 0
0.00.040.089 I print_info: ssm_d_inner      = 0
0.00.040.089 I print_info: ssm_d_state      = 0
0.00.040.090 I print_info: ssm_dt_rank      = 0
0.00.040.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.090 I print_info: model type       = 1.4B
0.00.040.090 I print_info: model params     = 1.41 B
0.00.040.090 I print_info: general.name     = 1.4B
0.00.040.091 I print_info: vocab type       = BPE
0.00.040.091 I print_info: n_vocab          = 50304
0.00.040.091 I print_info: n_merges         = 50009
0.00.040.092 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: LF token         = 187 'Ċ'
0.00.040.093 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.093 I print_info: max token length = 1024
0.00.584.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.083 I load_tensors: offloading output layer to GPU
0.00.584.084 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.116 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.117 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.585.564 I llama_init_from_model: n_seq_max     = 1
0.00.585.569 I llama_init_from_model: n_ctx         = 128
0.00.585.570 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.571 I llama_init_from_model: n_batch       = 128
0.00.585.571 I llama_init_from_model: n_ubatch      = 128
0.00.585.572 I llama_init_from_model: flash_attn    = 0
0.00.585.574 I llama_init_from_model: freq_base     = 10000.0
0.00.585.574 I llama_init_from_model: freq_scale    = 1
0.00.585.575 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.581 I ggml_metal_init: allocating
0.00.585.656 I ggml_metal_init: found device: Apple M4
0.00.585.670 I ggml_metal_init: picking default device: Apple M4
0.00.587.441 I ggml_metal_init: using embedded metal library
0.00.593.860 I ggml_metal_init: GPU name:   Apple M4
0.00.593.864 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.866 I ggml_metal_init: simdgroup reduction   = true
0.00.593.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.867 I ggml_metal_init: has residency sets    = true
0.00.593.867 I ggml_metal_init: has bfloat            = true
0.00.593.867 I ggml_metal_init: use bfloat            = true
0.00.593.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.769 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.330 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.336 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.670 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.618.672 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.618.672 I llama_init_from_model: graph nodes  = 967
0.00.618.673 I llama_init_from_model: graph splits = 2
0.00.618.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.131 I 
0.00.651.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.234 I perplexity: tokenizing the input ..
0.00.658.440 I perplexity: tokenization took 7.203 ms
0.00.658.448 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.241 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.773 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.794 I llama_perf_context_print:        load time =     641.13 ms
0.00.801.795 I llama_perf_context_print: prompt eval time =     140.89 ms /   128 tokens (    1.10 ms per token,   908.51 tokens per second)
0.00.801.796 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.796 I llama_perf_context_print:       total time =     150.67 ms /   129 tokens
0.00.802.170 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.081s
sys	0m0.132s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.651 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.451 I llama_model_loader: - type  f32:  194 tensors
0.00.025.451 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.451 I print_info: file format = GGUF V3 (latest)
0.00.025.452 I print_info: file type   = Q6_K
0.00.025.453 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.628 I load: special tokens cache size = 25
0.00.039.649 I load: token to piece cache size = 0.2984 MB
0.00.039.652 I print_info: arch             = gptneox
0.00.039.653 I print_info: vocab_only       = 0
0.00.039.653 I print_info: n_ctx_train      = 2048
0.00.039.653 I print_info: n_embd           = 2048
0.00.039.653 I print_info: n_layer          = 24
0.00.039.656 I print_info: n_head           = 16
0.00.039.657 I print_info: n_head_kv        = 16
0.00.039.657 I print_info: n_rot            = 32
0.00.039.657 I print_info: n_swa            = 0
0.00.039.658 I print_info: n_embd_head_k    = 128
0.00.039.658 I print_info: n_embd_head_v    = 128
0.00.039.661 I print_info: n_gqa            = 1
0.00.039.662 I print_info: n_embd_k_gqa     = 2048
0.00.039.663 I print_info: n_embd_v_gqa     = 2048
0.00.039.664 I print_info: f_norm_eps       = 1.0e-05
0.00.039.665 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.665 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.665 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.665 I print_info: f_logit_scale    = 0.0e+00
0.00.039.666 I print_info: n_ff             = 8192
0.00.039.666 I print_info: n_expert         = 0
0.00.039.666 I print_info: n_expert_used    = 0
0.00.039.666 I print_info: causal attn      = 1
0.00.039.666 I print_info: pooling type     = 0
0.00.039.667 I print_info: rope type        = 2
0.00.039.668 I print_info: rope scaling     = linear
0.00.039.669 I print_info: freq_base_train  = 10000.0
0.00.039.669 I print_info: freq_scale_train = 1
0.00.039.669 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.669 I print_info: rope_finetuned   = unknown
0.00.039.669 I print_info: ssm_d_conv       = 0
0.00.039.669 I print_info: ssm_d_inner      = 0
0.00.039.670 I print_info: ssm_d_state      = 0
0.00.039.670 I print_info: ssm_dt_rank      = 0
0.00.039.670 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.670 I print_info: model type       = 1.4B
0.00.039.671 I print_info: model params     = 1.41 B
0.00.039.671 I print_info: general.name     = 1.4B
0.00.039.671 I print_info: vocab type       = BPE
0.00.039.672 I print_info: n_vocab          = 50304
0.00.039.672 I print_info: n_merges         = 50009
0.00.039.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: LF token         = 187 'Ċ'
0.00.039.673 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: max token length = 1024
0.00.636.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.871 I load_tensors: offloading output layer to GPU
0.00.636.873 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.898 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.636.899 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.637.986 I llama_init_from_model: n_seq_max     = 1
0.00.637.988 I llama_init_from_model: n_ctx         = 2048
0.00.637.988 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.989 I llama_init_from_model: n_batch       = 2048
0.00.637.989 I llama_init_from_model: n_ubatch      = 512
0.00.637.990 I llama_init_from_model: flash_attn    = 0
0.00.637.991 I llama_init_from_model: freq_base     = 10000.0
0.00.637.991 I llama_init_from_model: freq_scale    = 1
0.00.637.993 I ggml_metal_init: allocating
0.00.638.011 I ggml_metal_init: found device: Apple M4
0.00.638.020 I ggml_metal_init: picking default device: Apple M4
0.00.639.302 I ggml_metal_init: using embedded metal library
0.00.644.957 I ggml_metal_init: GPU name:   Apple M4
0.00.644.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.962 I ggml_metal_init: simdgroup reduction   = true
0.00.644.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.962 I ggml_metal_init: has residency sets    = true
0.00.644.963 I ggml_metal_init: has bfloat            = true
0.00.644.963 I ggml_metal_init: use bfloat            = true
0.00.644.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.842 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.907 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.915 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.950 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.098 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.718.100 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.718.100 I llama_init_from_model: graph nodes  = 967
0.00.718.100 I llama_init_from_model: graph splits = 2
0.00.718.105 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.880 I main: llama threadpool init, n_threads = 4
0.00.784.934 I 
0.00.784.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.959 I 
0.00.785.113 I sampler seed: 1234
0.00.785.117 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.138 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.138 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.138 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.667.646 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.667.647 I llama_perf_context_print:        load time =     775.28 ms
0.01.667.648 I llama_perf_context_print: prompt eval time =      54.19 ms /     7 tokens (    7.74 ms per token,   129.17 tokens per second)
0.01.667.649 I llama_perf_context_print:        eval time =     825.37 ms /    63 runs   (   13.10 ms per token,    76.33 tokens per second)
0.01.667.650 I llama_perf_context_print:       total time =     883.51 ms /    70 tokens
0.01.667.927 I ggml_metal_free: deallocating

real	0m1.684s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4650 (8d4d2be1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.801 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.802 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.803 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.803 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.804 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.804 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.805 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.805 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.805 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.806 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.806 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.808 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.809 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.713 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.716 I llama_model_loader: - type  f32:  194 tensors
0.00.024.716 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.717 I print_info: file format = GGUF V3 (latest)
0.00.024.717 I print_info: file type   = Q6_K
0.00.024.718 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.833 I load: special tokens cache size = 25
0.00.038.867 I load: token to piece cache size = 0.2984 MB
0.00.038.870 I print_info: arch             = gptneox
0.00.038.870 I print_info: vocab_only       = 0
0.00.038.870 I print_info: n_ctx_train      = 2048
0.00.038.870 I print_info: n_embd           = 2048
0.00.038.871 I print_info: n_layer          = 24
0.00.038.874 I print_info: n_head           = 16
0.00.038.875 I print_info: n_head_kv        = 16
0.00.038.875 I print_info: n_rot            = 32
0.00.038.875 I print_info: n_swa            = 0
0.00.038.875 I print_info: n_embd_head_k    = 128
0.00.038.876 I print_info: n_embd_head_v    = 128
0.00.038.876 I print_info: n_gqa            = 1
0.00.038.877 I print_info: n_embd_k_gqa     = 2048
0.00.038.878 I print_info: n_embd_v_gqa     = 2048
0.00.038.878 I print_info: f_norm_eps       = 1.0e-05
0.00.038.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.879 I print_info: f_logit_scale    = 0.0e+00
0.00.038.880 I print_info: n_ff             = 8192
0.00.038.880 I print_info: n_expert         = 0
0.00.038.880 I print_info: n_expert_used    = 0
0.00.038.882 I print_info: causal attn      = 1
0.00.038.883 I print_info: pooling type     = 0
0.00.038.883 I print_info: rope type        = 2
0.00.038.883 I print_info: rope scaling     = linear
0.00.038.883 I print_info: freq_base_train  = 10000.0
0.00.038.884 I print_info: freq_scale_train = 1
0.00.038.884 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.884 I print_info: rope_finetuned   = unknown
0.00.038.884 I print_info: ssm_d_conv       = 0
0.00.038.884 I print_info: ssm_d_inner      = 0
0.00.038.884 I print_info: ssm_d_state      = 0
0.00.038.885 I print_info: ssm_dt_rank      = 0
0.00.038.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.885 I print_info: model type       = 1.4B
0.00.038.885 I print_info: model params     = 1.41 B
0.00.038.885 I print_info: general.name     = 1.4B
0.00.038.886 I print_info: vocab type       = BPE
0.00.038.886 I print_info: n_vocab          = 50304
0.00.038.886 I print_info: n_merges         = 50009
0.00.038.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.891 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.891 I print_info: LF token         = 187 'Ċ'
0.00.038.891 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.891 I print_info: max token length = 1024
0.00.633.262 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.273 I load_tensors: offloading output layer to GPU
0.00.633.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.307 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.633.309 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.634.716 I llama_init_from_model: n_seq_max     = 1
0.00.634.722 I llama_init_from_model: n_ctx         = 128
0.00.634.722 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.723 I llama_init_from_model: n_batch       = 128
0.00.634.723 I llama_init_from_model: n_ubatch      = 128
0.00.634.724 I llama_init_from_model: flash_attn    = 0
0.00.634.725 I llama_init_from_model: freq_base     = 10000.0
0.00.634.726 I llama_init_from_model: freq_scale    = 1
0.00.634.727 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.729 I ggml_metal_init: allocating
0.00.634.779 I ggml_metal_init: found device: Apple M4
0.00.634.792 I ggml_metal_init: picking default device: Apple M4
0.00.636.552 I ggml_metal_init: using embedded metal library
0.00.643.437 I ggml_metal_init: GPU name:   Apple M4
0.00.643.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.444 I ggml_metal_init: simdgroup reduction   = true
0.00.643.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.445 I ggml_metal_init: has residency sets    = true
0.00.643.445 I ggml_metal_init: has bfloat            = true
0.00.643.445 I ggml_metal_init: use bfloat            = true
0.00.643.447 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.448 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.261 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.910 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.914 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.311 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.314 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.315 I llama_init_from_model: graph nodes  = 967
0.00.668.315 I llama_init_from_model: graph splits = 2
0.00.668.319 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.405 I 
0.00.702.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.486 I perplexity: tokenizing the input ..
0.00.709.776 I perplexity: tokenization took 7.287 ms
0.00.709.784 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.905 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.854.112 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.854.130 I llama_perf_context_print:        load time =     693.45 ms
0.00.854.131 I llama_perf_context_print: prompt eval time =     141.22 ms /   128 tokens (    1.10 ms per token,   906.41 tokens per second)
0.00.854.131 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.131 I llama_perf_context_print:       total time =     151.73 ms /   129 tokens
0.00.854.618 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.082s
sys	0m0.146s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4650 (8d4d2be1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e048f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e04d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e051d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e05640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e05ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e05f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e06390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e06800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e06c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e08a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e09290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e0b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e0d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e0dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e0e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e0f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e0f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e0ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e10240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e10f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e12850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e12cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e13130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e13a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e13e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e14b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e1b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e1fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e23990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e25ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e27750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e28e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e2b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e2c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e30320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e32120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e32b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e34420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e35d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e37b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e3bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e3f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e40220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e42f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e44030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e45810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e46760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e4a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e4a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e4eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e53b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e55ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e56ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e57020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e58010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e59000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e59550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e59aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e5aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e5ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e5bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e5c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e5cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e5df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e5e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e5ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e5f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e60530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e60c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e62470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e63530 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.701.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116104b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116104f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116105400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116105870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116105ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116106150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1161065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116106a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116106ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116107310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116107780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116107e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116108990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116109140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116109950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11610a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11610a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11610aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11610b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11610bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11610c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11610cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11610d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11610d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11610e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11610e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11610e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11610ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11610ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11610f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11610f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11610fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116110180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116110440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1161108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116110d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116111190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116111600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116111a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116111ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116112350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1161127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116112c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1161130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116113510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116113980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116113df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116114260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1161146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116114b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116114fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116115420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116115890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116115d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116116170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1161165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116116b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116117050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1161174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116117930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116117da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116118210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116118680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116118af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116118f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1161193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116119840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116119cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11611a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11611a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11611aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11611ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11611b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11611b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11611bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11611c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11611c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11611c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11611cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11611d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11611d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11611dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11611df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11611e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11611e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11611ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11611f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11611f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11611f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11611fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1161202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116120730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116120ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116121010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116121480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1161218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116121d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1161221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116122640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116122ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116122f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116123390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116123800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116123c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1161240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116124550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1161249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116124e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1161252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116125710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116125b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116125ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116126460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1161268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116126d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1161271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116127620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116127a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116127f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116128370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1161287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116128c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1161290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116129530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1161299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116129e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11612a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11612a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11612ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11612afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11612b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11612b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11612bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11612c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11612c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11612ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11612cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11612d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11612d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11612dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11612e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11612e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11612e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11612edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11612f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11612f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11612fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11612ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116130420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116130890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116130d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116131170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1161315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116131a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116131ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116132330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1161327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116132c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116133080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1161334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116133960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116133dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116134240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1161346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116134b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116134f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116135bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116135e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116136140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1161365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116136a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116136e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116137300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116137770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116137be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116138050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1161384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116138930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116138da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116139210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116139680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116139af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116139f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11613a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11613a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11613acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11613b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11613b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11613ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11613be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11613c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11613c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11613cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11613d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11613d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11613d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11613dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11613e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11613e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11613ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11613ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11613f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11613f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11613fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116140290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116140700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116140b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116140fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116141500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116141a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116142580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116142840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116142e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1161433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116143980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116143f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116144500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116144ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116145080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116145640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116145c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1161461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116146780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116146d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116147300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1161478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116147e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116148440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116148a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116148fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116149580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116149b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11614a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11614a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11614ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11614b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11614b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11614bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11614c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11614c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11614cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11614d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11614da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11614e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11614e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11614ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11614f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11614f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11614fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1161502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116150880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116150e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116151400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1161519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116151f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116152540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116152b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1161530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116153680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116153c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116154200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1161547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116154d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116155340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116155900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116155ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116156480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116156a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116156f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116157440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116157940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116157e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116158340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116158840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116158d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116159240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116159740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116159c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11615a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11615a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11615ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11615b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11615b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11615bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11615c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11615cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11615d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11615d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11615df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11615e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11615e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e23c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e26900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e43790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e1e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e25da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e22b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e26eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e073a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e63c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e63f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e64a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e64f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e65510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e65a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e65d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e66010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e66590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e66850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e66b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e66dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e67090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e67350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e67610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e67b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e67e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e68950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e68c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e68ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e69190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e69450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e69710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e69c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e69f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e6a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e6a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e6a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e6aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e6ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e6afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e6b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e6b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e6b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e6bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e6bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e6c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e6c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e6c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e6c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e6cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e6ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e6d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e6d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e6d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e6d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e6dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e6de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e6e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e6e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e6e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e6e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e6ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e6ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e6f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e6f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e6f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e6fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e6fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e6ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e70250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e70510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e70a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e70d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e71010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e71590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e71850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e71b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e71dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e72090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e72350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e72610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e728d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e72b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e72e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e73110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e733d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e73690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e73950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e73c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e73ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e74190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e74450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e74710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e749d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e74c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e74f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e75210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e754d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e75790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e75a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e75d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e75fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e76290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e76550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e76810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e76ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e76d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e77050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e77310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e775d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e77890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e77b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e77e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e780d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e78390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e78650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e78910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e78bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e78e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e79150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e79410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e796d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e79990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e79c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e79f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e7a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e7a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e7a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e7ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e7ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e7b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e7b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e7b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e7be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e7c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e7c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e7cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e7cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e7d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e7d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e7dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e7e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e7e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e7ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e7eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e7f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e7fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e800b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e80670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e80b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e80ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e81460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e818d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e81d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e82260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e82770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e83b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e84120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e846e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e84ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e85260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e85820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e85de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e863a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e86960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e86f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e874e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e87aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e88060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e88620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e88be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e89760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e89d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e8a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e8a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e8ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e8b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e8b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e8bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e8c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e8cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e8d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e8d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e8dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e8e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e8e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e8eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e8f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e8f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e8fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e904a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e90a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e91020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e91ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e92160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e92720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e92ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e932a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e93860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e93e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e943e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e949a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e94f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e95ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e960a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e96660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e96c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e971e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e977a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e97ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e981a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e986a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e98ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e990a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e995a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e99aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e99fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e9a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e9a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e9aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e9b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e9b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e9bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e9c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e9ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e9d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e9daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e9e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e9e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e9ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e9ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e9f590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.780s
user	0m0.256s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4650 (8d4d2be1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13460d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13460de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13460e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13460e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13460ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13460f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13460fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134610070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134610620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134610b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134611020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134611520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1346127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134613720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134614c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134615450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134615b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1346169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134617250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134617c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1346193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1346196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134619b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13461a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13461abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13461aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13461b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13461b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13461bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13461c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13461c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13461ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13461cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13461d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13461d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13461db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13461e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13461e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13461f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13461f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13461fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134620270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134620880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1346214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134621c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134622130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1346225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134622890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134622ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134623690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134623950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134624730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134624bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134625510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1346259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134625e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1346262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134626790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1346270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134627620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134627b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1346280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134628b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1346290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134629600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134629b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13462a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13462a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13462ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13462b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13462b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13462bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13462c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13462c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13462cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13462d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13462d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13462db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13462e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13462e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13462eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13462f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13461ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13462f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13462fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1346301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134630c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1346311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134631700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134631c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1346321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1346326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134632c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134633190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1346336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134634180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134634620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134634ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134634f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134635400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1346358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134635d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1346361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134636680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134636b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134636fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134637900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134637da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134638240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1346386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134638b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134639020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1346394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134639960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134639e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13463a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13463a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13463abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13463b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13463b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13463b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13463be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13463c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13463c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13463cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13463d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13463d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13463da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13463dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13463e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13463e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13463eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13463f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13463f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13463fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13463ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1346403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134640860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1346411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134641ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134641f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134642420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1346428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134642d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134643200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1346436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134643b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134643fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134644920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134644dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134645260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134645700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134645ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1346464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134646980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134646e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1346472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134647760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1346480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1346489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134648e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134649320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1346497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13464a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13464a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13464aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13464aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13464b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13464b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13464be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13464c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13464c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13464cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13464d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13464d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13464ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13464e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13464ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13464ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13464f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13464f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134650110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1346505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134650a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134650ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1346516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134651bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134652140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134652690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134652be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134653130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134653680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134653bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134654670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134654bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134655110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134655660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134656100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134656650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134656ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1346570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134657640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134657b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1346580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134658630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134658b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1346590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134659620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134659b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13465a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13465a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13465ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13465b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13465b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13465bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13465c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13465c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13465cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13465d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13465d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13465db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13465e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13465e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13465eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13465f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13465f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13465fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134660060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1346605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134660b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134661050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1346615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134661af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134662040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134662590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134662ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134663030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134663580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134663ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134664020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1346644c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134664960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134664e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1346652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134665740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134665be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134666080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134666520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1346669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134666e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134667300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1346677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134667c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1346680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134668580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134668ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1346691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134669910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13466a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13466a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13466aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13466b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13466b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13466bad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13466b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13464d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13464ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13464da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134620530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134622b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13464f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134617ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13461e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13461f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13461f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13461ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13461ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134616ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134623160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13462f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13466acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13461a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13461a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13464fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13464e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134618500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1346187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134618a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13466bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13466c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13466c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13466c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13466ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13466ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13466cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13466d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13466d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13466d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13466dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13466dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13466e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13466e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13466e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13466e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13466eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13466edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13466f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13466f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13466f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13466f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13466fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13466fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134670130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1346703f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1346706b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134670970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134670c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134670ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1346711b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134671470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134671730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1346719f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134671cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134671f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134672230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1346724f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1346727b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134672a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134672d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134672ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1346732b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134673570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134673830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134673af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134673db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134674070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134674330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1346745f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1346748b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134674b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134674e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1346750f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1346753b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134675670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134675930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134675bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134675eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134676170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134676430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1346766f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1346769b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134676c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134676f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1346771f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1346774b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134677770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134677a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134677cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134677fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134678270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134678530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1346787f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134678ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134678d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134679030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1346792f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1346795b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134679870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134679b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134679df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13467a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13467a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13467a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13467a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13467abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13467ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13467b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13467b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13467b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13467b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13467bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13467bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13467c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13467c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13467c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13467c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13467ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13467cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13467d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13467d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13467d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13467da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13467dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13467dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13467e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13467e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13467e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13467eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13467edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13467f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13467f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13467f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13467f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13467fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13467fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1346800f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1346803b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134680670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134680930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134680bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134680eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134681170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134681430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1346816f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1346819b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134681c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134681f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1346821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1346824b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134682770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134682a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134682cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134682fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134683270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134683530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1346837f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134683ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134683d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134684030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1346842f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1346845b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134684870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134684b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134684df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1346850b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134685370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134685630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1346858f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134685bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134685e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134686130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1346863f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1346866b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134686970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134686c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134686ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1346871b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134687470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134687730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1346879f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134687cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134687f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134688230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1346884f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1346887b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134688a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134688d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134688ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1346892b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134689570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134689830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134689af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134689db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13468a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13468a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13468a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13468a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13468ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13468b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13468b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13468b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13468bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13468c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13468c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13468cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13468ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13468d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13468d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13468dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13468e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13468e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13468e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13468ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13468f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13468f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13468fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13468ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1346903c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134690830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134690ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134691110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134691580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1346919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134691e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1346922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134692740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134692bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134693020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134693490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134693900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134693d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1346941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134694650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134694ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134694f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1346953a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134695810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134695c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1346960f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134696560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1346969d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134696e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1346972b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134697720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134697b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134698000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134698470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1346988e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134698d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1346991c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134699630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134699aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134699f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13469a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13469a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13469ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13469b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13469b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13469b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13469be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13469c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13469c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13469cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13469cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13469d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13469d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13469dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13469e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13469e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13469ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13469eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13469f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13469f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13469fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1346a00b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1346a0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1346a0f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1346a16b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1346a1dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1346a24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1346a27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1346a2fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1346a3260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1346a3870 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1358044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1358056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1358063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1358092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13580a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13580a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13580af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13580b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13580be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13580c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13580cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13580d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13580dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13580dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13580e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13580e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13580e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13580edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13580f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13580f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13580fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13580fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1358102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1358114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1358133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1358149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1358152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1358177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1358180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1358189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1358196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13581a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13581a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13581ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13581b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13581b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13581ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13581bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13581c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13581c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13581cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13581d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13581d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13581d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13581ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13581e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13581e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13581eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13581efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13581f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13581f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13581fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1358205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1358217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1358224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1358253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1358269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1358272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1358291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13582a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13582a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13582ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13582b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13582b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13582b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13582be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13582c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13582c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13582cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13582d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13582d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13582d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13582dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13582e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13582e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13582eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13582ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13582f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13582f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13582fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1358300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1358309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1358328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1358331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1358347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1358350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1358366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1358378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1358385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1358397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13583a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13583a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13583a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13583ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13583b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13583b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13583bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13583bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13583c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13583c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13583ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13583d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13583d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13583da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13583deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13583e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13583e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13583ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13583f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13583f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13583f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13583fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1358406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1358424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1358436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1358455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1358474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1358493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13584a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13584a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13584aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13584ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13584b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13584b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13584bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13584c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13584c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13584c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13584cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13584d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13584d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13584dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13584df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13584e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13584e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13584eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13584f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13584f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13584f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13584fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1358502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1358521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1358533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1358540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1358549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1358552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1358568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1358576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1358579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.959s
user	0m0.235s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.35 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.80 sec*proc (2 tests)

Total Test time (real) =   1.82 sec
        1.84 real         0.52 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
