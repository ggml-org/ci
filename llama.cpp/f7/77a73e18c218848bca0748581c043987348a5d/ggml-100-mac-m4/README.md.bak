### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.08 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.59 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.28 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.49 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.93 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.77 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.40 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 163.83 sec*proc (29 tests)

Total Test time (real) = 163.84 sec

real	2m43.873s
user	4m39.106s
sys	0m5.681s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.04 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.75 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.35 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.07 sec*proc (29 tests)

Total Test time (real) =  48.08 sec

real	0m48.094s
user	0m54.589s
sys	0m5.075s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.097 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.289 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.298 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.299 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.300 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.301 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.302 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.302 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.303 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.304 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.304 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.307 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.307 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.308 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.309 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.309 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.310 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.310 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.659 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.660 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.661 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.661 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.662 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.662 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.663 I llama_model_loader: - type  f32:  124 tensors
0.00.025.663 I llama_model_loader: - type  f16:   73 tensors
0.00.025.664 I print_info: file format = GGUF V3 (latest)
0.00.025.664 I print_info: file type   = F16
0.00.025.666 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.710 I load: special tokens cache size = 5
0.00.031.853 I load: token to piece cache size = 0.2032 MB
0.00.031.857 I print_info: arch             = bert
0.00.031.858 I print_info: vocab_only       = 0
0.00.031.858 I print_info: n_ctx_train      = 512
0.00.031.858 I print_info: n_embd           = 384
0.00.031.858 I print_info: n_layer          = 12
0.00.031.862 I print_info: n_head           = 12
0.00.031.863 I print_info: n_head_kv        = 12
0.00.031.863 I print_info: n_rot            = 32
0.00.031.863 I print_info: n_swa            = 0
0.00.031.864 I print_info: n_embd_head_k    = 32
0.00.031.864 I print_info: n_embd_head_v    = 32
0.00.031.865 I print_info: n_gqa            = 1
0.00.031.869 I print_info: n_embd_k_gqa     = 384
0.00.031.869 I print_info: n_embd_v_gqa     = 384
0.00.031.870 I print_info: f_norm_eps       = 1.0e-12
0.00.031.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.871 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.871 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.871 I print_info: f_logit_scale    = 0.0e+00
0.00.031.872 I print_info: n_ff             = 1536
0.00.031.873 I print_info: n_expert         = 0
0.00.031.873 I print_info: n_expert_used    = 0
0.00.031.873 I print_info: causal attn      = 0
0.00.031.873 I print_info: pooling type     = 2
0.00.031.873 I print_info: rope type        = 2
0.00.031.875 I print_info: rope scaling     = linear
0.00.031.876 I print_info: freq_base_train  = 10000.0
0.00.031.876 I print_info: freq_scale_train = 1
0.00.031.876 I print_info: n_ctx_orig_yarn  = 512
0.00.031.877 I print_info: rope_finetuned   = unknown
0.00.031.877 I print_info: ssm_d_conv       = 0
0.00.031.877 I print_info: ssm_d_inner      = 0
0.00.031.877 I print_info: ssm_d_state      = 0
0.00.031.877 I print_info: ssm_dt_rank      = 0
0.00.031.877 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.878 I print_info: model type       = 33M
0.00.031.878 I print_info: model params     = 33.21 M
0.00.031.879 I print_info: general.name     = Bge Small
0.00.031.879 I print_info: vocab type       = WPM
0.00.031.880 I print_info: n_vocab          = 30522
0.00.031.880 I print_info: n_merges         = 0
0.00.031.880 I print_info: BOS token        = 101 '[CLS]'
0.00.031.880 I print_info: UNK token        = 100 '[UNK]'
0.00.031.881 I print_info: SEP token        = 102 '[SEP]'
0.00.031.881 I print_info: PAD token        = 0 '[PAD]'
0.00.031.881 I print_info: MASK token       = 103 '[MASK]'
0.00.031.881 I print_info: LF token         = 0 '[PAD]'
0.00.031.887 I print_info: max token length = 21
0.00.031.887 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.035.193 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.195 I load_tensors: offloading output layer to GPU
0.00.035.196 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.220 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.222 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.483 I llama_init_from_model: n_seq_max     = 1
0.00.035.484 I llama_init_from_model: n_ctx         = 512
0.00.035.485 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.485 I llama_init_from_model: n_batch       = 2048
0.00.035.485 I llama_init_from_model: n_ubatch      = 2048
0.00.035.486 I llama_init_from_model: flash_attn    = 0
0.00.035.486 I llama_init_from_model: freq_base     = 10000.0
0.00.035.486 I llama_init_from_model: freq_scale    = 1
0.00.035.487 I ggml_metal_init: allocating
0.00.035.492 I ggml_metal_init: found device: Apple M4
0.00.035.501 I ggml_metal_init: picking default device: Apple M4
0.00.036.282 I ggml_metal_init: using embedded metal library
0.00.040.291 I ggml_metal_init: GPU name:   Apple M4
0.00.040.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.295 I ggml_metal_init: simdgroup reduction   = true
0.00.040.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.296 I ggml_metal_init: has residency sets    = true
0.00.040.296 I ggml_metal_init: has bfloat            = true
0.00.040.296 I ggml_metal_init: use bfloat            = true
0.00.040.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.756 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.417 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.419 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.440 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.631 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.633 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.633 I llama_init_from_model: graph nodes  = 429
0.00.053.633 I llama_init_from_model: graph splits = 2
0.00.053.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.253 I 
0.00.059.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.936 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.103 I llama_perf_context_print:        load time =      44.33 ms
0.00.065.104 I llama_perf_context_print: prompt eval time =       5.02 ms /     9 tokens (    0.56 ms per token,  1792.83 tokens per second)
0.00.065.104 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.105 I llama_perf_context_print:       total time =       5.85 ms /    10 tokens
0.00.065.253 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.047s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.139 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.818 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.825 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.826 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.826 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.827 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.828 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.829 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.830 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.831 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.831 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.833 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.833 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.834 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.834 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.834 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.834 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.207 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.890 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.891 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.892 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.892 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.892 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.893 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.893 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.893 I llama_model_loader: - type  f32:  124 tensors
0.00.014.894 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.894 I print_info: file format = GGUF V3 (latest)
0.00.014.895 I print_info: file type   = Q8_0
0.00.014.895 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.384 I load: special tokens cache size = 5
0.00.018.685 I load: token to piece cache size = 0.2032 MB
0.00.018.688 I print_info: arch             = bert
0.00.018.688 I print_info: vocab_only       = 0
0.00.018.689 I print_info: n_ctx_train      = 512
0.00.018.689 I print_info: n_embd           = 384
0.00.018.689 I print_info: n_layer          = 12
0.00.018.693 I print_info: n_head           = 12
0.00.018.693 I print_info: n_head_kv        = 12
0.00.018.693 I print_info: n_rot            = 32
0.00.018.694 I print_info: n_swa            = 0
0.00.018.694 I print_info: n_embd_head_k    = 32
0.00.018.694 I print_info: n_embd_head_v    = 32
0.00.018.696 I print_info: n_gqa            = 1
0.00.018.697 I print_info: n_embd_k_gqa     = 384
0.00.018.698 I print_info: n_embd_v_gqa     = 384
0.00.018.698 I print_info: f_norm_eps       = 1.0e-12
0.00.018.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.699 I print_info: f_logit_scale    = 0.0e+00
0.00.018.700 I print_info: n_ff             = 1536
0.00.018.700 I print_info: n_expert         = 0
0.00.018.700 I print_info: n_expert_used    = 0
0.00.018.700 I print_info: causal attn      = 0
0.00.018.700 I print_info: pooling type     = 2
0.00.018.700 I print_info: rope type        = 2
0.00.018.700 I print_info: rope scaling     = linear
0.00.018.701 I print_info: freq_base_train  = 10000.0
0.00.018.701 I print_info: freq_scale_train = 1
0.00.018.701 I print_info: n_ctx_orig_yarn  = 512
0.00.018.701 I print_info: rope_finetuned   = unknown
0.00.018.701 I print_info: ssm_d_conv       = 0
0.00.018.701 I print_info: ssm_d_inner      = 0
0.00.018.701 I print_info: ssm_d_state      = 0
0.00.018.702 I print_info: ssm_dt_rank      = 0
0.00.018.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.702 I print_info: model type       = 33M
0.00.018.703 I print_info: model params     = 33.21 M
0.00.018.703 I print_info: general.name     = Bge Small
0.00.018.703 I print_info: vocab type       = WPM
0.00.018.704 I print_info: n_vocab          = 30522
0.00.018.704 I print_info: n_merges         = 0
0.00.018.704 I print_info: BOS token        = 101 '[CLS]'
0.00.018.704 I print_info: UNK token        = 100 '[UNK]'
0.00.018.705 I print_info: SEP token        = 102 '[SEP]'
0.00.018.705 I print_info: PAD token        = 0 '[PAD]'
0.00.018.705 I print_info: MASK token       = 103 '[MASK]'
0.00.018.705 I print_info: LF token         = 0 '[PAD]'
0.00.018.705 I print_info: max token length = 21
0.00.018.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.389 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.390 I load_tensors: offloading output layer to GPU
0.00.020.390 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.397 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.397 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.568 I llama_init_from_model: n_seq_max     = 1
0.00.020.569 I llama_init_from_model: n_ctx         = 512
0.00.020.569 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.569 I llama_init_from_model: n_batch       = 2048
0.00.020.569 I llama_init_from_model: n_ubatch      = 2048
0.00.020.569 I llama_init_from_model: flash_attn    = 0
0.00.020.570 I llama_init_from_model: freq_base     = 10000.0
0.00.020.570 I llama_init_from_model: freq_scale    = 1
0.00.020.570 I ggml_metal_init: allocating
0.00.020.573 I ggml_metal_init: found device: Apple M4
0.00.020.576 I ggml_metal_init: picking default device: Apple M4
0.00.021.104 I ggml_metal_init: using embedded metal library
0.00.023.624 I ggml_metal_init: GPU name:   Apple M4
0.00.023.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.627 I ggml_metal_init: simdgroup reduction   = true
0.00.023.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.628 I ggml_metal_init: has residency sets    = true
0.00.023.628 I ggml_metal_init: has bfloat            = true
0.00.023.628 I ggml_metal_init: use bfloat            = true
0.00.023.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.801 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.409 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.411 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.425 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.374 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.375 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.375 I llama_init_from_model: graph nodes  = 429
0.00.035.375 I llama_init_from_model: graph splits = 2
0.00.035.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.824 I 
0.00.039.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.372 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.793 I llama_perf_context_print:        load time =      30.68 ms
0.00.044.794 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2095.95 tokens per second)
0.00.044.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.795 I llama_perf_context_print:       total time =       4.97 ms /    10 tokens
0.00.045.002 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.261 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.675 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.712 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.720 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.729 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.730 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.730 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.732 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.732 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.734 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.735 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.735 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.738 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.739 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.740 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.563 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.242 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.053.244 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.244 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.053.245 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.053.245 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.053.245 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.053.246 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.053.246 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.053.247 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.053.247 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.053.247 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.053.248 I llama_model_loader: - type  f32:   40 tensors
0.00.053.248 I llama_model_loader: - type  f16:   30 tensors
0.00.053.249 I print_info: file format = GGUF V3 (latest)
0.00.053.249 I print_info: file type   = F16
0.00.053.250 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.057.519 W load: empty token at index 5
0.00.062.679 W load: model vocab missing newline token, using special_pad_id instead
0.00.064.208 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.064.245 I load: special tokens cache size = 5
0.00.327.570 I load: token to piece cache size = 1.5060 MB
0.00.327.577 I print_info: arch             = jina-bert-v2
0.00.327.580 I print_info: vocab_only       = 0
0.00.327.580 I print_info: n_ctx_train      = 8192
0.00.327.581 I print_info: n_embd           = 384
0.00.327.581 I print_info: n_layer          = 4
0.00.327.587 I print_info: n_head           = 12
0.00.327.587 I print_info: n_head_kv        = 12
0.00.327.588 I print_info: n_rot            = 32
0.00.327.588 I print_info: n_swa            = 0
0.00.327.588 I print_info: n_embd_head_k    = 32
0.00.327.588 I print_info: n_embd_head_v    = 32
0.00.327.588 I print_info: n_gqa            = 1
0.00.327.589 I print_info: n_embd_k_gqa     = 384
0.00.327.589 I print_info: n_embd_v_gqa     = 384
0.00.327.590 I print_info: f_norm_eps       = 1.0e-12
0.00.327.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.591 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.591 I print_info: f_logit_scale    = 0.0e+00
0.00.327.592 I print_info: n_ff             = 1536
0.00.327.592 I print_info: n_expert         = 0
0.00.327.592 I print_info: n_expert_used    = 0
0.00.327.593 I print_info: causal attn      = 0
0.00.327.595 I print_info: pooling type     = -1
0.00.327.595 I print_info: rope type        = -1
0.00.327.595 I print_info: rope scaling     = linear
0.00.327.595 I print_info: freq_base_train  = 10000.0
0.00.327.596 I print_info: freq_scale_train = 1
0.00.327.597 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.597 I print_info: rope_finetuned   = unknown
0.00.327.597 I print_info: ssm_d_conv       = 0
0.00.327.597 I print_info: ssm_d_inner      = 0
0.00.327.597 I print_info: ssm_d_state      = 0
0.00.327.598 I print_info: ssm_dt_rank      = 0
0.00.327.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.598 I print_info: model type       = 33M
0.00.327.598 I print_info: model params     = 32.90 M
0.00.327.598 I print_info: general.name     = Jina Bert Implementation
0.00.327.600 I print_info: vocab type       = BPE
0.00.327.600 I print_info: n_vocab          = 61056
0.00.327.600 I print_info: n_merges         = 39382
0.00.327.601 I print_info: BOS token        = 0 '<s>'
0.00.327.601 I print_info: EOS token        = 2 '</s>'
0.00.327.601 I print_info: UNK token        = 3 '<unk>'
0.00.327.601 I print_info: SEP token        = 2 '</s>'
0.00.327.601 I print_info: PAD token        = 1 '<pad>'
0.00.327.601 I print_info: MASK token       = 4 '<mask>'
0.00.327.602 I print_info: EOG token        = 2 '</s>'
0.00.327.602 I print_info: max token length = 45
0.00.327.602 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.569 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.571 I load_tensors: offloading output layer to GPU
0.00.329.571 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.594 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.595 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.329.882 I llama_init_from_model: n_seq_max     = 1
0.00.329.883 I llama_init_from_model: n_ctx         = 8192
0.00.329.883 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.883 I llama_init_from_model: n_batch       = 2048
0.00.329.883 I llama_init_from_model: n_ubatch      = 2048
0.00.329.884 I llama_init_from_model: flash_attn    = 0
0.00.329.884 I llama_init_from_model: freq_base     = 10000.0
0.00.329.884 I llama_init_from_model: freq_scale    = 1
0.00.329.885 I ggml_metal_init: allocating
0.00.329.889 I ggml_metal_init: found device: Apple M4
0.00.329.892 I ggml_metal_init: picking default device: Apple M4
0.00.330.815 I ggml_metal_init: using embedded metal library
0.00.333.303 I ggml_metal_init: GPU name:   Apple M4
0.00.333.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.305 I ggml_metal_init: simdgroup reduction   = true
0.00.333.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.306 I ggml_metal_init: has residency sets    = true
0.00.333.306 I ggml_metal_init: has bfloat            = true
0.00.333.306 I ggml_metal_init: use bfloat            = true
0.00.333.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.342.669 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.684 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.686 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.707 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.835 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.836 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.836 I llama_init_from_model: graph nodes  = 154
0.00.351.837 I llama_init_from_model: graph splits = 2
0.00.351.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.496 I 
0.00.359.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.624 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.625 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.628 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.628 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.632 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.632 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.137 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.887 I llama_perf_context_print:        load time =     333.81 ms
0.00.362.887 I llama_perf_context_print: prompt eval time =       2.74 ms /    62 tokens (    0.04 ms per token, 22602.99 tokens per second)
0.00.362.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.888 I llama_perf_context_print:       total time =       3.39 ms /    63 tokens
0.00.363.118 I ggml_metal_free: deallocating

real	0m1.061s
user	0m0.332s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.188 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.361 I main: llama backend init
0.00.000.368 I main: load the model and apply lora adapter, if any
0.00.060.359 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.073.123 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.073.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.073.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.073.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.073.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.073.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.073.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.073.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.073.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.073.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.073.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.073.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.073.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.073.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.073.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.073.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.073.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.089.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.115 I llama_model_loader: - type  f32:  194 tensors
0.00.089.120 I llama_model_loader: - type  f16:   98 tensors
0.00.089.124 I print_info: file format = GGUF V3 (latest)
0.00.089.127 I print_info: file type   = all F32 (guessed)
0.00.089.138 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.762 I load: special tokens cache size = 25
0.00.116.327 I load: token to piece cache size = 0.2984 MB
0.00.116.332 I print_info: arch             = gptneox
0.00.116.332 I print_info: vocab_only       = 0
0.00.116.332 I print_info: n_ctx_train      = 2048
0.00.116.332 I print_info: n_embd           = 2048
0.00.116.333 I print_info: n_layer          = 24
0.00.116.339 I print_info: n_head           = 16
0.00.116.340 I print_info: n_head_kv        = 16
0.00.116.340 I print_info: n_rot            = 32
0.00.116.340 I print_info: n_swa            = 0
0.00.116.341 I print_info: n_embd_head_k    = 128
0.00.116.341 I print_info: n_embd_head_v    = 128
0.00.116.342 I print_info: n_gqa            = 1
0.00.116.343 I print_info: n_embd_k_gqa     = 2048
0.00.116.344 I print_info: n_embd_v_gqa     = 2048
0.00.116.344 I print_info: f_norm_eps       = 1.0e-05
0.00.116.345 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.345 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.345 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.345 I print_info: f_logit_scale    = 0.0e+00
0.00.116.346 I print_info: n_ff             = 8192
0.00.116.347 I print_info: n_expert         = 0
0.00.116.347 I print_info: n_expert_used    = 0
0.00.116.347 I print_info: causal attn      = 1
0.00.116.347 I print_info: pooling type     = 0
0.00.116.347 I print_info: rope type        = 2
0.00.116.349 I print_info: rope scaling     = linear
0.00.116.349 I print_info: freq_base_train  = 10000.0
0.00.116.350 I print_info: freq_scale_train = 1
0.00.116.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.352 I print_info: rope_finetuned   = unknown
0.00.116.353 I print_info: ssm_d_conv       = 0
0.00.116.353 I print_info: ssm_d_inner      = 0
0.00.116.353 I print_info: ssm_d_state      = 0
0.00.116.353 I print_info: ssm_dt_rank      = 0
0.00.116.353 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.354 I print_info: model type       = 1.4B
0.00.116.354 I print_info: model params     = 1.41 B
0.00.116.354 I print_info: general.name     = 1.4B
0.00.116.355 I print_info: vocab type       = BPE
0.00.116.355 I print_info: n_vocab          = 50304
0.00.116.356 I print_info: n_merges         = 50009
0.00.116.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.357 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.359 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.359 I print_info: LF token         = 187 ''
0.00.116.359 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.360 I print_info: max token length = 1024
0.00.116.360 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.180.707 I load_tensors: offloading 24 repeating layers to GPU
0.00.180.711 I load_tensors: offloading output layer to GPU
0.00.180.712 I load_tensors: offloaded 25/25 layers to GPU
0.00.180.739 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.180.741 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.181.356 I llama_init_from_model: n_seq_max     = 1
0.00.181.357 I llama_init_from_model: n_ctx         = 2048
0.00.181.357 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.181.358 I llama_init_from_model: n_batch       = 2048
0.00.181.358 I llama_init_from_model: n_ubatch      = 512
0.00.181.358 I llama_init_from_model: flash_attn    = 0
0.00.181.359 I llama_init_from_model: freq_base     = 10000.0
0.00.181.359 I llama_init_from_model: freq_scale    = 1
0.00.181.360 I ggml_metal_init: allocating
0.00.181.442 I ggml_metal_init: found device: Apple M4
0.00.181.448 I ggml_metal_init: picking default device: Apple M4
0.00.182.213 I ggml_metal_init: using embedded metal library
0.00.433.075 I ggml_metal_init: GPU name:   Apple M4
0.00.433.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.433.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.433.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.433.095 I ggml_metal_init: simdgroup reduction   = true
0.00.433.095 I ggml_metal_init: simdgroup matrix mul. = true
0.00.433.096 I ggml_metal_init: has residency sets    = true
0.00.433.096 I ggml_metal_init: has bfloat            = true
0.00.433.096 I ggml_metal_init: use bfloat            = true
0.00.433.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.433.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.871 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.509.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.509.939 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.509.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.513.662 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.513.666 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.513.666 I llama_init_from_model: graph nodes  = 967
0.00.513.667 I llama_init_from_model: graph splits = 2
0.00.513.674 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.513.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.513.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.506 I main: llama threadpool init, n_threads = 4
0.00.581.548 I 
0.00.581.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.584 I 
0.00.581.766 I sampler seed: 1234
0.00.581.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.581.795 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.581.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.581.797 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.411.297 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.411.298 I llama_perf_context_print:        load time =     520.23 ms
0.02.411.299 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.19 tokens per second)
0.02.411.299 I llama_perf_context_print:        eval time =    1782.94 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.411.300 I llama_perf_context_print:       total time =    1830.70 ms /    70 tokens
0.02.411.524 I ggml_metal_free: deallocating

real	0m2.737s
user	0m0.144s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.819 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.592 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.089 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.101 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.103 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.104 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.291 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.292 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.292 I llama_model_loader: - type  f32:  194 tensors
0.00.058.293 I llama_model_loader: - type  f16:   98 tensors
0.00.058.294 I print_info: file format = GGUF V3 (latest)
0.00.058.294 I print_info: file type   = all F32 (guessed)
0.00.058.298 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.927 I load: special tokens cache size = 25
0.00.078.876 I load: token to piece cache size = 0.2984 MB
0.00.078.879 I print_info: arch             = gptneox
0.00.078.879 I print_info: vocab_only       = 0
0.00.078.880 I print_info: n_ctx_train      = 2048
0.00.078.880 I print_info: n_embd           = 2048
0.00.078.880 I print_info: n_layer          = 24
0.00.078.883 I print_info: n_head           = 16
0.00.078.884 I print_info: n_head_kv        = 16
0.00.078.884 I print_info: n_rot            = 32
0.00.078.884 I print_info: n_swa            = 0
0.00.078.885 I print_info: n_embd_head_k    = 128
0.00.078.885 I print_info: n_embd_head_v    = 128
0.00.078.886 I print_info: n_gqa            = 1
0.00.078.886 I print_info: n_embd_k_gqa     = 2048
0.00.078.887 I print_info: n_embd_v_gqa     = 2048
0.00.078.888 I print_info: f_norm_eps       = 1.0e-05
0.00.078.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.889 I print_info: f_logit_scale    = 0.0e+00
0.00.078.889 I print_info: n_ff             = 8192
0.00.078.889 I print_info: n_expert         = 0
0.00.078.890 I print_info: n_expert_used    = 0
0.00.078.890 I print_info: causal attn      = 1
0.00.078.890 I print_info: pooling type     = 0
0.00.078.890 I print_info: rope type        = 2
0.00.078.891 I print_info: rope scaling     = linear
0.00.078.891 I print_info: freq_base_train  = 10000.0
0.00.078.891 I print_info: freq_scale_train = 1
0.00.078.891 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.892 I print_info: rope_finetuned   = unknown
0.00.078.892 I print_info: ssm_d_conv       = 0
0.00.078.892 I print_info: ssm_d_inner      = 0
0.00.078.892 I print_info: ssm_d_state      = 0
0.00.078.892 I print_info: ssm_dt_rank      = 0
0.00.078.892 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.893 I print_info: model type       = 1.4B
0.00.078.893 I print_info: model params     = 1.41 B
0.00.078.893 I print_info: general.name     = 1.4B
0.00.078.894 I print_info: vocab type       = BPE
0.00.078.894 I print_info: n_vocab          = 50304
0.00.078.894 I print_info: n_merges         = 50009
0.00.078.894 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.894 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.895 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.895 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.895 I print_info: LF token         = 187 ''
0.00.078.895 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.895 I print_info: max token length = 1024
0.00.078.896 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.453.297 I load_tensors: offloading 24 repeating layers to GPU
0.01.453.303 I load_tensors: offloading output layer to GPU
0.01.453.303 I load_tensors: offloaded 25/25 layers to GPU
0.01.453.331 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.453.333 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.454.405 I llama_init_from_model: n_seq_max     = 1
0.01.454.407 I llama_init_from_model: n_ctx         = 128
0.01.454.407 I llama_init_from_model: n_ctx_per_seq = 128
0.01.454.407 I llama_init_from_model: n_batch       = 128
0.01.454.408 I llama_init_from_model: n_ubatch      = 128
0.01.454.408 I llama_init_from_model: flash_attn    = 0
0.01.454.409 I llama_init_from_model: freq_base     = 10000.0
0.01.454.410 I llama_init_from_model: freq_scale    = 1
0.01.454.410 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.454.412 I ggml_metal_init: allocating
0.01.454.504 I ggml_metal_init: found device: Apple M4
0.01.454.515 I ggml_metal_init: picking default device: Apple M4
0.01.455.763 I ggml_metal_init: using embedded metal library
0.01.459.894 I ggml_metal_init: GPU name:   Apple M4
0.01.459.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.459.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.459.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.459.898 I ggml_metal_init: simdgroup reduction   = true
0.01.459.898 I ggml_metal_init: simdgroup matrix mul. = true
0.01.459.898 I ggml_metal_init: has residency sets    = true
0.01.459.898 I ggml_metal_init: has bfloat            = true
0.01.459.898 I ggml_metal_init: use bfloat            = true
0.01.459.899 I ggml_metal_init: hasUnifiedMemory      = true
0.01.459.900 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.471.258 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.473.061 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.473.063 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.473.090 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.474.786 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.474.787 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.474.787 I llama_init_from_model: graph nodes  = 967
0.01.474.787 I llama_init_from_model: graph splits = 2
0.01.474.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.474.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.509.062 I 
0.01.509.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.509.101 I perplexity: tokenizing the input ..
0.01.512.997 I perplexity: tokenization took 3.894 ms
0.01.513.001 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.631.728 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.633.169 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.633.200 I llama_perf_context_print:        load time =    1482.46 ms
0.01.633.201 I llama_perf_context_print: prompt eval time =     118.49 ms /   128 tokens (    0.93 ms per token,  1080.26 tokens per second)
0.01.633.202 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.633.202 I llama_perf_context_print:       total time =     124.14 ms /   129 tokens
0.01.633.564 I ggml_metal_free: deallocating

real	0m1.827s
user	0m0.097s
sys	0m0.269s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.998 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.034.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.499 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.503 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.503 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.043.944 I llama_model_loader: - type  f32:  194 tensors
0.00.043.944 I llama_model_loader: - type q8_0:   98 tensors
0.00.043.944 I print_info: file format = GGUF V3 (latest)
0.00.043.945 I print_info: file type   = Q8_0
0.00.043.946 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.767 I load: special tokens cache size = 25
0.00.061.719 I load: token to piece cache size = 0.2984 MB
0.00.061.723 I print_info: arch             = gptneox
0.00.061.723 I print_info: vocab_only       = 0
0.00.061.724 I print_info: n_ctx_train      = 2048
0.00.061.724 I print_info: n_embd           = 2048
0.00.061.724 I print_info: n_layer          = 24
0.00.061.729 I print_info: n_head           = 16
0.00.061.732 I print_info: n_head_kv        = 16
0.00.061.732 I print_info: n_rot            = 32
0.00.061.732 I print_info: n_swa            = 0
0.00.061.733 I print_info: n_embd_head_k    = 128
0.00.061.733 I print_info: n_embd_head_v    = 128
0.00.061.734 I print_info: n_gqa            = 1
0.00.061.735 I print_info: n_embd_k_gqa     = 2048
0.00.061.736 I print_info: n_embd_v_gqa     = 2048
0.00.061.737 I print_info: f_norm_eps       = 1.0e-05
0.00.061.737 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.740 I print_info: f_logit_scale    = 0.0e+00
0.00.061.740 I print_info: n_ff             = 8192
0.00.061.740 I print_info: n_expert         = 0
0.00.061.741 I print_info: n_expert_used    = 0
0.00.061.741 I print_info: causal attn      = 1
0.00.061.741 I print_info: pooling type     = 0
0.00.061.741 I print_info: rope type        = 2
0.00.061.741 I print_info: rope scaling     = linear
0.00.061.742 I print_info: freq_base_train  = 10000.0
0.00.061.742 I print_info: freq_scale_train = 1
0.00.061.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.742 I print_info: rope_finetuned   = unknown
0.00.061.744 I print_info: ssm_d_conv       = 0
0.00.061.744 I print_info: ssm_d_inner      = 0
0.00.061.744 I print_info: ssm_d_state      = 0
0.00.061.744 I print_info: ssm_dt_rank      = 0
0.00.061.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.745 I print_info: model type       = 1.4B
0.00.061.745 I print_info: model params     = 1.41 B
0.00.061.745 I print_info: general.name     = 1.4B
0.00.061.746 I print_info: vocab type       = BPE
0.00.061.746 I print_info: n_vocab          = 50304
0.00.061.746 I print_info: n_merges         = 50009
0.00.061.747 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.748 I print_info: LF token         = 187 ''
0.00.061.748 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.748 I print_info: max token length = 1024
0.00.061.748 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.371.044 I load_tensors: offloading 24 repeating layers to GPU
0.01.371.049 I load_tensors: offloading output layer to GPU
0.01.371.050 I load_tensors: offloaded 25/25 layers to GPU
0.01.371.072 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.371.073 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.372.236 I llama_init_from_model: n_seq_max     = 1
0.01.372.238 I llama_init_from_model: n_ctx         = 2048
0.01.372.238 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.372.239 I llama_init_from_model: n_batch       = 2048
0.01.372.239 I llama_init_from_model: n_ubatch      = 512
0.01.372.239 I llama_init_from_model: flash_attn    = 0
0.01.372.240 I llama_init_from_model: freq_base     = 10000.0
0.01.372.241 I llama_init_from_model: freq_scale    = 1
0.01.372.242 I ggml_metal_init: allocating
0.01.372.256 I ggml_metal_init: found device: Apple M4
0.01.372.263 I ggml_metal_init: picking default device: Apple M4
0.01.373.617 I ggml_metal_init: using embedded metal library
0.01.378.988 I ggml_metal_init: GPU name:   Apple M4
0.01.378.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.378.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.378.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.378.994 I ggml_metal_init: simdgroup reduction   = true
0.01.378.994 I ggml_metal_init: simdgroup matrix mul. = true
0.01.378.994 I ggml_metal_init: has residency sets    = true
0.01.378.994 I ggml_metal_init: has bfloat            = true
0.01.378.995 I ggml_metal_init: use bfloat            = true
0.01.378.995 I ggml_metal_init: hasUnifiedMemory      = true
0.01.378.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.393.990 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.446.900 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.446.906 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.446.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.452.137 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.452.139 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.452.139 I llama_init_from_model: graph nodes  = 967
0.01.452.140 I llama_init_from_model: graph splits = 2
0.01.452.146 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.452.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.452.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.505.669 I main: llama threadpool init, n_threads = 4
0.01.505.712 I 
0.01.505.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.505.741 I 
0.01.505.895 I sampler seed: 1234
0.01.505.900 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.505.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.505.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.505.944 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.592.964 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.02.592.964 I llama_perf_context_print:        load time =    1494.94 ms
0.02.592.965 I llama_perf_context_print: prompt eval time =      39.79 ms /     7 tokens (    5.68 ms per token,   175.93 tokens per second)
0.02.592.965 I llama_perf_context_print:        eval time =    1044.36 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.592.966 I llama_perf_context_print:       total time =    1088.02 ms /    70 tokens
0.02.593.187 I ggml_metal_free: deallocating

real	0m2.613s
user	0m0.112s
sys	0m0.286s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.221 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.704 I llama_model_loader: - type  f32:  194 tensors
0.00.025.705 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.706 I print_info: file format = GGUF V3 (latest)
0.00.025.706 I print_info: file type   = Q8_0
0.00.025.707 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.613 I load: special tokens cache size = 25
0.00.039.619 I load: token to piece cache size = 0.2984 MB
0.00.039.623 I print_info: arch             = gptneox
0.00.039.623 I print_info: vocab_only       = 0
0.00.039.624 I print_info: n_ctx_train      = 2048
0.00.039.624 I print_info: n_embd           = 2048
0.00.039.624 I print_info: n_layer          = 24
0.00.039.629 I print_info: n_head           = 16
0.00.039.630 I print_info: n_head_kv        = 16
0.00.039.630 I print_info: n_rot            = 32
0.00.039.630 I print_info: n_swa            = 0
0.00.039.630 I print_info: n_embd_head_k    = 128
0.00.039.630 I print_info: n_embd_head_v    = 128
0.00.039.631 I print_info: n_gqa            = 1
0.00.039.632 I print_info: n_embd_k_gqa     = 2048
0.00.039.632 I print_info: n_embd_v_gqa     = 2048
0.00.039.633 I print_info: f_norm_eps       = 1.0e-05
0.00.039.633 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.634 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.634 I print_info: f_logit_scale    = 0.0e+00
0.00.039.634 I print_info: n_ff             = 8192
0.00.039.636 I print_info: n_expert         = 0
0.00.039.636 I print_info: n_expert_used    = 0
0.00.039.637 I print_info: causal attn      = 1
0.00.039.638 I print_info: pooling type     = 0
0.00.039.638 I print_info: rope type        = 2
0.00.039.638 I print_info: rope scaling     = linear
0.00.039.638 I print_info: freq_base_train  = 10000.0
0.00.039.638 I print_info: freq_scale_train = 1
0.00.039.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.640 I print_info: rope_finetuned   = unknown
0.00.039.640 I print_info: ssm_d_conv       = 0
0.00.039.640 I print_info: ssm_d_inner      = 0
0.00.039.640 I print_info: ssm_d_state      = 0
0.00.039.641 I print_info: ssm_dt_rank      = 0
0.00.039.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.641 I print_info: model type       = 1.4B
0.00.039.641 I print_info: model params     = 1.41 B
0.00.039.641 I print_info: general.name     = 1.4B
0.00.039.642 I print_info: vocab type       = BPE
0.00.039.642 I print_info: n_vocab          = 50304
0.00.039.642 I print_info: n_merges         = 50009
0.00.039.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: LF token         = 187 ''
0.00.039.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.647 I print_info: max token length = 1024
0.00.039.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.912.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.912.043 I load_tensors: offloading output layer to GPU
0.00.912.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.912.075 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.912.076 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.913.221 I llama_init_from_model: n_seq_max     = 1
0.00.913.223 I llama_init_from_model: n_ctx         = 128
0.00.913.224 I llama_init_from_model: n_ctx_per_seq = 128
0.00.913.224 I llama_init_from_model: n_batch       = 128
0.00.913.224 I llama_init_from_model: n_ubatch      = 128
0.00.913.225 I llama_init_from_model: flash_attn    = 0
0.00.913.226 I llama_init_from_model: freq_base     = 10000.0
0.00.913.226 I llama_init_from_model: freq_scale    = 1
0.00.913.227 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.913.229 I ggml_metal_init: allocating
0.00.913.270 I ggml_metal_init: found device: Apple M4
0.00.913.280 I ggml_metal_init: picking default device: Apple M4
0.00.914.521 I ggml_metal_init: using embedded metal library
0.00.919.744 I ggml_metal_init: GPU name:   Apple M4
0.00.919.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.919.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.919.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.919.750 I ggml_metal_init: simdgroup reduction   = true
0.00.919.750 I ggml_metal_init: simdgroup matrix mul. = true
0.00.919.750 I ggml_metal_init: has residency sets    = true
0.00.919.750 I ggml_metal_init: has bfloat            = true
0.00.919.750 I ggml_metal_init: use bfloat            = true
0.00.919.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.919.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.934.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.938.412 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.938.415 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.938.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.941.811 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.941.813 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.941.814 I llama_init_from_model: graph nodes  = 967
0.00.941.814 I llama_init_from_model: graph splits = 2
0.00.941.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.941.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.968.555 I 
0.00.968.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.968.628 I perplexity: tokenizing the input ..
0.00.975.988 I perplexity: tokenization took 7.356 ms
0.00.976.004 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.114.928 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.116.376 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.116.398 I llama_perf_context_print:        load time =     958.49 ms
0.01.116.399 I llama_perf_context_print: prompt eval time =     137.97 ms /   128 tokens (    1.08 ms per token,   927.75 tokens per second)
0.01.116.399 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.116.400 I llama_perf_context_print:       total time =     147.85 ms /   129 tokens
0.01.116.779 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.075s
sys	0m0.157s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.103 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.014.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.025 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.026 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.026 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.028 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.032 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.032 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.032 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.824 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.011 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.012 I llama_model_loader: - type  f32:  194 tensors
0.00.040.012 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.012 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.013 I print_info: file format = GGUF V3 (latest)
0.00.040.013 I print_info: file type   = Q4_0
0.00.040.014 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.970 I load: special tokens cache size = 25
0.00.055.884 I load: token to piece cache size = 0.2984 MB
0.00.055.887 I print_info: arch             = gptneox
0.00.055.887 I print_info: vocab_only       = 0
0.00.055.887 I print_info: n_ctx_train      = 2048
0.00.055.887 I print_info: n_embd           = 2048
0.00.055.888 I print_info: n_layer          = 24
0.00.055.891 I print_info: n_head           = 16
0.00.055.892 I print_info: n_head_kv        = 16
0.00.055.892 I print_info: n_rot            = 32
0.00.055.893 I print_info: n_swa            = 0
0.00.055.893 I print_info: n_embd_head_k    = 128
0.00.055.893 I print_info: n_embd_head_v    = 128
0.00.055.894 I print_info: n_gqa            = 1
0.00.055.894 I print_info: n_embd_k_gqa     = 2048
0.00.055.895 I print_info: n_embd_v_gqa     = 2048
0.00.055.896 I print_info: f_norm_eps       = 1.0e-05
0.00.055.896 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.897 I print_info: f_logit_scale    = 0.0e+00
0.00.055.898 I print_info: n_ff             = 8192
0.00.055.898 I print_info: n_expert         = 0
0.00.055.898 I print_info: n_expert_used    = 0
0.00.055.898 I print_info: causal attn      = 1
0.00.055.898 I print_info: pooling type     = 0
0.00.055.898 I print_info: rope type        = 2
0.00.055.899 I print_info: rope scaling     = linear
0.00.055.899 I print_info: freq_base_train  = 10000.0
0.00.055.899 I print_info: freq_scale_train = 1
0.00.055.900 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.900 I print_info: rope_finetuned   = unknown
0.00.055.900 I print_info: ssm_d_conv       = 0
0.00.055.900 I print_info: ssm_d_inner      = 0
0.00.055.900 I print_info: ssm_d_state      = 0
0.00.055.900 I print_info: ssm_dt_rank      = 0
0.00.055.901 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.901 I print_info: model type       = 1.4B
0.00.055.901 I print_info: model params     = 1.41 B
0.00.055.901 I print_info: general.name     = 1.4B
0.00.055.902 I print_info: vocab type       = BPE
0.00.055.902 I print_info: n_vocab          = 50304
0.00.055.902 I print_info: n_merges         = 50009
0.00.055.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.903 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.903 I print_info: LF token         = 187 ''
0.00.055.903 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.904 I print_info: max token length = 1024
0.00.055.904 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.174 I load_tensors: offloading output layer to GPU
0.00.636.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.208 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.636.209 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.637.874 I llama_init_from_model: n_seq_max     = 1
0.00.637.877 I llama_init_from_model: n_ctx         = 2048
0.00.637.877 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.878 I llama_init_from_model: n_batch       = 2048
0.00.637.878 I llama_init_from_model: n_ubatch      = 512
0.00.637.879 I llama_init_from_model: flash_attn    = 0
0.00.637.881 I llama_init_from_model: freq_base     = 10000.0
0.00.637.881 I llama_init_from_model: freq_scale    = 1
0.00.637.884 I ggml_metal_init: allocating
0.00.637.957 I ggml_metal_init: found device: Apple M4
0.00.637.972 I ggml_metal_init: picking default device: Apple M4
0.00.639.811 I ggml_metal_init: using embedded metal library
0.00.646.506 I ggml_metal_init: GPU name:   Apple M4
0.00.646.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.517 I ggml_metal_init: simdgroup reduction   = true
0.00.646.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.518 I ggml_metal_init: has residency sets    = true
0.00.646.523 I ggml_metal_init: has bfloat            = true
0.00.646.523 I ggml_metal_init: use bfloat            = true
0.00.646.524 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.718.572 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.611 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.709 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.712 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.712 I llama_init_from_model: graph nodes  = 967
0.00.722.712 I llama_init_from_model: graph splits = 2
0.00.722.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.234 I main: llama threadpool init, n_threads = 4
0.00.780.278 I 
0.00.780.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.303 I 
0.00.780.482 I sampler seed: 1234
0.00.780.487 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.507 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.507 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.456.317 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.456.317 I llama_perf_context_print:        load time =     764.74 ms
0.01.456.318 I llama_perf_context_print: prompt eval time =      45.24 ms /     7 tokens (    6.46 ms per token,   154.72 tokens per second)
0.01.456.319 I llama_perf_context_print:        eval time =     627.63 ms /    63 runs   (    9.96 ms per token,   100.38 tokens per second)
0.01.456.319 I llama_perf_context_print:       total time =     676.84 ms /    70 tokens
0.01.456.620 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.112s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.499 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.500 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.994 I llama_model_loader: - type  f32:  194 tensors
0.00.025.994 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.996 I print_info: file format = GGUF V3 (latest)
0.00.025.996 I print_info: file type   = Q4_0
0.00.025.997 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.991 I load: special tokens cache size = 25
0.00.040.019 I load: token to piece cache size = 0.2984 MB
0.00.040.023 I print_info: arch             = gptneox
0.00.040.023 I print_info: vocab_only       = 0
0.00.040.023 I print_info: n_ctx_train      = 2048
0.00.040.023 I print_info: n_embd           = 2048
0.00.040.024 I print_info: n_layer          = 24
0.00.040.028 I print_info: n_head           = 16
0.00.040.028 I print_info: n_head_kv        = 16
0.00.040.028 I print_info: n_rot            = 32
0.00.040.031 I print_info: n_swa            = 0
0.00.040.031 I print_info: n_embd_head_k    = 128
0.00.040.032 I print_info: n_embd_head_v    = 128
0.00.040.032 I print_info: n_gqa            = 1
0.00.040.033 I print_info: n_embd_k_gqa     = 2048
0.00.040.034 I print_info: n_embd_v_gqa     = 2048
0.00.040.034 I print_info: f_norm_eps       = 1.0e-05
0.00.040.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.035 I print_info: f_logit_scale    = 0.0e+00
0.00.040.036 I print_info: n_ff             = 8192
0.00.040.036 I print_info: n_expert         = 0
0.00.040.036 I print_info: n_expert_used    = 0
0.00.040.036 I print_info: causal attn      = 1
0.00.040.037 I print_info: pooling type     = 0
0.00.040.037 I print_info: rope type        = 2
0.00.040.037 I print_info: rope scaling     = linear
0.00.040.037 I print_info: freq_base_train  = 10000.0
0.00.040.037 I print_info: freq_scale_train = 1
0.00.040.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.038 I print_info: rope_finetuned   = unknown
0.00.040.038 I print_info: ssm_d_conv       = 0
0.00.040.038 I print_info: ssm_d_inner      = 0
0.00.040.038 I print_info: ssm_d_state      = 0
0.00.040.038 I print_info: ssm_dt_rank      = 0
0.00.040.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.039 I print_info: model type       = 1.4B
0.00.040.039 I print_info: model params     = 1.41 B
0.00.040.040 I print_info: general.name     = 1.4B
0.00.040.040 I print_info: vocab type       = BPE
0.00.040.041 I print_info: n_vocab          = 50304
0.00.040.041 I print_info: n_merges         = 50009
0.00.040.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: LF token         = 187 ''
0.00.040.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: max token length = 1024
0.00.040.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.008 I load_tensors: offloading output layer to GPU
0.00.588.009 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.044 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.045 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.589.529 I llama_init_from_model: n_seq_max     = 1
0.00.589.532 I llama_init_from_model: n_ctx         = 128
0.00.589.533 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.533 I llama_init_from_model: n_batch       = 128
0.00.589.534 I llama_init_from_model: n_ubatch      = 128
0.00.589.534 I llama_init_from_model: flash_attn    = 0
0.00.589.537 I llama_init_from_model: freq_base     = 10000.0
0.00.589.537 I llama_init_from_model: freq_scale    = 1
0.00.589.538 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.541 I ggml_metal_init: allocating
0.00.589.618 I ggml_metal_init: found device: Apple M4
0.00.589.632 I ggml_metal_init: picking default device: Apple M4
0.00.591.511 I ggml_metal_init: using embedded metal library
0.00.598.332 I ggml_metal_init: GPU name:   Apple M4
0.00.598.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.341 I ggml_metal_init: simdgroup reduction   = true
0.00.598.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.342 I ggml_metal_init: has residency sets    = true
0.00.598.342 I ggml_metal_init: has bfloat            = true
0.00.598.342 I ggml_metal_init: use bfloat            = true
0.00.598.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.730 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.281 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.288 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.609 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.610 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.611 I llama_init_from_model: graph nodes  = 967
0.00.623.611 I llama_init_from_model: graph splits = 2
0.00.623.614 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.667 I 
0.00.651.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.758 I perplexity: tokenizing the input ..
0.00.658.929 I perplexity: tokenization took 7.169 ms
0.00.658.939 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.573 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.795.903 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.795.928 I llama_perf_context_print:        load time =     641.34 ms
0.00.795.929 I llama_perf_context_print: prompt eval time =     135.25 ms /   128 tokens (    1.06 ms per token,   946.40 tokens per second)
0.00.795.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.930 I llama_perf_context_print:       total time =     144.27 ms /   129 tokens
0.00.796.308 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.079s
sys	0m0.118s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.393 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.073 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.074 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.074 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.075 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.077 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.850 I llama_model_loader: - type  f32:  194 tensors
0.00.036.851 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.851 I print_info: file format = GGUF V3 (latest)
0.00.036.852 I print_info: file type   = Q4_1
0.00.036.852 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.999 I load: special tokens cache size = 25
0.00.052.007 I load: token to piece cache size = 0.2984 MB
0.00.052.009 I print_info: arch             = gptneox
0.00.052.010 I print_info: vocab_only       = 0
0.00.052.010 I print_info: n_ctx_train      = 2048
0.00.052.010 I print_info: n_embd           = 2048
0.00.052.010 I print_info: n_layer          = 24
0.00.052.013 I print_info: n_head           = 16
0.00.052.014 I print_info: n_head_kv        = 16
0.00.052.015 I print_info: n_rot            = 32
0.00.052.015 I print_info: n_swa            = 0
0.00.052.016 I print_info: n_embd_head_k    = 128
0.00.052.016 I print_info: n_embd_head_v    = 128
0.00.052.017 I print_info: n_gqa            = 1
0.00.052.018 I print_info: n_embd_k_gqa     = 2048
0.00.052.018 I print_info: n_embd_v_gqa     = 2048
0.00.052.023 I print_info: f_norm_eps       = 1.0e-05
0.00.052.023 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.023 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.024 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.024 I print_info: f_logit_scale    = 0.0e+00
0.00.052.024 I print_info: n_ff             = 8192
0.00.052.025 I print_info: n_expert         = 0
0.00.052.025 I print_info: n_expert_used    = 0
0.00.052.025 I print_info: causal attn      = 1
0.00.052.025 I print_info: pooling type     = 0
0.00.052.025 I print_info: rope type        = 2
0.00.052.026 I print_info: rope scaling     = linear
0.00.052.026 I print_info: freq_base_train  = 10000.0
0.00.052.029 I print_info: freq_scale_train = 1
0.00.052.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.029 I print_info: rope_finetuned   = unknown
0.00.052.030 I print_info: ssm_d_conv       = 0
0.00.052.030 I print_info: ssm_d_inner      = 0
0.00.052.031 I print_info: ssm_d_state      = 0
0.00.052.031 I print_info: ssm_dt_rank      = 0
0.00.052.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.032 I print_info: model type       = 1.4B
0.00.052.032 I print_info: model params     = 1.41 B
0.00.052.032 I print_info: general.name     = 1.4B
0.00.052.032 I print_info: vocab type       = BPE
0.00.052.033 I print_info: n_vocab          = 50304
0.00.052.033 I print_info: n_merges         = 50009
0.00.052.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.034 I print_info: LF token         = 187 ''
0.00.052.034 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.034 I print_info: max token length = 1024
0.00.052.035 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.777.267 I load_tensors: offloading 24 repeating layers to GPU
0.00.777.280 I load_tensors: offloading output layer to GPU
0.00.777.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.777.314 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.777.315 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.778.752 I llama_init_from_model: n_seq_max     = 1
0.00.778.755 I llama_init_from_model: n_ctx         = 2048
0.00.778.756 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.778.756 I llama_init_from_model: n_batch       = 2048
0.00.778.757 I llama_init_from_model: n_ubatch      = 512
0.00.778.757 I llama_init_from_model: flash_attn    = 0
0.00.778.760 I llama_init_from_model: freq_base     = 10000.0
0.00.778.760 I llama_init_from_model: freq_scale    = 1
0.00.778.763 I ggml_metal_init: allocating
0.00.778.841 I ggml_metal_init: found device: Apple M4
0.00.778.854 I ggml_metal_init: picking default device: Apple M4
0.00.780.970 I ggml_metal_init: using embedded metal library
0.00.787.867 I ggml_metal_init: GPU name:   Apple M4
0.00.787.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.787.873 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.787.874 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.787.874 I ggml_metal_init: simdgroup reduction   = true
0.00.787.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.787.875 I ggml_metal_init: has residency sets    = true
0.00.787.875 I ggml_metal_init: has bfloat            = true
0.00.787.875 I ggml_metal_init: use bfloat            = true
0.00.787.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.787.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.806.027 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.865.131 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.865.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.865.178 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.869.326 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.869.328 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.869.329 I llama_init_from_model: graph nodes  = 967
0.00.869.329 I llama_init_from_model: graph splits = 2
0.00.869.339 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.869.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.869.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.925.587 I main: llama threadpool init, n_threads = 4
0.00.925.632 I 
0.00.925.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.925.657 I 
0.00.925.807 I sampler seed: 1234
0.00.925.812 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.925.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.925.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.925.831 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.654.469 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.654.470 I llama_perf_context_print:        load time =     914.44 ms
0.01.654.471 I llama_perf_context_print: prompt eval time =      48.86 ms /     7 tokens (    6.98 ms per token,   143.28 tokens per second)
0.01.654.472 I llama_perf_context_print:        eval time =     677.04 ms /    63 runs   (   10.75 ms per token,    93.05 tokens per second)
0.01.654.473 I llama_perf_context_print:       total time =     729.64 ms /    70 tokens
0.01.654.734 I ggml_metal_free: deallocating

real	0m1.675s
user	0m0.112s
sys	0m0.235s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.118 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.020 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.554 I llama_model_loader: - type  f32:  194 tensors
0.00.024.555 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.556 I print_info: file format = GGUF V3 (latest)
0.00.024.556 I print_info: file type   = Q4_1
0.00.024.557 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.840 I load: special tokens cache size = 25
0.00.039.012 I load: token to piece cache size = 0.2984 MB
0.00.039.016 I print_info: arch             = gptneox
0.00.039.016 I print_info: vocab_only       = 0
0.00.039.017 I print_info: n_ctx_train      = 2048
0.00.039.017 I print_info: n_embd           = 2048
0.00.039.017 I print_info: n_layer          = 24
0.00.039.021 I print_info: n_head           = 16
0.00.039.022 I print_info: n_head_kv        = 16
0.00.039.022 I print_info: n_rot            = 32
0.00.039.022 I print_info: n_swa            = 0
0.00.039.022 I print_info: n_embd_head_k    = 128
0.00.039.024 I print_info: n_embd_head_v    = 128
0.00.039.025 I print_info: n_gqa            = 1
0.00.039.026 I print_info: n_embd_k_gqa     = 2048
0.00.039.027 I print_info: n_embd_v_gqa     = 2048
0.00.039.027 I print_info: f_norm_eps       = 1.0e-05
0.00.039.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.028 I print_info: f_logit_scale    = 0.0e+00
0.00.039.029 I print_info: n_ff             = 8192
0.00.039.029 I print_info: n_expert         = 0
0.00.039.029 I print_info: n_expert_used    = 0
0.00.039.029 I print_info: causal attn      = 1
0.00.039.029 I print_info: pooling type     = 0
0.00.039.030 I print_info: rope type        = 2
0.00.039.030 I print_info: rope scaling     = linear
0.00.039.030 I print_info: freq_base_train  = 10000.0
0.00.039.031 I print_info: freq_scale_train = 1
0.00.039.031 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.031 I print_info: rope_finetuned   = unknown
0.00.039.031 I print_info: ssm_d_conv       = 0
0.00.039.031 I print_info: ssm_d_inner      = 0
0.00.039.031 I print_info: ssm_d_state      = 0
0.00.039.031 I print_info: ssm_dt_rank      = 0
0.00.039.032 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.032 I print_info: model type       = 1.4B
0.00.039.032 I print_info: model params     = 1.41 B
0.00.039.034 I print_info: general.name     = 1.4B
0.00.039.034 I print_info: vocab type       = BPE
0.00.039.034 I print_info: n_vocab          = 50304
0.00.039.034 I print_info: n_merges         = 50009
0.00.039.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: LF token         = 187 ''
0.00.039.036 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.036 I print_info: max token length = 1024
0.00.039.037 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.596 I load_tensors: offloading output layer to GPU
0.00.685.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.633 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.685.635 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.687.404 I llama_init_from_model: n_seq_max     = 1
0.00.687.408 I llama_init_from_model: n_ctx         = 128
0.00.687.409 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.409 I llama_init_from_model: n_batch       = 128
0.00.687.410 I llama_init_from_model: n_ubatch      = 128
0.00.687.411 I llama_init_from_model: flash_attn    = 0
0.00.687.413 I llama_init_from_model: freq_base     = 10000.0
0.00.687.413 I llama_init_from_model: freq_scale    = 1
0.00.687.414 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.417 I ggml_metal_init: allocating
0.00.687.516 I ggml_metal_init: found device: Apple M4
0.00.687.531 I ggml_metal_init: picking default device: Apple M4
0.00.689.443 I ggml_metal_init: using embedded metal library
0.00.696.269 I ggml_metal_init: GPU name:   Apple M4
0.00.696.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.279 I ggml_metal_init: simdgroup reduction   = true
0.00.696.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.280 I ggml_metal_init: has residency sets    = true
0.00.696.280 I ggml_metal_init: has bfloat            = true
0.00.696.280 I ggml_metal_init: use bfloat            = true
0.00.696.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.198 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.718.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.718.248 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.454 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.456 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.457 I llama_init_from_model: graph nodes  = 967
0.00.721.457 I llama_init_from_model: graph splits = 2
0.00.721.461 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.028 I 
0.00.751.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.120 I perplexity: tokenizing the input ..
0.00.758.390 I perplexity: tokenization took 7.267 ms
0.00.758.401 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.895.138 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.896.484 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.896.507 I llama_perf_context_print:        load time =     742.00 ms
0.00.896.508 I llama_perf_context_print: prompt eval time =     135.78 ms /   128 tokens (    1.06 ms per token,   942.68 tokens per second)
0.00.896.509 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.896.509 I llama_perf_context_print:       total time =     145.48 ms /   129 tokens
0.00.896.874 I ggml_metal_free: deallocating

real	0m0.911s
user	0m0.081s
sys	0m0.138s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.016.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.830 I llama_model_loader: - type  f32:  194 tensors
0.00.045.830 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.831 I print_info: file format = GGUF V3 (latest)
0.00.045.832 I print_info: file type   = Q5_0
0.00.045.833 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.058.041 I load: special tokens cache size = 25
0.00.068.301 I load: token to piece cache size = 0.2984 MB
0.00.068.306 I print_info: arch             = gptneox
0.00.068.306 I print_info: vocab_only       = 0
0.00.068.306 I print_info: n_ctx_train      = 2048
0.00.068.307 I print_info: n_embd           = 2048
0.00.068.307 I print_info: n_layer          = 24
0.00.068.311 I print_info: n_head           = 16
0.00.068.312 I print_info: n_head_kv        = 16
0.00.068.313 I print_info: n_rot            = 32
0.00.068.313 I print_info: n_swa            = 0
0.00.068.313 I print_info: n_embd_head_k    = 128
0.00.068.314 I print_info: n_embd_head_v    = 128
0.00.068.315 I print_info: n_gqa            = 1
0.00.068.316 I print_info: n_embd_k_gqa     = 2048
0.00.068.317 I print_info: n_embd_v_gqa     = 2048
0.00.068.318 I print_info: f_norm_eps       = 1.0e-05
0.00.068.318 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.319 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.319 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.319 I print_info: f_logit_scale    = 0.0e+00
0.00.068.320 I print_info: n_ff             = 8192
0.00.068.321 I print_info: n_expert         = 0
0.00.068.321 I print_info: n_expert_used    = 0
0.00.068.321 I print_info: causal attn      = 1
0.00.068.321 I print_info: pooling type     = 0
0.00.068.324 I print_info: rope type        = 2
0.00.068.326 I print_info: rope scaling     = linear
0.00.068.326 I print_info: freq_base_train  = 10000.0
0.00.068.327 I print_info: freq_scale_train = 1
0.00.068.327 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.327 I print_info: rope_finetuned   = unknown
0.00.068.327 I print_info: ssm_d_conv       = 0
0.00.068.328 I print_info: ssm_d_inner      = 0
0.00.068.328 I print_info: ssm_d_state      = 0
0.00.068.328 I print_info: ssm_dt_rank      = 0
0.00.068.328 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.329 I print_info: model type       = 1.4B
0.00.068.329 I print_info: model params     = 1.41 B
0.00.068.329 I print_info: general.name     = 1.4B
0.00.068.330 I print_info: vocab type       = BPE
0.00.068.331 I print_info: n_vocab          = 50304
0.00.068.331 I print_info: n_merges         = 50009
0.00.068.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.333 I print_info: LF token         = 187 ''
0.00.068.333 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.333 I print_info: max token length = 1024
0.00.068.334 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.746.892 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.905 I load_tensors: offloading output layer to GPU
0.00.746.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.940 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.746.941 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.748.431 I llama_init_from_model: n_seq_max     = 1
0.00.748.437 I llama_init_from_model: n_ctx         = 2048
0.00.748.438 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.748.438 I llama_init_from_model: n_batch       = 2048
0.00.748.438 I llama_init_from_model: n_ubatch      = 512
0.00.748.439 I llama_init_from_model: flash_attn    = 0
0.00.748.440 I llama_init_from_model: freq_base     = 10000.0
0.00.748.440 I llama_init_from_model: freq_scale    = 1
0.00.748.443 I ggml_metal_init: allocating
0.00.748.495 I ggml_metal_init: found device: Apple M4
0.00.748.506 I ggml_metal_init: picking default device: Apple M4
0.00.751.082 I ggml_metal_init: using embedded metal library
0.00.757.841 I ggml_metal_init: GPU name:   Apple M4
0.00.757.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.757.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.757.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.757.849 I ggml_metal_init: simdgroup reduction   = true
0.00.757.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.757.850 I ggml_metal_init: has residency sets    = true
0.00.757.850 I ggml_metal_init: has bfloat            = true
0.00.757.850 I ggml_metal_init: use bfloat            = true
0.00.757.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.757.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.775.403 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.826.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.826.039 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.826.076 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.830.832 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.830.833 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.830.834 I llama_init_from_model: graph nodes  = 967
0.00.830.834 I llama_init_from_model: graph splits = 2
0.00.830.839 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.830.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.830.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.890.004 I main: llama threadpool init, n_threads = 4
0.00.890.045 I 
0.00.890.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.890.068 I 
0.00.890.215 I sampler seed: 1234
0.00.890.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.890.230 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.890.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.890.232 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.681.743 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.01.681.744 I llama_perf_context_print:        load time =     872.31 ms
0.01.681.746 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.02 tokens per second)
0.01.681.747 I llama_perf_context_print:        eval time =     740.11 ms /    63 runs   (   11.75 ms per token,    85.12 tokens per second)
0.01.681.747 I llama_perf_context_print:       total time =     792.49 ms /    70 tokens
0.01.682.021 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.125s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.978 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.416 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.419 I llama_model_loader: - type  f32:  194 tensors
0.00.025.420 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.420 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.421 I print_info: file format = GGUF V3 (latest)
0.00.025.421 I print_info: file type   = Q5_0
0.00.025.423 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.677 I load: special tokens cache size = 25
0.00.039.607 I load: token to piece cache size = 0.2984 MB
0.00.039.612 I print_info: arch             = gptneox
0.00.039.612 I print_info: vocab_only       = 0
0.00.039.612 I print_info: n_ctx_train      = 2048
0.00.039.612 I print_info: n_embd           = 2048
0.00.039.613 I print_info: n_layer          = 24
0.00.039.617 I print_info: n_head           = 16
0.00.039.617 I print_info: n_head_kv        = 16
0.00.039.617 I print_info: n_rot            = 32
0.00.039.618 I print_info: n_swa            = 0
0.00.039.618 I print_info: n_embd_head_k    = 128
0.00.039.618 I print_info: n_embd_head_v    = 128
0.00.039.619 I print_info: n_gqa            = 1
0.00.039.620 I print_info: n_embd_k_gqa     = 2048
0.00.039.620 I print_info: n_embd_v_gqa     = 2048
0.00.039.621 I print_info: f_norm_eps       = 1.0e-05
0.00.039.621 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.622 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.622 I print_info: f_logit_scale    = 0.0e+00
0.00.039.622 I print_info: n_ff             = 8192
0.00.039.623 I print_info: n_expert         = 0
0.00.039.623 I print_info: n_expert_used    = 0
0.00.039.623 I print_info: causal attn      = 1
0.00.039.623 I print_info: pooling type     = 0
0.00.039.623 I print_info: rope type        = 2
0.00.039.624 I print_info: rope scaling     = linear
0.00.039.624 I print_info: freq_base_train  = 10000.0
0.00.039.624 I print_info: freq_scale_train = 1
0.00.039.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.625 I print_info: rope_finetuned   = unknown
0.00.039.625 I print_info: ssm_d_conv       = 0
0.00.039.625 I print_info: ssm_d_inner      = 0
0.00.039.625 I print_info: ssm_d_state      = 0
0.00.039.625 I print_info: ssm_dt_rank      = 0
0.00.039.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.626 I print_info: model type       = 1.4B
0.00.039.626 I print_info: model params     = 1.41 B
0.00.039.626 I print_info: general.name     = 1.4B
0.00.039.627 I print_info: vocab type       = BPE
0.00.039.627 I print_info: n_vocab          = 50304
0.00.039.627 I print_info: n_merges         = 50009
0.00.039.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: LF token         = 187 ''
0.00.039.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.632 I print_info: max token length = 1024
0.00.039.632 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.386 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.409 I load_tensors: offloading output layer to GPU
0.00.711.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.447 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.449 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.712.886 I llama_init_from_model: n_seq_max     = 1
0.00.712.890 I llama_init_from_model: n_ctx         = 128
0.00.712.890 I llama_init_from_model: n_ctx_per_seq = 128
0.00.712.891 I llama_init_from_model: n_batch       = 128
0.00.712.891 I llama_init_from_model: n_ubatch      = 128
0.00.712.891 I llama_init_from_model: flash_attn    = 0
0.00.712.895 I llama_init_from_model: freq_base     = 10000.0
0.00.712.895 I llama_init_from_model: freq_scale    = 1
0.00.712.896 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.712.903 I ggml_metal_init: allocating
0.00.712.980 I ggml_metal_init: found device: Apple M4
0.00.712.994 I ggml_metal_init: picking default device: Apple M4
0.00.715.451 I ggml_metal_init: using embedded metal library
0.00.722.402 I ggml_metal_init: GPU name:   Apple M4
0.00.722.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.722.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.722.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.722.412 I ggml_metal_init: simdgroup reduction   = true
0.00.722.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.722.412 I ggml_metal_init: has residency sets    = true
0.00.722.412 I ggml_metal_init: has bfloat            = true
0.00.722.413 I ggml_metal_init: use bfloat            = true
0.00.722.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.722.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.081 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.744.084 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.744.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.332 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.747.335 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.747.335 I llama_init_from_model: graph nodes  = 967
0.00.747.336 I llama_init_from_model: graph splits = 2
0.00.747.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.747.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.605 I 
0.00.776.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.668 I perplexity: tokenizing the input ..
0.00.783.043 I perplexity: tokenization took 6.372 ms
0.00.783.048 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.928.271 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.929.621 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.929.648 I llama_perf_context_print:        load time =     766.81 ms
0.00.929.649 I llama_perf_context_print: prompt eval time =     144.77 ms /   128 tokens (    1.13 ms per token,   884.16 tokens per second)
0.00.929.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.929.650 I llama_perf_context_print:       total time =     153.05 ms /   129 tokens
0.00.930.128 I ggml_metal_free: deallocating

real	0m0.945s
user	0m0.080s
sys	0m0.145s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.182 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.830 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.605 I llama_model_loader: - type  f32:  194 tensors
0.00.025.605 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.605 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.606 I print_info: file format = GGUF V3 (latest)
0.00.025.606 I print_info: file type   = Q5_1
0.00.025.608 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.759 I load: special tokens cache size = 25
0.00.039.825 I load: token to piece cache size = 0.2984 MB
0.00.039.829 I print_info: arch             = gptneox
0.00.039.830 I print_info: vocab_only       = 0
0.00.039.830 I print_info: n_ctx_train      = 2048
0.00.039.830 I print_info: n_embd           = 2048
0.00.039.830 I print_info: n_layer          = 24
0.00.039.834 I print_info: n_head           = 16
0.00.039.835 I print_info: n_head_kv        = 16
0.00.039.835 I print_info: n_rot            = 32
0.00.039.835 I print_info: n_swa            = 0
0.00.039.835 I print_info: n_embd_head_k    = 128
0.00.039.836 I print_info: n_embd_head_v    = 128
0.00.039.841 I print_info: n_gqa            = 1
0.00.039.841 I print_info: n_embd_k_gqa     = 2048
0.00.039.842 I print_info: n_embd_v_gqa     = 2048
0.00.039.843 I print_info: f_norm_eps       = 1.0e-05
0.00.039.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.843 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.843 I print_info: f_logit_scale    = 0.0e+00
0.00.039.844 I print_info: n_ff             = 8192
0.00.039.844 I print_info: n_expert         = 0
0.00.039.844 I print_info: n_expert_used    = 0
0.00.039.844 I print_info: causal attn      = 1
0.00.039.844 I print_info: pooling type     = 0
0.00.039.845 I print_info: rope type        = 2
0.00.039.846 I print_info: rope scaling     = linear
0.00.039.847 I print_info: freq_base_train  = 10000.0
0.00.039.847 I print_info: freq_scale_train = 1
0.00.039.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.847 I print_info: rope_finetuned   = unknown
0.00.039.847 I print_info: ssm_d_conv       = 0
0.00.039.847 I print_info: ssm_d_inner      = 0
0.00.039.848 I print_info: ssm_d_state      = 0
0.00.039.848 I print_info: ssm_dt_rank      = 0
0.00.039.848 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.848 I print_info: model type       = 1.4B
0.00.039.849 I print_info: model params     = 1.41 B
0.00.039.849 I print_info: general.name     = 1.4B
0.00.039.849 I print_info: vocab type       = BPE
0.00.039.849 I print_info: n_vocab          = 50304
0.00.039.850 I print_info: n_merges         = 50009
0.00.039.850 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: LF token         = 187 ''
0.00.039.851 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: max token length = 1024
0.00.039.851 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.438 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.444 I load_tensors: offloading output layer to GPU
0.00.631.445 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.463 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.631.464 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.632.342 I llama_init_from_model: n_seq_max     = 1
0.00.632.347 I llama_init_from_model: n_ctx         = 2048
0.00.632.347 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.632.348 I llama_init_from_model: n_batch       = 2048
0.00.632.348 I llama_init_from_model: n_ubatch      = 512
0.00.632.348 I llama_init_from_model: flash_attn    = 0
0.00.632.349 I llama_init_from_model: freq_base     = 10000.0
0.00.632.350 I llama_init_from_model: freq_scale    = 1
0.00.632.351 I ggml_metal_init: allocating
0.00.632.388 I ggml_metal_init: found device: Apple M4
0.00.632.399 I ggml_metal_init: picking default device: Apple M4
0.00.633.497 I ggml_metal_init: using embedded metal library
0.00.637.776 I ggml_metal_init: GPU name:   Apple M4
0.00.637.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.782 I ggml_metal_init: simdgroup reduction   = true
0.00.637.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.783 I ggml_metal_init: has residency sets    = true
0.00.637.783 I ggml_metal_init: has bfloat            = true
0.00.637.783 I ggml_metal_init: use bfloat            = true
0.00.637.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.203 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.898 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.903 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.045 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.046 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.047 I llama_init_from_model: graph nodes  = 967
0.00.688.047 I llama_init_from_model: graph splits = 2
0.00.688.052 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.292 I main: llama threadpool init, n_threads = 4
0.00.742.326 I 
0.00.742.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.346 I 
0.00.742.497 I sampler seed: 1234
0.00.742.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.528 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.529 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.529 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.572.539 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47747.14 tokens per second)
0.01.572.539 I llama_perf_context_print:        load time =     732.41 ms
0.01.572.540 I llama_perf_context_print: prompt eval time =      41.82 ms /     7 tokens (    5.97 ms per token,   167.38 tokens per second)
0.01.572.542 I llama_perf_context_print:        eval time =     785.29 ms /    63 runs   (   12.46 ms per token,    80.23 tokens per second)
0.01.572.542 I llama_perf_context_print:       total time =     830.94 ms /    70 tokens
0.01.572.789 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.105s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.395 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.394 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.396 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.396 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.930 I llama_model_loader: - type  f32:  194 tensors
0.00.024.930 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.930 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.931 I print_info: file format = GGUF V3 (latest)
0.00.024.931 I print_info: file type   = Q5_1
0.00.024.933 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.825 I load: special tokens cache size = 25
0.00.038.562 I load: token to piece cache size = 0.2984 MB
0.00.038.566 I print_info: arch             = gptneox
0.00.038.566 I print_info: vocab_only       = 0
0.00.038.567 I print_info: n_ctx_train      = 2048
0.00.038.567 I print_info: n_embd           = 2048
0.00.038.567 I print_info: n_layer          = 24
0.00.038.572 I print_info: n_head           = 16
0.00.038.573 I print_info: n_head_kv        = 16
0.00.038.573 I print_info: n_rot            = 32
0.00.038.573 I print_info: n_swa            = 0
0.00.038.573 I print_info: n_embd_head_k    = 128
0.00.038.574 I print_info: n_embd_head_v    = 128
0.00.038.575 I print_info: n_gqa            = 1
0.00.038.575 I print_info: n_embd_k_gqa     = 2048
0.00.038.576 I print_info: n_embd_v_gqa     = 2048
0.00.038.576 I print_info: f_norm_eps       = 1.0e-05
0.00.038.577 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.577 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.577 I print_info: f_logit_scale    = 0.0e+00
0.00.038.578 I print_info: n_ff             = 8192
0.00.038.578 I print_info: n_expert         = 0
0.00.038.578 I print_info: n_expert_used    = 0
0.00.038.578 I print_info: causal attn      = 1
0.00.038.579 I print_info: pooling type     = 0
0.00.038.579 I print_info: rope type        = 2
0.00.038.580 I print_info: rope scaling     = linear
0.00.038.580 I print_info: freq_base_train  = 10000.0
0.00.038.581 I print_info: freq_scale_train = 1
0.00.038.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.581 I print_info: rope_finetuned   = unknown
0.00.038.581 I print_info: ssm_d_conv       = 0
0.00.038.581 I print_info: ssm_d_inner      = 0
0.00.038.582 I print_info: ssm_d_state      = 0
0.00.038.582 I print_info: ssm_dt_rank      = 0
0.00.038.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.584 I print_info: model type       = 1.4B
0.00.038.585 I print_info: model params     = 1.41 B
0.00.038.585 I print_info: general.name     = 1.4B
0.00.038.585 I print_info: vocab type       = BPE
0.00.038.586 I print_info: n_vocab          = 50304
0.00.038.586 I print_info: n_merges         = 50009
0.00.038.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.586 I print_info: LF token         = 187 ''
0.00.038.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.588 I print_info: max token length = 1024
0.00.038.588 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.680 I load_tensors: offloading output layer to GPU
0.00.614.680 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.717 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.614.718 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.616.201 I llama_init_from_model: n_seq_max     = 1
0.00.616.205 I llama_init_from_model: n_ctx         = 128
0.00.616.205 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.206 I llama_init_from_model: n_batch       = 128
0.00.616.206 I llama_init_from_model: n_ubatch      = 128
0.00.616.207 I llama_init_from_model: flash_attn    = 0
0.00.616.211 I llama_init_from_model: freq_base     = 10000.0
0.00.616.211 I llama_init_from_model: freq_scale    = 1
0.00.616.212 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.214 I ggml_metal_init: allocating
0.00.616.301 I ggml_metal_init: found device: Apple M4
0.00.616.315 I ggml_metal_init: picking default device: Apple M4
0.00.618.218 I ggml_metal_init: using embedded metal library
0.00.625.254 I ggml_metal_init: GPU name:   Apple M4
0.00.625.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.266 I ggml_metal_init: simdgroup reduction   = true
0.00.625.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.266 I ggml_metal_init: has residency sets    = true
0.00.625.267 I ggml_metal_init: has bfloat            = true
0.00.625.267 I ggml_metal_init: use bfloat            = true
0.00.625.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.864 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.442 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.447 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.484 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.884 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.886 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.886 I llama_init_from_model: graph nodes  = 967
0.00.650.887 I llama_init_from_model: graph splits = 2
0.00.650.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.017 I 
0.00.681.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.086 I perplexity: tokenizing the input ..
0.00.687.588 I perplexity: tokenization took 6.498 ms
0.00.687.595 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.846 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.831.166 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.831.189 I llama_perf_context_print:        load time =     671.62 ms
0.00.831.190 I llama_perf_context_print: prompt eval time =     141.27 ms /   128 tokens (    1.10 ms per token,   906.09 tokens per second)
0.00.831.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.191 I llama_perf_context_print:       total time =     150.17 ms /   129 tokens
0.00.831.633 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.080s
sys	0m0.148s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.638 I llama_model_loader: - type  f32:  194 tensors
0.00.024.639 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.639 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.640 I print_info: file format = GGUF V3 (latest)
0.00.024.640 I print_info: file type   = Q2_K - Medium
0.00.024.641 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.333 I load: special tokens cache size = 25
0.00.038.253 I load: token to piece cache size = 0.2984 MB
0.00.038.256 I print_info: arch             = gptneox
0.00.038.256 I print_info: vocab_only       = 0
0.00.038.256 I print_info: n_ctx_train      = 2048
0.00.038.257 I print_info: n_embd           = 2048
0.00.038.257 I print_info: n_layer          = 24
0.00.038.259 I print_info: n_head           = 16
0.00.038.260 I print_info: n_head_kv        = 16
0.00.038.260 I print_info: n_rot            = 32
0.00.038.260 I print_info: n_swa            = 0
0.00.038.261 I print_info: n_embd_head_k    = 128
0.00.038.263 I print_info: n_embd_head_v    = 128
0.00.038.263 I print_info: n_gqa            = 1
0.00.038.264 I print_info: n_embd_k_gqa     = 2048
0.00.038.271 I print_info: n_embd_v_gqa     = 2048
0.00.038.272 I print_info: f_norm_eps       = 1.0e-05
0.00.038.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.274 I print_info: f_logit_scale    = 0.0e+00
0.00.038.279 I print_info: n_ff             = 8192
0.00.038.279 I print_info: n_expert         = 0
0.00.038.279 I print_info: n_expert_used    = 0
0.00.038.279 I print_info: causal attn      = 1
0.00.038.279 I print_info: pooling type     = 0
0.00.038.280 I print_info: rope type        = 2
0.00.038.281 I print_info: rope scaling     = linear
0.00.038.281 I print_info: freq_base_train  = 10000.0
0.00.038.281 I print_info: freq_scale_train = 1
0.00.038.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.282 I print_info: rope_finetuned   = unknown
0.00.038.282 I print_info: ssm_d_conv       = 0
0.00.038.282 I print_info: ssm_d_inner      = 0
0.00.038.283 I print_info: ssm_d_state      = 0
0.00.038.283 I print_info: ssm_dt_rank      = 0
0.00.038.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.283 I print_info: model type       = 1.4B
0.00.038.283 I print_info: model params     = 1.41 B
0.00.038.284 I print_info: general.name     = 1.4B
0.00.038.284 I print_info: vocab type       = BPE
0.00.038.284 I print_info: n_vocab          = 50304
0.00.038.285 I print_info: n_merges         = 50009
0.00.038.285 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.285 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.285 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.286 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.286 I print_info: LF token         = 187 ''
0.00.038.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: max token length = 1024
0.00.038.287 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.042 I load_tensors: offloading output layer to GPU
0.00.345.043 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.080 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.084 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.625 I llama_init_from_model: n_seq_max     = 1
0.00.346.627 I llama_init_from_model: n_ctx         = 2048
0.00.346.628 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.346.628 I llama_init_from_model: n_batch       = 2048
0.00.346.629 I llama_init_from_model: n_ubatch      = 512
0.00.346.630 I llama_init_from_model: flash_attn    = 0
0.00.346.632 I llama_init_from_model: freq_base     = 10000.0
0.00.346.633 I llama_init_from_model: freq_scale    = 1
0.00.346.636 I ggml_metal_init: allocating
0.00.346.762 I ggml_metal_init: found device: Apple M4
0.00.346.775 I ggml_metal_init: picking default device: Apple M4
0.00.348.812 I ggml_metal_init: using embedded metal library
0.00.354.502 I ggml_metal_init: GPU name:   Apple M4
0.00.354.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.516 I ggml_metal_init: simdgroup reduction   = true
0.00.354.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.517 I ggml_metal_init: has residency sets    = true
0.00.354.517 I ggml_metal_init: has bfloat            = true
0.00.354.518 I ggml_metal_init: use bfloat            = true
0.00.354.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.526 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.895 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.148 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.435.155 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.435.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.323 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.324 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.325 I llama_init_from_model: graph nodes  = 967
0.00.439.325 I llama_init_from_model: graph splits = 2
0.00.439.329 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.795 I main: llama threadpool init, n_threads = 4
0.00.498.841 I 
0.00.498.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.865 I 
0.00.499.038 I sampler seed: 1234
0.00.499.043 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.101 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.181.286 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.181.286 I llama_perf_context_print:        load time =     488.13 ms
0.01.181.289 I llama_perf_context_print: prompt eval time =      44.18 ms /     7 tokens (    6.31 ms per token,   158.46 tokens per second)
0.01.181.289 I llama_perf_context_print:        eval time =     635.23 ms /    63 runs   (   10.08 ms per token,    99.18 tokens per second)
0.01.181.290 I llama_perf_context_print:       total time =     683.20 ms /    70 tokens
0.01.181.520 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.110s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.476 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.765 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.767 I llama_model_loader: - type  f32:  194 tensors
0.00.025.768 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.768 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.768 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.769 I print_info: file format = GGUF V3 (latest)
0.00.025.769 I print_info: file type   = Q2_K - Medium
0.00.025.771 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.493 I load: special tokens cache size = 25
0.00.039.486 I load: token to piece cache size = 0.2984 MB
0.00.039.492 I print_info: arch             = gptneox
0.00.039.492 I print_info: vocab_only       = 0
0.00.039.492 I print_info: n_ctx_train      = 2048
0.00.039.492 I print_info: n_embd           = 2048
0.00.039.492 I print_info: n_layer          = 24
0.00.039.497 I print_info: n_head           = 16
0.00.039.497 I print_info: n_head_kv        = 16
0.00.039.498 I print_info: n_rot            = 32
0.00.039.498 I print_info: n_swa            = 0
0.00.039.498 I print_info: n_embd_head_k    = 128
0.00.039.498 I print_info: n_embd_head_v    = 128
0.00.039.499 I print_info: n_gqa            = 1
0.00.039.500 I print_info: n_embd_k_gqa     = 2048
0.00.039.501 I print_info: n_embd_v_gqa     = 2048
0.00.039.501 I print_info: f_norm_eps       = 1.0e-05
0.00.039.501 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.502 I print_info: f_logit_scale    = 0.0e+00
0.00.039.503 I print_info: n_ff             = 8192
0.00.039.503 I print_info: n_expert         = 0
0.00.039.503 I print_info: n_expert_used    = 0
0.00.039.503 I print_info: causal attn      = 1
0.00.039.503 I print_info: pooling type     = 0
0.00.039.504 I print_info: rope type        = 2
0.00.039.504 I print_info: rope scaling     = linear
0.00.039.504 I print_info: freq_base_train  = 10000.0
0.00.039.505 I print_info: freq_scale_train = 1
0.00.039.506 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.507 I print_info: rope_finetuned   = unknown
0.00.039.507 I print_info: ssm_d_conv       = 0
0.00.039.507 I print_info: ssm_d_inner      = 0
0.00.039.507 I print_info: ssm_d_state      = 0
0.00.039.507 I print_info: ssm_dt_rank      = 0
0.00.039.507 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.508 I print_info: model type       = 1.4B
0.00.039.510 I print_info: model params     = 1.41 B
0.00.039.510 I print_info: general.name     = 1.4B
0.00.039.511 I print_info: vocab type       = BPE
0.00.039.511 I print_info: n_vocab          = 50304
0.00.039.511 I print_info: n_merges         = 50009
0.00.039.512 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.512 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.512 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.513 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.514 I print_info: LF token         = 187 ''
0.00.039.514 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.514 I print_info: max token length = 1024
0.00.039.515 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.354.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.918 I load_tensors: offloading output layer to GPU
0.00.354.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.953 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.955 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.356.237 I llama_init_from_model: n_seq_max     = 1
0.00.356.246 I llama_init_from_model: n_ctx         = 128
0.00.356.247 I llama_init_from_model: n_ctx_per_seq = 128
0.00.356.247 I llama_init_from_model: n_batch       = 128
0.00.356.247 I llama_init_from_model: n_ubatch      = 128
0.00.356.248 I llama_init_from_model: flash_attn    = 0
0.00.356.250 I llama_init_from_model: freq_base     = 10000.0
0.00.356.251 I llama_init_from_model: freq_scale    = 1
0.00.356.251 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.356.254 I ggml_metal_init: allocating
0.00.356.366 I ggml_metal_init: found device: Apple M4
0.00.356.386 I ggml_metal_init: picking default device: Apple M4
0.00.358.407 I ggml_metal_init: using embedded metal library
0.00.364.320 I ggml_metal_init: GPU name:   Apple M4
0.00.364.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.364.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.364.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.364.344 I ggml_metal_init: simdgroup reduction   = true
0.00.364.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.364.345 I ggml_metal_init: has residency sets    = true
0.00.364.345 I ggml_metal_init: has bfloat            = true
0.00.364.346 I ggml_metal_init: use bfloat            = true
0.00.364.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.364.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.185 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.390.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.390.897 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.390.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.394.482 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.394.484 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.394.485 I llama_init_from_model: graph nodes  = 967
0.00.394.485 I llama_init_from_model: graph splits = 2
0.00.394.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.394.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.183 I 
0.00.421.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.249 I perplexity: tokenizing the input ..
0.00.427.654 I perplexity: tokenization took 6.4 ms
0.00.427.660 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.346 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.684 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.708 I llama_perf_context_print:        load time =     410.70 ms
0.00.561.709 I llama_perf_context_print: prompt eval time =     131.66 ms /   128 tokens (    1.03 ms per token,   972.23 tokens per second)
0.00.561.710 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.710 I llama_perf_context_print:       total time =     140.53 ms /   129 tokens
0.00.562.141 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.082s
sys	0m0.104s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.093 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.096 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.097 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.365 I llama_model_loader: - type  f32:  194 tensors
0.00.024.365 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.365 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.365 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.366 I print_info: file format = GGUF V3 (latest)
0.00.024.367 I print_info: file type   = Q3_K - Medium
0.00.024.368 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.990 I load: special tokens cache size = 25
0.00.037.683 I load: token to piece cache size = 0.2984 MB
0.00.037.685 I print_info: arch             = gptneox
0.00.037.686 I print_info: vocab_only       = 0
0.00.037.686 I print_info: n_ctx_train      = 2048
0.00.037.686 I print_info: n_embd           = 2048
0.00.037.686 I print_info: n_layer          = 24
0.00.037.689 I print_info: n_head           = 16
0.00.037.689 I print_info: n_head_kv        = 16
0.00.037.690 I print_info: n_rot            = 32
0.00.037.690 I print_info: n_swa            = 0
0.00.037.690 I print_info: n_embd_head_k    = 128
0.00.037.690 I print_info: n_embd_head_v    = 128
0.00.037.691 I print_info: n_gqa            = 1
0.00.037.692 I print_info: n_embd_k_gqa     = 2048
0.00.037.692 I print_info: n_embd_v_gqa     = 2048
0.00.037.693 I print_info: f_norm_eps       = 1.0e-05
0.00.037.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.693 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.694 I print_info: f_logit_scale    = 0.0e+00
0.00.037.694 I print_info: n_ff             = 8192
0.00.037.695 I print_info: n_expert         = 0
0.00.037.695 I print_info: n_expert_used    = 0
0.00.037.695 I print_info: causal attn      = 1
0.00.037.695 I print_info: pooling type     = 0
0.00.037.695 I print_info: rope type        = 2
0.00.037.695 I print_info: rope scaling     = linear
0.00.037.696 I print_info: freq_base_train  = 10000.0
0.00.037.696 I print_info: freq_scale_train = 1
0.00.037.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.696 I print_info: rope_finetuned   = unknown
0.00.037.697 I print_info: ssm_d_conv       = 0
0.00.037.699 I print_info: ssm_d_inner      = 0
0.00.037.699 I print_info: ssm_d_state      = 0
0.00.037.699 I print_info: ssm_dt_rank      = 0
0.00.037.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.699 I print_info: model type       = 1.4B
0.00.037.700 I print_info: model params     = 1.41 B
0.00.037.700 I print_info: general.name     = 1.4B
0.00.037.700 I print_info: vocab type       = BPE
0.00.037.701 I print_info: n_vocab          = 50304
0.00.037.701 I print_info: n_merges         = 50009
0.00.037.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: LF token         = 187 ''
0.00.037.702 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: max token length = 1024
0.00.037.703 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.167 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.181 I load_tensors: offloading output layer to GPU
0.00.439.182 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.216 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.217 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.679 I llama_init_from_model: n_seq_max     = 1
0.00.440.685 I llama_init_from_model: n_ctx         = 2048
0.00.440.686 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.440.686 I llama_init_from_model: n_batch       = 2048
0.00.440.687 I llama_init_from_model: n_ubatch      = 512
0.00.440.687 I llama_init_from_model: flash_attn    = 0
0.00.440.689 I llama_init_from_model: freq_base     = 10000.0
0.00.440.689 I llama_init_from_model: freq_scale    = 1
0.00.440.692 I ggml_metal_init: allocating
0.00.440.763 I ggml_metal_init: found device: Apple M4
0.00.440.776 I ggml_metal_init: picking default device: Apple M4
0.00.442.635 I ggml_metal_init: using embedded metal library
0.00.447.985 I ggml_metal_init: GPU name:   Apple M4
0.00.447.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.993 I ggml_metal_init: simdgroup reduction   = true
0.00.447.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.994 I ggml_metal_init: has residency sets    = true
0.00.447.994 I ggml_metal_init: has bfloat            = true
0.00.447.994 I ggml_metal_init: use bfloat            = true
0.00.447.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.821 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.952 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.525.960 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.525.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.530.464 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.530.464 I llama_init_from_model: graph nodes  = 967
0.00.530.464 I llama_init_from_model: graph splits = 2
0.00.530.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.530.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.530.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.354 I main: llama threadpool init, n_threads = 4
0.00.589.397 I 
0.00.589.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.423 I 
0.00.589.573 I sampler seed: 1234
0.00.589.577 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.588 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.588 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.669 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.341.670 I llama_perf_context_print:        load time =     579.79 ms
0.01.341.671 I llama_perf_context_print: prompt eval time =      50.08 ms /     7 tokens (    7.15 ms per token,   139.79 tokens per second)
0.01.341.671 I llama_perf_context_print:        eval time =     699.04 ms /    63 runs   (   11.10 ms per token,    90.12 tokens per second)
0.01.341.672 I llama_perf_context_print:       total time =     753.05 ms /    70 tokens
0.01.341.931 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.302 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.399 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.959 I llama_model_loader: - type  f32:  194 tensors
0.00.024.960 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.960 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.960 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.961 I print_info: file format = GGUF V3 (latest)
0.00.024.966 I print_info: file type   = Q3_K - Medium
0.00.024.967 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.820 I load: special tokens cache size = 25
0.00.038.986 I load: token to piece cache size = 0.2984 MB
0.00.038.990 I print_info: arch             = gptneox
0.00.038.990 I print_info: vocab_only       = 0
0.00.038.990 I print_info: n_ctx_train      = 2048
0.00.038.990 I print_info: n_embd           = 2048
0.00.038.991 I print_info: n_layer          = 24
0.00.038.995 I print_info: n_head           = 16
0.00.038.996 I print_info: n_head_kv        = 16
0.00.038.996 I print_info: n_rot            = 32
0.00.038.996 I print_info: n_swa            = 0
0.00.038.998 I print_info: n_embd_head_k    = 128
0.00.038.998 I print_info: n_embd_head_v    = 128
0.00.038.999 I print_info: n_gqa            = 1
0.00.038.999 I print_info: n_embd_k_gqa     = 2048
0.00.039.002 I print_info: n_embd_v_gqa     = 2048
0.00.039.003 I print_info: f_norm_eps       = 1.0e-05
0.00.039.003 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.003 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.003 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.003 I print_info: f_logit_scale    = 0.0e+00
0.00.039.004 I print_info: n_ff             = 8192
0.00.039.004 I print_info: n_expert         = 0
0.00.039.004 I print_info: n_expert_used    = 0
0.00.039.004 I print_info: causal attn      = 1
0.00.039.004 I print_info: pooling type     = 0
0.00.039.006 I print_info: rope type        = 2
0.00.039.006 I print_info: rope scaling     = linear
0.00.039.006 I print_info: freq_base_train  = 10000.0
0.00.039.006 I print_info: freq_scale_train = 1
0.00.039.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.007 I print_info: rope_finetuned   = unknown
0.00.039.007 I print_info: ssm_d_conv       = 0
0.00.039.007 I print_info: ssm_d_inner      = 0
0.00.039.007 I print_info: ssm_d_state      = 0
0.00.039.007 I print_info: ssm_dt_rank      = 0
0.00.039.007 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.008 I print_info: model type       = 1.4B
0.00.039.008 I print_info: model params     = 1.41 B
0.00.039.008 I print_info: general.name     = 1.4B
0.00.039.039 I print_info: vocab type       = BPE
0.00.039.041 I print_info: n_vocab          = 50304
0.00.039.041 I print_info: n_merges         = 50009
0.00.039.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.042 I print_info: LF token         = 187 ''
0.00.039.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.042 I print_info: max token length = 1024
0.00.039.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.936 I load_tensors: offloading output layer to GPU
0.00.451.937 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.977 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.979 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.453.341 I llama_init_from_model: n_seq_max     = 1
0.00.453.345 I llama_init_from_model: n_ctx         = 128
0.00.453.346 I llama_init_from_model: n_ctx_per_seq = 128
0.00.453.346 I llama_init_from_model: n_batch       = 128
0.00.453.346 I llama_init_from_model: n_ubatch      = 128
0.00.453.347 I llama_init_from_model: flash_attn    = 0
0.00.453.350 I llama_init_from_model: freq_base     = 10000.0
0.00.453.350 I llama_init_from_model: freq_scale    = 1
0.00.453.351 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.453.353 I ggml_metal_init: allocating
0.00.453.436 I ggml_metal_init: found device: Apple M4
0.00.453.451 I ggml_metal_init: picking default device: Apple M4
0.00.455.372 I ggml_metal_init: using embedded metal library
0.00.461.227 I ggml_metal_init: GPU name:   Apple M4
0.00.461.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.461.245 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.461.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.461.247 I ggml_metal_init: simdgroup reduction   = true
0.00.461.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.461.247 I ggml_metal_init: has residency sets    = true
0.00.461.248 I ggml_metal_init: has bfloat            = true
0.00.461.248 I ggml_metal_init: use bfloat            = true
0.00.461.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.461.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.196 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.797 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.485.803 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.485.877 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.073 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.489.074 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.489.075 I llama_init_from_model: graph nodes  = 967
0.00.489.075 I llama_init_from_model: graph splits = 2
0.00.489.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.489.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.841 I 
0.00.518.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.905 I perplexity: tokenizing the input ..
0.00.525.690 I perplexity: tokenization took 6.781 ms
0.00.525.701 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.517 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.672.873 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.672.899 I llama_perf_context_print:        load time =     509.53 ms
0.00.672.900 I llama_perf_context_print: prompt eval time =     144.84 ms /   128 tokens (    1.13 ms per token,   883.75 tokens per second)
0.00.672.901 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.672.901 I llama_perf_context_print:       total time =     154.06 ms /   129 tokens
0.00.673.335 I ggml_metal_free: deallocating

real	0m0.687s
user	0m0.081s
sys	0m0.116s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.784 I llama_model_loader: - type  f32:  194 tensors
0.00.024.785 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.785 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.785 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.786 I print_info: file format = GGUF V3 (latest)
0.00.024.786 I print_info: file type   = Q4_K - Medium
0.00.024.787 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.825 I load: special tokens cache size = 25
0.00.038.646 I load: token to piece cache size = 0.2984 MB
0.00.038.649 I print_info: arch             = gptneox
0.00.038.649 I print_info: vocab_only       = 0
0.00.038.649 I print_info: n_ctx_train      = 2048
0.00.038.650 I print_info: n_embd           = 2048
0.00.038.650 I print_info: n_layer          = 24
0.00.038.653 I print_info: n_head           = 16
0.00.038.653 I print_info: n_head_kv        = 16
0.00.038.654 I print_info: n_rot            = 32
0.00.038.654 I print_info: n_swa            = 0
0.00.038.654 I print_info: n_embd_head_k    = 128
0.00.038.654 I print_info: n_embd_head_v    = 128
0.00.038.655 I print_info: n_gqa            = 1
0.00.038.656 I print_info: n_embd_k_gqa     = 2048
0.00.038.657 I print_info: n_embd_v_gqa     = 2048
0.00.038.657 I print_info: f_norm_eps       = 1.0e-05
0.00.038.657 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.659 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.660 I print_info: f_logit_scale    = 0.0e+00
0.00.038.660 I print_info: n_ff             = 8192
0.00.038.660 I print_info: n_expert         = 0
0.00.038.661 I print_info: n_expert_used    = 0
0.00.038.661 I print_info: causal attn      = 1
0.00.038.661 I print_info: pooling type     = 0
0.00.038.661 I print_info: rope type        = 2
0.00.038.661 I print_info: rope scaling     = linear
0.00.038.662 I print_info: freq_base_train  = 10000.0
0.00.038.662 I print_info: freq_scale_train = 1
0.00.038.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.663 I print_info: rope_finetuned   = unknown
0.00.038.663 I print_info: ssm_d_conv       = 0
0.00.038.663 I print_info: ssm_d_inner      = 0
0.00.038.663 I print_info: ssm_d_state      = 0
0.00.038.663 I print_info: ssm_dt_rank      = 0
0.00.038.667 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.668 I print_info: model type       = 1.4B
0.00.038.669 I print_info: model params     = 1.41 B
0.00.038.669 I print_info: general.name     = 1.4B
0.00.038.669 I print_info: vocab type       = BPE
0.00.038.670 I print_info: n_vocab          = 50304
0.00.038.670 I print_info: n_merges         = 50009
0.00.038.670 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.671 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.671 I print_info: LF token         = 187 ''
0.00.038.671 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.671 I print_info: max token length = 1024
0.00.038.672 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.891 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.903 I load_tensors: offloading output layer to GPU
0.00.528.904 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.937 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.938 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.530.520 I llama_init_from_model: n_seq_max     = 1
0.00.530.522 I llama_init_from_model: n_ctx         = 2048
0.00.530.523 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.530.523 I llama_init_from_model: n_batch       = 2048
0.00.530.524 I llama_init_from_model: n_ubatch      = 512
0.00.530.524 I llama_init_from_model: flash_attn    = 0
0.00.530.527 I llama_init_from_model: freq_base     = 10000.0
0.00.530.527 I llama_init_from_model: freq_scale    = 1
0.00.530.532 I ggml_metal_init: allocating
0.00.530.609 I ggml_metal_init: found device: Apple M4
0.00.530.623 I ggml_metal_init: picking default device: Apple M4
0.00.532.557 I ggml_metal_init: using embedded metal library
0.00.539.250 I ggml_metal_init: GPU name:   Apple M4
0.00.539.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.257 I ggml_metal_init: simdgroup reduction   = true
0.00.539.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.258 I ggml_metal_init: has residency sets    = true
0.00.539.258 I ggml_metal_init: has bfloat            = true
0.00.539.258 I ggml_metal_init: use bfloat            = true
0.00.539.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.321 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.327 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.724 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.618.726 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.618.726 I llama_init_from_model: graph nodes  = 967
0.00.618.726 I llama_init_from_model: graph splits = 2
0.00.618.732 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.618.856 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.618.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.548 I main: llama threadpool init, n_threads = 4
0.00.674.597 I 
0.00.674.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.623 I 
0.00.674.776 I sampler seed: 1234
0.00.674.780 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.674.802 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.674.802 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.674.802 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.556 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47364.91 tokens per second)
0.01.426.557 I llama_perf_context_print:        load time =     664.89 ms
0.01.426.558 I llama_perf_context_print: prompt eval time =      47.21 ms /     7 tokens (    6.74 ms per token,   148.28 tokens per second)
0.01.426.559 I llama_perf_context_print:        eval time =     701.51 ms /    63 runs   (   11.14 ms per token,    89.81 tokens per second)
0.01.426.560 I llama_perf_context_print:       total time =     752.74 ms /    70 tokens
0.01.426.823 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.109s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.412 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.415 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.415 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.914 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.917 I llama_model_loader: - type  f32:  194 tensors
0.00.024.917 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.917 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.917 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.918 I print_info: file format = GGUF V3 (latest)
0.00.024.919 I print_info: file type   = Q4_K - Medium
0.00.024.920 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.163 I load: special tokens cache size = 25
0.00.039.256 I load: token to piece cache size = 0.2984 MB
0.00.039.260 I print_info: arch             = gptneox
0.00.039.261 I print_info: vocab_only       = 0
0.00.039.261 I print_info: n_ctx_train      = 2048
0.00.039.261 I print_info: n_embd           = 2048
0.00.039.261 I print_info: n_layer          = 24
0.00.039.265 I print_info: n_head           = 16
0.00.039.266 I print_info: n_head_kv        = 16
0.00.039.268 I print_info: n_rot            = 32
0.00.039.268 I print_info: n_swa            = 0
0.00.039.269 I print_info: n_embd_head_k    = 128
0.00.039.269 I print_info: n_embd_head_v    = 128
0.00.039.269 I print_info: n_gqa            = 1
0.00.039.270 I print_info: n_embd_k_gqa     = 2048
0.00.039.271 I print_info: n_embd_v_gqa     = 2048
0.00.039.271 I print_info: f_norm_eps       = 1.0e-05
0.00.039.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.272 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.272 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.272 I print_info: f_logit_scale    = 0.0e+00
0.00.039.273 I print_info: n_ff             = 8192
0.00.039.274 I print_info: n_expert         = 0
0.00.039.274 I print_info: n_expert_used    = 0
0.00.039.274 I print_info: causal attn      = 1
0.00.039.275 I print_info: pooling type     = 0
0.00.039.275 I print_info: rope type        = 2
0.00.039.275 I print_info: rope scaling     = linear
0.00.039.275 I print_info: freq_base_train  = 10000.0
0.00.039.276 I print_info: freq_scale_train = 1
0.00.039.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.276 I print_info: rope_finetuned   = unknown
0.00.039.276 I print_info: ssm_d_conv       = 0
0.00.039.277 I print_info: ssm_d_inner      = 0
0.00.039.277 I print_info: ssm_d_state      = 0
0.00.039.277 I print_info: ssm_dt_rank      = 0
0.00.039.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.277 I print_info: model type       = 1.4B
0.00.039.278 I print_info: model params     = 1.41 B
0.00.039.278 I print_info: general.name     = 1.4B
0.00.039.279 I print_info: vocab type       = BPE
0.00.039.279 I print_info: n_vocab          = 50304
0.00.039.279 I print_info: n_merges         = 50009
0.00.039.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: LF token         = 187 ''
0.00.039.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.281 I print_info: max token length = 1024
0.00.039.281 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.090 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.114 I load_tensors: offloading output layer to GPU
0.00.554.115 I load_tensors: offloaded 25/25 layers to GPU
0.00.554.157 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.554.159 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.555.583 I llama_init_from_model: n_seq_max     = 1
0.00.555.587 I llama_init_from_model: n_ctx         = 128
0.00.555.588 I llama_init_from_model: n_ctx_per_seq = 128
0.00.555.588 I llama_init_from_model: n_batch       = 128
0.00.555.589 I llama_init_from_model: n_ubatch      = 128
0.00.555.589 I llama_init_from_model: flash_attn    = 0
0.00.555.592 I llama_init_from_model: freq_base     = 10000.0
0.00.555.592 I llama_init_from_model: freq_scale    = 1
0.00.555.593 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.555.598 I ggml_metal_init: allocating
0.00.555.714 I ggml_metal_init: found device: Apple M4
0.00.555.729 I ggml_metal_init: picking default device: Apple M4
0.00.557.699 I ggml_metal_init: using embedded metal library
0.00.564.737 I ggml_metal_init: GPU name:   Apple M4
0.00.564.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.564.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.564.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.564.748 I ggml_metal_init: simdgroup reduction   = true
0.00.564.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.564.749 I ggml_metal_init: has residency sets    = true
0.00.564.749 I ggml_metal_init: has bfloat            = true
0.00.564.749 I ggml_metal_init: use bfloat            = true
0.00.564.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.564.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.582.980 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.586.532 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.586.536 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.586.573 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.589.679 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.589.681 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.589.681 I llama_init_from_model: graph nodes  = 967
0.00.589.681 I llama_init_from_model: graph splits = 2
0.00.589.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.589.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.360 I 
0.00.618.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.430 I perplexity: tokenizing the input ..
0.00.626.060 I perplexity: tokenization took 7.625 ms
0.00.626.069 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.848 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.774.137 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.774.158 I llama_perf_context_print:        load time =     608.94 ms
0.00.774.159 I llama_perf_context_print: prompt eval time =     145.85 ms /   128 tokens (    1.14 ms per token,   877.61 tokens per second)
0.00.774.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.160 I llama_perf_context_print:       total time =     155.80 ms /   129 tokens
0.00.774.583 I ggml_metal_free: deallocating

real	0m0.788s
user	0m0.082s
sys	0m0.139s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.219 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.594 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.308 I llama_model_loader: - type  f32:  194 tensors
0.00.026.308 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.308 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.309 I print_info: file format = GGUF V3 (latest)
0.00.026.309 I print_info: file type   = Q5_K - Medium
0.00.026.310 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.967 I load: special tokens cache size = 25
0.00.039.847 I load: token to piece cache size = 0.2984 MB
0.00.039.849 I print_info: arch             = gptneox
0.00.039.849 I print_info: vocab_only       = 0
0.00.039.850 I print_info: n_ctx_train      = 2048
0.00.039.850 I print_info: n_embd           = 2048
0.00.039.850 I print_info: n_layer          = 24
0.00.039.853 I print_info: n_head           = 16
0.00.039.854 I print_info: n_head_kv        = 16
0.00.039.854 I print_info: n_rot            = 32
0.00.039.854 I print_info: n_swa            = 0
0.00.039.855 I print_info: n_embd_head_k    = 128
0.00.039.855 I print_info: n_embd_head_v    = 128
0.00.039.858 I print_info: n_gqa            = 1
0.00.039.859 I print_info: n_embd_k_gqa     = 2048
0.00.039.859 I print_info: n_embd_v_gqa     = 2048
0.00.039.864 I print_info: f_norm_eps       = 1.0e-05
0.00.039.864 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.864 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.865 I print_info: f_logit_scale    = 0.0e+00
0.00.039.866 I print_info: n_ff             = 8192
0.00.039.866 I print_info: n_expert         = 0
0.00.039.866 I print_info: n_expert_used    = 0
0.00.039.867 I print_info: causal attn      = 1
0.00.039.867 I print_info: pooling type     = 0
0.00.039.868 I print_info: rope type        = 2
0.00.039.870 I print_info: rope scaling     = linear
0.00.039.870 I print_info: freq_base_train  = 10000.0
0.00.039.870 I print_info: freq_scale_train = 1
0.00.039.871 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.871 I print_info: rope_finetuned   = unknown
0.00.039.872 I print_info: ssm_d_conv       = 0
0.00.039.872 I print_info: ssm_d_inner      = 0
0.00.039.872 I print_info: ssm_d_state      = 0
0.00.039.873 I print_info: ssm_dt_rank      = 0
0.00.039.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.873 I print_info: model type       = 1.4B
0.00.039.873 I print_info: model params     = 1.41 B
0.00.039.873 I print_info: general.name     = 1.4B
0.00.039.874 I print_info: vocab type       = BPE
0.00.039.875 I print_info: n_vocab          = 50304
0.00.039.875 I print_info: n_merges         = 50009
0.00.039.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.876 I print_info: LF token         = 187 ''
0.00.039.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.876 I print_info: max token length = 1024
0.00.039.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.983 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.998 I load_tensors: offloading output layer to GPU
0.00.603.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.035 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.603.037 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.604.335 I llama_init_from_model: n_seq_max     = 1
0.00.604.337 I llama_init_from_model: n_ctx         = 2048
0.00.604.338 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.338 I llama_init_from_model: n_batch       = 2048
0.00.604.339 I llama_init_from_model: n_ubatch      = 512
0.00.604.339 I llama_init_from_model: flash_attn    = 0
0.00.604.340 I llama_init_from_model: freq_base     = 10000.0
0.00.604.341 I llama_init_from_model: freq_scale    = 1
0.00.604.342 I ggml_metal_init: allocating
0.00.604.355 I ggml_metal_init: found device: Apple M4
0.00.604.363 I ggml_metal_init: picking default device: Apple M4
0.00.605.903 I ggml_metal_init: using embedded metal library
0.00.612.107 I ggml_metal_init: GPU name:   Apple M4
0.00.612.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.111 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.112 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.112 I ggml_metal_init: simdgroup reduction   = true
0.00.612.113 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.113 I ggml_metal_init: has residency sets    = true
0.00.612.113 I ggml_metal_init: has bfloat            = true
0.00.612.113 I ggml_metal_init: use bfloat            = true
0.00.612.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.413 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.414 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.419 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.456 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.718 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.720 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.720 I llama_init_from_model: graph nodes  = 967
0.00.685.720 I llama_init_from_model: graph splits = 2
0.00.685.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.985 I main: llama threadpool init, n_threads = 4
0.00.748.028 I 
0.00.748.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.050 I 
0.00.748.190 I sampler seed: 1234
0.00.748.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.215 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.216 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.216 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.589.179 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.589.180 I llama_perf_context_print:        load time =     737.03 ms
0.01.589.181 I llama_perf_context_print: prompt eval time =      52.57 ms /     7 tokens (    7.51 ms per token,   133.17 tokens per second)
0.01.589.181 I llama_perf_context_print:        eval time =     785.64 ms /    63 runs   (   12.47 ms per token,    80.19 tokens per second)
0.01.589.183 I llama_perf_context_print:       total time =     841.93 ms /    70 tokens
0.01.589.415 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.107s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.265 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.311 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.319 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.320 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.324 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.655 I llama_model_loader: - type  f32:  194 tensors
0.00.025.655 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.656 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.656 I print_info: file format = GGUF V3 (latest)
0.00.025.657 I print_info: file type   = Q5_K - Medium
0.00.025.658 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.058 I load: special tokens cache size = 25
0.00.040.316 I load: token to piece cache size = 0.2984 MB
0.00.040.320 I print_info: arch             = gptneox
0.00.040.320 I print_info: vocab_only       = 0
0.00.040.320 I print_info: n_ctx_train      = 2048
0.00.040.321 I print_info: n_embd           = 2048
0.00.040.321 I print_info: n_layer          = 24
0.00.040.325 I print_info: n_head           = 16
0.00.040.325 I print_info: n_head_kv        = 16
0.00.040.325 I print_info: n_rot            = 32
0.00.040.325 I print_info: n_swa            = 0
0.00.040.326 I print_info: n_embd_head_k    = 128
0.00.040.326 I print_info: n_embd_head_v    = 128
0.00.040.327 I print_info: n_gqa            = 1
0.00.040.327 I print_info: n_embd_k_gqa     = 2048
0.00.040.328 I print_info: n_embd_v_gqa     = 2048
0.00.040.328 I print_info: f_norm_eps       = 1.0e-05
0.00.040.329 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.329 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.329 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.329 I print_info: f_logit_scale    = 0.0e+00
0.00.040.330 I print_info: n_ff             = 8192
0.00.040.330 I print_info: n_expert         = 0
0.00.040.330 I print_info: n_expert_used    = 0
0.00.040.330 I print_info: causal attn      = 1
0.00.040.330 I print_info: pooling type     = 0
0.00.040.330 I print_info: rope type        = 2
0.00.040.331 I print_info: rope scaling     = linear
0.00.040.331 I print_info: freq_base_train  = 10000.0
0.00.040.331 I print_info: freq_scale_train = 1
0.00.040.331 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.332 I print_info: rope_finetuned   = unknown
0.00.040.332 I print_info: ssm_d_conv       = 0
0.00.040.332 I print_info: ssm_d_inner      = 0
0.00.040.332 I print_info: ssm_d_state      = 0
0.00.040.332 I print_info: ssm_dt_rank      = 0
0.00.040.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.335 I print_info: model type       = 1.4B
0.00.040.336 I print_info: model params     = 1.41 B
0.00.040.336 I print_info: general.name     = 1.4B
0.00.040.336 I print_info: vocab type       = BPE
0.00.040.336 I print_info: n_vocab          = 50304
0.00.040.337 I print_info: n_merges         = 50009
0.00.040.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.337 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.337 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.337 I print_info: LF token         = 187 ''
0.00.040.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: max token length = 1024
0.00.040.347 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.315 I load_tensors: offloading output layer to GPU
0.00.632.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.355 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.632.356 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.633.772 I llama_init_from_model: n_seq_max     = 1
0.00.633.776 I llama_init_from_model: n_ctx         = 128
0.00.633.777 I llama_init_from_model: n_ctx_per_seq = 128
0.00.633.777 I llama_init_from_model: n_batch       = 128
0.00.633.778 I llama_init_from_model: n_ubatch      = 128
0.00.633.778 I llama_init_from_model: flash_attn    = 0
0.00.633.781 I llama_init_from_model: freq_base     = 10000.0
0.00.633.781 I llama_init_from_model: freq_scale    = 1
0.00.633.782 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.633.787 I ggml_metal_init: allocating
0.00.633.871 I ggml_metal_init: found device: Apple M4
0.00.633.886 I ggml_metal_init: picking default device: Apple M4
0.00.635.823 I ggml_metal_init: using embedded metal library
0.00.642.346 I ggml_metal_init: GPU name:   Apple M4
0.00.642.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.355 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.357 I ggml_metal_init: simdgroup reduction   = true
0.00.642.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.357 I ggml_metal_init: has residency sets    = true
0.00.642.358 I ggml_metal_init: has bfloat            = true
0.00.642.358 I ggml_metal_init: use bfloat            = true
0.00.642.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.366 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.777 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.174 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.358 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.360 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.361 I llama_init_from_model: graph nodes  = 967
0.00.667.361 I llama_init_from_model: graph splits = 2
0.00.667.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.758 I 
0.00.699.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.824 I perplexity: tokenizing the input ..
0.00.706.906 I perplexity: tokenization took 7.077 ms
0.00.706.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.292 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.846.594 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.846.619 I llama_perf_context_print:        load time =     689.49 ms
0.00.846.622 I llama_perf_context_print: prompt eval time =     137.36 ms /   128 tokens (    1.07 ms per token,   931.86 tokens per second)
0.00.846.626 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.626 I llama_perf_context_print:       total time =     146.86 ms /   129 tokens
0.00.847.065 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.081s
sys	0m0.152s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.663 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.669 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.220 I llama_model_loader: - type  f32:  194 tensors
0.00.025.220 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.221 I print_info: file format = GGUF V3 (latest)
0.00.025.221 I print_info: file type   = Q6_K
0.00.025.222 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.254 I load: special tokens cache size = 25
0.00.039.228 I load: token to piece cache size = 0.2984 MB
0.00.039.231 I print_info: arch             = gptneox
0.00.039.231 I print_info: vocab_only       = 0
0.00.039.231 I print_info: n_ctx_train      = 2048
0.00.039.231 I print_info: n_embd           = 2048
0.00.039.232 I print_info: n_layer          = 24
0.00.039.234 I print_info: n_head           = 16
0.00.039.235 I print_info: n_head_kv        = 16
0.00.039.235 I print_info: n_rot            = 32
0.00.039.235 I print_info: n_swa            = 0
0.00.039.235 I print_info: n_embd_head_k    = 128
0.00.039.238 I print_info: n_embd_head_v    = 128
0.00.039.238 I print_info: n_gqa            = 1
0.00.039.239 I print_info: n_embd_k_gqa     = 2048
0.00.039.240 I print_info: n_embd_v_gqa     = 2048
0.00.039.240 I print_info: f_norm_eps       = 1.0e-05
0.00.039.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.241 I print_info: f_logit_scale    = 0.0e+00
0.00.039.242 I print_info: n_ff             = 8192
0.00.039.242 I print_info: n_expert         = 0
0.00.039.242 I print_info: n_expert_used    = 0
0.00.039.242 I print_info: causal attn      = 1
0.00.039.243 I print_info: pooling type     = 0
0.00.039.243 I print_info: rope type        = 2
0.00.039.243 I print_info: rope scaling     = linear
0.00.039.244 I print_info: freq_base_train  = 10000.0
0.00.039.248 I print_info: freq_scale_train = 1
0.00.039.248 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.249 I print_info: rope_finetuned   = unknown
0.00.039.249 I print_info: ssm_d_conv       = 0
0.00.039.249 I print_info: ssm_d_inner      = 0
0.00.039.249 I print_info: ssm_d_state      = 0
0.00.039.250 I print_info: ssm_dt_rank      = 0
0.00.039.250 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.250 I print_info: model type       = 1.4B
0.00.039.251 I print_info: model params     = 1.41 B
0.00.039.251 I print_info: general.name     = 1.4B
0.00.039.251 I print_info: vocab type       = BPE
0.00.039.252 I print_info: n_vocab          = 50304
0.00.039.252 I print_info: n_merges         = 50009
0.00.039.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: LF token         = 187 ''
0.00.039.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: max token length = 1024
0.00.039.254 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.991 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.994 I load_tensors: offloading output layer to GPU
0.00.664.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.020 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.665.023 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.666.707 I llama_init_from_model: n_seq_max     = 1
0.00.666.709 I llama_init_from_model: n_ctx         = 2048
0.00.666.710 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.666.710 I llama_init_from_model: n_batch       = 2048
0.00.666.710 I llama_init_from_model: n_ubatch      = 512
0.00.666.711 I llama_init_from_model: flash_attn    = 0
0.00.666.712 I llama_init_from_model: freq_base     = 10000.0
0.00.666.713 I llama_init_from_model: freq_scale    = 1
0.00.666.714 I ggml_metal_init: allocating
0.00.666.768 I ggml_metal_init: found device: Apple M4
0.00.666.780 I ggml_metal_init: picking default device: Apple M4
0.00.668.353 I ggml_metal_init: using embedded metal library
0.00.674.221 I ggml_metal_init: GPU name:   Apple M4
0.00.674.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.227 I ggml_metal_init: simdgroup reduction   = true
0.00.674.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.227 I ggml_metal_init: has residency sets    = true
0.00.674.228 I ggml_metal_init: has bfloat            = true
0.00.674.228 I ggml_metal_init: use bfloat            = true
0.00.674.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.386 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.193 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.229 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.385 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.388 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.388 I llama_init_from_model: graph nodes  = 967
0.00.750.388 I llama_init_from_model: graph splits = 2
0.00.750.396 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.765 I main: llama threadpool init, n_threads = 4
0.00.814.811 I 
0.00.814.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.835 I 
0.00.815.004 I sampler seed: 1234
0.00.815.008 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.056 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.059 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.687.911 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.687.912 I llama_perf_context_print:        load time =     805.20 ms
0.01.687.913 I llama_perf_context_print: prompt eval time =      57.77 ms /     7 tokens (    8.25 ms per token,   121.18 tokens per second)
0.01.687.913 I llama_perf_context_print:        eval time =     812.21 ms /    63 runs   (   12.89 ms per token,    77.57 tokens per second)
0.01.687.914 I llama_perf_context_print:       total time =     873.86 ms /    70 tokens
0.01.688.198 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.108s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4763 (f777a73e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.433 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.945 I llama_model_loader: - type  f32:  194 tensors
0.00.024.946 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.946 I print_info: file format = GGUF V3 (latest)
0.00.024.947 I print_info: file type   = Q6_K
0.00.024.948 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.098 I load: special tokens cache size = 25
0.00.039.092 I load: token to piece cache size = 0.2984 MB
0.00.039.097 I print_info: arch             = gptneox
0.00.039.097 I print_info: vocab_only       = 0
0.00.039.097 I print_info: n_ctx_train      = 2048
0.00.039.097 I print_info: n_embd           = 2048
0.00.039.097 I print_info: n_layer          = 24
0.00.039.102 I print_info: n_head           = 16
0.00.039.103 I print_info: n_head_kv        = 16
0.00.039.103 I print_info: n_rot            = 32
0.00.039.103 I print_info: n_swa            = 0
0.00.039.104 I print_info: n_embd_head_k    = 128
0.00.039.104 I print_info: n_embd_head_v    = 128
0.00.039.105 I print_info: n_gqa            = 1
0.00.039.105 I print_info: n_embd_k_gqa     = 2048
0.00.039.106 I print_info: n_embd_v_gqa     = 2048
0.00.039.107 I print_info: f_norm_eps       = 1.0e-05
0.00.039.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.111 I print_info: f_logit_scale    = 0.0e+00
0.00.039.112 I print_info: n_ff             = 8192
0.00.039.112 I print_info: n_expert         = 0
0.00.039.112 I print_info: n_expert_used    = 0
0.00.039.112 I print_info: causal attn      = 1
0.00.039.112 I print_info: pooling type     = 0
0.00.039.112 I print_info: rope type        = 2
0.00.039.113 I print_info: rope scaling     = linear
0.00.039.113 I print_info: freq_base_train  = 10000.0
0.00.039.113 I print_info: freq_scale_train = 1
0.00.039.113 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.114 I print_info: rope_finetuned   = unknown
0.00.039.114 I print_info: ssm_d_conv       = 0
0.00.039.114 I print_info: ssm_d_inner      = 0
0.00.039.114 I print_info: ssm_d_state      = 0
0.00.039.114 I print_info: ssm_dt_rank      = 0
0.00.039.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.114 I print_info: model type       = 1.4B
0.00.039.116 I print_info: model params     = 1.41 B
0.00.039.116 I print_info: general.name     = 1.4B
0.00.039.117 I print_info: vocab type       = BPE
0.00.039.117 I print_info: n_vocab          = 50304
0.00.039.117 I print_info: n_merges         = 50009
0.00.039.117 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.118 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.118 I print_info: LF token         = 187 ''
0.00.039.118 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.118 I print_info: max token length = 1024
0.00.039.119 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.009 I load_tensors: offloading output layer to GPU
0.00.610.010 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.047 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.610.049 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.611.377 I llama_init_from_model: n_seq_max     = 1
0.00.611.380 I llama_init_from_model: n_ctx         = 128
0.00.611.380 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.381 I llama_init_from_model: n_batch       = 128
0.00.611.381 I llama_init_from_model: n_ubatch      = 128
0.00.611.381 I llama_init_from_model: flash_attn    = 0
0.00.611.384 I llama_init_from_model: freq_base     = 10000.0
0.00.611.385 I llama_init_from_model: freq_scale    = 1
0.00.611.386 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.388 I ggml_metal_init: allocating
0.00.611.469 I ggml_metal_init: found device: Apple M4
0.00.611.487 I ggml_metal_init: picking default device: Apple M4
0.00.613.317 I ggml_metal_init: using embedded metal library
0.00.619.775 I ggml_metal_init: GPU name:   Apple M4
0.00.619.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.783 I ggml_metal_init: simdgroup reduction   = true
0.00.619.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.784 I ggml_metal_init: has residency sets    = true
0.00.619.784 I ggml_metal_init: has bfloat            = true
0.00.619.784 I ggml_metal_init: use bfloat            = true
0.00.619.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.016 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.019 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.487 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.489 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.489 I llama_init_from_model: graph nodes  = 967
0.00.643.489 I llama_init_from_model: graph splits = 2
0.00.643.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.688 I 
0.00.674.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.759 I perplexity: tokenizing the input ..
0.00.681.740 I perplexity: tokenization took 6.976 ms
0.00.681.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.881 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.815.213 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.815.237 I llama_perf_context_print:        load time =     665.25 ms
0.00.815.238 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.45 tokens per second)
0.00.815.238 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.239 I llama_perf_context_print:       total time =     140.55 ms /   129 tokens
0.00.815.615 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.079s
sys	0m0.147s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4763 (f777a73e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ae079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ae07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ae082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ae08740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ae08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ae09020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ae09490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ae09900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ae09d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ae0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ae0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ae0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ae0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ae0bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ae0c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ae0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ae0d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ae0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ae0e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ae0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ae0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ae0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ae10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ae10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ae11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ae11400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ae116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ae11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ae12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ae126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ae12c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ae13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ae13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ae138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ae13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ae141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ae14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ae14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ae14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ae15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ae157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ae15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ae160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ae16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ae16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ae16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ae17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ae176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ae17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ae182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ae18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ae18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ae19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ae194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ae19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ae1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ae1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ae1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ae1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ae1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ae1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ae1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ae1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ae1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ae1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ae1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ae1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ae1d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ae1dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ae1e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ae1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ae1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ae1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ae1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ae1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ae200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ae206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ae20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ae21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ae217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ae21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ae22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ae228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ae22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ae23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ae239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ae23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ae24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ae24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ae25090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ae25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ae25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ae261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ae26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ae26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ae272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ae27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ae179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ae27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ae28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ae288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ae28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ae29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ae299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ae29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ae2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ae2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ae2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ae2b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ae2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ae2c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ae2c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ae2cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ae2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ae2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ae2dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ae2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ae2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ae2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ae2f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ae2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ae2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ae2ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ae30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ae30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ae30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ae31390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ae31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ae31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ae32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ae32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ae32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ae33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ae33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ae33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ae34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ae34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ae34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ae34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ae35490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ae35990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ae35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ae36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ae36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ae36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ae37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ae37790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ae37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ae38190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ae38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ae38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ae39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ae39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ae39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ae39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ae3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ae3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ae3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ae3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ae3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ae3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ae3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ae3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ae3cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ae3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ae3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ae3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ae3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ae3e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ae3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ae3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ae3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ae3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ae3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ae40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ae40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ae40d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ae41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ae41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ae41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ae42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ae42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ae42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ae43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ae43590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ae43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ae43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ae44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ae44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ae44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ae45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ae45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ae45d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ae46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ae46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ae46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ae473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ae47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ae47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ae48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ae48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ae49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ae49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ae49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ae4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ae4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ae4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ae4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ae4bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ae4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ae4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ae4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ae4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ae4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ae4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ae4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ae4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ae4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ae4f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ae4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ae50430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ae50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ae50ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ae51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ae51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ae51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ae52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ae52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ae52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ae53400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ae53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ae53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ae543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ae54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ae54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ae553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ae55930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae55e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ae56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ae56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ae573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae57910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ae57e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ae58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ae58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ae593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ae598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ae59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ae5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ae5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ae5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ae5b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ae5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ae5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ae5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ae5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ae5d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ae5de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ae5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ae5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ae5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ae5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ae5f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ae5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ae60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ae60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ae609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ae60e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ae612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ae61790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ae61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ae620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ae62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ae62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ae62eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ae63350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ae638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ae63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ae646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ae64e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ae65520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ae657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ae65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ae66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ae668a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.750.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10af04bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10af05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10af054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10af05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10af05d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10af061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10af06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10af06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10af06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10af073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10af07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10af07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10af08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10af091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10af099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10af0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10af0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10af0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10af0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10af0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10af0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10af0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10af0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10af0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10af0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10af0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10af0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10af0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10af0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10af0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10af0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10af0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10af10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10af10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10af109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10af10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10af112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10af11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10af11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10af11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10af12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10af128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10af12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10af131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10af13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10af13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10af13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10af14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10af147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10af14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10af150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10af15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10af159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10af15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10af16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10af166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10af16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10af17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10af175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10af17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10af17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10af18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10af18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10af18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10af19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10af194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10af19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10af19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10af1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10af1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10af1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10af1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10af1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10af1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10af1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10af1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10af1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10af1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10af1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10af1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10af1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10af1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10af1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10af1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10af1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10af1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10af1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10af1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10af1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10af1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10af203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10af20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10af20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10af21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10af21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10af21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10af21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10af222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10af22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10af22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10af23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10af234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10af23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10af23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10af241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10af24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10af24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10af24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10af253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10af25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10af25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10af26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10af26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10af269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10af26e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10af272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10af27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10af27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10af28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10af28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10af288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10af28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10af291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10af29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10af29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10af29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10af2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10af2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10af2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10af2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10af2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10af2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10af2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10af2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10af2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10af2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10af2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10af2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10af2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10af2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10af2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10af2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10af2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10af2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10af2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10af2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10af2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10af300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10af30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10af309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10af30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10af31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10af316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10af31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10af31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10af32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10af328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10af32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10af33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10af33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10af33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10af33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10af34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10af347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10af34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10af350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10af35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10af35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10af36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10af366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10af36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10af36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10af37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10af37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10af37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10af38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10af385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10af38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10af38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10af39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10af39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10af39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10af3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10af3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10af3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10af3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10af3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10af3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10af3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10af3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10af3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10af3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10af3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10af3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10af3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10af3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10af3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10af3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10af3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10af3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10af3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10af3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10af3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10af3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10af403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10af40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10af40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10af410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10af41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10af41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10af42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10af42950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10af42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10af434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10af43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10af44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10af44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10af44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10af45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10af45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10af45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10af462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10af46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10af46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10af47410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10af479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10af47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10af48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10af48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10af490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10af49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10af49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10af4a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10af4a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10af4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10af4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10af4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10af4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10af4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10af4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10af4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10af4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10af4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10af4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10af4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10af4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10af4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10af4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10af4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10af503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10af50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10af50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10af51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10af51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10af52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10af52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10af52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10af531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10af53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10af53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10af54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10af548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10af54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10af55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10af55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10af55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10af56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10af56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10af57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10af57550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10af57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10af57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10af58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10af58950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10af58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10af59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10af59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10af59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10af5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10af5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10af5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10af5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10af5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10af5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10af5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10af5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10af5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10af5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10af5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10af5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10af5e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1093046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109304b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109304fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109305430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1093058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109305d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109306180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1093065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109306a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109306ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109307340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109307a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109308580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109308d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109309540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109309c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10930a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10930aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10930b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10930b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10930c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10930c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10930ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10930d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10930dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10930df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10930e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10930e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10930eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10930ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10930f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10930f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10930fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109310030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1093104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109310910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109310d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1093111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109311660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109311ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109311f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1093123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109312820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109312c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109313100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109313570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1093139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109313e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1093142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109314730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109314ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109315010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109315480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1093158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109315d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1093161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109316740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109316c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1093170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109317520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109317990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109317e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109318270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1093186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109318b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109318fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109319430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1093198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109319d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10931a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10931a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10931aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10931aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10931b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10931b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10931bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10931c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10931c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10931c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10931cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10931d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10931d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10931db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10931dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10931e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10931e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10931ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10931f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10931f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10931fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10931feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109320320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109320790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109320c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109321070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1093214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109321950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109321dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109322230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1093226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109322b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109322f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1093233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109323860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1093241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109324490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109324900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109324d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1093251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109325650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109325ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109325f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1093263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109326810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109326c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1093270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109327560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1093279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109327e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1093282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109328720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109328b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109329000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109329470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1093298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109329d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10932a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10932a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10932aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10932af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10932b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10932b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10932bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10932c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10932c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10932c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10932ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10932d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10932d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10932db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10932dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10932e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10932e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10932ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10932f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10932f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10932fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10932fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109330360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1093307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109330c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1093310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109331520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109331990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109331e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109332270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1093326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109332b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109332fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109333430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1093338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109333d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109334180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1093345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109334a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109334ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109335340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1093357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109335c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109336090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109336500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109336970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109336de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109337250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1093376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109337b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109337fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109338410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109338880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109338cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109339160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1093395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109339a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109339eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10933a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10933a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10933ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10933b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10933b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10933b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10933bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10933c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10933c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10933cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10933cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10933d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10933d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10933dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10933e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10933e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10933ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10933ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10933f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10933f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10933fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109340050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1093404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109340a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109340ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109341330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109341e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109342140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109342400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109342870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109342ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109343150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1093435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109343a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109343ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109344310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109344780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109344bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109345060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1093454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109345940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109345db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109346220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109346690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109346b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109346f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1093473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109347850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109347cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109348130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1093485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109348a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109348e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1093492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109349760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109349bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10934a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10934a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10934a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10934ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10934b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10934b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10934bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10934bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10934c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10934c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10934cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10934d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10934d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10934d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10934de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10934e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10934e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10934ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10934f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10934f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10934f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10934fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1093501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109350650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109350ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109350f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1093513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109351810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109351c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1093520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109352560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1093529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109352e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1093532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109353720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109353b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109354000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109354470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1093548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109354d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1093551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109355630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109355aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109356510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109356c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109357350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109357a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109357d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1093581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1093587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109358db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.803s
user	0m0.285s
sys	0m0.302s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4763 (f777a73e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15070d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15070db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15070e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15070e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15070ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15070f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15070f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15070fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150710340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150710840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150710d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150711d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150712510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150712d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150713440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150713b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150714280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1507149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150715170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1507166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150716f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150717950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150717f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150718bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150719110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1507193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150719870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150719b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15071a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15071a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15071abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15071b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15071b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15071b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15071be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15071c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15071c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15071cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15071d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15071d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15071d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15071de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15071e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15071ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15071f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15071f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15071ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1507205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150720bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1507211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1507219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150721e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1507222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1507225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1507233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150723670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150723b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150723fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150724450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1507248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150724d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150725230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1507256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150725b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150726010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1507264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150726950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150726df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150727340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150727890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150727de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150728330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150728880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150728dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150729320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150729870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150729dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15072a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15072a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15072adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15072b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15072b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15072bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15072c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15072c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15072cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15072d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15072d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15072dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15072e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15072e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15072ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15071ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15072f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15072f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15072fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150730430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150730980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150731970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150732410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150732960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150732eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150733400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150733950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1507347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150734c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150735120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1507355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150735a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150735f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1507363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150736840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150736ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150737180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150737620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150737ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150737f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150738400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1507388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150738d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1507391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150739b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15073a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15073a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15073ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15073b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15073b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15073bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15073c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15073c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15073c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15073ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15073d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15073d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15073dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15073e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15073e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15073e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15073ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15073f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15073f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15073fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1507400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150740580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150740a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150741360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150741800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150741ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1507425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150742a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1507433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150743860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150743d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1507441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150744640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150744ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150744f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150745420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1507458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150745d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150746200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1507466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150746b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150746fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150747480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150747920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150747dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150748260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150748700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150748ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150749040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1507494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150749980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150749e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15074a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15074a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15074ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15074b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15074b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15074bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15074c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15074c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15074c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15074ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15074d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15074dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15074e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15074e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15074ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15074f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15074f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15074fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1507502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150750770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150750c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1507513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150751910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150751e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1507523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150752900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150752e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1507533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1507538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150753e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150754390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1507548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150754e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1507558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150755e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150756370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1507568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150756e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150757360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1507578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150757e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150758350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1507588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150758df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150759340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150759890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150759de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15075a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15075a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15075add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15075b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15075b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15075bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15075c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15075c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15075cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15075d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15075d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15075dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15075e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15075e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15075ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15075f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15075f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15075fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1507602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150760820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150760d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1507612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150761810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150761d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1507622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150762800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150762d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1507632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1507637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1507641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150764680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150764b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150764fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150765460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150765900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150765da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150766240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1507666e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150766b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150767020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1507674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150767960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150767e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1507682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1507687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150768f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150769630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150769d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15076a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15076a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15076af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15076b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15076b7f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15076b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15074d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15074cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15074d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150720860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150720250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150722870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15074f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150717c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15071e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15071f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15071f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15071dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15071fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150716c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150722e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15072f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15076a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150719df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15071a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15074f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15074dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150718220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1507184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1507187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15076bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15076bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15076c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15076c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15076c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15076ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15076ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15076cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15076d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15076d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15076d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15076da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15076dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15076e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15076e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15076e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15076e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15076eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15076edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15076f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15076f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15076f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15076f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15076fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15076fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150770110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1507703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150770690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150770950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150770c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150770ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150771190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150771450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150771710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1507719d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150771c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150771f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150772210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1507724d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150772790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150772a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150772d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150772fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150773290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150773550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150773810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150773ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150773d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150774050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150774310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1507745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150774890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150774b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150774e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1507750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150775390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150775650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150775910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150775bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150775e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150776150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150776410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1507766d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x150776990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150776c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150776f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1507771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150777490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150777750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150777a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150777cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150777f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150778250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150778510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1507787d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150778a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150778d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150779010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1507792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150779590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150779850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150779b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150779dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15077a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15077a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15077a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15077a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15077ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15077ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15077b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15077b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15077b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15077b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15077bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15077bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15077c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15077c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15077c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15077c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15077cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15077cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15077d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15077d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15077d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15077da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15077dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15077dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15077e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15077e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15077e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15077ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15077ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15077f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15077f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15077f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15077f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15077fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15077fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1507800d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150780390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150780650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150780910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150780bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150780e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150781150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150781410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1507816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150781990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150781c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150781f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1507821d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150782490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150782750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150782a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150782cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150782f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150783250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150783510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1507837d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150783a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150783d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150784010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1507842d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150784590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150784850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150784b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150784dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150785090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150785350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150785610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1507858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150785b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150785e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150786110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1507863d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150786690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150786950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150786c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150786ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150787190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150787450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150787710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1507879d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150787c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150787f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150788210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1507884d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150788790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150788a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150788d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150788fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150789290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150789550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150789810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150789ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150789d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15078a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15078a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15078a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15078a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15078add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15078b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15078b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15078b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15078be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15078c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15078c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15078cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15078d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15078d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15078d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15078dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15078e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15078e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15078eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15078ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15078f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15078f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15078fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1507900e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150790550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1507909c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150790e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1507912a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150791710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150791b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150791ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150792460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1507928d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150792d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1507931b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150793620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150793a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150793f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150794370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1507947e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150794c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1507950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150795530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1507959a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150795e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150796280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1507966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150796b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150796fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150797440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1507978b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150797d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150798190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150798600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150798a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150798ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150799350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1507997c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150799c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15079a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15079a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15079a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15079adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15079b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15079b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15079bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15079bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15079c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15079c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15079cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15079d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15079d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15079da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15079dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15079e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15079e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15079ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15079f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15079f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15079f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15079fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1507a0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1507a0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1507a13d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1507a1af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1507a2210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1507a24d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1507a2cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1507a2f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1507a3590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1518044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1518056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1518063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1518092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15180a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15180a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15180af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15180b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15180be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15180c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15180cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15180d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15180dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15180dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15180e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15180e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15180e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15180edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15180f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15180f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15180fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15180fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1518102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1518114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1518133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1518149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1518152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1518177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1518180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1518189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1518196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15181a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15181a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15181ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15181b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15181b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15181ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15181bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15181c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15181c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15181cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15181d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15181d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15181d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15181ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15181e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15181e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15181eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15181efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15181f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15181f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15181fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1518205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1518217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1518224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1518236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151824020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1518242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151824750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151824bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151825030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1518254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151825910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151825d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1518261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151826660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151826ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151826f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1518273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151827820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151827c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151828100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151828570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1518289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151828e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1518292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151829730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151829ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15182a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15182a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15182a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15182ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15182b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15182b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15182bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15182bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15182c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15182c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15182cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15182d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15182d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15182d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15182de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15182e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15182e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15182eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15182eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15182f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15182f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15182fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1518301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151830620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151830a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151830f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151831370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1518317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151831c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1518320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151832530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1518329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151832e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151833280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1518336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151833b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151833fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151834440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1518348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151834d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151835190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151835600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151835a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151835ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151836350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1518367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151836c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1518370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151837510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151837980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151838260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1518386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151838b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151838fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151839420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151839890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151839d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15183a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15183a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15183aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15183aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15183b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15183b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15183bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15183c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15183c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15183c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15183cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15183d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15183d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15183db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15183df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15183e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15183e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15183ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15183f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15183f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15183fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15183fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151840310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1518408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151840d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151841cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151841f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151842250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1518426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151842b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151842fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151843880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151843cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151844160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1518445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151844a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151844eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151845320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151845790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151845c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151846070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1518464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151846950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151846dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151847230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1518476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151847b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151847f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1518483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151848860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151848cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151849140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1518495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151849a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151849e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15184a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15184a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15184abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15184b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15184b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15184b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15184bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15184c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15184c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15184caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15184cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15184d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15184d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15184dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15184e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15184e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15184ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15184ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15184f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15184f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15184fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151850030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1518504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151850910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151850d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1518511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151851ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151851f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1518523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151852820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151852c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151853100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151853570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1518539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151853e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1518542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151854730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151854ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151855010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151855480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1518558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151856360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151856a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1518571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1518578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151857b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151857ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1518585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151858c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.231s
sys	0m0.158s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.77 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.20 sec*proc (2 tests)

Total Test time (real) =   2.21 sec
        2.23 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.12 user         0.08 sys
```
