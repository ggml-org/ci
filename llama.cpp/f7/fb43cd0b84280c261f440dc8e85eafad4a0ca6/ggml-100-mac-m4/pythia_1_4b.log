Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.511s
user	0m0.874s
sys	0m1.199s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Built target test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-gguf
[ 64%] Built target test-chat-template
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target llama-batched-bench
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Generating index.html.gz.hpp
[ 87%] Built target llama-quantize
[ 87%] Built target llama-retrieval
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.128s
user	0m6.187s
sys	0m9.910s

main: quantize time =  3438.57 ms
main:    total time =  3438.57 ms

main: quantize time =  1349.87 ms
main:    total time =  1349.87 ms

main: quantize time =  1439.42 ms
main:    total time =  1439.42 ms

main: quantize time =  2874.76 ms
main:    total time =  2874.76 ms

main: quantize time =  2616.83 ms
main:    total time =  2616.83 ms

main: quantize time =  4906.48 ms
main:    total time =  4906.48 ms

main: quantize time =  5552.57 ms
main:    total time =  5552.57 ms

main: quantize time =  7003.44 ms
main:    total time =  7003.44 ms

main: quantize time =  5954.18 ms
main:    total time =  5954.18 ms

main: quantize time =  4498.13 ms
main:    total time =  4498.13 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.147 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.330 I main: llama backend init
0.00.000.342 I main: load the model and apply lora adapter, if any
0.00.091.445 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.103.923 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.103.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.103.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.103.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.103.943 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.103.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.103.944 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.103.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.103.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.103.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.103.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.103.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.103.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.103.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.103.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.103.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.103.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.110.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.112.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.120.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.120.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.120.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.120.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.120.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.120.451 I llama_model_loader: - type  f32:  194 tensors
0.00.120.452 I llama_model_loader: - type  f16:   98 tensors
0.00.120.453 I print_info: file format = GGUF V3 (latest)
0.00.120.455 I print_info: file type   = all F32 (guessed)
0.00.120.457 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.156.107 I load: special tokens cache size = 25
0.00.163.786 I load: token to piece cache size = 0.2984 MB
0.00.163.789 I print_info: arch             = gptneox
0.00.163.790 I print_info: vocab_only       = 0
0.00.163.790 I print_info: n_ctx_train      = 2048
0.00.163.790 I print_info: n_embd           = 2048
0.00.163.790 I print_info: n_layer          = 24
0.00.163.794 I print_info: n_head           = 16
0.00.163.795 I print_info: n_head_kv        = 16
0.00.163.795 I print_info: n_rot            = 32
0.00.163.796 I print_info: n_swa            = 0
0.00.163.796 I print_info: n_embd_head_k    = 128
0.00.163.796 I print_info: n_embd_head_v    = 128
0.00.163.797 I print_info: n_gqa            = 1
0.00.163.797 I print_info: n_embd_k_gqa     = 2048
0.00.163.798 I print_info: n_embd_v_gqa     = 2048
0.00.163.799 I print_info: f_norm_eps       = 1.0e-05
0.00.163.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.163.799 I print_info: f_clamp_kqv      = 0.0e+00
0.00.163.799 I print_info: f_max_alibi_bias = 0.0e+00
0.00.163.800 I print_info: f_logit_scale    = 0.0e+00
0.00.163.800 I print_info: n_ff             = 8192
0.00.163.802 I print_info: n_expert         = 0
0.00.163.802 I print_info: n_expert_used    = 0
0.00.163.804 I print_info: causal attn      = 1
0.00.163.804 I print_info: pooling type     = 0
0.00.163.804 I print_info: rope type        = 2
0.00.163.804 I print_info: rope scaling     = linear
0.00.163.805 I print_info: freq_base_train  = 10000.0
0.00.163.805 I print_info: freq_scale_train = 1
0.00.163.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.163.806 I print_info: rope_finetuned   = unknown
0.00.163.806 I print_info: ssm_d_conv       = 0
0.00.163.806 I print_info: ssm_d_inner      = 0
0.00.163.806 I print_info: ssm_d_state      = 0
0.00.163.806 I print_info: ssm_dt_rank      = 0
0.00.163.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.163.807 I print_info: model type       = 1.4B
0.00.163.809 I print_info: model params     = 1.41 B
0.00.163.809 I print_info: general.name     = 1.4B
0.00.163.809 I print_info: vocab type       = BPE
0.00.163.809 I print_info: n_vocab          = 50304
0.00.163.810 I print_info: n_merges         = 50009
0.00.163.810 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.163.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.163.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.163.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.163.811 I print_info: LF token         = 128 'Ä'
0.00.163.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.163.811 I print_info: max token length = 1024
0.00.166.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.166.649 I load_tensors: offloading output layer to GPU
0.00.166.649 I load_tensors: offloaded 25/25 layers to GPU
0.00.166.668 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.166.670 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.167.002 I llama_init_from_model: n_seq_max     = 1
0.00.167.003 I llama_init_from_model: n_ctx         = 2048
0.00.167.003 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.167.003 I llama_init_from_model: n_batch       = 2048
0.00.167.003 I llama_init_from_model: n_ubatch      = 512
0.00.167.003 I llama_init_from_model: flash_attn    = 0
0.00.167.004 I llama_init_from_model: freq_base     = 10000.0
0.00.167.004 I llama_init_from_model: freq_scale    = 1
0.00.167.005 I ggml_metal_init: allocating
0.00.167.008 I ggml_metal_init: found device: Apple M4
0.00.167.010 I ggml_metal_init: picking default device: Apple M4
0.00.167.761 I ggml_metal_init: using embedded metal library
0.00.191.555 I ggml_metal_init: GPU name:   Apple M4
0.00.191.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.191.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.191.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.191.558 I ggml_metal_init: simdgroup reduction   = true
0.00.191.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.191.559 I ggml_metal_init: has bfloat            = true
0.00.191.559 I ggml_metal_init: use bfloat            = true
0.00.191.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.191.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.925 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.362.338 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.362.345 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.362.367 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.363.289 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.363.291 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.363.291 I llama_init_from_model: graph nodes  = 967
0.00.363.291 I llama_init_from_model: graph splits = 2
0.00.363.294 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.363.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.363.406 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.225 I main: llama threadpool init, n_threads = 4
0.00.444.261 I 
0.00.444.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.298 I 
0.00.444.374 I sampler seed: 1234
0.00.444.379 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.408 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.410 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.410 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.283.468 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.283.469 I llama_perf_context_print:        load time =     351.43 ms
0.02.283.469 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.73 tokens per second)
0.02.283.470 I llama_perf_context_print:        eval time =    1792.25 ms /    63 runs   (   28.45 ms per token,    35.15 tokens per second)
0.02.283.470 I llama_perf_context_print:       total time =    1840.59 ms /    70 tokens
0.02.283.649 I ggml_metal_free: deallocating

real	0m2.616s
user	0m0.160s
sys	0m0.110s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.637 I llama_model_loader: - type  f32:  194 tensors
0.00.038.637 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.638 I print_info: file format = GGUF V3 (latest)
0.00.038.640 I print_info: file type   = Q8_0
0.00.038.641 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.132 I load: special tokens cache size = 25
0.00.066.524 I load: token to piece cache size = 0.2984 MB
0.00.066.529 I print_info: arch             = gptneox
0.00.066.531 I print_info: vocab_only       = 0
0.00.066.531 I print_info: n_ctx_train      = 2048
0.00.066.531 I print_info: n_embd           = 2048
0.00.066.531 I print_info: n_layer          = 24
0.00.066.536 I print_info: n_head           = 16
0.00.066.536 I print_info: n_head_kv        = 16
0.00.066.536 I print_info: n_rot            = 32
0.00.066.537 I print_info: n_swa            = 0
0.00.066.537 I print_info: n_embd_head_k    = 128
0.00.066.537 I print_info: n_embd_head_v    = 128
0.00.066.538 I print_info: n_gqa            = 1
0.00.066.539 I print_info: n_embd_k_gqa     = 2048
0.00.066.539 I print_info: n_embd_v_gqa     = 2048
0.00.066.540 I print_info: f_norm_eps       = 1.0e-05
0.00.066.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.541 I print_info: f_logit_scale    = 0.0e+00
0.00.066.542 I print_info: n_ff             = 8192
0.00.066.542 I print_info: n_expert         = 0
0.00.066.542 I print_info: n_expert_used    = 0
0.00.066.542 I print_info: causal attn      = 1
0.00.066.542 I print_info: pooling type     = 0
0.00.066.542 I print_info: rope type        = 2
0.00.066.543 I print_info: rope scaling     = linear
0.00.066.543 I print_info: freq_base_train  = 10000.0
0.00.066.544 I print_info: freq_scale_train = 1
0.00.066.544 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.544 I print_info: rope_finetuned   = unknown
0.00.066.544 I print_info: ssm_d_conv       = 0
0.00.066.544 I print_info: ssm_d_inner      = 0
0.00.066.544 I print_info: ssm_d_state      = 0
0.00.066.544 I print_info: ssm_dt_rank      = 0
0.00.066.544 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.545 I print_info: model type       = 1.4B
0.00.066.546 I print_info: model params     = 1.41 B
0.00.066.546 I print_info: general.name     = 1.4B
0.00.066.546 I print_info: vocab type       = BPE
0.00.066.547 I print_info: n_vocab          = 50304
0.00.066.549 I print_info: n_merges         = 50009
0.00.066.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.550 I print_info: LF token         = 128 'Ä'
0.00.066.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.550 I print_info: max token length = 1024
0.00.069.016 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.016 I load_tensors: offloading output layer to GPU
0.00.069.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.028 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.029 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.370 I llama_init_from_model: n_seq_max     = 1
0.00.069.370 I llama_init_from_model: n_ctx         = 2048
0.00.069.371 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.069.371 I llama_init_from_model: n_batch       = 2048
0.00.069.371 I llama_init_from_model: n_ubatch      = 512
0.00.069.371 I llama_init_from_model: flash_attn    = 0
0.00.069.371 I llama_init_from_model: freq_base     = 10000.0
0.00.069.372 I llama_init_from_model: freq_scale    = 1
0.00.069.372 I ggml_metal_init: allocating
0.00.069.375 I ggml_metal_init: found device: Apple M4
0.00.069.377 I ggml_metal_init: picking default device: Apple M4
0.00.070.154 I ggml_metal_init: using embedded metal library
0.00.072.822 I ggml_metal_init: GPU name:   Apple M4
0.00.072.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.825 I ggml_metal_init: simdgroup reduction   = true
0.00.072.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.825 I ggml_metal_init: has bfloat            = true
0.00.072.825 I ggml_metal_init: use bfloat            = true
0.00.072.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.200 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.640 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.648 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.923 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.109.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.109.926 I llama_init_from_model: graph nodes  = 967
0.00.109.926 I llama_init_from_model: graph splits = 2
0.00.109.930 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.329.941 I main: llama threadpool init, n_threads = 4
0.01.330.005 I 
0.01.330.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.330.053 I 
0.01.330.578 I sampler seed: 1234
0.01.330.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.330.648 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.330.650 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.330.650 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.427.383 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.02.427.383 I llama_perf_context_print:        load time =    1318.66 ms
0.02.427.384 I llama_perf_context_print: prompt eval time =      50.78 ms /     7 tokens (    7.25 ms per token,   137.84 tokens per second)
0.02.427.385 I llama_perf_context_print:        eval time =    1043.09 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.427.386 I llama_perf_context_print:       total time =    1098.91 ms /    70 tokens
0.02.427.619 I ggml_metal_free: deallocating

real	0m2.448s
user	0m0.125s
sys	0m0.244s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.016.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.488 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.191 I llama_model_loader: - type  f32:  194 tensors
0.00.044.191 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.193 I print_info: file format = GGUF V3 (latest)
0.00.044.193 I print_info: file type   = Q4_0
0.00.044.194 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.073.780 I load: special tokens cache size = 25
0.00.085.212 I load: token to piece cache size = 0.2984 MB
0.00.085.217 I print_info: arch             = gptneox
0.00.085.218 I print_info: vocab_only       = 0
0.00.085.218 I print_info: n_ctx_train      = 2048
0.00.085.218 I print_info: n_embd           = 2048
0.00.085.218 I print_info: n_layer          = 24
0.00.085.223 I print_info: n_head           = 16
0.00.085.224 I print_info: n_head_kv        = 16
0.00.085.228 I print_info: n_rot            = 32
0.00.085.228 I print_info: n_swa            = 0
0.00.085.229 I print_info: n_embd_head_k    = 128
0.00.085.229 I print_info: n_embd_head_v    = 128
0.00.085.230 I print_info: n_gqa            = 1
0.00.085.231 I print_info: n_embd_k_gqa     = 2048
0.00.085.232 I print_info: n_embd_v_gqa     = 2048
0.00.085.233 I print_info: f_norm_eps       = 1.0e-05
0.00.085.233 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.236 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.236 I print_info: f_logit_scale    = 0.0e+00
0.00.085.237 I print_info: n_ff             = 8192
0.00.085.237 I print_info: n_expert         = 0
0.00.085.237 I print_info: n_expert_used    = 0
0.00.085.237 I print_info: causal attn      = 1
0.00.085.238 I print_info: pooling type     = 0
0.00.085.238 I print_info: rope type        = 2
0.00.085.238 I print_info: rope scaling     = linear
0.00.085.239 I print_info: freq_base_train  = 10000.0
0.00.085.239 I print_info: freq_scale_train = 1
0.00.085.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.240 I print_info: rope_finetuned   = unknown
0.00.085.240 I print_info: ssm_d_conv       = 0
0.00.085.241 I print_info: ssm_d_inner      = 0
0.00.085.241 I print_info: ssm_d_state      = 0
0.00.085.241 I print_info: ssm_dt_rank      = 0
0.00.085.246 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.247 I print_info: model type       = 1.4B
0.00.085.247 I print_info: model params     = 1.41 B
0.00.085.248 I print_info: general.name     = 1.4B
0.00.085.249 I print_info: vocab type       = BPE
0.00.085.249 I print_info: n_vocab          = 50304
0.00.085.249 I print_info: n_merges         = 50009
0.00.085.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.252 I print_info: LF token         = 128 'Ä'
0.00.085.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.252 I print_info: max token length = 1024
0.00.088.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.298 I load_tensors: offloading output layer to GPU
0.00.088.299 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.311 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.313 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.088.769 I llama_init_from_model: n_seq_max     = 1
0.00.088.770 I llama_init_from_model: n_ctx         = 2048
0.00.088.770 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.088.771 I llama_init_from_model: n_batch       = 2048
0.00.088.771 I llama_init_from_model: n_ubatch      = 512
0.00.088.771 I llama_init_from_model: flash_attn    = 0
0.00.088.772 I llama_init_from_model: freq_base     = 10000.0
0.00.088.772 I llama_init_from_model: freq_scale    = 1
0.00.088.773 I ggml_metal_init: allocating
0.00.088.778 I ggml_metal_init: found device: Apple M4
0.00.088.781 I ggml_metal_init: picking default device: Apple M4
0.00.089.742 I ggml_metal_init: using embedded metal library
0.00.093.705 I ggml_metal_init: GPU name:   Apple M4
0.00.093.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.707 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.708 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.708 I ggml_metal_init: simdgroup reduction   = true
0.00.093.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.709 I ggml_metal_init: has bfloat            = true
0.00.093.709 I ggml_metal_init: use bfloat            = true
0.00.093.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.544 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.137.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.137.187 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.137.222 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.138.396 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.138.399 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.138.399 I llama_init_from_model: graph nodes  = 967
0.00.138.399 I llama_init_from_model: graph splits = 2
0.00.138.404 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.138.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.373 I main: llama threadpool init, n_threads = 4
0.00.800.453 I 
0.00.800.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.499 I 
0.00.800.980 I sampler seed: 1234
0.00.800.991 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.021 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.023 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.023 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.486.672 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.486.672 I llama_perf_context_print:        load time =     782.43 ms
0.01.486.673 I llama_perf_context_print: prompt eval time =      47.35 ms /     7 tokens (    6.76 ms per token,   147.83 tokens per second)
0.01.486.676 I llama_perf_context_print:        eval time =     635.36 ms /    63 runs   (   10.09 ms per token,    99.16 tokens per second)
0.01.486.676 I llama_perf_context_print:       total time =     687.61 ms /    70 tokens
0.01.486.951 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.140s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.916 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.917 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.990 I llama_model_loader: - type  f32:  194 tensors
0.00.027.990 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.991 I print_info: file format = GGUF V3 (latest)
0.00.027.991 I print_info: file type   = Q4_1
0.00.027.992 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.426 I load: special tokens cache size = 25
0.00.053.540 I load: token to piece cache size = 0.2984 MB
0.00.053.543 I print_info: arch             = gptneox
0.00.053.543 I print_info: vocab_only       = 0
0.00.053.543 I print_info: n_ctx_train      = 2048
0.00.053.544 I print_info: n_embd           = 2048
0.00.053.544 I print_info: n_layer          = 24
0.00.053.547 I print_info: n_head           = 16
0.00.053.548 I print_info: n_head_kv        = 16
0.00.053.548 I print_info: n_rot            = 32
0.00.053.548 I print_info: n_swa            = 0
0.00.053.548 I print_info: n_embd_head_k    = 128
0.00.053.548 I print_info: n_embd_head_v    = 128
0.00.053.549 I print_info: n_gqa            = 1
0.00.053.550 I print_info: n_embd_k_gqa     = 2048
0.00.053.551 I print_info: n_embd_v_gqa     = 2048
0.00.053.551 I print_info: f_norm_eps       = 1.0e-05
0.00.053.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.553 I print_info: f_logit_scale    = 0.0e+00
0.00.053.554 I print_info: n_ff             = 8192
0.00.053.554 I print_info: n_expert         = 0
0.00.053.555 I print_info: n_expert_used    = 0
0.00.053.555 I print_info: causal attn      = 1
0.00.053.555 I print_info: pooling type     = 0
0.00.053.555 I print_info: rope type        = 2
0.00.053.557 I print_info: rope scaling     = linear
0.00.053.557 I print_info: freq_base_train  = 10000.0
0.00.053.557 I print_info: freq_scale_train = 1
0.00.053.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.558 I print_info: rope_finetuned   = unknown
0.00.053.558 I print_info: ssm_d_conv       = 0
0.00.053.558 I print_info: ssm_d_inner      = 0
0.00.053.558 I print_info: ssm_d_state      = 0
0.00.053.558 I print_info: ssm_dt_rank      = 0
0.00.053.558 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.559 I print_info: model type       = 1.4B
0.00.053.559 I print_info: model params     = 1.41 B
0.00.053.559 I print_info: general.name     = 1.4B
0.00.053.560 I print_info: vocab type       = BPE
0.00.053.560 I print_info: n_vocab          = 50304
0.00.053.560 I print_info: n_merges         = 50009
0.00.053.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.565 I print_info: LF token         = 128 'Ä'
0.00.053.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.572 I print_info: max token length = 1024
0.00.055.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.479 I load_tensors: offloading output layer to GPU
0.00.055.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.490 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.491 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.770 I llama_init_from_model: n_seq_max     = 1
0.00.055.770 I llama_init_from_model: n_ctx         = 2048
0.00.055.770 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.771 I llama_init_from_model: n_batch       = 2048
0.00.055.771 I llama_init_from_model: n_ubatch      = 512
0.00.055.771 I llama_init_from_model: flash_attn    = 0
0.00.055.771 I llama_init_from_model: freq_base     = 10000.0
0.00.055.772 I llama_init_from_model: freq_scale    = 1
0.00.055.772 I ggml_metal_init: allocating
0.00.055.775 I ggml_metal_init: found device: Apple M4
0.00.055.777 I ggml_metal_init: picking default device: Apple M4
0.00.056.365 I ggml_metal_init: using embedded metal library
0.00.058.694 I ggml_metal_init: GPU name:   Apple M4
0.00.058.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.696 I ggml_metal_init: simdgroup reduction   = true
0.00.058.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.696 I ggml_metal_init: has bfloat            = true
0.00.058.697 I ggml_metal_init: use bfloat            = true
0.00.058.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.459 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.140 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.146 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.245 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.247 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.247 I llama_init_from_model: graph nodes  = 967
0.00.089.247 I llama_init_from_model: graph splits = 2
0.00.089.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.453 I main: llama threadpool init, n_threads = 4
0.00.734.491 I 
0.00.734.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.512 I 
0.00.734.725 I sampler seed: 1234
0.00.734.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.778 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.779 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.456.099 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67362.43 tokens per second)
0.01.456.100 I llama_perf_context_print:        load time =     724.80 ms
0.01.456.101 I llama_perf_context_print: prompt eval time =      45.44 ms /     7 tokens (    6.49 ms per token,   154.04 tokens per second)
0.01.456.101 I llama_perf_context_print:        eval time =     673.12 ms /    63 runs   (   10.68 ms per token,    93.59 tokens per second)
0.01.456.103 I llama_perf_context_print:       total time =     722.51 ms /    70 tokens
0.01.456.331 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.211 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.211 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.211 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.217 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.315 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.328 I llama_model_loader: - type  f32:  194 tensors
0.00.027.329 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.329 I print_info: file format = GGUF V3 (latest)
0.00.027.330 I print_info: file type   = Q5_0
0.00.027.331 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.028 I load: special tokens cache size = 25
0.00.052.282 I load: token to piece cache size = 0.2984 MB
0.00.052.285 I print_info: arch             = gptneox
0.00.052.285 I print_info: vocab_only       = 0
0.00.052.285 I print_info: n_ctx_train      = 2048
0.00.052.285 I print_info: n_embd           = 2048
0.00.052.286 I print_info: n_layer          = 24
0.00.052.289 I print_info: n_head           = 16
0.00.052.289 I print_info: n_head_kv        = 16
0.00.052.290 I print_info: n_rot            = 32
0.00.052.290 I print_info: n_swa            = 0
0.00.052.290 I print_info: n_embd_head_k    = 128
0.00.052.290 I print_info: n_embd_head_v    = 128
0.00.052.291 I print_info: n_gqa            = 1
0.00.052.292 I print_info: n_embd_k_gqa     = 2048
0.00.052.294 I print_info: n_embd_v_gqa     = 2048
0.00.052.295 I print_info: f_norm_eps       = 1.0e-05
0.00.052.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.297 I print_info: f_logit_scale    = 0.0e+00
0.00.052.297 I print_info: n_ff             = 8192
0.00.052.298 I print_info: n_expert         = 0
0.00.052.298 I print_info: n_expert_used    = 0
0.00.052.298 I print_info: causal attn      = 1
0.00.052.298 I print_info: pooling type     = 0
0.00.052.300 I print_info: rope type        = 2
0.00.052.301 I print_info: rope scaling     = linear
0.00.052.301 I print_info: freq_base_train  = 10000.0
0.00.052.301 I print_info: freq_scale_train = 1
0.00.052.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.302 I print_info: rope_finetuned   = unknown
0.00.052.302 I print_info: ssm_d_conv       = 0
0.00.052.302 I print_info: ssm_d_inner      = 0
0.00.052.302 I print_info: ssm_d_state      = 0
0.00.052.302 I print_info: ssm_dt_rank      = 0
0.00.052.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.303 I print_info: model type       = 1.4B
0.00.052.303 I print_info: model params     = 1.41 B
0.00.052.304 I print_info: general.name     = 1.4B
0.00.052.308 I print_info: vocab type       = BPE
0.00.052.308 I print_info: n_vocab          = 50304
0.00.052.309 I print_info: n_merges         = 50009
0.00.052.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.309 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.309 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.309 I print_info: LF token         = 128 'Ä'
0.00.052.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.311 I print_info: max token length = 1024
0.00.054.235 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.235 I load_tensors: offloading output layer to GPU
0.00.054.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.246 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.247 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.526 I llama_init_from_model: n_seq_max     = 1
0.00.054.527 I llama_init_from_model: n_ctx         = 2048
0.00.054.527 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.527 I llama_init_from_model: n_batch       = 2048
0.00.054.527 I llama_init_from_model: n_ubatch      = 512
0.00.054.527 I llama_init_from_model: flash_attn    = 0
0.00.054.528 I llama_init_from_model: freq_base     = 10000.0
0.00.054.528 I llama_init_from_model: freq_scale    = 1
0.00.054.528 I ggml_metal_init: allocating
0.00.054.531 I ggml_metal_init: found device: Apple M4
0.00.054.533 I ggml_metal_init: picking default device: Apple M4
0.00.055.134 I ggml_metal_init: using embedded metal library
0.00.057.483 I ggml_metal_init: GPU name:   Apple M4
0.00.057.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.485 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.486 I ggml_metal_init: simdgroup reduction   = true
0.00.057.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.486 I ggml_metal_init: has bfloat            = true
0.00.057.486 I ggml_metal_init: use bfloat            = true
0.00.057.486 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.004 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.444 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.457 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.481 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.474 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.475 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.476 I llama_init_from_model: graph nodes  = 967
0.00.087.476 I llama_init_from_model: graph splits = 2
0.00.087.479 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.268 I main: llama threadpool init, n_threads = 4
0.00.774.303 I 
0.00.774.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.325 I 
0.00.774.541 I sampler seed: 1234
0.00.774.545 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.587 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.588 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.588 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.556.701 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.556.702 I llama_perf_context_print:        load time =     762.46 ms
0.01.556.703 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.60 tokens per second)
0.01.556.703 I llama_perf_context_print:        eval time =     731.94 ms /    63 runs   (   11.62 ms per token,    86.07 tokens per second)
0.01.556.703 I llama_perf_context_print:       total time =     783.29 ms /    70 tokens
0.01.556.976 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.519 I llama_model_loader: - type  f32:  194 tensors
0.00.025.519 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.520 I print_info: file format = GGUF V3 (latest)
0.00.025.521 I print_info: file type   = Q5_1
0.00.025.521 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.100 I load: special tokens cache size = 25
0.00.050.151 I load: token to piece cache size = 0.2984 MB
0.00.050.154 I print_info: arch             = gptneox
0.00.050.154 I print_info: vocab_only       = 0
0.00.050.154 I print_info: n_ctx_train      = 2048
0.00.050.154 I print_info: n_embd           = 2048
0.00.050.155 I print_info: n_layer          = 24
0.00.050.157 I print_info: n_head           = 16
0.00.050.158 I print_info: n_head_kv        = 16
0.00.050.158 I print_info: n_rot            = 32
0.00.050.159 I print_info: n_swa            = 0
0.00.050.159 I print_info: n_embd_head_k    = 128
0.00.050.159 I print_info: n_embd_head_v    = 128
0.00.050.160 I print_info: n_gqa            = 1
0.00.050.161 I print_info: n_embd_k_gqa     = 2048
0.00.050.161 I print_info: n_embd_v_gqa     = 2048
0.00.050.162 I print_info: f_norm_eps       = 1.0e-05
0.00.050.162 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.163 I print_info: f_logit_scale    = 0.0e+00
0.00.050.163 I print_info: n_ff             = 8192
0.00.050.163 I print_info: n_expert         = 0
0.00.050.164 I print_info: n_expert_used    = 0
0.00.050.164 I print_info: causal attn      = 1
0.00.050.164 I print_info: pooling type     = 0
0.00.050.165 I print_info: rope type        = 2
0.00.050.167 I print_info: rope scaling     = linear
0.00.050.167 I print_info: freq_base_train  = 10000.0
0.00.050.168 I print_info: freq_scale_train = 1
0.00.050.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.169 I print_info: rope_finetuned   = unknown
0.00.050.170 I print_info: ssm_d_conv       = 0
0.00.050.170 I print_info: ssm_d_inner      = 0
0.00.050.170 I print_info: ssm_d_state      = 0
0.00.050.170 I print_info: ssm_dt_rank      = 0
0.00.050.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.170 I print_info: model type       = 1.4B
0.00.050.171 I print_info: model params     = 1.41 B
0.00.050.171 I print_info: general.name     = 1.4B
0.00.050.171 I print_info: vocab type       = BPE
0.00.050.171 I print_info: n_vocab          = 50304
0.00.050.172 I print_info: n_merges         = 50009
0.00.050.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.173 I print_info: LF token         = 128 'Ä'
0.00.050.176 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: max token length = 1024
0.00.052.157 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.157 I load_tensors: offloading output layer to GPU
0.00.052.157 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.167 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.169 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.441 I llama_init_from_model: n_seq_max     = 1
0.00.052.442 I llama_init_from_model: n_ctx         = 2048
0.00.052.442 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.442 I llama_init_from_model: n_batch       = 2048
0.00.052.443 I llama_init_from_model: n_ubatch      = 512
0.00.052.443 I llama_init_from_model: flash_attn    = 0
0.00.052.443 I llama_init_from_model: freq_base     = 10000.0
0.00.052.443 I llama_init_from_model: freq_scale    = 1
0.00.052.444 I ggml_metal_init: allocating
0.00.052.447 I ggml_metal_init: found device: Apple M4
0.00.052.449 I ggml_metal_init: picking default device: Apple M4
0.00.053.043 I ggml_metal_init: using embedded metal library
0.00.055.347 I ggml_metal_init: GPU name:   Apple M4
0.00.055.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.350 I ggml_metal_init: simdgroup reduction   = true
0.00.055.350 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.350 I ggml_metal_init: has bfloat            = true
0.00.055.350 I ggml_metal_init: use bfloat            = true
0.00.055.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.978 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.064 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.069 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.115 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.116 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.116 I llama_init_from_model: graph nodes  = 967
0.00.085.117 I llama_init_from_model: graph splits = 2
0.00.085.120 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.448 I main: llama threadpool init, n_threads = 4
0.00.702.485 I 
0.00.702.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.525 I 
0.00.702.758 I sampler seed: 1234
0.00.702.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.783 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.784 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.784 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.530.905 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.530.905 I llama_perf_context_print:        load time =     692.87 ms
0.01.530.906 I llama_perf_context_print: prompt eval time =      42.31 ms /     7 tokens (    6.04 ms per token,   165.46 tokens per second)
0.01.530.907 I llama_perf_context_print:        eval time =     782.95 ms /    63 runs   (   12.43 ms per token,    80.47 tokens per second)
0.01.530.907 I llama_perf_context_print:       total time =     829.31 ms /    70 tokens
0.01.531.129 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.264 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.265 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.265 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.266 I llama_model_loader: - type  f32:  194 tensors
0.00.025.266 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.266 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.266 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.267 I print_info: file format = GGUF V3 (latest)
0.00.025.267 I print_info: file type   = Q2_K - Medium
0.00.025.267 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.002 I load: special tokens cache size = 25
0.00.050.037 I load: token to piece cache size = 0.2984 MB
0.00.050.041 I print_info: arch             = gptneox
0.00.050.041 I print_info: vocab_only       = 0
0.00.050.041 I print_info: n_ctx_train      = 2048
0.00.050.041 I print_info: n_embd           = 2048
0.00.050.041 I print_info: n_layer          = 24
0.00.050.045 I print_info: n_head           = 16
0.00.050.045 I print_info: n_head_kv        = 16
0.00.050.046 I print_info: n_rot            = 32
0.00.050.046 I print_info: n_swa            = 0
0.00.050.046 I print_info: n_embd_head_k    = 128
0.00.050.049 I print_info: n_embd_head_v    = 128
0.00.050.049 I print_info: n_gqa            = 1
0.00.050.050 I print_info: n_embd_k_gqa     = 2048
0.00.050.051 I print_info: n_embd_v_gqa     = 2048
0.00.050.052 I print_info: f_norm_eps       = 1.0e-05
0.00.050.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.054 I print_info: f_logit_scale    = 0.0e+00
0.00.050.054 I print_info: n_ff             = 8192
0.00.050.055 I print_info: n_expert         = 0
0.00.050.055 I print_info: n_expert_used    = 0
0.00.050.055 I print_info: causal attn      = 1
0.00.050.057 I print_info: pooling type     = 0
0.00.050.057 I print_info: rope type        = 2
0.00.050.057 I print_info: rope scaling     = linear
0.00.050.057 I print_info: freq_base_train  = 10000.0
0.00.050.058 I print_info: freq_scale_train = 1
0.00.050.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.058 I print_info: rope_finetuned   = unknown
0.00.050.062 I print_info: ssm_d_conv       = 0
0.00.050.062 I print_info: ssm_d_inner      = 0
0.00.050.062 I print_info: ssm_d_state      = 0
0.00.050.062 I print_info: ssm_dt_rank      = 0
0.00.050.063 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.063 I print_info: model type       = 1.4B
0.00.050.063 I print_info: model params     = 1.41 B
0.00.050.063 I print_info: general.name     = 1.4B
0.00.050.064 I print_info: vocab type       = BPE
0.00.050.064 I print_info: n_vocab          = 50304
0.00.050.064 I print_info: n_merges         = 50009
0.00.050.064 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.064 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: LF token         = 128 'Ä'
0.00.050.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: max token length = 1024
0.00.051.843 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.843 I load_tensors: offloading output layer to GPU
0.00.051.843 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.849 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.849 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.277 I llama_init_from_model: n_seq_max     = 1
0.00.052.278 I llama_init_from_model: n_ctx         = 2048
0.00.052.278 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.278 I llama_init_from_model: n_batch       = 2048
0.00.052.278 I llama_init_from_model: n_ubatch      = 512
0.00.052.278 I llama_init_from_model: flash_attn    = 0
0.00.052.279 I llama_init_from_model: freq_base     = 10000.0
0.00.052.279 I llama_init_from_model: freq_scale    = 1
0.00.052.279 I ggml_metal_init: allocating
0.00.052.282 I ggml_metal_init: found device: Apple M4
0.00.052.284 I ggml_metal_init: picking default device: Apple M4
0.00.052.867 I ggml_metal_init: using embedded metal library
0.00.055.221 I ggml_metal_init: GPU name:   Apple M4
0.00.055.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.224 I ggml_metal_init: simdgroup reduction   = true
0.00.055.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.224 I ggml_metal_init: has bfloat            = true
0.00.055.224 I ggml_metal_init: use bfloat            = true
0.00.055.225 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.937 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.128 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.148 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.232 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.233 I llama_init_from_model: graph nodes  = 967
0.00.085.233 I llama_init_from_model: graph splits = 2
0.00.085.237 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.117 I main: llama threadpool init, n_threads = 4
0.00.433.150 I 
0.00.433.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.172 I 
0.00.433.389 I sampler seed: 1234
0.00.433.393 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.433.404 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.433.404 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.433.404 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.105.820 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.105.821 I llama_perf_context_print:        load time =     422.37 ms
0.01.105.822 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.41 tokens per second)
0.01.105.823 I llama_perf_context_print:        eval time =     633.60 ms /    63 runs   (   10.06 ms per token,    99.43 tokens per second)
0.01.105.823 I llama_perf_context_print:       total time =     673.57 ms /    70 tokens
0.01.106.007 I ggml_metal_free: deallocating

real	0m1.123s
user	0m0.109s
sys	0m0.102s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.343 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.978 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.981 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.086 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.044 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.045 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.046 I llama_model_loader: - type  f32:  194 tensors
0.00.027.046 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.046 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.046 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.047 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.047 I print_info: file format = GGUF V3 (latest)
0.00.027.049 I print_info: file type   = Q3_K - Medium
0.00.027.050 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.383 I load: special tokens cache size = 25
0.00.052.368 I load: token to piece cache size = 0.2984 MB
0.00.052.371 I print_info: arch             = gptneox
0.00.052.371 I print_info: vocab_only       = 0
0.00.052.371 I print_info: n_ctx_train      = 2048
0.00.052.372 I print_info: n_embd           = 2048
0.00.052.372 I print_info: n_layer          = 24
0.00.052.375 I print_info: n_head           = 16
0.00.052.376 I print_info: n_head_kv        = 16
0.00.052.376 I print_info: n_rot            = 32
0.00.052.376 I print_info: n_swa            = 0
0.00.052.376 I print_info: n_embd_head_k    = 128
0.00.052.376 I print_info: n_embd_head_v    = 128
0.00.052.377 I print_info: n_gqa            = 1
0.00.052.378 I print_info: n_embd_k_gqa     = 2048
0.00.052.378 I print_info: n_embd_v_gqa     = 2048
0.00.052.379 I print_info: f_norm_eps       = 1.0e-05
0.00.052.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.380 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.380 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.380 I print_info: f_logit_scale    = 0.0e+00
0.00.052.380 I print_info: n_ff             = 8192
0.00.052.381 I print_info: n_expert         = 0
0.00.052.381 I print_info: n_expert_used    = 0
0.00.052.381 I print_info: causal attn      = 1
0.00.052.381 I print_info: pooling type     = 0
0.00.052.381 I print_info: rope type        = 2
0.00.052.381 I print_info: rope scaling     = linear
0.00.052.384 I print_info: freq_base_train  = 10000.0
0.00.052.384 I print_info: freq_scale_train = 1
0.00.052.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.386 I print_info: rope_finetuned   = unknown
0.00.052.386 I print_info: ssm_d_conv       = 0
0.00.052.386 I print_info: ssm_d_inner      = 0
0.00.052.386 I print_info: ssm_d_state      = 0
0.00.052.386 I print_info: ssm_dt_rank      = 0
0.00.052.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.387 I print_info: model type       = 1.4B
0.00.052.387 I print_info: model params     = 1.41 B
0.00.052.388 I print_info: general.name     = 1.4B
0.00.052.388 I print_info: vocab type       = BPE
0.00.052.389 I print_info: n_vocab          = 50304
0.00.052.390 I print_info: n_merges         = 50009
0.00.052.390 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.390 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.390 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.391 I print_info: LF token         = 128 'Ä'
0.00.052.391 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.391 I print_info: max token length = 1024
0.00.054.349 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.349 I load_tensors: offloading output layer to GPU
0.00.054.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.360 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.361 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.644 I llama_init_from_model: n_seq_max     = 1
0.00.054.645 I llama_init_from_model: n_ctx         = 2048
0.00.054.645 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.645 I llama_init_from_model: n_batch       = 2048
0.00.054.645 I llama_init_from_model: n_ubatch      = 512
0.00.054.645 I llama_init_from_model: flash_attn    = 0
0.00.054.646 I llama_init_from_model: freq_base     = 10000.0
0.00.054.646 I llama_init_from_model: freq_scale    = 1
0.00.054.646 I ggml_metal_init: allocating
0.00.054.649 I ggml_metal_init: found device: Apple M4
0.00.054.651 I ggml_metal_init: picking default device: Apple M4
0.00.055.233 I ggml_metal_init: using embedded metal library
0.00.057.589 I ggml_metal_init: GPU name:   Apple M4
0.00.057.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.591 I ggml_metal_init: simdgroup reduction   = true
0.00.057.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.591 I ggml_metal_init: has bfloat            = true
0.00.057.592 I ggml_metal_init: use bfloat            = true
0.00.057.592 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.468 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.042 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.049 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.022 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.023 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.024 I llama_init_from_model: graph nodes  = 967
0.00.088.024 I llama_init_from_model: graph splits = 2
0.00.088.026 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.530.062 I main: llama threadpool init, n_threads = 4
0.00.530.096 I 
0.00.530.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.530.118 I 
0.00.530.343 I sampler seed: 1234
0.00.530.348 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.530.358 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.530.359 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.530.359 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.266.013 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.266.013 I llama_perf_context_print:        load time =     517.86 ms
0.01.266.014 I llama_perf_context_print: prompt eval time =      40.42 ms /     7 tokens (    5.77 ms per token,   173.19 tokens per second)
0.01.266.015 I llama_perf_context_print:        eval time =     692.19 ms /    63 runs   (   10.99 ms per token,    91.02 tokens per second)
0.01.266.015 I llama_perf_context_print:       total time =     736.81 ms /    70 tokens
0.01.266.208 I ggml_metal_free: deallocating

real	0m1.282s
user	0m0.111s
sys	0m0.121s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.491 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.493 I llama_model_loader: - type  f32:  194 tensors
0.00.024.493 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.493 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.493 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.494 I print_info: file format = GGUF V3 (latest)
0.00.024.494 I print_info: file type   = Q4_K - Medium
0.00.024.494 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.150 I load: special tokens cache size = 25
0.00.049.170 I load: token to piece cache size = 0.2984 MB
0.00.049.172 I print_info: arch             = gptneox
0.00.049.173 I print_info: vocab_only       = 0
0.00.049.173 I print_info: n_ctx_train      = 2048
0.00.049.173 I print_info: n_embd           = 2048
0.00.049.173 I print_info: n_layer          = 24
0.00.049.176 I print_info: n_head           = 16
0.00.049.177 I print_info: n_head_kv        = 16
0.00.049.177 I print_info: n_rot            = 32
0.00.049.177 I print_info: n_swa            = 0
0.00.049.177 I print_info: n_embd_head_k    = 128
0.00.049.177 I print_info: n_embd_head_v    = 128
0.00.049.178 I print_info: n_gqa            = 1
0.00.049.179 I print_info: n_embd_k_gqa     = 2048
0.00.049.180 I print_info: n_embd_v_gqa     = 2048
0.00.049.180 I print_info: f_norm_eps       = 1.0e-05
0.00.049.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.181 I print_info: f_logit_scale    = 0.0e+00
0.00.049.184 I print_info: n_ff             = 8192
0.00.049.184 I print_info: n_expert         = 0
0.00.049.185 I print_info: n_expert_used    = 0
0.00.049.185 I print_info: causal attn      = 1
0.00.049.185 I print_info: pooling type     = 0
0.00.049.185 I print_info: rope type        = 2
0.00.049.185 I print_info: rope scaling     = linear
0.00.049.186 I print_info: freq_base_train  = 10000.0
0.00.049.186 I print_info: freq_scale_train = 1
0.00.049.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.186 I print_info: rope_finetuned   = unknown
0.00.049.187 I print_info: ssm_d_conv       = 0
0.00.049.187 I print_info: ssm_d_inner      = 0
0.00.049.187 I print_info: ssm_d_state      = 0
0.00.049.187 I print_info: ssm_dt_rank      = 0
0.00.049.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.187 I print_info: model type       = 1.4B
0.00.049.188 I print_info: model params     = 1.41 B
0.00.049.188 I print_info: general.name     = 1.4B
0.00.049.188 I print_info: vocab type       = BPE
0.00.049.189 I print_info: n_vocab          = 50304
0.00.049.189 I print_info: n_merges         = 50009
0.00.049.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.190 I print_info: LF token         = 128 'Ä'
0.00.049.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.192 I print_info: max token length = 1024
0.00.050.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.951 I load_tensors: offloading output layer to GPU
0.00.050.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.957 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.957 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.259 I llama_init_from_model: n_seq_max     = 1
0.00.051.259 I llama_init_from_model: n_ctx         = 2048
0.00.051.259 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.260 I llama_init_from_model: n_batch       = 2048
0.00.051.260 I llama_init_from_model: n_ubatch      = 512
0.00.051.260 I llama_init_from_model: flash_attn    = 0
0.00.051.260 I llama_init_from_model: freq_base     = 10000.0
0.00.051.261 I llama_init_from_model: freq_scale    = 1
0.00.051.261 I ggml_metal_init: allocating
0.00.051.264 I ggml_metal_init: found device: Apple M4
0.00.051.266 I ggml_metal_init: picking default device: Apple M4
0.00.051.847 I ggml_metal_init: using embedded metal library
0.00.054.196 I ggml_metal_init: GPU name:   Apple M4
0.00.054.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.199 I ggml_metal_init: simdgroup reduction   = true
0.00.054.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.199 I ggml_metal_init: has bfloat            = true
0.00.054.199 I ggml_metal_init: use bfloat            = true
0.00.054.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.200 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.845 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.406 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.433 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.405 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.406 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.407 I llama_init_from_model: graph nodes  = 967
0.00.083.407 I llama_init_from_model: graph splits = 2
0.00.083.410 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.119 I main: llama threadpool init, n_threads = 4
0.00.617.155 I 
0.00.617.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.201 I 
0.00.617.424 I sampler seed: 1234
0.00.617.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.469 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.469 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.469 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.374.421 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.374.422 I llama_perf_context_print:        load time =     607.40 ms
0.01.374.423 I llama_perf_context_print: prompt eval time =      51.77 ms /     7 tokens (    7.40 ms per token,   135.20 tokens per second)
0.01.374.424 I llama_perf_context_print:        eval time =     702.00 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.374.424 I llama_perf_context_print:       total time =     758.16 ms /    70 tokens
0.01.374.634 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.133 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.143 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.143 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.094 I llama_model_loader: - type  f32:  194 tensors
0.00.025.094 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.094 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.095 I print_info: file format = GGUF V3 (latest)
0.00.025.095 I print_info: file type   = Q5_K - Medium
0.00.025.096 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.507 I load: special tokens cache size = 25
0.00.050.381 I load: token to piece cache size = 0.2984 MB
0.00.050.384 I print_info: arch             = gptneox
0.00.050.385 I print_info: vocab_only       = 0
0.00.050.385 I print_info: n_ctx_train      = 2048
0.00.050.385 I print_info: n_embd           = 2048
0.00.050.385 I print_info: n_layer          = 24
0.00.050.388 I print_info: n_head           = 16
0.00.050.388 I print_info: n_head_kv        = 16
0.00.050.388 I print_info: n_rot            = 32
0.00.050.389 I print_info: n_swa            = 0
0.00.050.389 I print_info: n_embd_head_k    = 128
0.00.050.391 I print_info: n_embd_head_v    = 128
0.00.050.392 I print_info: n_gqa            = 1
0.00.050.392 I print_info: n_embd_k_gqa     = 2048
0.00.050.398 I print_info: n_embd_v_gqa     = 2048
0.00.050.399 I print_info: f_norm_eps       = 1.0e-05
0.00.050.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.400 I print_info: f_logit_scale    = 0.0e+00
0.00.050.401 I print_info: n_ff             = 8192
0.00.050.401 I print_info: n_expert         = 0
0.00.050.401 I print_info: n_expert_used    = 0
0.00.050.401 I print_info: causal attn      = 1
0.00.050.402 I print_info: pooling type     = 0
0.00.050.402 I print_info: rope type        = 2
0.00.050.404 I print_info: rope scaling     = linear
0.00.050.404 I print_info: freq_base_train  = 10000.0
0.00.050.404 I print_info: freq_scale_train = 1
0.00.050.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.405 I print_info: rope_finetuned   = unknown
0.00.050.406 I print_info: ssm_d_conv       = 0
0.00.050.406 I print_info: ssm_d_inner      = 0
0.00.050.406 I print_info: ssm_d_state      = 0
0.00.050.406 I print_info: ssm_dt_rank      = 0
0.00.050.407 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.407 I print_info: model type       = 1.4B
0.00.050.407 I print_info: model params     = 1.41 B
0.00.050.407 I print_info: general.name     = 1.4B
0.00.050.410 I print_info: vocab type       = BPE
0.00.050.410 I print_info: n_vocab          = 50304
0.00.050.411 I print_info: n_merges         = 50009
0.00.050.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.411 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.411 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.411 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.411 I print_info: LF token         = 128 'Ä'
0.00.050.412 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.412 I print_info: max token length = 1024
0.00.052.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.394 I load_tensors: offloading output layer to GPU
0.00.052.394 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.405 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.406 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.690 I llama_init_from_model: n_seq_max     = 1
0.00.052.690 I llama_init_from_model: n_ctx         = 2048
0.00.052.690 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.691 I llama_init_from_model: n_batch       = 2048
0.00.052.691 I llama_init_from_model: n_ubatch      = 512
0.00.052.691 I llama_init_from_model: flash_attn    = 0
0.00.052.691 I llama_init_from_model: freq_base     = 10000.0
0.00.052.692 I llama_init_from_model: freq_scale    = 1
0.00.052.692 I ggml_metal_init: allocating
0.00.052.695 I ggml_metal_init: found device: Apple M4
0.00.052.697 I ggml_metal_init: picking default device: Apple M4
0.00.053.312 I ggml_metal_init: using embedded metal library
0.00.055.670 I ggml_metal_init: GPU name:   Apple M4
0.00.055.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.672 I ggml_metal_init: simdgroup reduction   = true
0.00.055.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.673 I ggml_metal_init: has bfloat            = true
0.00.055.673 I ggml_metal_init: use bfloat            = true
0.00.055.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.472 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.168 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.174 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.193 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.347 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.349 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.349 I llama_init_from_model: graph nodes  = 967
0.00.086.349 I llama_init_from_model: graph splits = 2
0.00.086.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.016 I main: llama threadpool init, n_threads = 4
0.00.682.053 I 
0.00.682.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.075 I 
0.00.682.302 I sampler seed: 1234
0.00.682.308 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.348 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.529.190 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.529.191 I llama_perf_context_print:        load time =     671.72 ms
0.01.529.192 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.61 tokens per second)
0.01.529.192 I llama_perf_context_print:        eval time =     792.26 ms /    63 runs   (   12.58 ms per token,    79.52 tokens per second)
0.01.529.192 I llama_perf_context_print:       total time =     848.03 ms /    70 tokens
0.01.529.376 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.669 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.466 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.467 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.469 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.627 I llama_model_loader: - type  f32:  194 tensors
0.00.024.627 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.628 I print_info: file format = GGUF V3 (latest)
0.00.024.628 I print_info: file type   = Q6_K
0.00.024.629 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.268 I load: special tokens cache size = 25
0.00.050.366 I load: token to piece cache size = 0.2984 MB
0.00.050.369 I print_info: arch             = gptneox
0.00.050.369 I print_info: vocab_only       = 0
0.00.050.369 I print_info: n_ctx_train      = 2048
0.00.050.370 I print_info: n_embd           = 2048
0.00.050.370 I print_info: n_layer          = 24
0.00.050.373 I print_info: n_head           = 16
0.00.050.373 I print_info: n_head_kv        = 16
0.00.050.374 I print_info: n_rot            = 32
0.00.050.374 I print_info: n_swa            = 0
0.00.050.374 I print_info: n_embd_head_k    = 128
0.00.050.374 I print_info: n_embd_head_v    = 128
0.00.050.375 I print_info: n_gqa            = 1
0.00.050.376 I print_info: n_embd_k_gqa     = 2048
0.00.050.376 I print_info: n_embd_v_gqa     = 2048
0.00.050.377 I print_info: f_norm_eps       = 1.0e-05
0.00.050.377 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.377 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.378 I print_info: f_logit_scale    = 0.0e+00
0.00.050.378 I print_info: n_ff             = 8192
0.00.050.379 I print_info: n_expert         = 0
0.00.050.379 I print_info: n_expert_used    = 0
0.00.050.379 I print_info: causal attn      = 1
0.00.050.379 I print_info: pooling type     = 0
0.00.050.379 I print_info: rope type        = 2
0.00.050.379 I print_info: rope scaling     = linear
0.00.050.380 I print_info: freq_base_train  = 10000.0
0.00.050.380 I print_info: freq_scale_train = 1
0.00.050.380 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.381 I print_info: rope_finetuned   = unknown
0.00.050.381 I print_info: ssm_d_conv       = 0
0.00.050.381 I print_info: ssm_d_inner      = 0
0.00.050.381 I print_info: ssm_d_state      = 0
0.00.050.381 I print_info: ssm_dt_rank      = 0
0.00.050.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.382 I print_info: model type       = 1.4B
0.00.050.382 I print_info: model params     = 1.41 B
0.00.050.382 I print_info: general.name     = 1.4B
0.00.050.383 I print_info: vocab type       = BPE
0.00.050.383 I print_info: n_vocab          = 50304
0.00.050.383 I print_info: n_merges         = 50009
0.00.050.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.384 I print_info: LF token         = 128 'Ä'
0.00.050.385 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.385 I print_info: max token length = 1024
0.00.052.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.473 I load_tensors: offloading output layer to GPU
0.00.052.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.484 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.485 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.878 I llama_init_from_model: n_seq_max     = 1
0.00.052.879 I llama_init_from_model: n_ctx         = 2048
0.00.052.879 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.879 I llama_init_from_model: n_batch       = 2048
0.00.052.879 I llama_init_from_model: n_ubatch      = 512
0.00.052.880 I llama_init_from_model: flash_attn    = 0
0.00.052.880 I llama_init_from_model: freq_base     = 10000.0
0.00.052.880 I llama_init_from_model: freq_scale    = 1
0.00.052.881 I ggml_metal_init: allocating
0.00.052.884 I ggml_metal_init: found device: Apple M4
0.00.052.886 I ggml_metal_init: picking default device: Apple M4
0.00.053.488 I ggml_metal_init: using embedded metal library
0.00.055.882 I ggml_metal_init: GPU name:   Apple M4
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.885 I ggml_metal_init: simdgroup reduction   = true
0.00.055.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.885 I ggml_metal_init: has bfloat            = true
0.00.055.885 I ggml_metal_init: use bfloat            = true
0.00.055.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.798 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.687 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.698 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.719 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.676 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.677 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.677 I llama_init_from_model: graph nodes  = 967
0.00.086.678 I llama_init_from_model: graph splits = 2
0.00.086.681 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.329 I main: llama threadpool init, n_threads = 4
0.00.727.374 I 
0.00.727.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.401 I 
0.00.727.622 I sampler seed: 1234
0.00.727.627 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.670 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.674 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.674 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.615.500 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48563.61 tokens per second)
0.01.615.501 I llama_perf_context_print:        load time =     717.75 ms
0.01.615.502 I llama_perf_context_print: prompt eval time =      54.16 ms /     7 tokens (    7.74 ms per token,   129.25 tokens per second)
0.01.615.503 I llama_perf_context_print:        eval time =     831.10 ms /    63 runs   (   13.19 ms per token,    75.80 tokens per second)
0.01.615.503 I llama_perf_context_print:       total time =     889.08 ms /    70 tokens
0.01.615.755 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.111s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.839 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.786 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.468 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.492 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.494 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.862 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.769 I llama_model_loader: - type  f32:  194 tensors
0.00.055.769 I llama_model_loader: - type  f16:   98 tensors
0.00.055.770 I print_info: file format = GGUF V3 (latest)
0.00.055.771 I print_info: file type   = all F32 (guessed)
0.00.055.772 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.687 I load: special tokens cache size = 25
0.00.088.214 I load: token to piece cache size = 0.2984 MB
0.00.088.217 I print_info: arch             = gptneox
0.00.088.217 I print_info: vocab_only       = 0
0.00.088.217 I print_info: n_ctx_train      = 2048
0.00.088.218 I print_info: n_embd           = 2048
0.00.088.218 I print_info: n_layer          = 24
0.00.088.221 I print_info: n_head           = 16
0.00.088.222 I print_info: n_head_kv        = 16
0.00.088.222 I print_info: n_rot            = 32
0.00.088.222 I print_info: n_swa            = 0
0.00.088.222 I print_info: n_embd_head_k    = 128
0.00.088.225 I print_info: n_embd_head_v    = 128
0.00.088.225 I print_info: n_gqa            = 1
0.00.088.226 I print_info: n_embd_k_gqa     = 2048
0.00.088.227 I print_info: n_embd_v_gqa     = 2048
0.00.088.228 I print_info: f_norm_eps       = 1.0e-05
0.00.088.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.228 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.228 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.228 I print_info: f_logit_scale    = 0.0e+00
0.00.088.229 I print_info: n_ff             = 8192
0.00.088.229 I print_info: n_expert         = 0
0.00.088.229 I print_info: n_expert_used    = 0
0.00.088.229 I print_info: causal attn      = 1
0.00.088.229 I print_info: pooling type     = 0
0.00.088.229 I print_info: rope type        = 2
0.00.088.230 I print_info: rope scaling     = linear
0.00.088.230 I print_info: freq_base_train  = 10000.0
0.00.088.230 I print_info: freq_scale_train = 1
0.00.088.230 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.231 I print_info: rope_finetuned   = unknown
0.00.088.231 I print_info: ssm_d_conv       = 0
0.00.088.231 I print_info: ssm_d_inner      = 0
0.00.088.231 I print_info: ssm_d_state      = 0
0.00.088.231 I print_info: ssm_dt_rank      = 0
0.00.088.231 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.231 I print_info: model type       = 1.4B
0.00.088.232 I print_info: model params     = 1.41 B
0.00.088.232 I print_info: general.name     = 1.4B
0.00.088.232 I print_info: vocab type       = BPE
0.00.088.233 I print_info: n_vocab          = 50304
0.00.088.233 I print_info: n_merges         = 50009
0.00.088.233 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.233 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.234 I print_info: LF token         = 128 'Ä'
0.00.088.239 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.242 I print_info: max token length = 1024
0.00.090.707 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.707 I load_tensors: offloading output layer to GPU
0.00.090.707 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.718 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.719 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.004 I llama_init_from_model: n_seq_max     = 1
0.00.091.005 I llama_init_from_model: n_ctx         = 128
0.00.091.005 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.005 I llama_init_from_model: n_batch       = 128
0.00.091.005 I llama_init_from_model: n_ubatch      = 128
0.00.091.005 I llama_init_from_model: flash_attn    = 0
0.00.091.006 I llama_init_from_model: freq_base     = 10000.0
0.00.091.006 I llama_init_from_model: freq_scale    = 1
0.00.091.006 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.007 I ggml_metal_init: allocating
0.00.091.010 I ggml_metal_init: found device: Apple M4
0.00.091.012 I ggml_metal_init: picking default device: Apple M4
0.00.091.615 I ggml_metal_init: using embedded metal library
0.00.094.170 I ggml_metal_init: GPU name:   Apple M4
0.00.094.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.173 I ggml_metal_init: simdgroup reduction   = true
0.00.094.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.173 I ggml_metal_init: has bfloat            = true
0.00.094.173 I ggml_metal_init: use bfloat            = true
0.00.094.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.137 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.384 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.386 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.419 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.344 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.345 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.345 I llama_init_from_model: graph nodes  = 967
0.00.105.345 I llama_init_from_model: graph splits = 2
0.00.105.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.100.177 I 
0.01.100.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.100.289 I perplexity: tokenizing the input ..
0.01.112.196 I perplexity: tokenization took 11.903 ms
0.01.112.220 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.233.483 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.235.446 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.235.475 I llama_perf_context_print:        load time =    1076.38 ms
0.01.235.477 I llama_perf_context_print: prompt eval time =     120.88 ms /   128 tokens (    0.94 ms per token,  1058.88 tokens per second)
0.01.235.478 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.235.479 I llama_perf_context_print:       total time =     135.30 ms /   129 tokens
0.01.236.387 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.121s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.303 I llama_model_loader: - type  f32:  194 tensors
0.00.036.303 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.304 I print_info: file format = GGUF V3 (latest)
0.00.036.306 I print_info: file type   = Q8_0
0.00.036.308 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.888 I load: special tokens cache size = 25
0.00.067.194 I load: token to piece cache size = 0.2984 MB
0.00.067.197 I print_info: arch             = gptneox
0.00.067.198 I print_info: vocab_only       = 0
0.00.067.198 I print_info: n_ctx_train      = 2048
0.00.067.198 I print_info: n_embd           = 2048
0.00.067.198 I print_info: n_layer          = 24
0.00.067.202 I print_info: n_head           = 16
0.00.067.205 I print_info: n_head_kv        = 16
0.00.067.206 I print_info: n_rot            = 32
0.00.067.206 I print_info: n_swa            = 0
0.00.067.206 I print_info: n_embd_head_k    = 128
0.00.067.206 I print_info: n_embd_head_v    = 128
0.00.067.207 I print_info: n_gqa            = 1
0.00.067.212 I print_info: n_embd_k_gqa     = 2048
0.00.067.213 I print_info: n_embd_v_gqa     = 2048
0.00.067.213 I print_info: f_norm_eps       = 1.0e-05
0.00.067.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.214 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.214 I print_info: f_logit_scale    = 0.0e+00
0.00.067.215 I print_info: n_ff             = 8192
0.00.067.215 I print_info: n_expert         = 0
0.00.067.215 I print_info: n_expert_used    = 0
0.00.067.215 I print_info: causal attn      = 1
0.00.067.215 I print_info: pooling type     = 0
0.00.067.215 I print_info: rope type        = 2
0.00.067.216 I print_info: rope scaling     = linear
0.00.067.216 I print_info: freq_base_train  = 10000.0
0.00.067.217 I print_info: freq_scale_train = 1
0.00.067.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.223 I print_info: rope_finetuned   = unknown
0.00.067.224 I print_info: ssm_d_conv       = 0
0.00.067.224 I print_info: ssm_d_inner      = 0
0.00.067.225 I print_info: ssm_d_state      = 0
0.00.067.225 I print_info: ssm_dt_rank      = 0
0.00.067.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.225 I print_info: model type       = 1.4B
0.00.067.226 I print_info: model params     = 1.41 B
0.00.067.226 I print_info: general.name     = 1.4B
0.00.067.226 I print_info: vocab type       = BPE
0.00.067.227 I print_info: n_vocab          = 50304
0.00.067.228 I print_info: n_merges         = 50009
0.00.067.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.228 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.228 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.228 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.229 I print_info: LF token         = 128 'Ä'
0.00.067.229 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.229 I print_info: max token length = 1024
0.00.069.515 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.515 I load_tensors: offloading output layer to GPU
0.00.069.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.526 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.528 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.847 I llama_init_from_model: n_seq_max     = 1
0.00.069.847 I llama_init_from_model: n_ctx         = 128
0.00.069.848 I llama_init_from_model: n_ctx_per_seq = 128
0.00.069.848 I llama_init_from_model: n_batch       = 128
0.00.069.848 I llama_init_from_model: n_ubatch      = 128
0.00.069.848 I llama_init_from_model: flash_attn    = 0
0.00.069.848 I llama_init_from_model: freq_base     = 10000.0
0.00.069.849 I llama_init_from_model: freq_scale    = 1
0.00.069.849 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.850 I ggml_metal_init: allocating
0.00.069.852 I ggml_metal_init: found device: Apple M4
0.00.069.854 I ggml_metal_init: picking default device: Apple M4
0.00.070.490 I ggml_metal_init: using embedded metal library
0.00.073.072 I ggml_metal_init: GPU name:   Apple M4
0.00.073.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.074 I ggml_metal_init: simdgroup reduction   = true
0.00.073.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.075 I ggml_metal_init: has bfloat            = true
0.00.073.075 I ggml_metal_init: use bfloat            = true
0.00.073.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.482 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.936 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.951 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.834 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.834 I llama_init_from_model: graph nodes  = 967
0.00.084.834 I llama_init_from_model: graph splits = 2
0.00.084.836 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.811 I 
0.00.928.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.874 I perplexity: tokenizing the input ..
0.00.937.165 I perplexity: tokenization took 8.289 ms
0.00.937.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.060.989 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.062.329 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.062.343 I llama_perf_context_print:        load time =     917.66 ms
0.01.062.344 I llama_perf_context_print: prompt eval time =     123.59 ms /   128 tokens (    0.97 ms per token,  1035.70 tokens per second)
0.01.062.345 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.062.345 I llama_perf_context_print:       total time =     133.53 ms /   129 tokens
0.01.062.824 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.095s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.084 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.980 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.985 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.987 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.988 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.988 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.989 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.990 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.990 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.991 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.991 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.067 I llama_model_loader: - type  f32:  194 tensors
0.00.026.068 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.068 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.069 I print_info: file format = GGUF V3 (latest)
0.00.026.069 I print_info: file type   = Q4_0
0.00.026.070 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.730 I load: special tokens cache size = 25
0.00.050.671 I load: token to piece cache size = 0.2984 MB
0.00.050.675 I print_info: arch             = gptneox
0.00.050.675 I print_info: vocab_only       = 0
0.00.050.675 I print_info: n_ctx_train      = 2048
0.00.050.676 I print_info: n_embd           = 2048
0.00.050.676 I print_info: n_layer          = 24
0.00.050.678 I print_info: n_head           = 16
0.00.050.679 I print_info: n_head_kv        = 16
0.00.050.679 I print_info: n_rot            = 32
0.00.050.680 I print_info: n_swa            = 0
0.00.050.680 I print_info: n_embd_head_k    = 128
0.00.050.680 I print_info: n_embd_head_v    = 128
0.00.050.681 I print_info: n_gqa            = 1
0.00.050.682 I print_info: n_embd_k_gqa     = 2048
0.00.050.682 I print_info: n_embd_v_gqa     = 2048
0.00.050.683 I print_info: f_norm_eps       = 1.0e-05
0.00.050.683 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.683 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.685 I print_info: f_logit_scale    = 0.0e+00
0.00.050.686 I print_info: n_ff             = 8192
0.00.050.686 I print_info: n_expert         = 0
0.00.050.686 I print_info: n_expert_used    = 0
0.00.050.686 I print_info: causal attn      = 1
0.00.050.686 I print_info: pooling type     = 0
0.00.050.687 I print_info: rope type        = 2
0.00.050.687 I print_info: rope scaling     = linear
0.00.050.687 I print_info: freq_base_train  = 10000.0
0.00.050.688 I print_info: freq_scale_train = 1
0.00.050.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.688 I print_info: rope_finetuned   = unknown
0.00.050.688 I print_info: ssm_d_conv       = 0
0.00.050.689 I print_info: ssm_d_inner      = 0
0.00.050.689 I print_info: ssm_d_state      = 0
0.00.050.689 I print_info: ssm_dt_rank      = 0
0.00.050.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.689 I print_info: model type       = 1.4B
0.00.050.690 I print_info: model params     = 1.41 B
0.00.050.690 I print_info: general.name     = 1.4B
0.00.050.691 I print_info: vocab type       = BPE
0.00.050.691 I print_info: n_vocab          = 50304
0.00.050.691 I print_info: n_merges         = 50009
0.00.050.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: LF token         = 128 'Ä'
0.00.050.692 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: max token length = 1024
0.00.052.664 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.664 I load_tensors: offloading output layer to GPU
0.00.052.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.675 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.676 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.019 I llama_init_from_model: n_seq_max     = 1
0.00.053.020 I llama_init_from_model: n_ctx         = 128
0.00.053.020 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.020 I llama_init_from_model: n_batch       = 128
0.00.053.020 I llama_init_from_model: n_ubatch      = 128
0.00.053.020 I llama_init_from_model: flash_attn    = 0
0.00.053.021 I llama_init_from_model: freq_base     = 10000.0
0.00.053.021 I llama_init_from_model: freq_scale    = 1
0.00.053.021 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.022 I ggml_metal_init: allocating
0.00.053.024 I ggml_metal_init: found device: Apple M4
0.00.053.026 I ggml_metal_init: picking default device: Apple M4
0.00.053.577 I ggml_metal_init: using embedded metal library
0.00.055.925 I ggml_metal_init: GPU name:   Apple M4
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.928 I ggml_metal_init: simdgroup reduction   = true
0.00.055.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.928 I ggml_metal_init: has bfloat            = true
0.00.055.928 I ggml_metal_init: use bfloat            = true
0.00.055.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.641 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.926 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.928 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.893 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.894 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.894 I llama_init_from_model: graph nodes  = 967
0.00.067.895 I llama_init_from_model: graph splits = 2
0.00.067.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.866 I 
0.00.666.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.913 I perplexity: tokenizing the input ..
0.00.674.721 I perplexity: tokenization took 7.806 ms
0.00.674.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.654 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.799 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.798.814 I llama_perf_context_print:        load time =     656.78 ms
0.00.798.815 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.23 tokens per second)
0.00.798.816 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.816 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.799.230 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.269 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.270 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.270 I print_info: file format = GGUF V3 (latest)
0.00.024.271 I print_info: file type   = Q4_1
0.00.024.272 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.847 I load: special tokens cache size = 25
0.00.048.875 I load: token to piece cache size = 0.2984 MB
0.00.048.877 I print_info: arch             = gptneox
0.00.048.877 I print_info: vocab_only       = 0
0.00.048.878 I print_info: n_ctx_train      = 2048
0.00.048.878 I print_info: n_embd           = 2048
0.00.048.878 I print_info: n_layer          = 24
0.00.048.881 I print_info: n_head           = 16
0.00.048.882 I print_info: n_head_kv        = 16
0.00.048.882 I print_info: n_rot            = 32
0.00.048.882 I print_info: n_swa            = 0
0.00.048.882 I print_info: n_embd_head_k    = 128
0.00.048.882 I print_info: n_embd_head_v    = 128
0.00.048.883 I print_info: n_gqa            = 1
0.00.048.884 I print_info: n_embd_k_gqa     = 2048
0.00.048.885 I print_info: n_embd_v_gqa     = 2048
0.00.048.885 I print_info: f_norm_eps       = 1.0e-05
0.00.048.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.886 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.886 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.886 I print_info: f_logit_scale    = 0.0e+00
0.00.048.887 I print_info: n_ff             = 8192
0.00.048.887 I print_info: n_expert         = 0
0.00.048.887 I print_info: n_expert_used    = 0
0.00.048.887 I print_info: causal attn      = 1
0.00.048.887 I print_info: pooling type     = 0
0.00.048.887 I print_info: rope type        = 2
0.00.048.888 I print_info: rope scaling     = linear
0.00.048.890 I print_info: freq_base_train  = 10000.0
0.00.048.892 I print_info: freq_scale_train = 1
0.00.048.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.892 I print_info: rope_finetuned   = unknown
0.00.048.892 I print_info: ssm_d_conv       = 0
0.00.048.893 I print_info: ssm_d_inner      = 0
0.00.048.893 I print_info: ssm_d_state      = 0
0.00.048.893 I print_info: ssm_dt_rank      = 0
0.00.048.893 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.893 I print_info: model type       = 1.4B
0.00.048.893 I print_info: model params     = 1.41 B
0.00.048.894 I print_info: general.name     = 1.4B
0.00.048.894 I print_info: vocab type       = BPE
0.00.048.894 I print_info: n_vocab          = 50304
0.00.048.899 I print_info: n_merges         = 50009
0.00.048.899 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.899 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.899 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.899 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.900 I print_info: LF token         = 128 'Ä'
0.00.048.900 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.900 I print_info: max token length = 1024
0.00.050.827 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.827 I load_tensors: offloading output layer to GPU
0.00.050.827 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.838 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.839 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.119 I llama_init_from_model: n_seq_max     = 1
0.00.051.120 I llama_init_from_model: n_ctx         = 128
0.00.051.120 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.120 I llama_init_from_model: n_batch       = 128
0.00.051.120 I llama_init_from_model: n_ubatch      = 128
0.00.051.121 I llama_init_from_model: flash_attn    = 0
0.00.051.121 I llama_init_from_model: freq_base     = 10000.0
0.00.051.121 I llama_init_from_model: freq_scale    = 1
0.00.051.122 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.122 I ggml_metal_init: allocating
0.00.051.125 I ggml_metal_init: found device: Apple M4
0.00.051.127 I ggml_metal_init: picking default device: Apple M4
0.00.051.724 I ggml_metal_init: using embedded metal library
0.00.054.031 I ggml_metal_init: GPU name:   Apple M4
0.00.054.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.033 I ggml_metal_init: simdgroup reduction   = true
0.00.054.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.034 I ggml_metal_init: has bfloat            = true
0.00.054.034 I ggml_metal_init: use bfloat            = true
0.00.054.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.690 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.931 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.935 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.949 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.870 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.871 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.871 I llama_init_from_model: graph nodes  = 967
0.00.065.871 I llama_init_from_model: graph splits = 2
0.00.065.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.872 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.840 I 
0.00.715.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.954 I perplexity: tokenizing the input ..
0.00.723.978 I perplexity: tokenization took 8.023 ms
0.00.723.990 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.476 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.625 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.641 I llama_perf_context_print:        load time =     706.97 ms
0.00.847.642 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.94 tokens per second)
0.00.847.647 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.647 I llama_perf_context_print:       total time =     131.80 ms /   129 tokens
0.00.848.010 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.077s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.145 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.281 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.330 I llama_model_loader: - type  f32:  194 tensors
0.00.026.331 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.331 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.331 I print_info: file format = GGUF V3 (latest)
0.00.026.332 I print_info: file type   = Q5_0
0.00.026.333 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.591 I load: special tokens cache size = 25
0.00.051.631 I load: token to piece cache size = 0.2984 MB
0.00.051.634 I print_info: arch             = gptneox
0.00.051.634 I print_info: vocab_only       = 0
0.00.051.635 I print_info: n_ctx_train      = 2048
0.00.051.635 I print_info: n_embd           = 2048
0.00.051.635 I print_info: n_layer          = 24
0.00.051.638 I print_info: n_head           = 16
0.00.051.639 I print_info: n_head_kv        = 16
0.00.051.639 I print_info: n_rot            = 32
0.00.051.639 I print_info: n_swa            = 0
0.00.051.639 I print_info: n_embd_head_k    = 128
0.00.051.640 I print_info: n_embd_head_v    = 128
0.00.051.640 I print_info: n_gqa            = 1
0.00.051.641 I print_info: n_embd_k_gqa     = 2048
0.00.051.642 I print_info: n_embd_v_gqa     = 2048
0.00.051.642 I print_info: f_norm_eps       = 1.0e-05
0.00.051.643 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.643 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.643 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.643 I print_info: f_logit_scale    = 0.0e+00
0.00.051.644 I print_info: n_ff             = 8192
0.00.051.644 I print_info: n_expert         = 0
0.00.051.644 I print_info: n_expert_used    = 0
0.00.051.644 I print_info: causal attn      = 1
0.00.051.645 I print_info: pooling type     = 0
0.00.051.645 I print_info: rope type        = 2
0.00.051.645 I print_info: rope scaling     = linear
0.00.051.647 I print_info: freq_base_train  = 10000.0
0.00.051.649 I print_info: freq_scale_train = 1
0.00.051.649 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.649 I print_info: rope_finetuned   = unknown
0.00.051.650 I print_info: ssm_d_conv       = 0
0.00.051.650 I print_info: ssm_d_inner      = 0
0.00.051.650 I print_info: ssm_d_state      = 0
0.00.051.650 I print_info: ssm_dt_rank      = 0
0.00.051.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.650 I print_info: model type       = 1.4B
0.00.051.651 I print_info: model params     = 1.41 B
0.00.051.651 I print_info: general.name     = 1.4B
0.00.051.652 I print_info: vocab type       = BPE
0.00.051.653 I print_info: n_vocab          = 50304
0.00.051.653 I print_info: n_merges         = 50009
0.00.051.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: LF token         = 128 'Ä'
0.00.051.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.658 I print_info: max token length = 1024
0.00.053.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.478 I load_tensors: offloading output layer to GPU
0.00.053.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.483 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.484 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.850 I llama_init_from_model: n_seq_max     = 1
0.00.053.850 I llama_init_from_model: n_ctx         = 128
0.00.053.851 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.851 I llama_init_from_model: n_batch       = 128
0.00.053.851 I llama_init_from_model: n_ubatch      = 128
0.00.053.851 I llama_init_from_model: flash_attn    = 0
0.00.053.851 I llama_init_from_model: freq_base     = 10000.0
0.00.053.852 I llama_init_from_model: freq_scale    = 1
0.00.053.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.852 I ggml_metal_init: allocating
0.00.053.855 I ggml_metal_init: found device: Apple M4
0.00.053.857 I ggml_metal_init: picking default device: Apple M4
0.00.054.423 I ggml_metal_init: using embedded metal library
0.00.056.742 I ggml_metal_init: GPU name:   Apple M4
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.745 I ggml_metal_init: simdgroup reduction   = true
0.00.056.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.745 I ggml_metal_init: has bfloat            = true
0.00.056.745 I ggml_metal_init: use bfloat            = true
0.00.056.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.543 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.547 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.566 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.462 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.464 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.464 I llama_init_from_model: graph nodes  = 967
0.00.068.464 I llama_init_from_model: graph splits = 2
0.00.068.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.915 I 
0.00.718.948 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.960 I perplexity: tokenizing the input ..
0.00.726.923 I perplexity: tokenization took 7.962 ms
0.00.726.934 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.109 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.863.268 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.863.284 I llama_perf_context_print:        load time =     708.77 ms
0.00.863.285 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.61 tokens per second)
0.00.863.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.287 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.863.700 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.078s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.786 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.787 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.788 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.788 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.788 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.789 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.790 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.791 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.791 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.791 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.075 I llama_model_loader: - type  f32:  194 tensors
0.00.025.075 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.076 I print_info: file format = GGUF V3 (latest)
0.00.025.076 I print_info: file type   = Q5_1
0.00.025.077 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.326 I load: special tokens cache size = 25
0.00.050.467 I load: token to piece cache size = 0.2984 MB
0.00.050.471 I print_info: arch             = gptneox
0.00.050.471 I print_info: vocab_only       = 0
0.00.050.471 I print_info: n_ctx_train      = 2048
0.00.050.471 I print_info: n_embd           = 2048
0.00.050.472 I print_info: n_layer          = 24
0.00.050.475 I print_info: n_head           = 16
0.00.050.476 I print_info: n_head_kv        = 16
0.00.050.476 I print_info: n_rot            = 32
0.00.050.476 I print_info: n_swa            = 0
0.00.050.476 I print_info: n_embd_head_k    = 128
0.00.050.476 I print_info: n_embd_head_v    = 128
0.00.050.478 I print_info: n_gqa            = 1
0.00.050.478 I print_info: n_embd_k_gqa     = 2048
0.00.050.479 I print_info: n_embd_v_gqa     = 2048
0.00.050.480 I print_info: f_norm_eps       = 1.0e-05
0.00.050.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.480 I print_info: f_logit_scale    = 0.0e+00
0.00.050.481 I print_info: n_ff             = 8192
0.00.050.481 I print_info: n_expert         = 0
0.00.050.481 I print_info: n_expert_used    = 0
0.00.050.482 I print_info: causal attn      = 1
0.00.050.482 I print_info: pooling type     = 0
0.00.050.482 I print_info: rope type        = 2
0.00.050.482 I print_info: rope scaling     = linear
0.00.050.484 I print_info: freq_base_train  = 10000.0
0.00.050.485 I print_info: freq_scale_train = 1
0.00.050.485 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.485 I print_info: rope_finetuned   = unknown
0.00.050.485 I print_info: ssm_d_conv       = 0
0.00.050.486 I print_info: ssm_d_inner      = 0
0.00.050.486 I print_info: ssm_d_state      = 0
0.00.050.486 I print_info: ssm_dt_rank      = 0
0.00.050.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.486 I print_info: model type       = 1.4B
0.00.050.486 I print_info: model params     = 1.41 B
0.00.050.487 I print_info: general.name     = 1.4B
0.00.050.491 I print_info: vocab type       = BPE
0.00.050.491 I print_info: n_vocab          = 50304
0.00.050.492 I print_info: n_merges         = 50009
0.00.050.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.493 I print_info: LF token         = 128 'Ä'
0.00.050.493 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.493 I print_info: max token length = 1024
0.00.052.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.473 I load_tensors: offloading output layer to GPU
0.00.052.473 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.483 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.484 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.766 I llama_init_from_model: n_seq_max     = 1
0.00.052.767 I llama_init_from_model: n_ctx         = 128
0.00.052.767 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.767 I llama_init_from_model: n_batch       = 128
0.00.052.767 I llama_init_from_model: n_ubatch      = 128
0.00.052.767 I llama_init_from_model: flash_attn    = 0
0.00.052.768 I llama_init_from_model: freq_base     = 10000.0
0.00.052.768 I llama_init_from_model: freq_scale    = 1
0.00.052.768 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.769 I ggml_metal_init: allocating
0.00.052.771 I ggml_metal_init: found device: Apple M4
0.00.052.773 I ggml_metal_init: picking default device: Apple M4
0.00.053.352 I ggml_metal_init: using embedded metal library
0.00.055.701 I ggml_metal_init: GPU name:   Apple M4
0.00.055.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.703 I ggml_metal_init: simdgroup reduction   = true
0.00.055.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.703 I ggml_metal_init: has bfloat            = true
0.00.055.703 I ggml_metal_init: use bfloat            = true
0.00.055.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.509 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.786 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.789 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.802 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.659 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.660 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.660 I llama_init_from_model: graph nodes  = 967
0.00.067.660 I llama_init_from_model: graph splits = 2
0.00.067.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.378 I 
0.00.683.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.473 I perplexity: tokenizing the input ..
0.00.691.566 I perplexity: tokenization took 8.093 ms
0.00.691.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.404 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.827.712 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.827.729 I llama_perf_context_print:        load time =     674.54 ms
0.00.827.730 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.18 tokens per second)
0.00.827.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.731 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.828.225 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.078s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.534 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.256 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.271 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.272 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.274 I llama_model_loader: - type  f32:  194 tensors
0.00.026.275 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.275 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.275 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.276 I print_info: file format = GGUF V3 (latest)
0.00.026.276 I print_info: file type   = Q2_K - Medium
0.00.026.277 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.737 I load: special tokens cache size = 25
0.00.050.982 I load: token to piece cache size = 0.2984 MB
0.00.050.984 I print_info: arch             = gptneox
0.00.050.985 I print_info: vocab_only       = 0
0.00.050.985 I print_info: n_ctx_train      = 2048
0.00.050.985 I print_info: n_embd           = 2048
0.00.050.985 I print_info: n_layer          = 24
0.00.050.988 I print_info: n_head           = 16
0.00.050.989 I print_info: n_head_kv        = 16
0.00.050.989 I print_info: n_rot            = 32
0.00.050.990 I print_info: n_swa            = 0
0.00.050.990 I print_info: n_embd_head_k    = 128
0.00.050.991 I print_info: n_embd_head_v    = 128
0.00.050.991 I print_info: n_gqa            = 1
0.00.050.992 I print_info: n_embd_k_gqa     = 2048
0.00.050.993 I print_info: n_embd_v_gqa     = 2048
0.00.050.993 I print_info: f_norm_eps       = 1.0e-05
0.00.050.994 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.994 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.994 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.996 I print_info: f_logit_scale    = 0.0e+00
0.00.050.997 I print_info: n_ff             = 8192
0.00.050.997 I print_info: n_expert         = 0
0.00.050.997 I print_info: n_expert_used    = 0
0.00.050.997 I print_info: causal attn      = 1
0.00.050.997 I print_info: pooling type     = 0
0.00.050.997 I print_info: rope type        = 2
0.00.051.004 I print_info: rope scaling     = linear
0.00.051.007 I print_info: freq_base_train  = 10000.0
0.00.051.007 I print_info: freq_scale_train = 1
0.00.051.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.008 I print_info: rope_finetuned   = unknown
0.00.051.008 I print_info: ssm_d_conv       = 0
0.00.051.008 I print_info: ssm_d_inner      = 0
0.00.051.009 I print_info: ssm_d_state      = 0
0.00.051.009 I print_info: ssm_dt_rank      = 0
0.00.051.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.010 I print_info: model type       = 1.4B
0.00.051.010 I print_info: model params     = 1.41 B
0.00.051.010 I print_info: general.name     = 1.4B
0.00.051.011 I print_info: vocab type       = BPE
0.00.051.011 I print_info: n_vocab          = 50304
0.00.051.011 I print_info: n_merges         = 50009
0.00.051.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.012 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.012 I print_info: LF token         = 128 'Ä'
0.00.051.012 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.013 I print_info: max token length = 1024
0.00.052.746 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.746 I load_tensors: offloading output layer to GPU
0.00.052.746 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.752 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.752 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.022 I llama_init_from_model: n_seq_max     = 1
0.00.053.022 I llama_init_from_model: n_ctx         = 128
0.00.053.023 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.023 I llama_init_from_model: n_batch       = 128
0.00.053.023 I llama_init_from_model: n_ubatch      = 128
0.00.053.023 I llama_init_from_model: flash_attn    = 0
0.00.053.023 I llama_init_from_model: freq_base     = 10000.0
0.00.053.024 I llama_init_from_model: freq_scale    = 1
0.00.053.024 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.024 I ggml_metal_init: allocating
0.00.053.028 I ggml_metal_init: found device: Apple M4
0.00.053.030 I ggml_metal_init: picking default device: Apple M4
0.00.053.586 I ggml_metal_init: using embedded metal library
0.00.055.904 I ggml_metal_init: GPU name:   Apple M4
0.00.055.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.906 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.906 I ggml_metal_init: simdgroup reduction   = true
0.00.055.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.906 I ggml_metal_init: has bfloat            = true
0.00.055.906 I ggml_metal_init: use bfloat            = true
0.00.055.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.489 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.701 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.724 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.632 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.633 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.633 I llama_init_from_model: graph nodes  = 967
0.00.067.634 I llama_init_from_model: graph splits = 2
0.00.067.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.377.122 I 
0.00.377.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.377.165 I perplexity: tokenizing the input ..
0.00.384.694 I perplexity: tokenization took 7.527 ms
0.00.384.710 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.516.974 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.518.128 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.518.146 I llama_perf_context_print:        load time =     366.58 ms
0.00.518.147 I llama_perf_context_print: prompt eval time =     132.04 ms /   128 tokens (    1.03 ms per token,   969.43 tokens per second)
0.00.518.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.518.149 I llama_perf_context_print:       total time =     141.02 ms /   129 tokens
0.00.518.583 I ggml_metal_free: deallocating

real	0m0.535s
user	0m0.077s
sys	0m0.063s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.445 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.240 I llama_model_loader: - type  f32:  194 tensors
0.00.025.240 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.241 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.241 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.242 I print_info: file format = GGUF V3 (latest)
0.00.025.242 I print_info: file type   = Q3_K - Medium
0.00.025.243 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.797 I load: special tokens cache size = 25
0.00.049.755 I load: token to piece cache size = 0.2984 MB
0.00.049.757 I print_info: arch             = gptneox
0.00.049.758 I print_info: vocab_only       = 0
0.00.049.758 I print_info: n_ctx_train      = 2048
0.00.049.758 I print_info: n_embd           = 2048
0.00.049.758 I print_info: n_layer          = 24
0.00.049.761 I print_info: n_head           = 16
0.00.049.762 I print_info: n_head_kv        = 16
0.00.049.762 I print_info: n_rot            = 32
0.00.049.762 I print_info: n_swa            = 0
0.00.049.762 I print_info: n_embd_head_k    = 128
0.00.049.762 I print_info: n_embd_head_v    = 128
0.00.049.763 I print_info: n_gqa            = 1
0.00.049.764 I print_info: n_embd_k_gqa     = 2048
0.00.049.764 I print_info: n_embd_v_gqa     = 2048
0.00.049.765 I print_info: f_norm_eps       = 1.0e-05
0.00.049.765 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.765 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.766 I print_info: f_logit_scale    = 0.0e+00
0.00.049.766 I print_info: n_ff             = 8192
0.00.049.767 I print_info: n_expert         = 0
0.00.049.767 I print_info: n_expert_used    = 0
0.00.049.767 I print_info: causal attn      = 1
0.00.049.767 I print_info: pooling type     = 0
0.00.049.767 I print_info: rope type        = 2
0.00.049.767 I print_info: rope scaling     = linear
0.00.049.768 I print_info: freq_base_train  = 10000.0
0.00.049.768 I print_info: freq_scale_train = 1
0.00.049.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.768 I print_info: rope_finetuned   = unknown
0.00.049.769 I print_info: ssm_d_conv       = 0
0.00.049.769 I print_info: ssm_d_inner      = 0
0.00.049.769 I print_info: ssm_d_state      = 0
0.00.049.769 I print_info: ssm_dt_rank      = 0
0.00.049.769 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.770 I print_info: model type       = 1.4B
0.00.049.770 I print_info: model params     = 1.41 B
0.00.049.770 I print_info: general.name     = 1.4B
0.00.049.771 I print_info: vocab type       = BPE
0.00.049.771 I print_info: n_vocab          = 50304
0.00.049.771 I print_info: n_merges         = 50009
0.00.049.771 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.771 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.772 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.772 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.772 I print_info: LF token         = 128 'Ä'
0.00.049.773 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.773 I print_info: max token length = 1024
0.00.051.665 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.666 I load_tensors: offloading output layer to GPU
0.00.051.666 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.676 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.677 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.961 I llama_init_from_model: n_seq_max     = 1
0.00.051.962 I llama_init_from_model: n_ctx         = 128
0.00.051.962 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.962 I llama_init_from_model: n_batch       = 128
0.00.051.962 I llama_init_from_model: n_ubatch      = 128
0.00.051.962 I llama_init_from_model: flash_attn    = 0
0.00.051.962 I llama_init_from_model: freq_base     = 10000.0
0.00.051.963 I llama_init_from_model: freq_scale    = 1
0.00.051.963 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.964 I ggml_metal_init: allocating
0.00.051.966 I ggml_metal_init: found device: Apple M4
0.00.051.968 I ggml_metal_init: picking default device: Apple M4
0.00.052.537 I ggml_metal_init: using embedded metal library
0.00.054.843 I ggml_metal_init: GPU name:   Apple M4
0.00.054.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.846 I ggml_metal_init: simdgroup reduction   = true
0.00.054.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.846 I ggml_metal_init: has bfloat            = true
0.00.054.846 I ggml_metal_init: use bfloat            = true
0.00.054.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.437 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.800 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.805 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.820 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.708 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.709 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.709 I llama_init_from_model: graph nodes  = 967
0.00.066.709 I llama_init_from_model: graph splits = 2
0.00.066.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.451 I 
0.00.532.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.498 I perplexity: tokenizing the input ..
0.00.540.462 I perplexity: tokenization took 7.962 ms
0.00.540.473 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.672.733 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.673.970 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.673.988 I llama_perf_context_print:        load time =     522.71 ms
0.00.673.989 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.45 tokens per second)
0.00.673.990 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.673.990 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.674.486 I ggml_metal_free: deallocating

real	0m0.690s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.137 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.123 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.125 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.126 I llama_model_loader: - type  f32:  194 tensors
0.00.025.126 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.126 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.127 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.127 I print_info: file format = GGUF V3 (latest)
0.00.025.127 I print_info: file type   = Q4_K - Medium
0.00.025.128 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.614 I load: special tokens cache size = 25
0.00.050.701 I load: token to piece cache size = 0.2984 MB
0.00.050.703 I print_info: arch             = gptneox
0.00.050.704 I print_info: vocab_only       = 0
0.00.050.704 I print_info: n_ctx_train      = 2048
0.00.050.704 I print_info: n_embd           = 2048
0.00.050.704 I print_info: n_layer          = 24
0.00.050.708 I print_info: n_head           = 16
0.00.050.708 I print_info: n_head_kv        = 16
0.00.050.709 I print_info: n_rot            = 32
0.00.050.709 I print_info: n_swa            = 0
0.00.050.709 I print_info: n_embd_head_k    = 128
0.00.050.709 I print_info: n_embd_head_v    = 128
0.00.050.710 I print_info: n_gqa            = 1
0.00.050.711 I print_info: n_embd_k_gqa     = 2048
0.00.050.712 I print_info: n_embd_v_gqa     = 2048
0.00.050.713 I print_info: f_norm_eps       = 1.0e-05
0.00.050.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.718 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.718 I print_info: f_logit_scale    = 0.0e+00
0.00.050.719 I print_info: n_ff             = 8192
0.00.050.719 I print_info: n_expert         = 0
0.00.050.721 I print_info: n_expert_used    = 0
0.00.050.721 I print_info: causal attn      = 1
0.00.050.721 I print_info: pooling type     = 0
0.00.050.721 I print_info: rope type        = 2
0.00.050.723 I print_info: rope scaling     = linear
0.00.050.726 I print_info: freq_base_train  = 10000.0
0.00.050.726 I print_info: freq_scale_train = 1
0.00.050.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.727 I print_info: rope_finetuned   = unknown
0.00.050.727 I print_info: ssm_d_conv       = 0
0.00.050.727 I print_info: ssm_d_inner      = 0
0.00.050.728 I print_info: ssm_d_state      = 0
0.00.050.728 I print_info: ssm_dt_rank      = 0
0.00.050.728 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.729 I print_info: model type       = 1.4B
0.00.050.729 I print_info: model params     = 1.41 B
0.00.050.729 I print_info: general.name     = 1.4B
0.00.050.730 I print_info: vocab type       = BPE
0.00.050.730 I print_info: n_vocab          = 50304
0.00.050.730 I print_info: n_merges         = 50009
0.00.050.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.730 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.730 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: LF token         = 128 'Ä'
0.00.050.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: max token length = 1024
0.00.052.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.789 I load_tensors: offloading output layer to GPU
0.00.052.790 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.800 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.802 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.164 I llama_init_from_model: n_seq_max     = 1
0.00.053.164 I llama_init_from_model: n_ctx         = 128
0.00.053.165 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.165 I llama_init_from_model: n_batch       = 128
0.00.053.165 I llama_init_from_model: n_ubatch      = 128
0.00.053.165 I llama_init_from_model: flash_attn    = 0
0.00.053.165 I llama_init_from_model: freq_base     = 10000.0
0.00.053.166 I llama_init_from_model: freq_scale    = 1
0.00.053.166 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.166 I ggml_metal_init: allocating
0.00.053.169 I ggml_metal_init: found device: Apple M4
0.00.053.171 I ggml_metal_init: picking default device: Apple M4
0.00.053.760 I ggml_metal_init: using embedded metal library
0.00.056.144 I ggml_metal_init: GPU name:   Apple M4
0.00.056.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.146 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.146 I ggml_metal_init: simdgroup reduction   = true
0.00.056.146 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.147 I ggml_metal_init: has bfloat            = true
0.00.056.147 I ggml_metal_init: use bfloat            = true
0.00.056.147 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.148 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.053 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.373 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.376 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.389 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.354 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.355 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.355 I llama_init_from_model: graph nodes  = 967
0.00.068.355 I llama_init_from_model: graph splits = 2
0.00.068.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.386 I 
0.00.580.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.452 I perplexity: tokenizing the input ..
0.00.588.617 I perplexity: tokenization took 8.164 ms
0.00.588.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.722.505 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.723.689 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.723.706 I llama_perf_context_print:        load time =     571.25 ms
0.00.723.707 I llama_perf_context_print: prompt eval time =     133.65 ms /   128 tokens (    1.04 ms per token,   957.74 tokens per second)
0.00.723.707 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.708 I llama_perf_context_print:       total time =     143.32 ms /   129 tokens
0.00.724.104 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.079s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.132 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.527 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.527 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.528 I print_info: file format = GGUF V3 (latest)
0.00.025.528 I print_info: file type   = Q5_K - Medium
0.00.025.529 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.055 I load: special tokens cache size = 25
0.00.050.126 I load: token to piece cache size = 0.2984 MB
0.00.050.129 I print_info: arch             = gptneox
0.00.050.129 I print_info: vocab_only       = 0
0.00.050.129 I print_info: n_ctx_train      = 2048
0.00.050.129 I print_info: n_embd           = 2048
0.00.050.130 I print_info: n_layer          = 24
0.00.050.133 I print_info: n_head           = 16
0.00.050.133 I print_info: n_head_kv        = 16
0.00.050.134 I print_info: n_rot            = 32
0.00.050.134 I print_info: n_swa            = 0
0.00.050.134 I print_info: n_embd_head_k    = 128
0.00.050.134 I print_info: n_embd_head_v    = 128
0.00.050.135 I print_info: n_gqa            = 1
0.00.050.136 I print_info: n_embd_k_gqa     = 2048
0.00.050.137 I print_info: n_embd_v_gqa     = 2048
0.00.050.138 I print_info: f_norm_eps       = 1.0e-05
0.00.050.138 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.138 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.145 I print_info: f_logit_scale    = 0.0e+00
0.00.050.146 I print_info: n_ff             = 8192
0.00.050.146 I print_info: n_expert         = 0
0.00.050.146 I print_info: n_expert_used    = 0
0.00.050.146 I print_info: causal attn      = 1
0.00.050.147 I print_info: pooling type     = 0
0.00.050.147 I print_info: rope type        = 2
0.00.050.147 I print_info: rope scaling     = linear
0.00.050.147 I print_info: freq_base_train  = 10000.0
0.00.050.148 I print_info: freq_scale_train = 1
0.00.050.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.148 I print_info: rope_finetuned   = unknown
0.00.050.149 I print_info: ssm_d_conv       = 0
0.00.050.149 I print_info: ssm_d_inner      = 0
0.00.050.149 I print_info: ssm_d_state      = 0
0.00.050.149 I print_info: ssm_dt_rank      = 0
0.00.050.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.149 I print_info: model type       = 1.4B
0.00.050.150 I print_info: model params     = 1.41 B
0.00.050.150 I print_info: general.name     = 1.4B
0.00.050.151 I print_info: vocab type       = BPE
0.00.050.151 I print_info: n_vocab          = 50304
0.00.050.151 I print_info: n_merges         = 50009
0.00.050.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: LF token         = 128 'Ä'
0.00.050.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: max token length = 1024
0.00.052.136 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.136 I load_tensors: offloading output layer to GPU
0.00.052.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.147 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.148 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.435 I llama_init_from_model: n_seq_max     = 1
0.00.052.435 I llama_init_from_model: n_ctx         = 128
0.00.052.436 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.436 I llama_init_from_model: n_batch       = 128
0.00.052.436 I llama_init_from_model: n_ubatch      = 128
0.00.052.436 I llama_init_from_model: flash_attn    = 0
0.00.052.436 I llama_init_from_model: freq_base     = 10000.0
0.00.052.437 I llama_init_from_model: freq_scale    = 1
0.00.052.437 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.437 I ggml_metal_init: allocating
0.00.052.444 I ggml_metal_init: found device: Apple M4
0.00.052.446 I ggml_metal_init: picking default device: Apple M4
0.00.053.027 I ggml_metal_init: using embedded metal library
0.00.055.337 I ggml_metal_init: GPU name:   Apple M4
0.00.055.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.340 I ggml_metal_init: simdgroup reduction   = true
0.00.055.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.340 I ggml_metal_init: has bfloat            = true
0.00.055.340 I ggml_metal_init: use bfloat            = true
0.00.055.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.910 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.157 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.171 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.095 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.096 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.096 I llama_init_from_model: graph nodes  = 967
0.00.067.096 I llama_init_from_model: graph splits = 2
0.00.067.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.023 I 
0.00.616.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.074 I perplexity: tokenizing the input ..
0.00.623.344 I perplexity: tokenization took 7.268 ms
0.00.623.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.363 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.765.632 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.765.650 I llama_perf_context_print:        load time =     605.89 ms
0.00.765.651 I llama_perf_context_print: prompt eval time =     140.78 ms /   128 tokens (    1.10 ms per token,   909.21 tokens per second)
0.00.765.652 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.652 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.766.180 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.076s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.734 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.735 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.735 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.736 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.643 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.643 I llama_model_loader: - type  f32:  194 tensors
0.00.025.644 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.644 I print_info: file format = GGUF V3 (latest)
0.00.025.645 I print_info: file type   = Q6_K
0.00.025.646 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.256 I load: special tokens cache size = 25
0.00.050.445 I load: token to piece cache size = 0.2984 MB
0.00.050.448 I print_info: arch             = gptneox
0.00.050.448 I print_info: vocab_only       = 0
0.00.050.449 I print_info: n_ctx_train      = 2048
0.00.050.449 I print_info: n_embd           = 2048
0.00.050.449 I print_info: n_layer          = 24
0.00.050.452 I print_info: n_head           = 16
0.00.050.453 I print_info: n_head_kv        = 16
0.00.050.453 I print_info: n_rot            = 32
0.00.050.453 I print_info: n_swa            = 0
0.00.050.454 I print_info: n_embd_head_k    = 128
0.00.050.454 I print_info: n_embd_head_v    = 128
0.00.050.454 I print_info: n_gqa            = 1
0.00.050.455 I print_info: n_embd_k_gqa     = 2048
0.00.050.456 I print_info: n_embd_v_gqa     = 2048
0.00.050.456 I print_info: f_norm_eps       = 1.0e-05
0.00.050.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.457 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.457 I print_info: f_logit_scale    = 0.0e+00
0.00.050.458 I print_info: n_ff             = 8192
0.00.050.458 I print_info: n_expert         = 0
0.00.050.458 I print_info: n_expert_used    = 0
0.00.050.458 I print_info: causal attn      = 1
0.00.050.458 I print_info: pooling type     = 0
0.00.050.458 I print_info: rope type        = 2
0.00.050.459 I print_info: rope scaling     = linear
0.00.050.459 I print_info: freq_base_train  = 10000.0
0.00.050.459 I print_info: freq_scale_train = 1
0.00.050.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.460 I print_info: rope_finetuned   = unknown
0.00.050.460 I print_info: ssm_d_conv       = 0
0.00.050.460 I print_info: ssm_d_inner      = 0
0.00.050.460 I print_info: ssm_d_state      = 0
0.00.050.460 I print_info: ssm_dt_rank      = 0
0.00.050.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.463 I print_info: model type       = 1.4B
0.00.050.463 I print_info: model params     = 1.41 B
0.00.050.463 I print_info: general.name     = 1.4B
0.00.050.464 I print_info: vocab type       = BPE
0.00.050.464 I print_info: n_vocab          = 50304
0.00.050.464 I print_info: n_merges         = 50009
0.00.050.465 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: LF token         = 128 'Ä'
0.00.050.466 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.470 I print_info: max token length = 1024
0.00.052.457 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.457 I load_tensors: offloading output layer to GPU
0.00.052.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.468 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.469 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.741 I llama_init_from_model: n_seq_max     = 1
0.00.052.742 I llama_init_from_model: n_ctx         = 128
0.00.052.742 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.742 I llama_init_from_model: n_batch       = 128
0.00.052.742 I llama_init_from_model: n_ubatch      = 128
0.00.052.743 I llama_init_from_model: flash_attn    = 0
0.00.052.743 I llama_init_from_model: freq_base     = 10000.0
0.00.052.743 I llama_init_from_model: freq_scale    = 1
0.00.052.744 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.744 I ggml_metal_init: allocating
0.00.052.747 I ggml_metal_init: found device: Apple M4
0.00.052.749 I ggml_metal_init: picking default device: Apple M4
0.00.053.305 I ggml_metal_init: using embedded metal library
0.00.055.628 I ggml_metal_init: GPU name:   Apple M4
0.00.055.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.630 I ggml_metal_init: simdgroup reduction   = true
0.00.055.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.631 I ggml_metal_init: has bfloat            = true
0.00.055.631 I ggml_metal_init: use bfloat            = true
0.00.055.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.235 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.511 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.513 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.423 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.424 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.424 I llama_init_from_model: graph nodes  = 967
0.00.067.425 I llama_init_from_model: graph splits = 2
0.00.067.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.636 I 
0.00.397.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.397.695 I perplexity: tokenizing the input ..
0.00.405.161 I perplexity: tokenization took 7.463 ms
0.00.405.173 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.221 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.545.637 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.545.650 I llama_perf_context_print:        load time =     387.48 ms
0.00.545.651 I llama_perf_context_print: prompt eval time =     138.82 ms /   128 tokens (    1.08 ms per token,   922.03 tokens per second)
0.00.545.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.545.652 I llama_perf_context_print:       total time =     148.02 ms /   129 tokens
0.00.546.012 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.186 I build: 4537 (f7fb43cd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.038 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.875 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.888 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.889 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.889 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.890 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.890 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.892 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.893 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.248 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.250 I llama_model_loader: - type  f32:  194 tensors
0.00.035.250 I llama_model_loader: - type  f16:   98 tensors
0.00.035.251 I print_info: file format = GGUF V3 (latest)
0.00.035.251 I print_info: file type   = all F32 (guessed)
0.00.035.253 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.055.812 I load: special tokens cache size = 25
0.00.061.666 I load: token to piece cache size = 0.2984 MB
0.00.061.671 I print_info: arch             = gptneox
0.00.061.671 I print_info: vocab_only       = 0
0.00.061.671 I print_info: n_ctx_train      = 2048
0.00.061.671 I print_info: n_embd           = 2048
0.00.061.672 I print_info: n_layer          = 24
0.00.061.675 I print_info: n_head           = 16
0.00.061.676 I print_info: n_head_kv        = 16
0.00.061.676 I print_info: n_rot            = 32
0.00.061.676 I print_info: n_swa            = 0
0.00.061.679 I print_info: n_embd_head_k    = 128
0.00.061.679 I print_info: n_embd_head_v    = 128
0.00.061.680 I print_info: n_gqa            = 1
0.00.061.680 I print_info: n_embd_k_gqa     = 2048
0.00.061.681 I print_info: n_embd_v_gqa     = 2048
0.00.061.682 I print_info: f_norm_eps       = 1.0e-05
0.00.061.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.688 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.689 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.689 I print_info: f_logit_scale    = 0.0e+00
0.00.061.690 I print_info: n_ff             = 8192
0.00.061.690 I print_info: n_expert         = 0
0.00.061.690 I print_info: n_expert_used    = 0
0.00.061.690 I print_info: causal attn      = 1
0.00.061.690 I print_info: pooling type     = 0
0.00.061.690 I print_info: rope type        = 2
0.00.061.690 I print_info: rope scaling     = linear
0.00.061.691 I print_info: freq_base_train  = 10000.0
0.00.061.692 I print_info: freq_scale_train = 1
0.00.061.692 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.692 I print_info: rope_finetuned   = unknown
0.00.061.692 I print_info: ssm_d_conv       = 0
0.00.061.692 I print_info: ssm_d_inner      = 0
0.00.061.692 I print_info: ssm_d_state      = 0
0.00.061.692 I print_info: ssm_dt_rank      = 0
0.00.061.693 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.693 I print_info: model type       = 1.4B
0.00.061.693 I print_info: model params     = 1.41 B
0.00.061.693 I print_info: general.name     = 1.4B
0.00.061.694 I print_info: vocab type       = BPE
0.00.061.694 I print_info: n_vocab          = 50304
0.00.061.694 I print_info: n_merges         = 50009
0.00.061.696 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.696 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.697 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.697 I print_info: LF token         = 128 'Ä'
0.00.061.697 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.697 I print_info: max token length = 1024
0.00.064.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.214 I load_tensors: offloading output layer to GPU
0.00.064.215 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.226 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.064.227 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.064.537 I llama_init_from_model: n_seq_max     = 1
0.00.064.538 I llama_init_from_model: n_ctx         = 128
0.00.064.538 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.538 I llama_init_from_model: n_batch       = 128
0.00.064.538 I llama_init_from_model: n_ubatch      = 128
0.00.064.539 I llama_init_from_model: flash_attn    = 0
0.00.064.539 I llama_init_from_model: freq_base     = 10000.0
0.00.064.539 I llama_init_from_model: freq_scale    = 1
0.00.064.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.540 I ggml_metal_init: allocating
0.00.064.544 I ggml_metal_init: found device: Apple M4
0.00.064.546 I ggml_metal_init: picking default device: Apple M4
0.00.065.186 I ggml_metal_init: using embedded metal library
0.00.067.572 I ggml_metal_init: GPU name:   Apple M4
0.00.067.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.575 I ggml_metal_init: simdgroup reduction   = true
0.00.067.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.575 I ggml_metal_init: has bfloat            = true
0.00.067.575 I ggml_metal_init: use bfloat            = true
0.00.067.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.812 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.133 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.135 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.153 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.089 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.089 I llama_init_from_model: graph nodes  = 967
0.00.080.089 I llama_init_from_model: graph splits = 2
0.00.080.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.091 I 
0.00.080.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.080.130 I compute_imatrix: tokenizing the input ..
0.00.087.654 I compute_imatrix: tokenization took 7.522 ms
0.00.087.657 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.687.439 I compute_imatrix: 1.60 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.690.020 I llama_perf_context_print:        load time =    1671.40 ms
0.01.690.021 I llama_perf_context_print: prompt eval time =    1599.13 ms /   128 tokens (   12.49 ms per token,    80.04 tokens per second)
0.01.690.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.690.024 I llama_perf_context_print:       total time =    1673.97 ms /   129 tokens
0.01.690.621 I ggml_metal_free: deallocating

real	0m1.870s
user	0m0.149s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4537 (f7fb43cd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14160a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14160aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14160b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14160b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14160bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14160c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14160c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14160cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14160d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14160d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14160dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14160e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14160ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14160f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14160fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141610340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141610a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141611180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1416118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141612790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141612eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1416135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141613e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141614590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141614850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141614e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141616010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1416162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141616770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141616a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1416172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141617800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141617ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141618400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1416188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141618d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1416191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141619b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141619fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14161a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14161a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14161ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14161b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14161bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14161c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14161c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14161ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14161d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14161dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14161e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14161e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14161ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14161f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14161f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14161fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1416202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141620eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141621350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1416217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141621c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141622130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1416225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141622a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141622f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1416233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141623cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141625230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141625780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141625cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141626220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141626770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141626cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141627210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141627760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141627cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141628200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141628750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141628ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1416291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141629740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141629c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14162a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14162a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14162ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14162b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14162b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14162bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14161b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14162c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14162c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14162cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14162d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14162d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14162ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14162e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14162e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14162edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14162f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14162f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14162fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141630850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141630da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141631240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1416316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141631b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1416324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141632960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1416332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141633be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141634520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1416349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141634e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1416357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141635c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1416360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141636a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141637800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1416385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141638a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141638f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1416393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141639d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14163a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14163a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14163aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14163af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14163b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14163b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14163bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14163c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14163c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14163cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14163cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14163d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14163d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14163ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14163e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14163e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14163eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14163f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14163f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14163f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14163fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1416402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141640760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1416410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141641540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1416419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141641e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1416427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1416435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141643a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141643ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141644820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141644cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141645160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141645600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141645aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141645f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1416463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141646880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141646d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1416471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141647b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141647fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1416484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141648a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141648f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1416494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1416497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141649db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14164a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14164a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14164b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14164b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14164b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14164bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14164c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14164cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14164d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14164d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14164db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14164e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14164e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14164ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14164f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14164f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14164fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1416502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1416507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141650d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141651290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1416517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141651d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141652280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1416527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141652d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141653270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1416537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141653d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141654260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1416547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141654d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141655250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1416557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141655cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141656240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141656790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141656ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141657230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141657780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141657cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141658220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141658770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141658cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141659210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141659760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141659cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14165a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14165a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14165aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14165b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14165b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14165bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14165c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14165c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14165cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14165d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14165d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14165dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14165e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14165e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14165ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14165f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14165f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14165fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1416601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1416606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141660c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1416610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141661580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141661a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141661ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141662360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141662800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141662ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141663140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1416635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141663a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141663f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1416643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141664d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1416651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1416656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141665e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141666530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141666c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141667370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141667e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1416680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1416686f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.165.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.165.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1416683a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14164a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141649a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14164a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14161d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14161d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14161f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14164c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141614b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14161b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14161bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14161c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14161a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14161cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141613b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14161e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14161fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14162c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1416678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141616cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141616fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14164c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14164ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1416153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1416156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141668b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141668e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1416690d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141669390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141669650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141669910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141669bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141669e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14166a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14166a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14166a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14166a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14166ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14166af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14166b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14166b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14166b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14166ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14166bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14166bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14166c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14166c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14166c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14166ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14166cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14166d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14166d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14166d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14166d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14166db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14166ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14166e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14166e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14166e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14166e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14166eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14166ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14166f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14166f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14166f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14166f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14166fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14166fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141670190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141670450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141670710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1416709d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141670c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141670f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141671210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1416714d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141671790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141671a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141671d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141671fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141672290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141672550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141672810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141672ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141672d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141673050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141673310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1416735d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141673890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141673b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141673e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1416740d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141674390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141674650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141674910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141674bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141674e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141675150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141675410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1416756d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141675990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141675c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141675f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1416761d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141676490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141676750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141676a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141676cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141676f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141677250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141677510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1416777d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141677a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141677d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141678010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1416782d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141678590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141678850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141678b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141678dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141679090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141679350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141679610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1416798d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141679b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141679e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14167a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14167a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14167a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14167a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14167ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14167aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14167b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14167b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14167b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14167b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14167bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14167bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14167c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14167c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14167c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14167ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14167cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14167cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14167d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14167d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14167d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14167dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14167dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14167e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14167e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14167e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14167e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14167eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14167ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14167f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14167f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14167f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14167f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14167fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14167fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141680150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141680410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1416806d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141680990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141680c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141680f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1416811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141681490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141681750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141681a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141681cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141681f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141682250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141682510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1416827d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141682a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141682d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141683010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1416832d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141683590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141683850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141683b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141683dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141684090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141684350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141684610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1416848d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141684b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141684e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141685110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1416853d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141685690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141685950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141685c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141685ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141686190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141686450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141686710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1416869d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141686c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141686f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141687210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1416874d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141687790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141687a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141687d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141687fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1416885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141688860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141688b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141688de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1416890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141689360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141689620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1416898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141689ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141689e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14168a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14168a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14168a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14168a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14168ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14168aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14168b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14168b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14168b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14168b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14168bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14168bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14168c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14168c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14168c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14168ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14168cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14168cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14168d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14168d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14168d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14168dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14168dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14168e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14168e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14168ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14168f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14168f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14168fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1416902d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141690820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141690d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1416912c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141691810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141691d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1416922b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141692800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141692d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1416932a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1416937f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141693d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141694290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1416947e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141694d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141695280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1416957d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141695d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141695fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1416962a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1416967a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141696ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1416971a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1416976a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141697ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1416980a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1416985a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141698aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141698fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1416994a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1416999a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141699ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14169a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14169a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14169b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14169b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14169c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14169c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14169cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14169d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14169d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14169db90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14169d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14164bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14169cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14169dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14169e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14169e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14169e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14169eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14169edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14169f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14169f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14169f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14169fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1416a0190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1416a07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1416a0a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1416a0d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1416a1000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1416a12c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1416a1580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1416a1840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1416a1b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1416a1dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1416a2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1416a2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1416a2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1416a28c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1416a2b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1416a2e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1416a3100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1416a33c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1416a3680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1416a3940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1416a3c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1416a3ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1416a4180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1416a4440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1416a4700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1416a49c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1416a4c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1416a4f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1416a5200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1416a54c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1416a5780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1416a5a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1416a5d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1416a5fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1416a6280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1416a6540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1416a6800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1416a6ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1416a6d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1416a7040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1416a7300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1416a75c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1416a7880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1416a7b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1416a7e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1416a80c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1416a8380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1416a8640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1416a8900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1416a8bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1416a8e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1416a9140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1416a9400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1416a96c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1416a9980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1416a9c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1416a9f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1416aa1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1416aa480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1416aa740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1416aaa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1416aacc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1416aaf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131604870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131608550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131608c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1316090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131609560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1316099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131609e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13160a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13160a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13160ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13160b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13160b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13160b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13160bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13160c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13160c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13160caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13160cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13160d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13160d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13160dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13160e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13160e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13160e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13160ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13160f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13160fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13160fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131610330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1316107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131610c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131611080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1316114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131611960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131611dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131612240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1316126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131612b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131613400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131613870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131613ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131614150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1316145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131614ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131615780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131615bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1316164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1316183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1316195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13161a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13161a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13161abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13161b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13161b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13161b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13161bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13161c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13161c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13161cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13161cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13161d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13161d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13161dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13161e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13161e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13161e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13161ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13161f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13161f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13161fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131620020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1316211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131621650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131621f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1316223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1316230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131623560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1316239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131623e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1316242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131624720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131625000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131625470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1316258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1316261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131626630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131626f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131627380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1316277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1316280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1316289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131628e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131629290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131629b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131629fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13162a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13162a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13162ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13162b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13162b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13162ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13162bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13162c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13162c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13162cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13162d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13162dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13162df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13162e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13162e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13162eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13162ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13162f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13162f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13162fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1316300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131630540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1316309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131631700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131631b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131632450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1316328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1316331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131633a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131633ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131634360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1316347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131634c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1316350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131635520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131635990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131635e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1316366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131636b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131636fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1316378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131638180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1316385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131638a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131638ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1316397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131639c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13163a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13163a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13163a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13163ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13163b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13163b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13163bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13163bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13163c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13163c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13163ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13163d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13163d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13163da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13163deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13163e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13163e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13163ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13163f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13163f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13163f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13163fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1316406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131640b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1316413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1316422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1316429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131643110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131643af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131644560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131644b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.000s
user	0m0.303s
sys	0m0.330s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4537 (f7fb43cd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12880a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12880b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12880b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12880bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12880c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12880c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12880ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12880d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12880d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12880dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12880e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12880e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12880f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12880fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128810210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128810930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128811050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128811770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128811e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128812660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128812d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1288134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128813bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128814460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128814b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128815450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1288160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128816600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1288168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128816d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128817020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1288178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1288180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128818550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1288189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128818e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128819330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1288197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128819c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12881a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12881a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12881aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12881ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12881b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12881b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12881c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12881c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12881ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12881d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12881da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12881e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12881e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12881eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12881f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12881f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12881faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1288200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1288208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128820b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128821000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1288214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128821de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128822280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128822720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128822bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128823060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128823500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1288239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128823e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1288242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128824830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128824d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1288252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128825d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1288262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128826810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128826d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1288272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128827800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128827d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1288282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1288287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128828d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128829290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1288297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128829d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12882a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12882a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12882ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12882b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12882b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12882bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12882c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12881bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12882c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12882ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12882d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12882d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12882de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12882e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12882e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12882ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12882f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12882f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12882fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1288303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1288308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128830e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128831390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128831cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128832170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128832610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128832ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128832f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1288333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128833890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128833d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1288341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128834670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128834b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128834fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1288358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128835d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128836230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1288366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128836b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128837010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1288374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128837950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128838290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128838730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128838bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128839070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128839510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1288399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128839e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12883a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12883a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12883ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12883b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12883b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12883ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12883beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12883c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12883c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12883cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12883d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12883d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12883da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12883df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12883e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12883e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12883ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12883f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12883f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12883fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12883ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128840410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1288408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128840d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1288411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128841690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128841b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128841fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128842470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128842910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128842db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128843250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1288436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128843b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128844030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1288444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128844970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128844e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1288452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128845750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128845bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128846090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128846530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1288469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128846e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128847310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1288477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128847c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1288480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128848590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128848ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128849030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128849580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128849ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128849d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12884a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12884a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12884afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12884b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12884bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12884bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12884c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12884cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12884d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12884d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12884dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12884e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12884e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12884ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12884f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12884f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12884fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128850340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128850890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128850de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128851330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128851880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128851dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128852320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128852870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128852dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128853310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128853860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128853db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128854300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128854850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128854da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1288552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128855840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128855d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1288562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128856830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128856d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1288572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128857820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128857d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1288582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128858810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128858d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1288592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128859800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128859d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12885a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12885a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12885ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12885b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12885b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12885bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12885c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12885c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12885cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12885d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12885d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12885dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12885e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12885e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12885ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12885f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12885f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12885fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128860240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128860790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128860ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128861230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1288616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128861b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128862010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1288624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128862950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128862df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128863290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128863730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128863bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128864070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128864510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1288649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128864e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1288652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128865790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128865ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128866400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128866b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128867240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128867960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128867c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128868410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1288686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128868ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.089.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128868990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12884a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12884a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12884ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12881dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12881d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12881fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12884c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128815100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12881bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12881c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12881cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12881afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12881d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128814100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128809f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12881e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12882c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128867ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1288172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1288175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12884cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12884b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128815710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1288159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128815c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128869140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128869400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1288696c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128869980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128869c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128869f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12886a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12886a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12886a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12886aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12886acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12886af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12886b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12886b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12886b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12886ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12886bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12886c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12886c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12886c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12886c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12886cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12886cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12886d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12886d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12886d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12886d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12886db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12886de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12886e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12886e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12886e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12886e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12886ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12886eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12886f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12886f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12886f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12886f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12886fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12886ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128870200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1288704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128870780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128870a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128870d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128870fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128871280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128871540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128871800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128871ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128871d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128872040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128872300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1288725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128872880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128872b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128872e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1288730c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128873380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128873640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128873900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128873bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128873e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128874140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128874400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1288746c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128874980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128874c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128874f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1288751c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128875480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128875740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128875a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128875cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128875f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128876240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128876500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1288767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128876a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128876d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128877000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1288772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128877580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128877840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128877b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128877dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128878080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128878340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128878600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1288788c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128878b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128878e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128879100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1288793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128879680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128879940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128879c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128879ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12887a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12887a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12887a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12887a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12887ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12887af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12887b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12887b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12887b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12887ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12887bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12887bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12887c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12887c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12887c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12887cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12887cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12887d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12887d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12887d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12887d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12887db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12887de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12887e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12887e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12887e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12887e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12887ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12887ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12887f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12887f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12887f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12887f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12887fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12887ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1288801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128880480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128880740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128880a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128880cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128880f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128881240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128881500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1288817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128881a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128881d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128882000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1288822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128882580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128882840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128882b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128882dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128883080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128883340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128883600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1288838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128883b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128883e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128884100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1288843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128884680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128884940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128884c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128884ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128885180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128885440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128885700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1288859c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128885c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128885f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128886200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1288864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128886780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128886a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128886d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128886fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128887280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128887540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128887800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128887ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128887d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128888040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128888300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1288885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128888b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128888e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128889110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1288893d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128889690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128889950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128889c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128889ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12888a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12888a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12888a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12888a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12888ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12888af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12888b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12888b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12888b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12888ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12888bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12888bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12888c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12888c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12888c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12888cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12888cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12888d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12888d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12888d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12888d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12888db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12888de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12888e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12888e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12888e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12888ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12888f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12888f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12888fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128890370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1288908c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128890e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128891360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1288918b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128891e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128892350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1288928a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128892df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128893340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128893890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128893de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128894330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128894880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128894dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128895320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128895870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128895dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128896310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1288965d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128896890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128896d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128897290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128897790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128897c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128898190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128898690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128898b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128899090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128899590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128899a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128899f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12889a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12889a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12889ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12889b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12889bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12889c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12889ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12889d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12889d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12889db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12889e180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1276085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127608a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127608e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127609300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127609770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127609be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12760a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12760a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12760a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12760ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12760b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12760b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12760c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12760cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12760d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12760dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12760e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12760e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12760f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12760f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12760ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127610650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127611e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1276125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127612a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1276148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127615d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127616700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127617a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127618350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1276187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1276190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127619510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127619980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12761a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12761aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12761aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12761b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12761b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12761bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12761c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12761c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12761cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12761d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12761d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12761db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12761dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12761e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12761e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12761edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12761f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12761f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12761fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127620650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127620ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1276210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1276220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127622630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127622b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1276230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1276240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127624b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1276250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1276260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1276265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127626b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1276275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127627b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1276285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127628b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127629070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1276295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127629b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12762a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12762a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12762ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12762b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12762b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12762baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12762c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12762c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12762cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12762d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12762d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12762d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12762de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12762e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12762e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12762ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12762f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12762f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12762f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12762fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127630310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1276307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1276310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1276335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1276343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1276351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1276368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1276376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12763a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12763a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12763a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12763ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12763b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12763b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12763bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12763c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12763c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12763c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12763ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12763d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12763d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12763dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12763e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12763e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12763ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12763eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12763f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12763f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12763fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1276413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1276421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1276438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127644780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127644cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127645220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127645770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127645a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127646650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1276478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1276481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1276487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127648fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127649900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127649da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12764a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12764aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12764aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12764b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12764ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12764bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12764c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12764ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12764cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12764d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12764da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12764dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12764e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12764ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12764efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12764f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12764fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12764ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1276504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127650a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127650f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1276514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127651a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127651f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1276524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127652a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127652f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1276534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127653a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127653f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1276544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127654f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1276554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1276559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127655f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127656490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1276569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127656f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127657480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1276579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127657f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127658470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1276589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127658f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127659460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1276599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12765a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12765a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12765aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12765b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12765b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12765bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12765c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12765c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12765ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12765d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12765d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12765dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12765e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12765e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12765ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12765ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12765f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12765f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12765fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1276601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127660650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127661980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1276620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1276627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127662ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127663600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1276638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1276640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127664370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127664980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.922s
user	0m0.244s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
