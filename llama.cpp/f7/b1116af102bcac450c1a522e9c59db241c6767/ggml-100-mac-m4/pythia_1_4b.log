Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.468s
user	0m0.863s
sys	0m1.188s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Built target llava_shared
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-arg-parser
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target test-rope
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-batched
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-parallel
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-cli
[ 80%] Built target llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-perplexity
[ 81%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative
[ 90%] Built target llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tts
[ 91%] Built target llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.246s
user	0m6.548s
sys	0m10.063s

main: quantize time =  5909.01 ms
main:    total time =  5909.01 ms

main: quantize time =  3109.43 ms
main:    total time =  3109.43 ms

main: quantize time =  3507.37 ms
main:    total time =  3507.37 ms

main: quantize time =  2347.73 ms
main:    total time =  2347.73 ms

main: quantize time =  1442.78 ms
main:    total time =  1442.78 ms

main: quantize time =  5717.93 ms
main:    total time =  5717.93 ms

main: quantize time =  5861.87 ms
main:    total time =  5861.87 ms

main: quantize time =  7402.90 ms
main:    total time =  7402.90 ms

main: quantize time =  6192.98 ms
main:    total time =  6192.98 ms

main: quantize time =  5002.39 ms
main:    total time =  5002.39 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.141 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.334 I main: llama backend init
0.00.000.339 I main: load the model and apply lora adapter, if any
0.00.092.438 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.104.759 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.104.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.104.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.104.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.104.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.104.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.104.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.104.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.104.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.104.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.104.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.104.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.104.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.104.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.104.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.104.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.104.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.111.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.113.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.120.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.120.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.120.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.120.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.120.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.120.587 I llama_model_loader: - type  f32:  194 tensors
0.00.120.587 I llama_model_loader: - type  f16:   98 tensors
0.00.120.589 I print_info: file format = GGUF V3 (latest)
0.00.120.591 I print_info: file type   = all F32 (guessed)
0.00.120.594 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.138.272 I load: special tokens cache size = 25
0.00.148.468 I load: token to piece cache size = 0.2984 MB
0.00.148.474 I print_info: arch             = gptneox
0.00.148.474 I print_info: vocab_only       = 0
0.00.148.474 I print_info: n_ctx_train      = 2048
0.00.148.474 I print_info: n_embd           = 2048
0.00.148.475 I print_info: n_layer          = 24
0.00.148.481 I print_info: n_head           = 16
0.00.148.482 I print_info: n_head_kv        = 16
0.00.148.482 I print_info: n_rot            = 32
0.00.148.482 I print_info: n_swa            = 0
0.00.148.483 I print_info: n_embd_head_k    = 128
0.00.148.483 I print_info: n_embd_head_v    = 128
0.00.148.484 I print_info: n_gqa            = 1
0.00.148.485 I print_info: n_embd_k_gqa     = 2048
0.00.148.485 I print_info: n_embd_v_gqa     = 2048
0.00.148.486 I print_info: f_norm_eps       = 1.0e-05
0.00.148.487 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.148.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.148.487 I print_info: f_max_alibi_bias = 0.0e+00
0.00.148.487 I print_info: f_logit_scale    = 0.0e+00
0.00.148.490 I print_info: n_ff             = 8192
0.00.148.490 I print_info: n_expert         = 0
0.00.148.490 I print_info: n_expert_used    = 0
0.00.148.491 I print_info: causal attn      = 1
0.00.148.491 I print_info: pooling type     = 0
0.00.148.491 I print_info: rope type        = 2
0.00.148.491 I print_info: rope scaling     = linear
0.00.148.494 I print_info: freq_base_train  = 10000.0
0.00.148.494 I print_info: freq_scale_train = 1
0.00.148.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.148.495 I print_info: rope_finetuned   = unknown
0.00.148.495 I print_info: ssm_d_conv       = 0
0.00.148.495 I print_info: ssm_d_inner      = 0
0.00.148.495 I print_info: ssm_d_state      = 0
0.00.148.495 I print_info: ssm_dt_rank      = 0
0.00.148.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.148.496 I print_info: model type       = 1.4B
0.00.148.496 I print_info: model params     = 1.41 B
0.00.148.496 I print_info: general.name     = 1.4B
0.00.148.497 I print_info: vocab type       = BPE
0.00.148.498 I print_info: n_vocab          = 50304
0.00.148.498 I print_info: n_merges         = 50009
0.00.148.499 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.148.499 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.148.504 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.148.504 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.148.504 I print_info: LF token         = 187 'Ċ'
0.00.148.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.148.507 I print_info: max token length = 1024
0.00.148.507 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.202.562 I load_tensors: offloading 24 repeating layers to GPU
0.00.202.566 I load_tensors: offloading output layer to GPU
0.00.202.566 I load_tensors: offloaded 25/25 layers to GPU
0.00.202.587 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.202.588 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.202.978 I llama_init_from_model: n_seq_max     = 1
0.00.202.979 I llama_init_from_model: n_ctx         = 2048
0.00.202.979 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.202.979 I llama_init_from_model: n_batch       = 2048
0.00.202.979 I llama_init_from_model: n_ubatch      = 512
0.00.202.979 I llama_init_from_model: flash_attn    = 0
0.00.202.980 I llama_init_from_model: freq_base     = 10000.0
0.00.202.980 I llama_init_from_model: freq_scale    = 1
0.00.202.981 I ggml_metal_init: allocating
0.00.203.007 I ggml_metal_init: found device: Apple M4
0.00.203.012 I ggml_metal_init: picking default device: Apple M4
0.00.203.628 I ggml_metal_init: using embedded metal library
0.00.220.461 I ggml_metal_init: GPU name:   Apple M4
0.00.220.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.220.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.220.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.220.465 I ggml_metal_init: simdgroup reduction   = true
0.00.220.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.220.465 I ggml_metal_init: has residency sets    = true
0.00.220.465 I ggml_metal_init: has bfloat            = true
0.00.220.465 I ggml_metal_init: use bfloat            = true
0.00.220.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.220.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.289.521 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.322.956 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.322.962 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.323.008 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.327.351 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.327.353 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.327.353 I llama_init_from_model: graph nodes  = 967
0.00.327.353 I llama_init_from_model: graph splits = 2
0.00.327.357 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.327.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.327.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.415 I main: llama threadpool init, n_threads = 4
0.00.393.455 I 
0.00.393.472 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.393.472 I 
0.00.393.515 I sampler seed: 1234
0.00.393.520 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.393.544 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.393.546 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.393.546 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.224.556 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.224.557 I llama_perf_context_print:        load time =     300.14 ms
0.02.224.557 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.51 tokens per second)
0.02.224.558 I llama_perf_context_print:        eval time =    1784.59 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.224.559 I llama_perf_context_print:       total time =    1831.95 ms /    70 tokens
0.02.224.846 I ggml_metal_free: deallocating

real	0m2.539s
user	0m0.135s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.344 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.345 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.347 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.059 I llama_model_loader: - type  f32:  194 tensors
0.00.029.059 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.060 I print_info: file format = GGUF V3 (latest)
0.00.029.061 I print_info: file type   = Q8_0
0.00.029.062 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.007 I load: special tokens cache size = 25
0.00.043.169 I load: token to piece cache size = 0.2984 MB
0.00.043.175 I print_info: arch             = gptneox
0.00.043.175 I print_info: vocab_only       = 0
0.00.043.175 I print_info: n_ctx_train      = 2048
0.00.043.177 I print_info: n_embd           = 2048
0.00.043.177 I print_info: n_layer          = 24
0.00.043.183 I print_info: n_head           = 16
0.00.043.184 I print_info: n_head_kv        = 16
0.00.043.184 I print_info: n_rot            = 32
0.00.043.184 I print_info: n_swa            = 0
0.00.043.185 I print_info: n_embd_head_k    = 128
0.00.043.185 I print_info: n_embd_head_v    = 128
0.00.043.186 I print_info: n_gqa            = 1
0.00.043.186 I print_info: n_embd_k_gqa     = 2048
0.00.043.187 I print_info: n_embd_v_gqa     = 2048
0.00.043.188 I print_info: f_norm_eps       = 1.0e-05
0.00.043.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.189 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.191 I print_info: f_logit_scale    = 0.0e+00
0.00.043.192 I print_info: n_ff             = 8192
0.00.043.192 I print_info: n_expert         = 0
0.00.043.192 I print_info: n_expert_used    = 0
0.00.043.192 I print_info: causal attn      = 1
0.00.043.192 I print_info: pooling type     = 0
0.00.043.193 I print_info: rope type        = 2
0.00.043.193 I print_info: rope scaling     = linear
0.00.043.193 I print_info: freq_base_train  = 10000.0
0.00.043.194 I print_info: freq_scale_train = 1
0.00.043.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.194 I print_info: rope_finetuned   = unknown
0.00.043.195 I print_info: ssm_d_conv       = 0
0.00.043.196 I print_info: ssm_d_inner      = 0
0.00.043.196 I print_info: ssm_d_state      = 0
0.00.043.196 I print_info: ssm_dt_rank      = 0
0.00.043.196 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.196 I print_info: model type       = 1.4B
0.00.043.197 I print_info: model params     = 1.41 B
0.00.043.197 I print_info: general.name     = 1.4B
0.00.043.198 I print_info: vocab type       = BPE
0.00.043.198 I print_info: n_vocab          = 50304
0.00.043.198 I print_info: n_merges         = 50009
0.00.043.198 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.199 I print_info: LF token         = 187 'Ċ'
0.00.043.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.199 I print_info: max token length = 1024
0.00.043.200 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.249.799 I load_tensors: offloading 24 repeating layers to GPU
0.01.249.806 I load_tensors: offloading output layer to GPU
0.01.249.807 I load_tensors: offloaded 25/25 layers to GPU
0.01.249.828 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.249.830 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.250.625 I llama_init_from_model: n_seq_max     = 1
0.01.250.627 I llama_init_from_model: n_ctx         = 2048
0.01.250.627 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.250.628 I llama_init_from_model: n_batch       = 2048
0.01.250.628 I llama_init_from_model: n_ubatch      = 512
0.01.250.629 I llama_init_from_model: flash_attn    = 0
0.01.250.629 I llama_init_from_model: freq_base     = 10000.0
0.01.250.630 I llama_init_from_model: freq_scale    = 1
0.01.250.631 I ggml_metal_init: allocating
0.01.250.649 I ggml_metal_init: found device: Apple M4
0.01.250.658 I ggml_metal_init: picking default device: Apple M4
0.01.252.037 I ggml_metal_init: using embedded metal library
0.01.257.490 I ggml_metal_init: GPU name:   Apple M4
0.01.257.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.257.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.257.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.257.495 I ggml_metal_init: simdgroup reduction   = true
0.01.257.495 I ggml_metal_init: simdgroup matrix mul. = true
0.01.257.495 I ggml_metal_init: has residency sets    = true
0.01.257.496 I ggml_metal_init: has bfloat            = true
0.01.257.496 I ggml_metal_init: use bfloat            = true
0.01.257.497 I ggml_metal_init: hasUnifiedMemory      = true
0.01.257.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.274.060 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.329.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.329.863 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.329.899 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.333.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.333.932 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.333.932 I llama_init_from_model: graph nodes  = 967
0.01.333.932 I llama_init_from_model: graph splits = 2
0.01.333.936 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.334.070 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.334.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.390.517 I main: llama threadpool init, n_threads = 4
0.01.390.559 I 
0.01.390.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.390.577 I 
0.01.390.730 I sampler seed: 1234
0.01.390.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.390.777 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.390.780 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.390.780 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.482.575 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.02.482.575 I llama_perf_context_print:        load time =    1379.89 ms
0.02.482.577 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.29 tokens per second)
0.02.482.577 I llama_perf_context_print:        eval time =    1039.67 ms /    63 runs   (   16.50 ms per token,    60.60 tokens per second)
0.02.482.578 I llama_perf_context_print:       total time =    1092.74 ms /    70 tokens
0.02.482.836 I ggml_metal_free: deallocating

real	0m2.501s
user	0m0.108s
sys	0m0.281s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.015.273 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.891 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.906 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.906 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.970 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.202 I llama_model_loader: - type  f32:  194 tensors
0.00.047.202 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.202 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.203 I print_info: file format = GGUF V3 (latest)
0.00.047.204 I print_info: file type   = Q4_0
0.00.047.205 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.059.535 I load: special tokens cache size = 25
0.00.069.444 I load: token to piece cache size = 0.2984 MB
0.00.069.448 I print_info: arch             = gptneox
0.00.069.449 I print_info: vocab_only       = 0
0.00.069.449 I print_info: n_ctx_train      = 2048
0.00.069.449 I print_info: n_embd           = 2048
0.00.069.449 I print_info: n_layer          = 24
0.00.069.454 I print_info: n_head           = 16
0.00.069.455 I print_info: n_head_kv        = 16
0.00.069.455 I print_info: n_rot            = 32
0.00.069.455 I print_info: n_swa            = 0
0.00.069.456 I print_info: n_embd_head_k    = 128
0.00.069.456 I print_info: n_embd_head_v    = 128
0.00.069.457 I print_info: n_gqa            = 1
0.00.069.458 I print_info: n_embd_k_gqa     = 2048
0.00.069.459 I print_info: n_embd_v_gqa     = 2048
0.00.069.460 I print_info: f_norm_eps       = 1.0e-05
0.00.069.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.461 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.461 I print_info: f_logit_scale    = 0.0e+00
0.00.069.462 I print_info: n_ff             = 8192
0.00.069.462 I print_info: n_expert         = 0
0.00.069.462 I print_info: n_expert_used    = 0
0.00.069.464 I print_info: causal attn      = 1
0.00.069.464 I print_info: pooling type     = 0
0.00.069.466 I print_info: rope type        = 2
0.00.069.466 I print_info: rope scaling     = linear
0.00.069.466 I print_info: freq_base_train  = 10000.0
0.00.069.467 I print_info: freq_scale_train = 1
0.00.069.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.467 I print_info: rope_finetuned   = unknown
0.00.069.468 I print_info: ssm_d_conv       = 0
0.00.069.468 I print_info: ssm_d_inner      = 0
0.00.069.468 I print_info: ssm_d_state      = 0
0.00.069.468 I print_info: ssm_dt_rank      = 0
0.00.069.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.468 I print_info: model type       = 1.4B
0.00.069.469 I print_info: model params     = 1.41 B
0.00.069.469 I print_info: general.name     = 1.4B
0.00.069.470 I print_info: vocab type       = BPE
0.00.069.471 I print_info: n_vocab          = 50304
0.00.069.471 I print_info: n_merges         = 50009
0.00.069.477 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.477 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.477 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.478 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.478 I print_info: LF token         = 187 'Ċ'
0.00.069.479 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.479 I print_info: max token length = 1024
0.00.069.479 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.414 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.429 I load_tensors: offloading output layer to GPU
0.00.639.430 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.468 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.639.470 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.640.787 I llama_init_from_model: n_seq_max     = 1
0.00.640.789 I llama_init_from_model: n_ctx         = 2048
0.00.640.790 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.790 I llama_init_from_model: n_batch       = 2048
0.00.640.791 I llama_init_from_model: n_ubatch      = 512
0.00.640.791 I llama_init_from_model: flash_attn    = 0
0.00.640.794 I llama_init_from_model: freq_base     = 10000.0
0.00.640.794 I llama_init_from_model: freq_scale    = 1
0.00.640.797 I ggml_metal_init: allocating
0.00.640.873 I ggml_metal_init: found device: Apple M4
0.00.640.886 I ggml_metal_init: picking default device: Apple M4
0.00.642.633 I ggml_metal_init: using embedded metal library
0.00.648.133 I ggml_metal_init: GPU name:   Apple M4
0.00.648.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.152 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.154 I ggml_metal_init: simdgroup reduction   = true
0.00.648.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.154 I ggml_metal_init: has residency sets    = true
0.00.648.155 I ggml_metal_init: has bfloat            = true
0.00.648.155 I ggml_metal_init: use bfloat            = true
0.00.648.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.181 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.557 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.592 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.468 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.470 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.470 I llama_init_from_model: graph nodes  = 967
0.00.729.470 I llama_init_from_model: graph splits = 2
0.00.729.476 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.239 I main: llama threadpool init, n_threads = 4
0.00.782.284 I 
0.00.782.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.300 I 
0.00.782.456 I sampler seed: 1234
0.00.782.461 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.472 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.472 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.474 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.455.646 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.455.647 I llama_perf_context_print:        load time =     766.25 ms
0.01.455.649 I llama_perf_context_print: prompt eval time =      39.44 ms /     7 tokens (    5.63 ms per token,   177.47 tokens per second)
0.01.455.649 I llama_perf_context_print:        eval time =     630.88 ms /    63 runs   (   10.01 ms per token,    99.86 tokens per second)
0.01.455.650 I llama_perf_context_print:       total time =     674.11 ms /    70 tokens
0.01.455.852 I ggml_metal_free: deallocating

real	0m1.486s
user	0m0.124s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.336 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.004 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.006 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.008 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.009 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.009 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.621 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.622 I llama_model_loader: - type  f32:  194 tensors
0.00.028.623 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.623 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.623 I print_info: file format = GGUF V3 (latest)
0.00.028.624 I print_info: file type   = Q4_1
0.00.028.624 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.406 I load: special tokens cache size = 25
0.00.042.451 I load: token to piece cache size = 0.2984 MB
0.00.042.455 I print_info: arch             = gptneox
0.00.042.455 I print_info: vocab_only       = 0
0.00.042.455 I print_info: n_ctx_train      = 2048
0.00.042.455 I print_info: n_embd           = 2048
0.00.042.455 I print_info: n_layer          = 24
0.00.042.458 I print_info: n_head           = 16
0.00.042.459 I print_info: n_head_kv        = 16
0.00.042.459 I print_info: n_rot            = 32
0.00.042.459 I print_info: n_swa            = 0
0.00.042.459 I print_info: n_embd_head_k    = 128
0.00.042.462 I print_info: n_embd_head_v    = 128
0.00.042.463 I print_info: n_gqa            = 1
0.00.042.464 I print_info: n_embd_k_gqa     = 2048
0.00.042.470 I print_info: n_embd_v_gqa     = 2048
0.00.042.470 I print_info: f_norm_eps       = 1.0e-05
0.00.042.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.471 I print_info: f_logit_scale    = 0.0e+00
0.00.042.475 I print_info: n_ff             = 8192
0.00.042.475 I print_info: n_expert         = 0
0.00.042.475 I print_info: n_expert_used    = 0
0.00.042.475 I print_info: causal attn      = 1
0.00.042.475 I print_info: pooling type     = 0
0.00.042.478 I print_info: rope type        = 2
0.00.042.479 I print_info: rope scaling     = linear
0.00.042.480 I print_info: freq_base_train  = 10000.0
0.00.042.480 I print_info: freq_scale_train = 1
0.00.042.480 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.480 I print_info: rope_finetuned   = unknown
0.00.042.480 I print_info: ssm_d_conv       = 0
0.00.042.481 I print_info: ssm_d_inner      = 0
0.00.042.481 I print_info: ssm_d_state      = 0
0.00.042.481 I print_info: ssm_dt_rank      = 0
0.00.042.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.481 I print_info: model type       = 1.4B
0.00.042.481 I print_info: model params     = 1.41 B
0.00.042.481 I print_info: general.name     = 1.4B
0.00.042.482 I print_info: vocab type       = BPE
0.00.042.482 I print_info: n_vocab          = 50304
0.00.042.482 I print_info: n_merges         = 50009
0.00.042.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.484 I print_info: LF token         = 187 'Ċ'
0.00.042.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.486 I print_info: max token length = 1024
0.00.042.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.691.203 I load_tensors: offloading 24 repeating layers to GPU
0.00.691.217 I load_tensors: offloading output layer to GPU
0.00.691.218 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.257 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.691.259 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.692.932 I llama_init_from_model: n_seq_max     = 1
0.00.692.935 I llama_init_from_model: n_ctx         = 2048
0.00.692.936 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.692.936 I llama_init_from_model: n_batch       = 2048
0.00.692.937 I llama_init_from_model: n_ubatch      = 512
0.00.692.938 I llama_init_from_model: flash_attn    = 0
0.00.692.941 I llama_init_from_model: freq_base     = 10000.0
0.00.692.941 I llama_init_from_model: freq_scale    = 1
0.00.692.943 I ggml_metal_init: allocating
0.00.693.020 I ggml_metal_init: found device: Apple M4
0.00.693.032 I ggml_metal_init: picking default device: Apple M4
0.00.694.865 I ggml_metal_init: using embedded metal library
0.00.701.457 I ggml_metal_init: GPU name:   Apple M4
0.00.701.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.463 I ggml_metal_init: simdgroup reduction   = true
0.00.701.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.464 I ggml_metal_init: has residency sets    = true
0.00.701.464 I ggml_metal_init: has bfloat            = true
0.00.701.464 I ggml_metal_init: use bfloat            = true
0.00.701.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.595 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.774.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.774.314 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.774.350 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.778.587 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.778.589 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.778.589 I llama_init_from_model: graph nodes  = 967
0.00.778.590 I llama_init_from_model: graph splits = 2
0.00.778.596 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.778.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.778.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.549 I main: llama threadpool init, n_threads = 4
0.00.833.593 I 
0.00.833.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.607 I 
0.00.833.749 I sampler seed: 1234
0.00.833.754 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.774 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.774 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.559.309 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.559.310 I llama_perf_context_print:        load time =     822.52 ms
0.01.559.310 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.42 tokens per second)
0.01.559.312 I llama_perf_context_print:        eval time =     674.04 ms /    63 runs   (   10.70 ms per token,    93.47 tokens per second)
0.01.559.312 I llama_perf_context_print:       total time =     726.45 ms /    70 tokens
0.01.559.511 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.109s
sys	0m0.227s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.755 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.756 I llama_model_loader: - type  f32:  194 tensors
0.00.025.756 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.756 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.757 I print_info: file format = GGUF V3 (latest)
0.00.025.757 I print_info: file type   = Q5_0
0.00.025.758 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.882 I load: special tokens cache size = 25
0.00.039.994 I load: token to piece cache size = 0.2984 MB
0.00.039.997 I print_info: arch             = gptneox
0.00.039.998 I print_info: vocab_only       = 0
0.00.039.998 I print_info: n_ctx_train      = 2048
0.00.039.998 I print_info: n_embd           = 2048
0.00.039.998 I print_info: n_layer          = 24
0.00.040.001 I print_info: n_head           = 16
0.00.040.002 I print_info: n_head_kv        = 16
0.00.040.002 I print_info: n_rot            = 32
0.00.040.003 I print_info: n_swa            = 0
0.00.040.003 I print_info: n_embd_head_k    = 128
0.00.040.003 I print_info: n_embd_head_v    = 128
0.00.040.004 I print_info: n_gqa            = 1
0.00.040.005 I print_info: n_embd_k_gqa     = 2048
0.00.040.005 I print_info: n_embd_v_gqa     = 2048
0.00.040.006 I print_info: f_norm_eps       = 1.0e-05
0.00.040.008 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.008 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.008 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.008 I print_info: f_logit_scale    = 0.0e+00
0.00.040.009 I print_info: n_ff             = 8192
0.00.040.009 I print_info: n_expert         = 0
0.00.040.009 I print_info: n_expert_used    = 0
0.00.040.009 I print_info: causal attn      = 1
0.00.040.010 I print_info: pooling type     = 0
0.00.040.011 I print_info: rope type        = 2
0.00.040.013 I print_info: rope scaling     = linear
0.00.040.014 I print_info: freq_base_train  = 10000.0
0.00.040.014 I print_info: freq_scale_train = 1
0.00.040.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.015 I print_info: rope_finetuned   = unknown
0.00.040.015 I print_info: ssm_d_conv       = 0
0.00.040.015 I print_info: ssm_d_inner      = 0
0.00.040.015 I print_info: ssm_d_state      = 0
0.00.040.015 I print_info: ssm_dt_rank      = 0
0.00.040.015 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.016 I print_info: model type       = 1.4B
0.00.040.016 I print_info: model params     = 1.41 B
0.00.040.017 I print_info: general.name     = 1.4B
0.00.040.017 I print_info: vocab type       = BPE
0.00.040.018 I print_info: n_vocab          = 50304
0.00.040.018 I print_info: n_merges         = 50009
0.00.040.018 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.018 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.018 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.019 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.019 I print_info: LF token         = 187 'Ċ'
0.00.040.019 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.019 I print_info: max token length = 1024
0.00.040.020 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.743.078 I load_tensors: offloading 24 repeating layers to GPU
0.00.743.091 I load_tensors: offloading output layer to GPU
0.00.743.092 I load_tensors: offloaded 25/25 layers to GPU
0.00.743.129 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.743.130 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.744.731 I llama_init_from_model: n_seq_max     = 1
0.00.744.741 I llama_init_from_model: n_ctx         = 2048
0.00.744.742 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.744.742 I llama_init_from_model: n_batch       = 2048
0.00.744.743 I llama_init_from_model: n_ubatch      = 512
0.00.744.743 I llama_init_from_model: flash_attn    = 0
0.00.744.745 I llama_init_from_model: freq_base     = 10000.0
0.00.744.745 I llama_init_from_model: freq_scale    = 1
0.00.744.748 I ggml_metal_init: allocating
0.00.744.795 I ggml_metal_init: found device: Apple M4
0.00.744.806 I ggml_metal_init: picking default device: Apple M4
0.00.747.883 I ggml_metal_init: using embedded metal library
0.00.754.812 I ggml_metal_init: GPU name:   Apple M4
0.00.754.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.818 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.820 I ggml_metal_init: simdgroup reduction   = true
0.00.754.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.820 I ggml_metal_init: has residency sets    = true
0.00.754.820 I ggml_metal_init: has bfloat            = true
0.00.754.821 I ggml_metal_init: use bfloat            = true
0.00.754.822 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.823 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.772.396 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.836.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.836.270 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.836.311 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.840.764 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.840.765 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.840.766 I llama_init_from_model: graph nodes  = 967
0.00.840.766 I llama_init_from_model: graph splits = 2
0.00.840.772 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.840.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.840.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.897.659 I main: llama threadpool init, n_threads = 4
0.00.897.701 I 
0.00.897.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.897.715 I 
0.00.897.862 I sampler seed: 1234
0.00.897.866 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.897.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.897.879 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.897.879 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.682.369 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47207.45 tokens per second)
0.01.682.370 I llama_perf_context_print:        load time =     888.06 ms
0.01.682.370 I llama_perf_context_print: prompt eval time =      42.87 ms /     7 tokens (    6.12 ms per token,   163.29 tokens per second)
0.01.682.371 I llama_perf_context_print:        eval time =     739.08 ms /    63 runs   (   11.73 ms per token,    85.24 tokens per second)
0.01.682.372 I llama_perf_context_print:       total time =     785.41 ms /    70 tokens
0.01.682.606 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.110s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.757 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.944 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.673 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.673 I llama_model_loader: - type  f32:  194 tensors
0.00.025.673 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.674 I print_info: file format = GGUF V3 (latest)
0.00.025.675 I print_info: file type   = Q5_1
0.00.025.676 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.481 I load: special tokens cache size = 25
0.00.039.481 I load: token to piece cache size = 0.2984 MB
0.00.039.483 I print_info: arch             = gptneox
0.00.039.483 I print_info: vocab_only       = 0
0.00.039.484 I print_info: n_ctx_train      = 2048
0.00.039.484 I print_info: n_embd           = 2048
0.00.039.484 I print_info: n_layer          = 24
0.00.039.487 I print_info: n_head           = 16
0.00.039.487 I print_info: n_head_kv        = 16
0.00.039.488 I print_info: n_rot            = 32
0.00.039.488 I print_info: n_swa            = 0
0.00.039.488 I print_info: n_embd_head_k    = 128
0.00.039.488 I print_info: n_embd_head_v    = 128
0.00.039.489 I print_info: n_gqa            = 1
0.00.039.492 I print_info: n_embd_k_gqa     = 2048
0.00.039.492 I print_info: n_embd_v_gqa     = 2048
0.00.039.493 I print_info: f_norm_eps       = 1.0e-05
0.00.039.493 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.494 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.494 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.495 I print_info: f_logit_scale    = 0.0e+00
0.00.039.495 I print_info: n_ff             = 8192
0.00.039.495 I print_info: n_expert         = 0
0.00.039.496 I print_info: n_expert_used    = 0
0.00.039.496 I print_info: causal attn      = 1
0.00.039.496 I print_info: pooling type     = 0
0.00.039.496 I print_info: rope type        = 2
0.00.039.496 I print_info: rope scaling     = linear
0.00.039.497 I print_info: freq_base_train  = 10000.0
0.00.039.497 I print_info: freq_scale_train = 1
0.00.039.497 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.497 I print_info: rope_finetuned   = unknown
0.00.039.498 I print_info: ssm_d_conv       = 0
0.00.039.498 I print_info: ssm_d_inner      = 0
0.00.039.498 I print_info: ssm_d_state      = 0
0.00.039.498 I print_info: ssm_dt_rank      = 0
0.00.039.498 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.498 I print_info: model type       = 1.4B
0.00.039.499 I print_info: model params     = 1.41 B
0.00.039.500 I print_info: general.name     = 1.4B
0.00.039.500 I print_info: vocab type       = BPE
0.00.039.500 I print_info: n_vocab          = 50304
0.00.039.501 I print_info: n_merges         = 50009
0.00.039.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.503 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: LF token         = 187 'Ċ'
0.00.039.504 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: max token length = 1024
0.00.039.505 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.233 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.249 I load_tensors: offloading output layer to GPU
0.00.630.250 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.284 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.630.286 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.631.807 I llama_init_from_model: n_seq_max     = 1
0.00.631.810 I llama_init_from_model: n_ctx         = 2048
0.00.631.811 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.811 I llama_init_from_model: n_batch       = 2048
0.00.631.812 I llama_init_from_model: n_ubatch      = 512
0.00.631.813 I llama_init_from_model: flash_attn    = 0
0.00.631.814 I llama_init_from_model: freq_base     = 10000.0
0.00.631.814 I llama_init_from_model: freq_scale    = 1
0.00.631.816 I ggml_metal_init: allocating
0.00.631.833 I ggml_metal_init: found device: Apple M4
0.00.631.843 I ggml_metal_init: picking default device: Apple M4
0.00.633.333 I ggml_metal_init: using embedded metal library
0.00.639.622 I ggml_metal_init: GPU name:   Apple M4
0.00.639.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.628 I ggml_metal_init: simdgroup reduction   = true
0.00.639.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.628 I ggml_metal_init: has residency sets    = true
0.00.639.629 I ggml_metal_init: has bfloat            = true
0.00.639.629 I ggml_metal_init: use bfloat            = true
0.00.639.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.380 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.274 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.281 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.316 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.663 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.665 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.665 I llama_init_from_model: graph nodes  = 967
0.00.714.665 I llama_init_from_model: graph splits = 2
0.00.714.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.805 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.517 I main: llama threadpool init, n_threads = 4
0.00.772.564 I 
0.00.772.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.580 I 
0.00.772.757 I sampler seed: 1234
0.00.772.762 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.773 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.773 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.624.575 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.624.576 I llama_perf_context_print:        load time =     762.06 ms
0.01.624.578 I llama_perf_context_print: prompt eval time =      51.95 ms /     7 tokens (    7.42 ms per token,   134.75 tokens per second)
0.01.624.579 I llama_perf_context_print:        eval time =     796.86 ms /    63 runs   (   12.65 ms per token,    79.06 tokens per second)
0.01.624.579 I llama_perf_context_print:       total time =     852.75 ms /    70 tokens
0.01.624.849 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.109s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.251 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.086 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.087 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.088 I llama_model_loader: - type  f32:  194 tensors
0.00.024.088 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.088 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.089 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.089 I print_info: file format = GGUF V3 (latest)
0.00.024.090 I print_info: file type   = Q2_K - Medium
0.00.024.091 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.840 I load: special tokens cache size = 25
0.00.037.795 I load: token to piece cache size = 0.2984 MB
0.00.037.798 I print_info: arch             = gptneox
0.00.037.799 I print_info: vocab_only       = 0
0.00.037.799 I print_info: n_ctx_train      = 2048
0.00.037.799 I print_info: n_embd           = 2048
0.00.037.799 I print_info: n_layer          = 24
0.00.037.803 I print_info: n_head           = 16
0.00.037.804 I print_info: n_head_kv        = 16
0.00.037.804 I print_info: n_rot            = 32
0.00.037.804 I print_info: n_swa            = 0
0.00.037.804 I print_info: n_embd_head_k    = 128
0.00.037.804 I print_info: n_embd_head_v    = 128
0.00.037.805 I print_info: n_gqa            = 1
0.00.037.806 I print_info: n_embd_k_gqa     = 2048
0.00.037.808 I print_info: n_embd_v_gqa     = 2048
0.00.037.808 I print_info: f_norm_eps       = 1.0e-05
0.00.037.809 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.811 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.811 I print_info: f_logit_scale    = 0.0e+00
0.00.037.812 I print_info: n_ff             = 8192
0.00.037.812 I print_info: n_expert         = 0
0.00.037.812 I print_info: n_expert_used    = 0
0.00.037.812 I print_info: causal attn      = 1
0.00.037.812 I print_info: pooling type     = 0
0.00.037.812 I print_info: rope type        = 2
0.00.037.813 I print_info: rope scaling     = linear
0.00.037.813 I print_info: freq_base_train  = 10000.0
0.00.037.813 I print_info: freq_scale_train = 1
0.00.037.814 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.814 I print_info: rope_finetuned   = unknown
0.00.037.814 I print_info: ssm_d_conv       = 0
0.00.037.814 I print_info: ssm_d_inner      = 0
0.00.037.814 I print_info: ssm_d_state      = 0
0.00.037.814 I print_info: ssm_dt_rank      = 0
0.00.037.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.815 I print_info: model type       = 1.4B
0.00.037.815 I print_info: model params     = 1.41 B
0.00.037.815 I print_info: general.name     = 1.4B
0.00.037.816 I print_info: vocab type       = BPE
0.00.037.816 I print_info: n_vocab          = 50304
0.00.037.816 I print_info: n_merges         = 50009
0.00.037.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.817 I print_info: LF token         = 187 'Ċ'
0.00.037.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.818 I print_info: max token length = 1024
0.00.037.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.347.831 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.845 I load_tensors: offloading output layer to GPU
0.00.347.846 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.876 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.877 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.402 I llama_init_from_model: n_seq_max     = 1
0.00.349.407 I llama_init_from_model: n_ctx         = 2048
0.00.349.408 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.349.408 I llama_init_from_model: n_batch       = 2048
0.00.349.408 I llama_init_from_model: n_ubatch      = 512
0.00.349.409 I llama_init_from_model: flash_attn    = 0
0.00.349.411 I llama_init_from_model: freq_base     = 10000.0
0.00.349.411 I llama_init_from_model: freq_scale    = 1
0.00.349.413 I ggml_metal_init: allocating
0.00.349.512 I ggml_metal_init: found device: Apple M4
0.00.349.526 I ggml_metal_init: picking default device: Apple M4
0.00.351.390 I ggml_metal_init: using embedded metal library
0.00.356.886 I ggml_metal_init: GPU name:   Apple M4
0.00.356.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.902 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.903 I ggml_metal_init: simdgroup reduction   = true
0.00.356.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.903 I ggml_metal_init: has residency sets    = true
0.00.356.904 I ggml_metal_init: has bfloat            = true
0.00.356.904 I ggml_metal_init: use bfloat            = true
0.00.356.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.378.220 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.438.334 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.438.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.443.194 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.443.196 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.443.197 I llama_init_from_model: graph nodes  = 967
0.00.443.197 I llama_init_from_model: graph splits = 2
0.00.443.208 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.443.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.443.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.263 I main: llama threadpool init, n_threads = 4
0.00.500.305 I 
0.00.500.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.323 I 
0.00.500.500 I sampler seed: 1234
0.00.500.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.525 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.525 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.525 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.174.401 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.174.401 I llama_perf_context_print:        load time =     490.29 ms
0.01.174.402 I llama_perf_context_print: prompt eval time =      35.48 ms /     7 tokens (    5.07 ms per token,   197.30 tokens per second)
0.01.174.403 I llama_perf_context_print:        eval time =     635.54 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.174.403 I llama_perf_context_print:       total time =     674.85 ms /    70 tokens
0.01.174.636 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.112s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.412 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.412 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.412 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.104 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.104 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.105 I llama_model_loader: - type  f32:  194 tensors
0.00.025.106 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.106 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.106 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.106 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.107 I print_info: file format = GGUF V3 (latest)
0.00.025.108 I print_info: file type   = Q3_K - Medium
0.00.025.108 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.263 I load: special tokens cache size = 25
0.00.039.274 I load: token to piece cache size = 0.2984 MB
0.00.039.277 I print_info: arch             = gptneox
0.00.039.277 I print_info: vocab_only       = 0
0.00.039.277 I print_info: n_ctx_train      = 2048
0.00.039.277 I print_info: n_embd           = 2048
0.00.039.277 I print_info: n_layer          = 24
0.00.039.280 I print_info: n_head           = 16
0.00.039.281 I print_info: n_head_kv        = 16
0.00.039.281 I print_info: n_rot            = 32
0.00.039.281 I print_info: n_swa            = 0
0.00.039.281 I print_info: n_embd_head_k    = 128
0.00.039.281 I print_info: n_embd_head_v    = 128
0.00.039.282 I print_info: n_gqa            = 1
0.00.039.283 I print_info: n_embd_k_gqa     = 2048
0.00.039.283 I print_info: n_embd_v_gqa     = 2048
0.00.039.284 I print_info: f_norm_eps       = 1.0e-05
0.00.039.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.285 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.285 I print_info: f_logit_scale    = 0.0e+00
0.00.039.286 I print_info: n_ff             = 8192
0.00.039.286 I print_info: n_expert         = 0
0.00.039.286 I print_info: n_expert_used    = 0
0.00.039.286 I print_info: causal attn      = 1
0.00.039.287 I print_info: pooling type     = 0
0.00.039.287 I print_info: rope type        = 2
0.00.039.287 I print_info: rope scaling     = linear
0.00.039.287 I print_info: freq_base_train  = 10000.0
0.00.039.288 I print_info: freq_scale_train = 1
0.00.039.288 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.288 I print_info: rope_finetuned   = unknown
0.00.039.288 I print_info: ssm_d_conv       = 0
0.00.039.291 I print_info: ssm_d_inner      = 0
0.00.039.291 I print_info: ssm_d_state      = 0
0.00.039.291 I print_info: ssm_dt_rank      = 0
0.00.039.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.291 I print_info: model type       = 1.4B
0.00.039.292 I print_info: model params     = 1.41 B
0.00.039.292 I print_info: general.name     = 1.4B
0.00.039.292 I print_info: vocab type       = BPE
0.00.039.293 I print_info: n_vocab          = 50304
0.00.039.293 I print_info: n_merges         = 50009
0.00.039.293 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.294 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.294 I print_info: LF token         = 187 'Ċ'
0.00.039.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.294 I print_info: max token length = 1024
0.00.039.295 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.579 I load_tensors: offloading output layer to GPU
0.00.435.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.614 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.626 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.091 I llama_init_from_model: n_seq_max     = 1
0.00.437.094 I llama_init_from_model: n_ctx         = 2048
0.00.437.095 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.437.095 I llama_init_from_model: n_batch       = 2048
0.00.437.095 I llama_init_from_model: n_ubatch      = 512
0.00.437.096 I llama_init_from_model: flash_attn    = 0
0.00.437.098 I llama_init_from_model: freq_base     = 10000.0
0.00.437.099 I llama_init_from_model: freq_scale    = 1
0.00.437.101 I ggml_metal_init: allocating
0.00.437.173 I ggml_metal_init: found device: Apple M4
0.00.437.186 I ggml_metal_init: picking default device: Apple M4
0.00.439.056 I ggml_metal_init: using embedded metal library
0.00.445.068 I ggml_metal_init: GPU name:   Apple M4
0.00.445.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.076 I ggml_metal_init: simdgroup reduction   = true
0.00.445.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.076 I ggml_metal_init: has residency sets    = true
0.00.445.076 I ggml_metal_init: has bfloat            = true
0.00.445.077 I ggml_metal_init: use bfloat            = true
0.00.445.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.518.997 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.519.003 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.519.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.524.327 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.524.329 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.524.329 I llama_init_from_model: graph nodes  = 967
0.00.524.329 I llama_init_from_model: graph splits = 2
0.00.524.335 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.524.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.524.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.784 I main: llama threadpool init, n_threads = 4
0.00.582.832 I 
0.00.582.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.846 I 
0.00.583.000 I sampler seed: 1234
0.00.583.005 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.583.050 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.583.054 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.583.054 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.322.713 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.322.714 I llama_perf_context_print:        load time =     573.31 ms
0.01.322.714 I llama_perf_context_print: prompt eval time =      45.25 ms /     7 tokens (    6.46 ms per token,   154.70 tokens per second)
0.01.322.715 I llama_perf_context_print:        eval time =     691.45 ms /    63 runs   (   10.98 ms per token,    91.11 tokens per second)
0.01.322.715 I llama_perf_context_print:       total time =     740.62 ms /    70 tokens
0.01.322.908 I ggml_metal_free: deallocating

real	0m1.340s
user	0m0.110s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.020.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.292 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.029.174 I llama_model_loader: - type  f32:  194 tensors
0.00.029.175 I llama_model_loader: - type q4_K:   61 tensors
0.00.029.175 I llama_model_loader: - type q5_K:   24 tensors
0.00.029.175 I llama_model_loader: - type q6_K:   13 tensors
0.00.029.176 I print_info: file format = GGUF V3 (latest)
0.00.029.176 I print_info: file type   = Q4_K - Medium
0.00.029.176 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.978 I load: special tokens cache size = 25
0.00.043.073 I load: token to piece cache size = 0.2984 MB
0.00.043.076 I print_info: arch             = gptneox
0.00.043.076 I print_info: vocab_only       = 0
0.00.043.077 I print_info: n_ctx_train      = 2048
0.00.043.077 I print_info: n_embd           = 2048
0.00.043.077 I print_info: n_layer          = 24
0.00.043.080 I print_info: n_head           = 16
0.00.043.081 I print_info: n_head_kv        = 16
0.00.043.081 I print_info: n_rot            = 32
0.00.043.081 I print_info: n_swa            = 0
0.00.043.081 I print_info: n_embd_head_k    = 128
0.00.043.081 I print_info: n_embd_head_v    = 128
0.00.043.082 I print_info: n_gqa            = 1
0.00.043.083 I print_info: n_embd_k_gqa     = 2048
0.00.043.084 I print_info: n_embd_v_gqa     = 2048
0.00.043.086 I print_info: f_norm_eps       = 1.0e-05
0.00.043.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.087 I print_info: f_logit_scale    = 0.0e+00
0.00.043.088 I print_info: n_ff             = 8192
0.00.043.088 I print_info: n_expert         = 0
0.00.043.088 I print_info: n_expert_used    = 0
0.00.043.088 I print_info: causal attn      = 1
0.00.043.089 I print_info: pooling type     = 0
0.00.043.091 I print_info: rope type        = 2
0.00.043.091 I print_info: rope scaling     = linear
0.00.043.091 I print_info: freq_base_train  = 10000.0
0.00.043.092 I print_info: freq_scale_train = 1
0.00.043.092 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.092 I print_info: rope_finetuned   = unknown
0.00.043.093 I print_info: ssm_d_conv       = 0
0.00.043.093 I print_info: ssm_d_inner      = 0
0.00.043.093 I print_info: ssm_d_state      = 0
0.00.043.093 I print_info: ssm_dt_rank      = 0
0.00.043.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.093 I print_info: model type       = 1.4B
0.00.043.094 I print_info: model params     = 1.41 B
0.00.043.094 I print_info: general.name     = 1.4B
0.00.043.094 I print_info: vocab type       = BPE
0.00.043.094 I print_info: n_vocab          = 50304
0.00.043.095 I print_info: n_merges         = 50009
0.00.043.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.095 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.095 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.099 I print_info: LF token         = 187 'Ċ'
0.00.043.100 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.100 I print_info: max token length = 1024
0.00.043.100 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.636 I load_tensors: offloading output layer to GPU
0.00.533.637 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.671 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.673 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.981 I llama_init_from_model: n_seq_max     = 1
0.00.534.984 I llama_init_from_model: n_ctx         = 2048
0.00.534.985 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.534.985 I llama_init_from_model: n_batch       = 2048
0.00.534.985 I llama_init_from_model: n_ubatch      = 512
0.00.534.986 I llama_init_from_model: flash_attn    = 0
0.00.534.988 I llama_init_from_model: freq_base     = 10000.0
0.00.534.988 I llama_init_from_model: freq_scale    = 1
0.00.534.991 I ggml_metal_init: allocating
0.00.535.069 I ggml_metal_init: found device: Apple M4
0.00.535.086 I ggml_metal_init: picking default device: Apple M4
0.00.537.007 I ggml_metal_init: using embedded metal library
0.00.543.415 I ggml_metal_init: GPU name:   Apple M4
0.00.543.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.422 I ggml_metal_init: simdgroup reduction   = true
0.00.543.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.423 I ggml_metal_init: has residency sets    = true
0.00.543.423 I ggml_metal_init: has bfloat            = true
0.00.543.423 I ggml_metal_init: use bfloat            = true
0.00.543.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.454 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.619.596 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.619.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.988 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.623.990 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.623.991 I llama_init_from_model: graph nodes  = 967
0.00.623.991 I llama_init_from_model: graph splits = 2
0.00.623.995 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.120 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.154 I main: llama threadpool init, n_threads = 4
0.00.680.198 I 
0.00.680.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.214 I 
0.00.680.361 I sampler seed: 1234
0.00.680.366 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.392 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.430.921 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.430.922 I llama_perf_context_print:        load time =     666.68 ms
0.01.430.922 I llama_perf_context_print: prompt eval time =      47.54 ms /     7 tokens (    6.79 ms per token,   147.23 tokens per second)
0.01.430.923 I llama_perf_context_print:        eval time =     699.97 ms /    63 runs   (   11.11 ms per token,    90.00 tokens per second)
0.01.430.923 I llama_perf_context_print:       total time =     751.46 ms /    70 tokens
0.01.431.202 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.110s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.224 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.917 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.918 I llama_model_loader: - type  f32:  194 tensors
0.00.025.918 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.918 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.919 I print_info: file format = GGUF V3 (latest)
0.00.025.919 I print_info: file type   = Q5_K - Medium
0.00.025.920 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.084 I load: special tokens cache size = 25
0.00.040.058 I load: token to piece cache size = 0.2984 MB
0.00.040.061 I print_info: arch             = gptneox
0.00.040.062 I print_info: vocab_only       = 0
0.00.040.062 I print_info: n_ctx_train      = 2048
0.00.040.062 I print_info: n_embd           = 2048
0.00.040.062 I print_info: n_layer          = 24
0.00.040.065 I print_info: n_head           = 16
0.00.040.066 I print_info: n_head_kv        = 16
0.00.040.066 I print_info: n_rot            = 32
0.00.040.066 I print_info: n_swa            = 0
0.00.040.066 I print_info: n_embd_head_k    = 128
0.00.040.066 I print_info: n_embd_head_v    = 128
0.00.040.067 I print_info: n_gqa            = 1
0.00.040.068 I print_info: n_embd_k_gqa     = 2048
0.00.040.069 I print_info: n_embd_v_gqa     = 2048
0.00.040.069 I print_info: f_norm_eps       = 1.0e-05
0.00.040.070 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.070 I print_info: f_logit_scale    = 0.0e+00
0.00.040.071 I print_info: n_ff             = 8192
0.00.040.071 I print_info: n_expert         = 0
0.00.040.071 I print_info: n_expert_used    = 0
0.00.040.071 I print_info: causal attn      = 1
0.00.040.071 I print_info: pooling type     = 0
0.00.040.071 I print_info: rope type        = 2
0.00.040.072 I print_info: rope scaling     = linear
0.00.040.072 I print_info: freq_base_train  = 10000.0
0.00.040.075 I print_info: freq_scale_train = 1
0.00.040.075 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.075 I print_info: rope_finetuned   = unknown
0.00.040.075 I print_info: ssm_d_conv       = 0
0.00.040.075 I print_info: ssm_d_inner      = 0
0.00.040.075 I print_info: ssm_d_state      = 0
0.00.040.075 I print_info: ssm_dt_rank      = 0
0.00.040.082 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.082 I print_info: model type       = 1.4B
0.00.040.083 I print_info: model params     = 1.41 B
0.00.040.083 I print_info: general.name     = 1.4B
0.00.040.084 I print_info: vocab type       = BPE
0.00.040.084 I print_info: n_vocab          = 50304
0.00.040.084 I print_info: n_merges         = 50009
0.00.040.084 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: LF token         = 187 'Ċ'
0.00.040.085 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.087 I print_info: max token length = 1024
0.00.040.087 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.713 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.726 I load_tensors: offloading output layer to GPU
0.00.617.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.764 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.617.765 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.619.160 I llama_init_from_model: n_seq_max     = 1
0.00.619.162 I llama_init_from_model: n_ctx         = 2048
0.00.619.163 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.164 I llama_init_from_model: n_batch       = 2048
0.00.619.164 I llama_init_from_model: n_ubatch      = 512
0.00.619.164 I llama_init_from_model: flash_attn    = 0
0.00.619.166 I llama_init_from_model: freq_base     = 10000.0
0.00.619.167 I llama_init_from_model: freq_scale    = 1
0.00.619.171 I ggml_metal_init: allocating
0.00.619.242 I ggml_metal_init: found device: Apple M4
0.00.619.255 I ggml_metal_init: picking default device: Apple M4
0.00.621.096 I ggml_metal_init: using embedded metal library
0.00.627.838 I ggml_metal_init: GPU name:   Apple M4
0.00.627.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.843 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.845 I ggml_metal_init: simdgroup reduction   = true
0.00.627.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.845 I ggml_metal_init: has residency sets    = true
0.00.627.845 I ggml_metal_init: has bfloat            = true
0.00.627.846 I ggml_metal_init: use bfloat            = true
0.00.627.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.702.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.175 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.425 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.707.427 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.707.427 I llama_init_from_model: graph nodes  = 967
0.00.707.428 I llama_init_from_model: graph splits = 2
0.00.707.434 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.203 I main: llama threadpool init, n_threads = 4
0.00.768.253 I 
0.00.768.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.269 I 
0.00.768.422 I sampler seed: 1234
0.00.768.427 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.438 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.609.894 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.609.894 I llama_perf_context_print:        load time =     758.59 ms
0.01.609.895 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.03 tokens per second)
0.01.609.896 I llama_perf_context_print:        eval time =     785.96 ms /    63 runs   (   12.48 ms per token,    80.16 tokens per second)
0.01.609.897 I llama_perf_context_print:       total time =     842.39 ms /    70 tokens
0.01.610.183 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.108s
sys	0m0.230s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.424 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.426 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.427 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.429 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.052 I llama_model_loader: - type  f32:  194 tensors
0.00.026.052 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.052 I print_info: file format = GGUF V3 (latest)
0.00.026.053 I print_info: file type   = Q6_K
0.00.026.054 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.848 I load: special tokens cache size = 25
0.00.039.850 I load: token to piece cache size = 0.2984 MB
0.00.039.853 I print_info: arch             = gptneox
0.00.039.853 I print_info: vocab_only       = 0
0.00.039.853 I print_info: n_ctx_train      = 2048
0.00.039.853 I print_info: n_embd           = 2048
0.00.039.854 I print_info: n_layer          = 24
0.00.039.857 I print_info: n_head           = 16
0.00.039.857 I print_info: n_head_kv        = 16
0.00.039.858 I print_info: n_rot            = 32
0.00.039.858 I print_info: n_swa            = 0
0.00.039.858 I print_info: n_embd_head_k    = 128
0.00.039.858 I print_info: n_embd_head_v    = 128
0.00.039.859 I print_info: n_gqa            = 1
0.00.039.860 I print_info: n_embd_k_gqa     = 2048
0.00.039.860 I print_info: n_embd_v_gqa     = 2048
0.00.039.861 I print_info: f_norm_eps       = 1.0e-05
0.00.039.861 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.861 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.862 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.863 I print_info: f_logit_scale    = 0.0e+00
0.00.039.864 I print_info: n_ff             = 8192
0.00.039.864 I print_info: n_expert         = 0
0.00.039.864 I print_info: n_expert_used    = 0
0.00.039.864 I print_info: causal attn      = 1
0.00.039.865 I print_info: pooling type     = 0
0.00.039.865 I print_info: rope type        = 2
0.00.039.865 I print_info: rope scaling     = linear
0.00.039.865 I print_info: freq_base_train  = 10000.0
0.00.039.866 I print_info: freq_scale_train = 1
0.00.039.866 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.866 I print_info: rope_finetuned   = unknown
0.00.039.866 I print_info: ssm_d_conv       = 0
0.00.039.866 I print_info: ssm_d_inner      = 0
0.00.039.866 I print_info: ssm_d_state      = 0
0.00.039.867 I print_info: ssm_dt_rank      = 0
0.00.039.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.867 I print_info: model type       = 1.4B
0.00.039.867 I print_info: model params     = 1.41 B
0.00.039.867 I print_info: general.name     = 1.4B
0.00.039.868 I print_info: vocab type       = BPE
0.00.039.868 I print_info: n_vocab          = 50304
0.00.039.868 I print_info: n_merges         = 50009
0.00.039.869 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: LF token         = 187 'Ċ'
0.00.039.870 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.870 I print_info: max token length = 1024
0.00.039.870 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.671.329 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.343 I load_tensors: offloading output layer to GPU
0.00.671.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.377 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.671.378 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.673.078 I llama_init_from_model: n_seq_max     = 1
0.00.673.084 I llama_init_from_model: n_ctx         = 2048
0.00.673.084 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.673.085 I llama_init_from_model: n_batch       = 2048
0.00.673.085 I llama_init_from_model: n_ubatch      = 512
0.00.673.085 I llama_init_from_model: flash_attn    = 0
0.00.673.087 I llama_init_from_model: freq_base     = 10000.0
0.00.673.087 I llama_init_from_model: freq_scale    = 1
0.00.673.090 I ggml_metal_init: allocating
0.00.673.168 I ggml_metal_init: found device: Apple M4
0.00.673.181 I ggml_metal_init: picking default device: Apple M4
0.00.675.076 I ggml_metal_init: using embedded metal library
0.00.682.098 I ggml_metal_init: GPU name:   Apple M4
0.00.682.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.105 I ggml_metal_init: simdgroup reduction   = true
0.00.682.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.106 I ggml_metal_init: has residency sets    = true
0.00.682.106 I ggml_metal_init: has bfloat            = true
0.00.682.107 I ggml_metal_init: use bfloat            = true
0.00.682.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.759.031 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.759.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.759.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.763.630 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.763.633 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.763.633 I llama_init_from_model: graph nodes  = 967
0.00.763.633 I llama_init_from_model: graph splits = 2
0.00.763.638 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.763.779 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.763.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.259 I main: llama threadpool init, n_threads = 4
0.00.833.304 I 
0.00.833.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.319 I 
0.00.833.470 I sampler seed: 1234
0.00.833.475 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.521 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.522 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.706.974 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.706.975 I llama_perf_context_print:        load time =     822.71 ms
0.01.706.976 I llama_perf_context_print: prompt eval time =      57.86 ms /     7 tokens (    8.27 ms per token,   120.98 tokens per second)
0.01.706.976 I llama_perf_context_print:        eval time =     812.74 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.706.976 I llama_perf_context_print:       total time =     874.42 ms /    70 tokens
0.01.707.233 I ggml_metal_free: deallocating

real	0m1.728s
user	0m0.111s
sys	0m0.243s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.773 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.377 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.966 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.979 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.022 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.550 I llama_model_loader: - type  f32:  194 tensors
0.00.055.551 I llama_model_loader: - type  f16:   98 tensors
0.00.055.551 I print_info: file format = GGUF V3 (latest)
0.00.055.552 I print_info: file type   = all F32 (guessed)
0.00.055.553 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.350 I load: special tokens cache size = 25
0.00.074.945 I load: token to piece cache size = 0.2984 MB
0.00.074.948 I print_info: arch             = gptneox
0.00.074.948 I print_info: vocab_only       = 0
0.00.074.949 I print_info: n_ctx_train      = 2048
0.00.074.949 I print_info: n_embd           = 2048
0.00.074.949 I print_info: n_layer          = 24
0.00.074.952 I print_info: n_head           = 16
0.00.074.953 I print_info: n_head_kv        = 16
0.00.074.953 I print_info: n_rot            = 32
0.00.074.953 I print_info: n_swa            = 0
0.00.074.954 I print_info: n_embd_head_k    = 128
0.00.074.954 I print_info: n_embd_head_v    = 128
0.00.074.954 I print_info: n_gqa            = 1
0.00.074.955 I print_info: n_embd_k_gqa     = 2048
0.00.074.956 I print_info: n_embd_v_gqa     = 2048
0.00.074.956 I print_info: f_norm_eps       = 1.0e-05
0.00.074.957 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.957 I print_info: f_logit_scale    = 0.0e+00
0.00.074.958 I print_info: n_ff             = 8192
0.00.074.958 I print_info: n_expert         = 0
0.00.074.958 I print_info: n_expert_used    = 0
0.00.074.958 I print_info: causal attn      = 1
0.00.074.959 I print_info: pooling type     = 0
0.00.074.959 I print_info: rope type        = 2
0.00.074.959 I print_info: rope scaling     = linear
0.00.074.959 I print_info: freq_base_train  = 10000.0
0.00.074.960 I print_info: freq_scale_train = 1
0.00.074.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.960 I print_info: rope_finetuned   = unknown
0.00.074.963 I print_info: ssm_d_conv       = 0
0.00.074.963 I print_info: ssm_d_inner      = 0
0.00.074.963 I print_info: ssm_d_state      = 0
0.00.074.963 I print_info: ssm_dt_rank      = 0
0.00.074.964 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.964 I print_info: model type       = 1.4B
0.00.074.964 I print_info: model params     = 1.41 B
0.00.074.964 I print_info: general.name     = 1.4B
0.00.074.965 I print_info: vocab type       = BPE
0.00.074.965 I print_info: n_vocab          = 50304
0.00.074.965 I print_info: n_merges         = 50009
0.00.074.966 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.966 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.971 I print_info: LF token         = 187 'Ċ'
0.00.074.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.971 I print_info: max token length = 1024
0.00.074.972 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.077.861 I load_tensors: offloading 24 repeating layers to GPU
0.01.077.865 I load_tensors: offloading output layer to GPU
0.01.077.866 I load_tensors: offloaded 25/25 layers to GPU
0.01.077.884 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.077.886 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.078.476 I llama_init_from_model: n_seq_max     = 1
0.01.078.476 I llama_init_from_model: n_ctx         = 128
0.01.078.477 I llama_init_from_model: n_ctx_per_seq = 128
0.01.078.477 I llama_init_from_model: n_batch       = 128
0.01.078.477 I llama_init_from_model: n_ubatch      = 128
0.01.078.477 I llama_init_from_model: flash_attn    = 0
0.01.078.478 I llama_init_from_model: freq_base     = 10000.0
0.01.078.478 I llama_init_from_model: freq_scale    = 1
0.01.078.478 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.078.480 I ggml_metal_init: allocating
0.01.078.540 I ggml_metal_init: found device: Apple M4
0.01.078.545 I ggml_metal_init: picking default device: Apple M4
0.01.079.198 I ggml_metal_init: using embedded metal library
0.01.081.996 I ggml_metal_init: GPU name:   Apple M4
0.01.081.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.081.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.081.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.081.999 I ggml_metal_init: simdgroup reduction   = true
0.01.081.999 I ggml_metal_init: simdgroup matrix mul. = true
0.01.081.999 I ggml_metal_init: has residency sets    = true
0.01.081.999 I ggml_metal_init: has bfloat            = true
0.01.081.999 I ggml_metal_init: use bfloat            = true
0.01.082.000 I ggml_metal_init: hasUnifiedMemory      = true
0.01.082.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.091.622 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.093.246 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.093.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.093.278 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.094.873 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.094.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.094.874 I llama_init_from_model: graph nodes  = 967
0.01.094.874 I llama_init_from_model: graph splits = 2
0.01.094.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.094.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.129.349 I 
0.01.129.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.129.382 I perplexity: tokenizing the input ..
0.01.133.291 I perplexity: tokenization took 3.907 ms
0.01.133.294 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.265.911 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.270.414 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.270.461 I llama_perf_context_print:        load time =    1104.95 ms
0.01.270.462 I llama_perf_context_print: prompt eval time =     132.38 ms /   128 tokens (    1.03 ms per token,   966.89 tokens per second)
0.01.270.463 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.270.471 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.01.271.149 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.112s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.188 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.472 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.422 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.429 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.432 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.433 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.435 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.435 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.338 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.157 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.158 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.158 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.159 I llama_model_loader: - type  f32:  194 tensors
0.00.026.159 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.160 I print_info: file format = GGUF V3 (latest)
0.00.026.160 I print_info: file type   = Q8_0
0.00.026.161 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.413 I load: special tokens cache size = 25
0.00.040.595 I load: token to piece cache size = 0.2984 MB
0.00.040.601 I print_info: arch             = gptneox
0.00.040.602 I print_info: vocab_only       = 0
0.00.040.602 I print_info: n_ctx_train      = 2048
0.00.040.602 I print_info: n_embd           = 2048
0.00.040.602 I print_info: n_layer          = 24
0.00.040.607 I print_info: n_head           = 16
0.00.040.608 I print_info: n_head_kv        = 16
0.00.040.608 I print_info: n_rot            = 32
0.00.040.608 I print_info: n_swa            = 0
0.00.040.609 I print_info: n_embd_head_k    = 128
0.00.040.609 I print_info: n_embd_head_v    = 128
0.00.040.609 I print_info: n_gqa            = 1
0.00.040.610 I print_info: n_embd_k_gqa     = 2048
0.00.040.613 I print_info: n_embd_v_gqa     = 2048
0.00.040.614 I print_info: f_norm_eps       = 1.0e-05
0.00.040.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.614 I print_info: f_logit_scale    = 0.0e+00
0.00.040.616 I print_info: n_ff             = 8192
0.00.040.616 I print_info: n_expert         = 0
0.00.040.617 I print_info: n_expert_used    = 0
0.00.040.617 I print_info: causal attn      = 1
0.00.040.617 I print_info: pooling type     = 0
0.00.040.617 I print_info: rope type        = 2
0.00.040.617 I print_info: rope scaling     = linear
0.00.040.617 I print_info: freq_base_train  = 10000.0
0.00.040.618 I print_info: freq_scale_train = 1
0.00.040.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.618 I print_info: rope_finetuned   = unknown
0.00.040.618 I print_info: ssm_d_conv       = 0
0.00.040.618 I print_info: ssm_d_inner      = 0
0.00.040.618 I print_info: ssm_d_state      = 0
0.00.040.618 I print_info: ssm_dt_rank      = 0
0.00.040.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.619 I print_info: model type       = 1.4B
0.00.040.619 I print_info: model params     = 1.41 B
0.00.040.619 I print_info: general.name     = 1.4B
0.00.040.620 I print_info: vocab type       = BPE
0.00.040.620 I print_info: n_vocab          = 50304
0.00.040.620 I print_info: n_merges         = 50009
0.00.040.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: LF token         = 187 'Ċ'
0.00.040.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: max token length = 1024
0.00.040.622 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.871.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.871.018 I load_tensors: offloading output layer to GPU
0.00.871.019 I load_tensors: offloaded 25/25 layers to GPU
0.00.871.041 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.871.042 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.871.954 I llama_init_from_model: n_seq_max     = 1
0.00.871.956 I llama_init_from_model: n_ctx         = 128
0.00.871.957 I llama_init_from_model: n_ctx_per_seq = 128
0.00.871.957 I llama_init_from_model: n_batch       = 128
0.00.871.957 I llama_init_from_model: n_ubatch      = 128
0.00.871.958 I llama_init_from_model: flash_attn    = 0
0.00.871.959 I llama_init_from_model: freq_base     = 10000.0
0.00.871.960 I llama_init_from_model: freq_scale    = 1
0.00.871.960 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.871.961 I ggml_metal_init: allocating
0.00.871.997 I ggml_metal_init: found device: Apple M4
0.00.872.008 I ggml_metal_init: picking default device: Apple M4
0.00.872.981 I ggml_metal_init: using embedded metal library
0.00.877.156 I ggml_metal_init: GPU name:   Apple M4
0.00.877.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.877.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.877.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.877.163 I ggml_metal_init: simdgroup reduction   = true
0.00.877.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.877.164 I ggml_metal_init: has residency sets    = true
0.00.877.164 I ggml_metal_init: has bfloat            = true
0.00.877.165 I ggml_metal_init: use bfloat            = true
0.00.877.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.877.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.891.435 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.892.942 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.892.943 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.892.969 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.894.344 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.894.345 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.894.345 I llama_init_from_model: graph nodes  = 967
0.00.894.345 I llama_init_from_model: graph splits = 2
0.00.894.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.894.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.918.278 I 
0.00.918.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.918.299 I perplexity: tokenizing the input ..
0.00.921.745 I perplexity: tokenization took 3.445 ms
0.00.921.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.055.727 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.057.308 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.057.329 I llama_perf_context_print:        load time =     907.79 ms
0.01.057.330 I llama_perf_context_print: prompt eval time =     133.76 ms /   128 tokens (    1.05 ms per token,   956.91 tokens per second)
0.01.057.330 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.057.330 I llama_perf_context_print:       total time =     139.05 ms /   129 tokens
0.01.057.696 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.071s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.384 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.747 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.608 I llama_model_loader: - type  f32:  194 tensors
0.00.027.608 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.609 I print_info: file format = GGUF V3 (latest)
0.00.027.610 I print_info: file type   = Q4_0
0.00.027.611 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.522 I load: special tokens cache size = 25
0.00.041.536 I load: token to piece cache size = 0.2984 MB
0.00.041.539 I print_info: arch             = gptneox
0.00.041.540 I print_info: vocab_only       = 0
0.00.041.540 I print_info: n_ctx_train      = 2048
0.00.041.540 I print_info: n_embd           = 2048
0.00.041.540 I print_info: n_layer          = 24
0.00.041.545 I print_info: n_head           = 16
0.00.041.548 I print_info: n_head_kv        = 16
0.00.041.549 I print_info: n_rot            = 32
0.00.041.549 I print_info: n_swa            = 0
0.00.041.549 I print_info: n_embd_head_k    = 128
0.00.041.549 I print_info: n_embd_head_v    = 128
0.00.041.550 I print_info: n_gqa            = 1
0.00.041.550 I print_info: n_embd_k_gqa     = 2048
0.00.041.554 I print_info: n_embd_v_gqa     = 2048
0.00.041.555 I print_info: f_norm_eps       = 1.0e-05
0.00.041.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.555 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.556 I print_info: f_logit_scale    = 0.0e+00
0.00.041.556 I print_info: n_ff             = 8192
0.00.041.557 I print_info: n_expert         = 0
0.00.041.557 I print_info: n_expert_used    = 0
0.00.041.557 I print_info: causal attn      = 1
0.00.041.557 I print_info: pooling type     = 0
0.00.041.557 I print_info: rope type        = 2
0.00.041.557 I print_info: rope scaling     = linear
0.00.041.558 I print_info: freq_base_train  = 10000.0
0.00.041.558 I print_info: freq_scale_train = 1
0.00.041.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.558 I print_info: rope_finetuned   = unknown
0.00.041.558 I print_info: ssm_d_conv       = 0
0.00.041.558 I print_info: ssm_d_inner      = 0
0.00.041.559 I print_info: ssm_d_state      = 0
0.00.041.559 I print_info: ssm_dt_rank      = 0
0.00.041.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.559 I print_info: model type       = 1.4B
0.00.041.559 I print_info: model params     = 1.41 B
0.00.041.559 I print_info: general.name     = 1.4B
0.00.041.560 I print_info: vocab type       = BPE
0.00.041.560 I print_info: n_vocab          = 50304
0.00.041.561 I print_info: n_merges         = 50009
0.00.041.561 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.562 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.562 I print_info: LF token         = 187 'Ċ'
0.00.041.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.562 I print_info: max token length = 1024
0.00.041.564 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.083 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.089 I load_tensors: offloading output layer to GPU
0.00.631.089 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.108 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.631.109 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.631.946 I llama_init_from_model: n_seq_max     = 1
0.00.631.949 I llama_init_from_model: n_ctx         = 128
0.00.631.950 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.950 I llama_init_from_model: n_batch       = 128
0.00.631.950 I llama_init_from_model: n_ubatch      = 128
0.00.631.950 I llama_init_from_model: flash_attn    = 0
0.00.631.952 I llama_init_from_model: freq_base     = 10000.0
0.00.631.952 I llama_init_from_model: freq_scale    = 1
0.00.631.953 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.954 I ggml_metal_init: allocating
0.00.632.000 I ggml_metal_init: found device: Apple M4
0.00.632.011 I ggml_metal_init: picking default device: Apple M4
0.00.633.052 I ggml_metal_init: using embedded metal library
0.00.643.433 I ggml_metal_init: GPU name:   Apple M4
0.00.643.443 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.444 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.445 I ggml_metal_init: simdgroup reduction   = true
0.00.643.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.445 I ggml_metal_init: has residency sets    = true
0.00.643.446 I ggml_metal_init: has bfloat            = true
0.00.643.446 I ggml_metal_init: use bfloat            = true
0.00.643.447 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.813 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.669.865 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.172 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.673.178 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.673.179 I llama_init_from_model: graph nodes  = 967
0.00.673.179 I llama_init_from_model: graph splits = 2
0.00.673.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.205 I 
0.00.698.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.238 I perplexity: tokenizing the input ..
0.00.702.186 I perplexity: tokenization took 3.947 ms
0.00.702.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.855 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.838.006 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.838.028 I llama_perf_context_print:        load time =     687.81 ms
0.00.838.029 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.15 tokens per second)
0.00.838.030 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.030 I llama_perf_context_print:       total time =     139.82 ms /   129 tokens
0.00.838.373 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.075s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.120 I llama_model_loader: - type  f32:  194 tensors
0.00.025.120 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.120 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.121 I print_info: file format = GGUF V3 (latest)
0.00.025.121 I print_info: file type   = Q4_1
0.00.025.122 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.244 I load: special tokens cache size = 25
0.00.039.225 I load: token to piece cache size = 0.2984 MB
0.00.039.228 I print_info: arch             = gptneox
0.00.039.228 I print_info: vocab_only       = 0
0.00.039.228 I print_info: n_ctx_train      = 2048
0.00.039.229 I print_info: n_embd           = 2048
0.00.039.229 I print_info: n_layer          = 24
0.00.039.232 I print_info: n_head           = 16
0.00.039.233 I print_info: n_head_kv        = 16
0.00.039.233 I print_info: n_rot            = 32
0.00.039.233 I print_info: n_swa            = 0
0.00.039.233 I print_info: n_embd_head_k    = 128
0.00.039.233 I print_info: n_embd_head_v    = 128
0.00.039.234 I print_info: n_gqa            = 1
0.00.039.235 I print_info: n_embd_k_gqa     = 2048
0.00.039.236 I print_info: n_embd_v_gqa     = 2048
0.00.039.236 I print_info: f_norm_eps       = 1.0e-05
0.00.039.236 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.237 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.237 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.237 I print_info: f_logit_scale    = 0.0e+00
0.00.039.238 I print_info: n_ff             = 8192
0.00.039.238 I print_info: n_expert         = 0
0.00.039.238 I print_info: n_expert_used    = 0
0.00.039.238 I print_info: causal attn      = 1
0.00.039.238 I print_info: pooling type     = 0
0.00.039.238 I print_info: rope type        = 2
0.00.039.241 I print_info: rope scaling     = linear
0.00.039.241 I print_info: freq_base_train  = 10000.0
0.00.039.241 I print_info: freq_scale_train = 1
0.00.039.241 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.242 I print_info: rope_finetuned   = unknown
0.00.039.248 I print_info: ssm_d_conv       = 0
0.00.039.249 I print_info: ssm_d_inner      = 0
0.00.039.250 I print_info: ssm_d_state      = 0
0.00.039.250 I print_info: ssm_dt_rank      = 0
0.00.039.250 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.250 I print_info: model type       = 1.4B
0.00.039.251 I print_info: model params     = 1.41 B
0.00.039.251 I print_info: general.name     = 1.4B
0.00.039.251 I print_info: vocab type       = BPE
0.00.039.251 I print_info: n_vocab          = 50304
0.00.039.252 I print_info: n_merges         = 50009
0.00.039.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: LF token         = 187 'Ċ'
0.00.039.254 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.254 I print_info: max token length = 1024
0.00.039.255 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.589 I load_tensors: offloading output layer to GPU
0.00.645.590 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.626 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.645.627 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.647.321 I llama_init_from_model: n_seq_max     = 1
0.00.647.324 I llama_init_from_model: n_ctx         = 128
0.00.647.325 I llama_init_from_model: n_ctx_per_seq = 128
0.00.647.326 I llama_init_from_model: n_batch       = 128
0.00.647.326 I llama_init_from_model: n_ubatch      = 128
0.00.647.326 I llama_init_from_model: flash_attn    = 0
0.00.647.329 I llama_init_from_model: freq_base     = 10000.0
0.00.647.330 I llama_init_from_model: freq_scale    = 1
0.00.647.330 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.647.332 I ggml_metal_init: allocating
0.00.647.419 I ggml_metal_init: found device: Apple M4
0.00.647.433 I ggml_metal_init: picking default device: Apple M4
0.00.649.260 I ggml_metal_init: using embedded metal library
0.00.655.788 I ggml_metal_init: GPU name:   Apple M4
0.00.655.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.794 I ggml_metal_init: simdgroup reduction   = true
0.00.655.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.794 I ggml_metal_init: has residency sets    = true
0.00.655.795 I ggml_metal_init: has bfloat            = true
0.00.655.795 I ggml_metal_init: use bfloat            = true
0.00.655.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.334 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.676.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.676.388 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.625 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.627 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.627 I llama_init_from_model: graph nodes  = 967
0.00.679.628 I llama_init_from_model: graph splits = 2
0.00.679.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.553 I 
0.00.708.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.621 I perplexity: tokenizing the input ..
0.00.715.820 I perplexity: tokenization took 7.195 ms
0.00.715.827 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.805 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.246 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.281 I llama_perf_context_print:        load time =     699.81 ms
0.00.850.282 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.94 tokens per second)
0.00.850.283 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.283 I llama_perf_context_print:       total time =     141.73 ms /   129 tokens
0.00.850.663 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.079s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.477 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.156 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.157 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.158 I llama_model_loader: - type  f32:  194 tensors
0.00.026.158 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.159 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.159 I print_info: file format = GGUF V3 (latest)
0.00.026.160 I print_info: file type   = Q5_0
0.00.026.161 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.539 I load: special tokens cache size = 25
0.00.040.753 I load: token to piece cache size = 0.2984 MB
0.00.040.758 I print_info: arch             = gptneox
0.00.040.758 I print_info: vocab_only       = 0
0.00.040.759 I print_info: n_ctx_train      = 2048
0.00.040.759 I print_info: n_embd           = 2048
0.00.040.759 I print_info: n_layer          = 24
0.00.040.762 I print_info: n_head           = 16
0.00.040.763 I print_info: n_head_kv        = 16
0.00.040.763 I print_info: n_rot            = 32
0.00.040.767 I print_info: n_swa            = 0
0.00.040.767 I print_info: n_embd_head_k    = 128
0.00.040.768 I print_info: n_embd_head_v    = 128
0.00.040.768 I print_info: n_gqa            = 1
0.00.040.769 I print_info: n_embd_k_gqa     = 2048
0.00.040.769 I print_info: n_embd_v_gqa     = 2048
0.00.040.770 I print_info: f_norm_eps       = 1.0e-05
0.00.040.770 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.770 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.771 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.771 I print_info: f_logit_scale    = 0.0e+00
0.00.040.771 I print_info: n_ff             = 8192
0.00.040.772 I print_info: n_expert         = 0
0.00.040.772 I print_info: n_expert_used    = 0
0.00.040.772 I print_info: causal attn      = 1
0.00.040.773 I print_info: pooling type     = 0
0.00.040.773 I print_info: rope type        = 2
0.00.040.773 I print_info: rope scaling     = linear
0.00.040.774 I print_info: freq_base_train  = 10000.0
0.00.040.774 I print_info: freq_scale_train = 1
0.00.040.774 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.774 I print_info: rope_finetuned   = unknown
0.00.040.775 I print_info: ssm_d_conv       = 0
0.00.040.775 I print_info: ssm_d_inner      = 0
0.00.040.775 I print_info: ssm_d_state      = 0
0.00.040.775 I print_info: ssm_dt_rank      = 0
0.00.040.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.776 I print_info: model type       = 1.4B
0.00.040.776 I print_info: model params     = 1.41 B
0.00.040.776 I print_info: general.name     = 1.4B
0.00.040.777 I print_info: vocab type       = BPE
0.00.040.777 I print_info: n_vocab          = 50304
0.00.040.777 I print_info: n_merges         = 50009
0.00.040.777 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.777 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.777 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.778 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.778 I print_info: LF token         = 187 'Ċ'
0.00.040.778 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.778 I print_info: max token length = 1024
0.00.040.779 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.770.208 I load_tensors: offloading 24 repeating layers to GPU
0.00.770.219 I load_tensors: offloading output layer to GPU
0.00.770.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.770.254 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.770.257 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.771.822 I llama_init_from_model: n_seq_max     = 1
0.00.771.825 I llama_init_from_model: n_ctx         = 128
0.00.771.826 I llama_init_from_model: n_ctx_per_seq = 128
0.00.771.827 I llama_init_from_model: n_batch       = 128
0.00.771.827 I llama_init_from_model: n_ubatch      = 128
0.00.771.827 I llama_init_from_model: flash_attn    = 0
0.00.771.830 I llama_init_from_model: freq_base     = 10000.0
0.00.771.830 I llama_init_from_model: freq_scale    = 1
0.00.771.830 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.771.833 I ggml_metal_init: allocating
0.00.771.887 I ggml_metal_init: found device: Apple M4
0.00.771.901 I ggml_metal_init: picking default device: Apple M4
0.00.773.571 I ggml_metal_init: using embedded metal library
0.00.780.589 I ggml_metal_init: GPU name:   Apple M4
0.00.780.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.780.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.780.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.780.602 I ggml_metal_init: simdgroup reduction   = true
0.00.780.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.780.602 I ggml_metal_init: has residency sets    = true
0.00.780.602 I ggml_metal_init: has bfloat            = true
0.00.780.603 I ggml_metal_init: use bfloat            = true
0.00.780.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.780.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.798.559 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.802.158 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.802.165 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.802.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.805.509 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.805.511 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.805.511 I llama_init_from_model: graph nodes  = 967
0.00.805.512 I llama_init_from_model: graph splits = 2
0.00.805.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.805.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.805 I 
0.00.834.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.878 I perplexity: tokenizing the input ..
0.00.841.831 I perplexity: tokenization took 6.951 ms
0.00.841.835 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.975.784 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.977.185 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.977.206 I llama_perf_context_print:        load time =     824.95 ms
0.00.977.207 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.28 tokens per second)
0.00.977.208 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.977.208 I llama_perf_context_print:       total time =     142.41 ms /   129 tokens
0.00.977.557 I ggml_metal_free: deallocating

real	0m0.993s
user	0m0.078s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.887 I llama_model_loader: - type  f32:  194 tensors
0.00.024.888 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.889 I print_info: file format = GGUF V3 (latest)
0.00.024.889 I print_info: file type   = Q5_1
0.00.024.891 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.058 I load: special tokens cache size = 25
0.00.039.151 I load: token to piece cache size = 0.2984 MB
0.00.039.155 I print_info: arch             = gptneox
0.00.039.155 I print_info: vocab_only       = 0
0.00.039.156 I print_info: n_ctx_train      = 2048
0.00.039.156 I print_info: n_embd           = 2048
0.00.039.156 I print_info: n_layer          = 24
0.00.039.160 I print_info: n_head           = 16
0.00.039.161 I print_info: n_head_kv        = 16
0.00.039.161 I print_info: n_rot            = 32
0.00.039.164 I print_info: n_swa            = 0
0.00.039.164 I print_info: n_embd_head_k    = 128
0.00.039.165 I print_info: n_embd_head_v    = 128
0.00.039.165 I print_info: n_gqa            = 1
0.00.039.166 I print_info: n_embd_k_gqa     = 2048
0.00.039.166 I print_info: n_embd_v_gqa     = 2048
0.00.039.167 I print_info: f_norm_eps       = 1.0e-05
0.00.039.167 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.167 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.168 I print_info: f_logit_scale    = 0.0e+00
0.00.039.168 I print_info: n_ff             = 8192
0.00.039.169 I print_info: n_expert         = 0
0.00.039.169 I print_info: n_expert_used    = 0
0.00.039.169 I print_info: causal attn      = 1
0.00.039.169 I print_info: pooling type     = 0
0.00.039.169 I print_info: rope type        = 2
0.00.039.169 I print_info: rope scaling     = linear
0.00.039.170 I print_info: freq_base_train  = 10000.0
0.00.039.170 I print_info: freq_scale_train = 1
0.00.039.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.170 I print_info: rope_finetuned   = unknown
0.00.039.171 I print_info: ssm_d_conv       = 0
0.00.039.171 I print_info: ssm_d_inner      = 0
0.00.039.171 I print_info: ssm_d_state      = 0
0.00.039.171 I print_info: ssm_dt_rank      = 0
0.00.039.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.171 I print_info: model type       = 1.4B
0.00.039.171 I print_info: model params     = 1.41 B
0.00.039.172 I print_info: general.name     = 1.4B
0.00.039.173 I print_info: vocab type       = BPE
0.00.039.173 I print_info: n_vocab          = 50304
0.00.039.173 I print_info: n_merges         = 50009
0.00.039.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: LF token         = 187 'Ċ'
0.00.039.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: max token length = 1024
0.00.039.175 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.562 I load_tensors: offloading output layer to GPU
0.00.636.562 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.599 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.636.600 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.637.948 I llama_init_from_model: n_seq_max     = 1
0.00.637.950 I llama_init_from_model: n_ctx         = 128
0.00.637.951 I llama_init_from_model: n_ctx_per_seq = 128
0.00.637.951 I llama_init_from_model: n_batch       = 128
0.00.637.952 I llama_init_from_model: n_ubatch      = 128
0.00.637.952 I llama_init_from_model: flash_attn    = 0
0.00.637.955 I llama_init_from_model: freq_base     = 10000.0
0.00.637.955 I llama_init_from_model: freq_scale    = 1
0.00.637.956 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.958 I ggml_metal_init: allocating
0.00.638.082 I ggml_metal_init: found device: Apple M4
0.00.638.103 I ggml_metal_init: picking default device: Apple M4
0.00.639.988 I ggml_metal_init: using embedded metal library
0.00.646.779 I ggml_metal_init: GPU name:   Apple M4
0.00.646.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.790 I ggml_metal_init: simdgroup reduction   = true
0.00.646.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.791 I ggml_metal_init: has residency sets    = true
0.00.646.791 I ggml_metal_init: has bfloat            = true
0.00.646.791 I ggml_metal_init: use bfloat            = true
0.00.646.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.549 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.668.553 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.668.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.097 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.099 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.099 I llama_init_from_model: graph nodes  = 967
0.00.672.100 I llama_init_from_model: graph splits = 2
0.00.672.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.768 I 
0.00.703.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.836 I perplexity: tokenizing the input ..
0.00.710.586 I perplexity: tokenization took 6.747 ms
0.00.710.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.663 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.861.005 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.861.037 I llama_perf_context_print:        load time =     694.88 ms
0.00.861.038 I llama_perf_context_print: prompt eval time =     148.14 ms /   128 tokens (    1.16 ms per token,   864.07 tokens per second)
0.00.861.039 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.039 I llama_perf_context_print:       total time =     157.28 ms /   129 tokens
0.00.861.441 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.080s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.906 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.907 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.909 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.910 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.725 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.726 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.727 I print_info: file format = GGUF V3 (latest)
0.00.025.727 I print_info: file type   = Q2_K - Medium
0.00.025.728 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.677 I load: special tokens cache size = 25
0.00.039.733 I load: token to piece cache size = 0.2984 MB
0.00.039.737 I print_info: arch             = gptneox
0.00.039.737 I print_info: vocab_only       = 0
0.00.039.737 I print_info: n_ctx_train      = 2048
0.00.039.737 I print_info: n_embd           = 2048
0.00.039.737 I print_info: n_layer          = 24
0.00.039.741 I print_info: n_head           = 16
0.00.039.742 I print_info: n_head_kv        = 16
0.00.039.742 I print_info: n_rot            = 32
0.00.039.743 I print_info: n_swa            = 0
0.00.039.743 I print_info: n_embd_head_k    = 128
0.00.039.743 I print_info: n_embd_head_v    = 128
0.00.039.744 I print_info: n_gqa            = 1
0.00.039.744 I print_info: n_embd_k_gqa     = 2048
0.00.039.745 I print_info: n_embd_v_gqa     = 2048
0.00.039.746 I print_info: f_norm_eps       = 1.0e-05
0.00.039.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.746 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.747 I print_info: f_logit_scale    = 0.0e+00
0.00.039.747 I print_info: n_ff             = 8192
0.00.039.748 I print_info: n_expert         = 0
0.00.039.748 I print_info: n_expert_used    = 0
0.00.039.748 I print_info: causal attn      = 1
0.00.039.748 I print_info: pooling type     = 0
0.00.039.748 I print_info: rope type        = 2
0.00.039.748 I print_info: rope scaling     = linear
0.00.039.749 I print_info: freq_base_train  = 10000.0
0.00.039.749 I print_info: freq_scale_train = 1
0.00.039.749 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.750 I print_info: rope_finetuned   = unknown
0.00.039.750 I print_info: ssm_d_conv       = 0
0.00.039.750 I print_info: ssm_d_inner      = 0
0.00.039.753 I print_info: ssm_d_state      = 0
0.00.039.753 I print_info: ssm_dt_rank      = 0
0.00.039.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.753 I print_info: model type       = 1.4B
0.00.039.754 I print_info: model params     = 1.41 B
0.00.039.754 I print_info: general.name     = 1.4B
0.00.039.754 I print_info: vocab type       = BPE
0.00.039.755 I print_info: n_vocab          = 50304
0.00.039.755 I print_info: n_merges         = 50009
0.00.039.755 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: LF token         = 187 'Ċ'
0.00.039.756 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: max token length = 1024
0.00.039.757 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.350.960 I load_tensors: offloading 24 repeating layers to GPU
0.00.350.975 I load_tensors: offloading output layer to GPU
0.00.350.975 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.005 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.006 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.572 I llama_init_from_model: n_seq_max     = 1
0.00.352.578 I llama_init_from_model: n_ctx         = 128
0.00.352.579 I llama_init_from_model: n_ctx_per_seq = 128
0.00.352.579 I llama_init_from_model: n_batch       = 128
0.00.352.580 I llama_init_from_model: n_ubatch      = 128
0.00.352.580 I llama_init_from_model: flash_attn    = 0
0.00.352.582 I llama_init_from_model: freq_base     = 10000.0
0.00.352.582 I llama_init_from_model: freq_scale    = 1
0.00.352.583 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.352.585 I ggml_metal_init: allocating
0.00.352.678 I ggml_metal_init: found device: Apple M4
0.00.352.692 I ggml_metal_init: picking default device: Apple M4
0.00.354.495 I ggml_metal_init: using embedded metal library
0.00.359.841 I ggml_metal_init: GPU name:   Apple M4
0.00.359.854 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.856 I ggml_metal_init: simdgroup reduction   = true
0.00.359.857 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.857 I ggml_metal_init: has residency sets    = true
0.00.359.857 I ggml_metal_init: has bfloat            = true
0.00.359.858 I ggml_metal_init: use bfloat            = true
0.00.359.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.718 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.384.409 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.384.417 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.384.475 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.387.759 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.387.761 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.387.762 I llama_init_from_model: graph nodes  = 967
0.00.387.762 I llama_init_from_model: graph splits = 2
0.00.387.765 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.387.765 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.027 I 
0.00.414.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.065 I perplexity: tokenizing the input ..
0.00.420.023 I perplexity: tokenization took 5.954 ms
0.00.420.031 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.907 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.427 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.454 I llama_perf_context_print:        load time =     404.14 ms
0.00.553.455 I llama_perf_context_print: prompt eval time =     131.18 ms /   128 tokens (    1.02 ms per token,   975.74 tokens per second)
0.00.553.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.456 I llama_perf_context_print:       total time =     139.43 ms /   129 tokens
0.00.553.812 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.078s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.600 I llama_model_loader: - type  f32:  194 tensors
0.00.024.601 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.601 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.601 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.601 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.602 I print_info: file format = GGUF V3 (latest)
0.00.024.602 I print_info: file type   = Q3_K - Medium
0.00.024.603 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.562 I load: special tokens cache size = 25
0.00.038.651 I load: token to piece cache size = 0.2984 MB
0.00.038.659 I print_info: arch             = gptneox
0.00.038.660 I print_info: vocab_only       = 0
0.00.038.660 I print_info: n_ctx_train      = 2048
0.00.038.660 I print_info: n_embd           = 2048
0.00.038.660 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.665 I print_info: n_head_kv        = 16
0.00.038.665 I print_info: n_rot            = 32
0.00.038.666 I print_info: n_swa            = 0
0.00.038.666 I print_info: n_embd_head_k    = 128
0.00.038.666 I print_info: n_embd_head_v    = 128
0.00.038.666 I print_info: n_gqa            = 1
0.00.038.667 I print_info: n_embd_k_gqa     = 2048
0.00.038.668 I print_info: n_embd_v_gqa     = 2048
0.00.038.668 I print_info: f_norm_eps       = 1.0e-05
0.00.038.668 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.669 I print_info: f_logit_scale    = 0.0e+00
0.00.038.669 I print_info: n_ff             = 8192
0.00.038.670 I print_info: n_expert         = 0
0.00.038.670 I print_info: n_expert_used    = 0
0.00.038.670 I print_info: causal attn      = 1
0.00.038.670 I print_info: pooling type     = 0
0.00.038.670 I print_info: rope type        = 2
0.00.038.670 I print_info: rope scaling     = linear
0.00.038.671 I print_info: freq_base_train  = 10000.0
0.00.038.671 I print_info: freq_scale_train = 1
0.00.038.671 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.671 I print_info: rope_finetuned   = unknown
0.00.038.672 I print_info: ssm_d_conv       = 0
0.00.038.672 I print_info: ssm_d_inner      = 0
0.00.038.672 I print_info: ssm_d_state      = 0
0.00.038.672 I print_info: ssm_dt_rank      = 0
0.00.038.672 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.672 I print_info: model type       = 1.4B
0.00.038.673 I print_info: model params     = 1.41 B
0.00.038.673 I print_info: general.name     = 1.4B
0.00.038.673 I print_info: vocab type       = BPE
0.00.038.673 I print_info: n_vocab          = 50304
0.00.038.673 I print_info: n_merges         = 50009
0.00.038.674 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.674 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.674 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.674 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.674 I print_info: LF token         = 187 'Ċ'
0.00.038.675 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.675 I print_info: max token length = 1024
0.00.038.675 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.496 I load_tensors: offloading output layer to GPU
0.00.441.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.533 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.535 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.443.086 I llama_init_from_model: n_seq_max     = 1
0.00.443.088 I llama_init_from_model: n_ctx         = 128
0.00.443.089 I llama_init_from_model: n_ctx_per_seq = 128
0.00.443.089 I llama_init_from_model: n_batch       = 128
0.00.443.090 I llama_init_from_model: n_ubatch      = 128
0.00.443.090 I llama_init_from_model: flash_attn    = 0
0.00.443.092 I llama_init_from_model: freq_base     = 10000.0
0.00.443.092 I llama_init_from_model: freq_scale    = 1
0.00.443.093 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.443.095 I ggml_metal_init: allocating
0.00.443.186 I ggml_metal_init: found device: Apple M4
0.00.443.202 I ggml_metal_init: picking default device: Apple M4
0.00.445.073 I ggml_metal_init: using embedded metal library
0.00.451.283 I ggml_metal_init: GPU name:   Apple M4
0.00.451.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.295 I ggml_metal_init: simdgroup reduction   = true
0.00.451.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.295 I ggml_metal_init: has residency sets    = true
0.00.451.296 I ggml_metal_init: has bfloat            = true
0.00.451.296 I ggml_metal_init: use bfloat            = true
0.00.451.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.469.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.581 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.473.589 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.473.639 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.476.850 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.476.852 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.476.852 I llama_init_from_model: graph nodes  = 967
0.00.476.853 I llama_init_from_model: graph splits = 2
0.00.476.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.476.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.164 I 
0.00.507.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.233 I perplexity: tokenizing the input ..
0.00.514.448 I perplexity: tokenization took 7.212 ms
0.00.514.456 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.394 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.738 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.762 I llama_perf_context_print:        load time =     498.10 ms
0.00.654.763 I llama_perf_context_print: prompt eval time =     138.53 ms /   128 tokens (    1.08 ms per token,   923.95 tokens per second)
0.00.654.764 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.764 I llama_perf_context_print:       total time =     147.60 ms /   129 tokens
0.00.655.103 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.970 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.985 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.659 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.659 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.659 I llama_model_loader: - type  f32:  194 tensors
0.00.025.660 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.660 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.660 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.661 I print_info: file format = GGUF V3 (latest)
0.00.025.662 I print_info: file type   = Q4_K - Medium
0.00.025.663 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.032 I load: special tokens cache size = 25
0.00.040.100 I load: token to piece cache size = 0.2984 MB
0.00.040.104 I print_info: arch             = gptneox
0.00.040.104 I print_info: vocab_only       = 0
0.00.040.105 I print_info: n_ctx_train      = 2048
0.00.040.105 I print_info: n_embd           = 2048
0.00.040.105 I print_info: n_layer          = 24
0.00.040.109 I print_info: n_head           = 16
0.00.040.110 I print_info: n_head_kv        = 16
0.00.040.110 I print_info: n_rot            = 32
0.00.040.110 I print_info: n_swa            = 0
0.00.040.111 I print_info: n_embd_head_k    = 128
0.00.040.112 I print_info: n_embd_head_v    = 128
0.00.040.113 I print_info: n_gqa            = 1
0.00.040.113 I print_info: n_embd_k_gqa     = 2048
0.00.040.114 I print_info: n_embd_v_gqa     = 2048
0.00.040.115 I print_info: f_norm_eps       = 1.0e-05
0.00.040.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.116 I print_info: f_logit_scale    = 0.0e+00
0.00.040.117 I print_info: n_ff             = 8192
0.00.040.117 I print_info: n_expert         = 0
0.00.040.117 I print_info: n_expert_used    = 0
0.00.040.117 I print_info: causal attn      = 1
0.00.040.117 I print_info: pooling type     = 0
0.00.040.117 I print_info: rope type        = 2
0.00.040.118 I print_info: rope scaling     = linear
0.00.040.118 I print_info: freq_base_train  = 10000.0
0.00.040.118 I print_info: freq_scale_train = 1
0.00.040.119 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.119 I print_info: rope_finetuned   = unknown
0.00.040.119 I print_info: ssm_d_conv       = 0
0.00.040.119 I print_info: ssm_d_inner      = 0
0.00.040.119 I print_info: ssm_d_state      = 0
0.00.040.120 I print_info: ssm_dt_rank      = 0
0.00.040.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.120 I print_info: model type       = 1.4B
0.00.040.121 I print_info: model params     = 1.41 B
0.00.040.121 I print_info: general.name     = 1.4B
0.00.040.121 I print_info: vocab type       = BPE
0.00.040.122 I print_info: n_vocab          = 50304
0.00.040.122 I print_info: n_merges         = 50009
0.00.040.123 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: LF token         = 187 'Ċ'
0.00.040.124 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.125 I print_info: max token length = 1024
0.00.040.126 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.518.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.518.126 I load_tensors: offloading output layer to GPU
0.00.518.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.156 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.161 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.519.744 I llama_init_from_model: n_seq_max     = 1
0.00.519.747 I llama_init_from_model: n_ctx         = 128
0.00.519.748 I llama_init_from_model: n_ctx_per_seq = 128
0.00.519.748 I llama_init_from_model: n_batch       = 128
0.00.519.748 I llama_init_from_model: n_ubatch      = 128
0.00.519.749 I llama_init_from_model: flash_attn    = 0
0.00.519.751 I llama_init_from_model: freq_base     = 10000.0
0.00.519.751 I llama_init_from_model: freq_scale    = 1
0.00.519.752 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.519.754 I ggml_metal_init: allocating
0.00.519.805 I ggml_metal_init: found device: Apple M4
0.00.519.817 I ggml_metal_init: picking default device: Apple M4
0.00.521.579 I ggml_metal_init: using embedded metal library
0.00.528.366 I ggml_metal_init: GPU name:   Apple M4
0.00.528.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.373 I ggml_metal_init: simdgroup reduction   = true
0.00.528.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.374 I ggml_metal_init: has residency sets    = true
0.00.528.374 I ggml_metal_init: has bfloat            = true
0.00.528.374 I ggml_metal_init: use bfloat            = true
0.00.528.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.731 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.181 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.549.184 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.549.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.552.449 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.552.450 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.552.451 I llama_init_from_model: graph nodes  = 967
0.00.552.451 I llama_init_from_model: graph splits = 2
0.00.552.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.552.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.851 I 
0.00.582.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.930 I perplexity: tokenizing the input ..
0.00.588.907 I perplexity: tokenization took 5.975 ms
0.00.588.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.720 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.058 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.085 I llama_perf_context_print:        load time =     572.92 ms
0.00.732.086 I llama_perf_context_print: prompt eval time =     141.58 ms /   128 tokens (    1.11 ms per token,   904.10 tokens per second)
0.00.732.087 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.087 I llama_perf_context_print:       total time =     149.24 ms /   129 tokens
0.00.732.515 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.078s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.708 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.850 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.860 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.860 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.866 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.866 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.521 I llama_model_loader: - type  f32:  194 tensors
0.00.024.521 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.521 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.522 I print_info: file format = GGUF V3 (latest)
0.00.024.528 I print_info: file type   = Q5_K - Medium
0.00.024.529 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.528 I load: special tokens cache size = 25
0.00.038.628 I load: token to piece cache size = 0.2984 MB
0.00.038.632 I print_info: arch             = gptneox
0.00.038.632 I print_info: vocab_only       = 0
0.00.038.632 I print_info: n_ctx_train      = 2048
0.00.038.632 I print_info: n_embd           = 2048
0.00.038.633 I print_info: n_layer          = 24
0.00.038.636 I print_info: n_head           = 16
0.00.038.636 I print_info: n_head_kv        = 16
0.00.038.637 I print_info: n_rot            = 32
0.00.038.637 I print_info: n_swa            = 0
0.00.038.637 I print_info: n_embd_head_k    = 128
0.00.038.637 I print_info: n_embd_head_v    = 128
0.00.038.638 I print_info: n_gqa            = 1
0.00.038.639 I print_info: n_embd_k_gqa     = 2048
0.00.038.639 I print_info: n_embd_v_gqa     = 2048
0.00.038.640 I print_info: f_norm_eps       = 1.0e-05
0.00.038.640 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.641 I print_info: f_logit_scale    = 0.0e+00
0.00.038.642 I print_info: n_ff             = 8192
0.00.038.642 I print_info: n_expert         = 0
0.00.038.642 I print_info: n_expert_used    = 0
0.00.038.643 I print_info: causal attn      = 1
0.00.038.643 I print_info: pooling type     = 0
0.00.038.643 I print_info: rope type        = 2
0.00.038.643 I print_info: rope scaling     = linear
0.00.038.643 I print_info: freq_base_train  = 10000.0
0.00.038.644 I print_info: freq_scale_train = 1
0.00.038.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.644 I print_info: rope_finetuned   = unknown
0.00.038.644 I print_info: ssm_d_conv       = 0
0.00.038.644 I print_info: ssm_d_inner      = 0
0.00.038.645 I print_info: ssm_d_state      = 0
0.00.038.645 I print_info: ssm_dt_rank      = 0
0.00.038.646 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.646 I print_info: model type       = 1.4B
0.00.038.647 I print_info: model params     = 1.41 B
0.00.038.647 I print_info: general.name     = 1.4B
0.00.038.648 I print_info: vocab type       = BPE
0.00.038.649 I print_info: n_vocab          = 50304
0.00.038.650 I print_info: n_merges         = 50009
0.00.038.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: LF token         = 187 'Ċ'
0.00.038.651 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: max token length = 1024
0.00.038.652 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.003 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.007 I load_tensors: offloading output layer to GPU
0.00.590.008 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.031 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.032 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.582 I llama_init_from_model: n_seq_max     = 1
0.00.591.584 I llama_init_from_model: n_ctx         = 128
0.00.591.585 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.585 I llama_init_from_model: n_batch       = 128
0.00.591.586 I llama_init_from_model: n_ubatch      = 128
0.00.591.586 I llama_init_from_model: flash_attn    = 0
0.00.591.587 I llama_init_from_model: freq_base     = 10000.0
0.00.591.588 I llama_init_from_model: freq_scale    = 1
0.00.591.588 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.590 I ggml_metal_init: allocating
0.00.591.612 I ggml_metal_init: found device: Apple M4
0.00.591.621 I ggml_metal_init: picking default device: Apple M4
0.00.593.039 I ggml_metal_init: using embedded metal library
0.00.599.026 I ggml_metal_init: GPU name:   Apple M4
0.00.599.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.031 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.031 I ggml_metal_init: simdgroup reduction   = true
0.00.599.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.032 I ggml_metal_init: has residency sets    = true
0.00.599.032 I ggml_metal_init: has bfloat            = true
0.00.599.032 I ggml_metal_init: use bfloat            = true
0.00.599.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.013 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.499 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.507 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.551 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.686 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.688 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.689 I llama_init_from_model: graph nodes  = 967
0.00.622.689 I llama_init_from_model: graph splits = 2
0.00.622.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.084 I 
0.00.657.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.159 I perplexity: tokenizing the input ..
0.00.663.536 I perplexity: tokenization took 6.374 ms
0.00.663.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.541 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.088 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.110 I llama_perf_context_print:        load time =     648.36 ms
0.00.802.112 I llama_perf_context_print: prompt eval time =     136.15 ms /   128 tokens (    1.06 ms per token,   940.17 tokens per second)
0.00.802.113 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.113 I llama_perf_context_print:       total time =     145.03 ms /   129 tokens
0.00.802.467 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.077s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.904 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.917 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.924 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.606 I llama_model_loader: - type  f32:  194 tensors
0.00.028.607 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.608 I print_info: file format = GGUF V3 (latest)
0.00.028.612 I print_info: file type   = Q6_K
0.00.028.613 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.500 I load: special tokens cache size = 25
0.00.042.631 I load: token to piece cache size = 0.2984 MB
0.00.042.637 I print_info: arch             = gptneox
0.00.042.637 I print_info: vocab_only       = 0
0.00.042.642 I print_info: n_ctx_train      = 2048
0.00.042.642 I print_info: n_embd           = 2048
0.00.042.643 I print_info: n_layer          = 24
0.00.042.647 I print_info: n_head           = 16
0.00.042.647 I print_info: n_head_kv        = 16
0.00.042.648 I print_info: n_rot            = 32
0.00.042.648 I print_info: n_swa            = 0
0.00.042.648 I print_info: n_embd_head_k    = 128
0.00.042.648 I print_info: n_embd_head_v    = 128
0.00.042.649 I print_info: n_gqa            = 1
0.00.042.650 I print_info: n_embd_k_gqa     = 2048
0.00.042.650 I print_info: n_embd_v_gqa     = 2048
0.00.042.651 I print_info: f_norm_eps       = 1.0e-05
0.00.042.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.652 I print_info: f_logit_scale    = 0.0e+00
0.00.042.653 I print_info: n_ff             = 8192
0.00.042.653 I print_info: n_expert         = 0
0.00.042.653 I print_info: n_expert_used    = 0
0.00.042.653 I print_info: causal attn      = 1
0.00.042.654 I print_info: pooling type     = 0
0.00.042.654 I print_info: rope type        = 2
0.00.042.654 I print_info: rope scaling     = linear
0.00.042.654 I print_info: freq_base_train  = 10000.0
0.00.042.655 I print_info: freq_scale_train = 1
0.00.042.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.655 I print_info: rope_finetuned   = unknown
0.00.042.655 I print_info: ssm_d_conv       = 0
0.00.042.656 I print_info: ssm_d_inner      = 0
0.00.042.656 I print_info: ssm_d_state      = 0
0.00.042.656 I print_info: ssm_dt_rank      = 0
0.00.042.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.656 I print_info: model type       = 1.4B
0.00.042.656 I print_info: model params     = 1.41 B
0.00.042.657 I print_info: general.name     = 1.4B
0.00.042.657 I print_info: vocab type       = BPE
0.00.042.657 I print_info: n_vocab          = 50304
0.00.042.658 I print_info: n_merges         = 50009
0.00.042.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.658 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.659 I print_info: LF token         = 187 'Ċ'
0.00.042.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.659 I print_info: max token length = 1024
0.00.042.660 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.085 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.089 I load_tensors: offloading output layer to GPU
0.00.334.090 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.118 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.334.120 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.335.336 I llama_init_from_model: n_seq_max     = 1
0.00.335.339 I llama_init_from_model: n_ctx         = 128
0.00.335.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.335.339 I llama_init_from_model: n_batch       = 128
0.00.335.340 I llama_init_from_model: n_ubatch      = 128
0.00.335.340 I llama_init_from_model: flash_attn    = 0
0.00.335.341 I llama_init_from_model: freq_base     = 10000.0
0.00.335.342 I llama_init_from_model: freq_scale    = 1
0.00.335.343 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.335.345 I ggml_metal_init: allocating
0.00.335.381 I ggml_metal_init: found device: Apple M4
0.00.335.391 I ggml_metal_init: picking default device: Apple M4
0.00.336.820 I ggml_metal_init: using embedded metal library
0.00.342.576 I ggml_metal_init: GPU name:   Apple M4
0.00.342.579 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.580 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.581 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.581 I ggml_metal_init: simdgroup reduction   = true
0.00.342.582 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.582 I ggml_metal_init: has residency sets    = true
0.00.342.582 I ggml_metal_init: has bfloat            = true
0.00.342.582 I ggml_metal_init: use bfloat            = true
0.00.342.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.358.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.362.175 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.362.178 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.362.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.365.313 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.365.315 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.365.315 I llama_init_from_model: graph nodes  = 967
0.00.365.315 I llama_init_from_model: graph splits = 2
0.00.365.319 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.365.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.507 I 
0.00.404.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.570 I perplexity: tokenizing the input ..
0.00.410.469 I perplexity: tokenization took 5.896 ms
0.00.410.481 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.541.855 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.543.204 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.543.229 I llama_perf_context_print:        load time =     391.54 ms
0.00.543.229 I llama_perf_context_print: prompt eval time =     130.48 ms /   128 tokens (    1.02 ms per token,   981.00 tokens per second)
0.00.543.230 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.543.230 I llama_perf_context_print:       total time =     138.73 ms /   129 tokens
0.00.543.611 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.075s
sys	0m0.105s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.274 I build: 4734 (f7b1116a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.349 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.354 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.362 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.362 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.425 I llama_model_loader: - type  f32:  194 tensors
0.00.053.425 I llama_model_loader: - type  f16:   98 tensors
0.00.053.426 I print_info: file format = GGUF V3 (latest)
0.00.053.427 I print_info: file type   = all F32 (guessed)
0.00.053.428 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.981 I load: special tokens cache size = 25
0.00.074.087 I load: token to piece cache size = 0.2984 MB
0.00.074.090 I print_info: arch             = gptneox
0.00.074.090 I print_info: vocab_only       = 0
0.00.074.091 I print_info: n_ctx_train      = 2048
0.00.074.091 I print_info: n_embd           = 2048
0.00.074.091 I print_info: n_layer          = 24
0.00.074.094 I print_info: n_head           = 16
0.00.074.095 I print_info: n_head_kv        = 16
0.00.074.095 I print_info: n_rot            = 32
0.00.074.096 I print_info: n_swa            = 0
0.00.074.096 I print_info: n_embd_head_k    = 128
0.00.074.096 I print_info: n_embd_head_v    = 128
0.00.074.097 I print_info: n_gqa            = 1
0.00.074.098 I print_info: n_embd_k_gqa     = 2048
0.00.074.099 I print_info: n_embd_v_gqa     = 2048
0.00.074.100 I print_info: f_norm_eps       = 1.0e-05
0.00.074.100 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.100 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.101 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.101 I print_info: f_logit_scale    = 0.0e+00
0.00.074.102 I print_info: n_ff             = 8192
0.00.074.102 I print_info: n_expert         = 0
0.00.074.102 I print_info: n_expert_used    = 0
0.00.074.102 I print_info: causal attn      = 1
0.00.074.102 I print_info: pooling type     = 0
0.00.074.102 I print_info: rope type        = 2
0.00.074.103 I print_info: rope scaling     = linear
0.00.074.103 I print_info: freq_base_train  = 10000.0
0.00.074.103 I print_info: freq_scale_train = 1
0.00.074.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.104 I print_info: rope_finetuned   = unknown
0.00.074.104 I print_info: ssm_d_conv       = 0
0.00.074.104 I print_info: ssm_d_inner      = 0
0.00.074.104 I print_info: ssm_d_state      = 0
0.00.074.105 I print_info: ssm_dt_rank      = 0
0.00.074.106 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.106 I print_info: model type       = 1.4B
0.00.074.106 I print_info: model params     = 1.41 B
0.00.074.106 I print_info: general.name     = 1.4B
0.00.074.107 I print_info: vocab type       = BPE
0.00.074.107 I print_info: n_vocab          = 50304
0.00.074.107 I print_info: n_merges         = 50009
0.00.074.108 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.110 I print_info: LF token         = 187 'Ċ'
0.00.074.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.110 I print_info: max token length = 1024
0.00.074.111 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.287.607 I load_tensors: offloading 24 repeating layers to GPU
0.01.287.611 I load_tensors: offloading output layer to GPU
0.01.287.611 I load_tensors: offloaded 25/25 layers to GPU
0.01.287.634 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.287.636 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.288.500 I llama_init_from_model: n_seq_max     = 1
0.01.288.501 I llama_init_from_model: n_ctx         = 128
0.01.288.501 I llama_init_from_model: n_ctx_per_seq = 128
0.01.288.501 I llama_init_from_model: n_batch       = 128
0.01.288.502 I llama_init_from_model: n_ubatch      = 128
0.01.288.502 I llama_init_from_model: flash_attn    = 0
0.01.288.502 I llama_init_from_model: freq_base     = 10000.0
0.01.288.503 I llama_init_from_model: freq_scale    = 1
0.01.288.503 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.288.504 I ggml_metal_init: allocating
0.01.288.540 I ggml_metal_init: found device: Apple M4
0.01.288.546 I ggml_metal_init: picking default device: Apple M4
0.01.289.531 I ggml_metal_init: using embedded metal library
0.01.293.313 I ggml_metal_init: GPU name:   Apple M4
0.01.293.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.293.316 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.293.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.293.317 I ggml_metal_init: simdgroup reduction   = true
0.01.293.317 I ggml_metal_init: simdgroup matrix mul. = true
0.01.293.317 I ggml_metal_init: has residency sets    = true
0.01.293.317 I ggml_metal_init: has bfloat            = true
0.01.293.317 I ggml_metal_init: use bfloat            = true
0.01.293.318 I ggml_metal_init: hasUnifiedMemory      = true
0.01.293.319 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.304.357 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.306.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.306.178 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.306.204 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.307.887 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.307.889 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.307.889 I llama_init_from_model: graph nodes  = 967
0.01.307.889 I llama_init_from_model: graph splits = 2
0.01.307.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.307.891 I 
0.01.307.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.307.918 I compute_imatrix: tokenizing the input ..
0.01.311.974 I compute_imatrix: tokenization took 4.055 ms
0.01.311.976 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.578.170 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.580.920 I llama_perf_context_print:        load time =    1556.23 ms
0.01.580.921 I llama_perf_context_print: prompt eval time =     264.44 ms /   128 tokens (    2.07 ms per token,   484.05 tokens per second)
0.01.580.922 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.580.922 I llama_perf_context_print:       total time =    1558.98 ms /   129 tokens
0.01.581.477 I ggml_metal_free: deallocating

real	0m1.764s
user	0m0.126s
sys	0m0.245s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4734 (f7b1116a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c607ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c6085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c608ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c609150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c609cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c60a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c60a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c60adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c60b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c60b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c60bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c60c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c60cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c60d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c60dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c60e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c60ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c60f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c60fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c6119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c612110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c6123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c6129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c613650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c613b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c6142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c6145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c614e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c615f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c6168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c616d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c617200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c6176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c617b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c6182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c6188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c6197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c61a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c61aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c61b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c61b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c61bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c61c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c61c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c61cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c61d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c61e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c61e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c61ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c61eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c61f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c6205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c6213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c6242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c624d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c6252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c625d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c6262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c6272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c6282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c628800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c6292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c6297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c6194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c62a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c62a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c62aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c62b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c62b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c62bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c62c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c62c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c62ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c62d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c62d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c62de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c62e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c62e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c62edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c62f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c62f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c6304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c6320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c6337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c6373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c6381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c63a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c63bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c63c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c63d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c63d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c63e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c63ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c63f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c63fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c6423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c6448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c6451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c6465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c646b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c647320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c648d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c6491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c6494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c64a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c64a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c64ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c64b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c64b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c64be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c64c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c64c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c64ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c64d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c64d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c64de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c64e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c64e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c64ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c64f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c64f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c64fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c6508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c6562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c6572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c6582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c6592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c65a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c65a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c65ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c65b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c65b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c65bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c65c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c65c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c65cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c65d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c65d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c65dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c65e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c65e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c65ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c65f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c65f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c65fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c661160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c661aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c661f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c6623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c6640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c6647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c664ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c6651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c6659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c665c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c666270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.716.113 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c665f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c647bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c6475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c61b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c61acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c61d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c612690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c619180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c61a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c61a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c611690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c61d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c629f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c665470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c614870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c614b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c64a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c648810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c612ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c612f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c613220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c6666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c666c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c6671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c667490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c667750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c667a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c668250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c668510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c6687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c668a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c668d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c669010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c6692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c669590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c669850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c669b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c669dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c66a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c66a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c66a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c66a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c66ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c66ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c66b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c66b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c66b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c66b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c66bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c66bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c66c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c66c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c66c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c66c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c66cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c66cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c66d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c66d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c66d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c66da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c66dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c66dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c66e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c66e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c66e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c66ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c66ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c66f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c66f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c66f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c66f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c66fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c66fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c6700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c670390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c670650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c670910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c670bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c670e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c671150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c671410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c6716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c671990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c671c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c671f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c6721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c672490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c672750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c672a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c672cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c672f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c673250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c673510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c6737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c673a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c673d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c674010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c6742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c674590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c674850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c674b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c674dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c675090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c675350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c675610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c6758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c675b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c675e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c676110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c6763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c676690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c676950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c676c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c676ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c677190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c677450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c677710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c6779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c677c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c677f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c678210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c6784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c678790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c678a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c678d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c678fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c679290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c679550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c679810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c679ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c679d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c67a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c67a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c67a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c67a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c67ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c67ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c67b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c67b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c67b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c67b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c67bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c67be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c67c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c67c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c67c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c67c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c67cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c67cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c67d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c67d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c67d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c67da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c67dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c67df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c67e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c67e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c67e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c67ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c67ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c67f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c67f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c67f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c67f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c67fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c67fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c680090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c680350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c680610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c6808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c680b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c680e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c681110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c6813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c681690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c681950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c681c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c681ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c682190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c682450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c682710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c6829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c682f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c683210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c6834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c683790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c683a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c683d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c683fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c684290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c684550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c684810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c684ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c684d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c685050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c685850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c685b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c685fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c686450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c6868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c6870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c687360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c687620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c687a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c687f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c688370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c6887e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c688c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c6890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c689530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c6899a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c689e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c68a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c68a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c68ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c68afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c68b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c68b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c68bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c68c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c68c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c68ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c68cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c68d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c68d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c68dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c68e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c68e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c68e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c68edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c68f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c68f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c68fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c68ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c690420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c690890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c690d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c691170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c6915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c691a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c691ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c692330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c6927a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c692c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c693080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c6934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c693960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c693dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c694240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c6946b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c694b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c694f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c695400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c695870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c695ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c696150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c6965c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c696a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c696ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c697310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c697780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c697bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c698060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c6984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c698940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c698db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c699220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c699690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c699b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c699f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c69a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c69a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c69acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c69b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c69be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c69c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c69cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c69cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c69d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c69da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c69e010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x108f1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108f1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108f1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x108f1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108f1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x108f1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108f1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108f1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x108f1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108f1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108f1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108f1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108f1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108f1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108f1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108f1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108f20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108f20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108f20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108f21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108f214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108f21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108f21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108f22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108f226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108f22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108f22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108f233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108f23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108f241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108f24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108f24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108f24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108f251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108f25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108f25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108f25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108f263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108f3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108f3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108f40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f42140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f4bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108f4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108f4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108f4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108f4e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108f4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108f4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108f4f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108f4f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108f4f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108f4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108f501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108f50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108f50ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108f50f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108f513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108f51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108f520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108f52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108f529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108f52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108f532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108f53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108f53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108f54000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108f54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108f548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108f54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108f551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108f55630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108f55aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108f56510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108f56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108f57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108f57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108f57d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108f581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108f587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108f58db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.776s
user	0m0.278s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4734 (f7b1116a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f00a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f00ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f00b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f00b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f00bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f00c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f00ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f00d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f00d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f00daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f00dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f00e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f00f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f00f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f00ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f0106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f010e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f011530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f011c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f012420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f012b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f013260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f013980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f014220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f014940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f014c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f015210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f015e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f0163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f016680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f016b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f016de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f017670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f017bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f017e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f018310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f0187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f018c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f0190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f019590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f019a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f019ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f01a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f01a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f01aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f01b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f01b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f01c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f01c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f01d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f01d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f01de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f01e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f01ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f01f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f01f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f01f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f01fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f020660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f020920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f020dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f021700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f022040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f0224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f022980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f022e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f0232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f023760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f023c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f0240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f0245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f024b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f025090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f0255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f025b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f026080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f0265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f027070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f0275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f027b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f028060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f0285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f029050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f0295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f029af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f02a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f02a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f02aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f02b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f02b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f02bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f02c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f01bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f02c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f02cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f02d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f02d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f02dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f02e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f02e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f02ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f02f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f02f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f02fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f030160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f0306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f030c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f031150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f0315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f031a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f031f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f0323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f032870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f032d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f0331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f033650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f033af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f033f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f034430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f0348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f035210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f0356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f035b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f035ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f036490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f036930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f036dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f037270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f037710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f037bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f038050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f0384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f038990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f038e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f0392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f039770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f039c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f03a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f03a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f03a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f03ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f03b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f03b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f03bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f03c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f03c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f03ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f03cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f03d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f03d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f03dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f03e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f03e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f03eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f03ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f03f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f03f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f03fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f0401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f040670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f040fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f041450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f0418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f041d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f042230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f0426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f042b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f043010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f0434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f043950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f043df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f044290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f044730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f044bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f045070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f045510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f0459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f045e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f0462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f046790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f046c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f0470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f047570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f047a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f047eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f048350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f0488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f049b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f04a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f04a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f04ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f04b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f04ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f04bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f04c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f04c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f04d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f04d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f04da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f04dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f04e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f04ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f04f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f04fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f050100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f050650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f050ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f0510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f051640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f051b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f0520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f052630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f052b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f0530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f053620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f053b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f0540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f054610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f054b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f0550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f055600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f0560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f0565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f056b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f057090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f0575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f057b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f058080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f0585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f058b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f059070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f0595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f059b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f05a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f05a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f05ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f05b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f05b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f05baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f05c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f05c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f05cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f05d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f05d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f05dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f05e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f05e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f05eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f05f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f05f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f05fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f060000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f060550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f060aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f060ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f061490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f061930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f061dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f062270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f062710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f062bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f063050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f0634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f063990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f063e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f0642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f064770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f064c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f0650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f065550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f065aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f0661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f0668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f067000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f067720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f0679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f0681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f068490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f068aa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13df05690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13df05b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13df05f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13df063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13df06850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13df06cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13df07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13df075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13df07a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13df07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13df082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13df089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13df094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13df09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13df0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13df0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13df0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13df0b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13df0c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13df0c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13df0d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13df0d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13df0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13df0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13df0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13df0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13df0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13df0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13df0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13df0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13df103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13df108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13df10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13df11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13df11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13df11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13df11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13df121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13df12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13df12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13df12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13df133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13df13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13df13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13df140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13df14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13df149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13df14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13df152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13df15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13df15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13df16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13df16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13df168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13df16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13df171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13df17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13df17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13df180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13df18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13df18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13df18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13df19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13df196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13df19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13df19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13df1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13df1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13df1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13df1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13df1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13df1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13df1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13df1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13df1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13df1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13df1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13df1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13df1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13df1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13df1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13df1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13df1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13df1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13df1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13df1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13df1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13df20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13df205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13df20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13df20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13df21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13df21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13df21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13df22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13df224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13df22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13df22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13df23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13df23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13df23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13df23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13df243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13df24850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13df24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13df25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13df255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13df25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13df25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13df262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13df26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13df26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13df27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13df274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13df27920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13df27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13df28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13df28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13df28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13df28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13df293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13df29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13df29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13df2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13df2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13df2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13df2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13df2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13df2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13df2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13df2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13df2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13df2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13df2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13df2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13df2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13df2dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13df2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13df2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13df2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13df2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13df2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13df2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13df2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13df2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13df302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13df30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13df30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13df31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13df31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13df318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13df31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13df321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13df32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13df32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13df32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13df33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13df337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13df33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13df340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13df34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13df349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13df34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13df35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13df35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13df35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13df367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13df36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13df36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13df37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13df37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13df37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13df37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13df38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13df387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13df38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13df390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13df39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13df39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13df39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13df3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13df3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13df3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13df3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13df3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13df3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13df3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13df3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13df3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13df3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13df3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13df3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13df3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13df3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13df3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13df3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13df3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13df3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13df3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13df3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13df3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13df3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13df404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13df40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13df40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13df412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13df41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13df41bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13df420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13df425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13df43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13df43420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13df439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13df43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13df44560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13df44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13df450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13df456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13df45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13df46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13df467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13df46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13df47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13df47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13df47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13df484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13df48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13df49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13df495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13df49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13df4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13df4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13df4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13df4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13df4b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13df4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13df4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13df4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13df4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13df4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13df4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13df4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13df4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13df4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13df4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13df4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13df4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13df50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13df508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13df50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13df51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13df51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13df51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13df525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13df52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13df53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13df536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13df53ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13df54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13df54820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13df54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13df553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13df55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13df55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13df564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13df56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13df57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13df57620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13df57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13df58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13df58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13df58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13df58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13df59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13df59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13df59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13df5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13df5a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13df5ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13df5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13df5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13df5bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13df5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13df5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13df5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13df5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13df5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13df5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13df5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13df5ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13df5f410 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f04a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f04bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f068750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f049e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f04aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f01db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f01d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f01fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f04c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f014ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f01b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f01c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f01b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f01e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f01cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f013ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f02c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f067ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f0170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f04cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f04b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f0154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f015790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f015a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f068f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f0691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f069480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f069740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f069a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f069cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f069f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f06a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f06a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f06a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f06aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f06ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f06b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f06b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f06b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f06b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f06bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f06bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f06c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f06c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f06c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f06c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f06cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f06ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f06d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f06d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f06d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f06d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f06dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f06dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f06e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f06e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f06e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f06e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f06ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f06ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f06f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f06f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f06f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f06fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f06fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f06ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f070280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f070540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f070800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f070ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f070d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f071040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f071300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f0715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f071880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f071b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f071e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f0720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f072380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f072640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f072900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f072bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f072e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f073140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f073400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f0736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f073980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f073c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f073f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f0741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f074480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f074740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f074a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f074cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f074f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f075240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f075500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f0757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f075a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f075d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f076000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f0762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f076580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f076840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f076b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f076dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f077080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f077340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f077600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f0778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f077b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f077e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f078100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f0783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f078680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f078940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f078c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f078ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f079180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f079440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f079700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f0799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f079c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f079f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f07a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f07a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f07a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f07aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f07ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f07afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f07b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f07b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f07b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f07bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f07bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f07c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f07c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f07c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f07c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f07cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f07ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f07d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f07d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f07d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f07d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f07dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f07de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f07e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f07e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f07e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f07e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f07ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f07ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f07f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f07f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f07f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f07fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f07fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f07ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f080240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f080500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f0807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f080a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f080d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f081000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f0812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f081580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f081840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f081b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f081dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f082080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f082340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f082600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f0828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f082b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f082e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f083100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f0833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f083680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f083940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f083c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f083ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f084180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f084440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f084700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f0849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f084c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f084f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f085200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f0854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f085780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f085a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f085d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f085fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f086280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f086540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f086800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f086ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f086d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f087040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f087300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f0875c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f087880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f087b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f087e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f0880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f088380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f088640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f088a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f0891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f0894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f089770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f089be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f08a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f08a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f08a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f08ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f08b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f08b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f08baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f08bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f08c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f08c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f08ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f08d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f08d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f08da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f08de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f08e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f08e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f08ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f08f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f08f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f08f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f08fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f0901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f090660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f090ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f090f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f0913b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f091820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f091c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f092100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f092570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f0929e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f092e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f0932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f093730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f093ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f094010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f094480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f0948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f094d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f0951d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f095640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f095ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f095f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f096390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f096800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f096c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f0970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f097550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f0979c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f097e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f0982a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f098710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f098b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f098ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f099460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f0998d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f099d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f09a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f09a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f09aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f09af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f09b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f09b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f09bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f09c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f09c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f09c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f09ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f09d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f09dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f09e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f09ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f09f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f09f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f09fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f0a0160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.950s
user	0m0.230s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
