Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:49 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:305 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.602s
user	0m0.700s
sys	0m0.951s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target build_info
[  6%] Built target sha256
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target xxhash
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-cpu
[ 14%] Built target ggml-blas
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Linking CXX shared library libllama.dylib
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-run
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target llava
[ 34%] Built target test-c
[ 34%] Built target llama-quantize-stats
[ 34%] Built target llama-simple-chat
[ 34%] Built target llama-run
[ 34%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 35%] Built target common
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Built target test-chat-template
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Built target test-llama-grammar
[ 56%] Built target test-grammar-integration
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-barrier
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Built target llama-batched-bench
[ 69%] Built target llama-batched
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Built target test-json-schema-to-grammar
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-embedding
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-gguf-split
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Built target llama-eval-callback
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-infill
[ 76%] Built target llama-gritlm
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-bench
[ 79%] Built target llama-lookup
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Built target llama-lookahead
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Generating loading.html.hpp
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 86%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-perplexity
[ 86%] Generating index.html.hpp
[ 86%] Built target llama-lookup-create
[ 86%] Built target llama-lookup-merge
[ 86%] Built target llama-lookup-stats
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-quantize
[ 87%] Built target llama-cli
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 89%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-parallel
[ 89%] Built target llama-passkey
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-quantize
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-tokenize
[ 94%] Built target llama-retrieval
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-speculative
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.415s
user	0m5.192s
sys	0m8.314s

main: quantize time =  3902.12 ms
main:    total time =  3902.12 ms

main: quantize time =  2047.84 ms
main:    total time =  2047.84 ms

main: quantize time =  2320.96 ms
main:    total time =  2320.96 ms

main: quantize time =  3606.79 ms
main:    total time =  3606.79 ms

main: quantize time =  2437.97 ms
main:    total time =  2437.97 ms

main: quantize time =  5327.24 ms
main:    total time =  5327.24 ms

main: quantize time =  5585.25 ms
main:    total time =  5585.25 ms

main: quantize time =  6757.67 ms
main:    total time =  6757.67 ms

main: quantize time =  5782.88 ms
main:    total time =  5782.88 ms

main: quantize time =  4513.93 ms
main:    total time =  4513.93 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.110 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.231 I main: llama backend init
0.00.000.237 I main: load the model and apply lora adapter, if any
0.00.035.808 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.310 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.346 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.347 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.152 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.066.580 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.580 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.581 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.581 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.582 I llama_model_loader: - type  f32:  194 tensors
0.00.066.583 I llama_model_loader: - type  f16:   98 tensors
0.00.100.259 I llm_load_vocab: special tokens cache size = 25
0.00.107.377 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.107.380 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.107.380 I llm_load_print_meta: arch             = gptneox
0.00.107.381 I llm_load_print_meta: vocab type       = BPE
0.00.107.381 I llm_load_print_meta: n_vocab          = 50304
0.00.107.381 I llm_load_print_meta: n_merges         = 50009
0.00.107.381 I llm_load_print_meta: vocab_only       = 0
0.00.107.382 I llm_load_print_meta: n_ctx_train      = 2048
0.00.107.382 I llm_load_print_meta: n_embd           = 2048
0.00.107.382 I llm_load_print_meta: n_layer          = 24
0.00.107.385 I llm_load_print_meta: n_head           = 16
0.00.107.386 I llm_load_print_meta: n_head_kv        = 16
0.00.107.405 I llm_load_print_meta: n_rot            = 32
0.00.107.406 I llm_load_print_meta: n_swa            = 0
0.00.107.406 I llm_load_print_meta: n_embd_head_k    = 128
0.00.107.406 I llm_load_print_meta: n_embd_head_v    = 128
0.00.107.407 I llm_load_print_meta: n_gqa            = 1
0.00.107.408 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.107.408 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.107.409 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.107.409 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.107.409 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.107.409 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.107.410 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.107.410 I llm_load_print_meta: n_ff             = 8192
0.00.107.410 I llm_load_print_meta: n_expert         = 0
0.00.107.410 I llm_load_print_meta: n_expert_used    = 0
0.00.107.411 I llm_load_print_meta: causal attn      = 1
0.00.107.412 I llm_load_print_meta: pooling type     = 0
0.00.107.412 I llm_load_print_meta: rope type        = 2
0.00.107.412 I llm_load_print_meta: rope scaling     = linear
0.00.107.413 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.107.413 I llm_load_print_meta: freq_scale_train = 1
0.00.107.413 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.107.413 I llm_load_print_meta: rope_finetuned   = unknown
0.00.107.414 I llm_load_print_meta: ssm_d_conv       = 0
0.00.107.414 I llm_load_print_meta: ssm_d_inner      = 0
0.00.107.414 I llm_load_print_meta: ssm_d_state      = 0
0.00.107.414 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.107.414 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.107.423 I llm_load_print_meta: model type       = 1.4B
0.00.107.424 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.107.424 I llm_load_print_meta: model params     = 1.41 B
0.00.107.425 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.107.425 I llm_load_print_meta: general.name     = 1.4B
0.00.107.425 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.107.425 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.107.425 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.107.426 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.107.426 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.107.426 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.107.426 I llm_load_print_meta: max token length = 1024
0.00.110.003 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.110.003 I llm_load_tensors: offloading output layer to GPU
0.00.110.003 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.110.022 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.023 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.110.995 I llama_new_context_with_model: n_seq_max     = 1
0.00.110.996 I llama_new_context_with_model: n_ctx         = 2048
0.00.110.996 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.110.996 I llama_new_context_with_model: n_batch       = 2048
0.00.110.996 I llama_new_context_with_model: n_ubatch      = 512
0.00.110.997 I llama_new_context_with_model: flash_attn    = 0
0.00.110.997 I llama_new_context_with_model: freq_base     = 10000.0
0.00.110.997 I llama_new_context_with_model: freq_scale    = 1
0.00.110.998 I ggml_metal_init: allocating
0.00.111.001 I ggml_metal_init: found device: Apple M4
0.00.111.003 I ggml_metal_init: picking default device: Apple M4
0.00.111.670 I ggml_metal_init: using embedded metal library
0.00.122.234 I ggml_metal_init: GPU name:   Apple M4
0.00.122.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.122.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.122.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.122.236 I ggml_metal_init: simdgroup reduction   = true
0.00.122.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.122.237 I ggml_metal_init: has bfloat            = true
0.00.122.237 I ggml_metal_init: use bfloat            = true
0.00.122.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.122.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.167.850 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.167.857 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.167.878 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.168.875 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.168.877 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.168.878 I llama_new_context_with_model: graph nodes  = 967
0.00.168.878 I llama_new_context_with_model: graph splits = 2
0.00.168.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.244.813 I main: llama threadpool init, n_threads = 4
0.00.244.847 I 
0.00.244.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.244.884 I 
0.00.244.967 I sampler seed: 1234
0.00.244.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.244.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.244.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.244.997 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.085.257 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.02.085.258 I llama_perf_context_print:        load time =     208.99 ms
0.02.085.259 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.76 tokens per second)
0.02.085.259 I llama_perf_context_print:        eval time =    1793.56 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.085.260 I llama_perf_context_print:       total time =    1840.45 ms /    70 tokens
0.02.085.485 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.148s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.012 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.399 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.401 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.407 I llama_model_loader: - type  f32:  194 tensors
0.00.035.408 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.720 I llm_load_vocab: special tokens cache size = 25
0.00.064.876 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.879 I llm_load_print_meta: arch             = gptneox
0.00.064.880 I llm_load_print_meta: vocab type       = BPE
0.00.064.880 I llm_load_print_meta: n_vocab          = 50304
0.00.064.880 I llm_load_print_meta: n_merges         = 50009
0.00.064.880 I llm_load_print_meta: vocab_only       = 0
0.00.064.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.881 I llm_load_print_meta: n_embd           = 2048
0.00.064.883 I llm_load_print_meta: n_layer          = 24
0.00.064.887 I llm_load_print_meta: n_head           = 16
0.00.064.888 I llm_load_print_meta: n_head_kv        = 16
0.00.064.901 I llm_load_print_meta: n_rot            = 32
0.00.064.901 I llm_load_print_meta: n_swa            = 0
0.00.064.902 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.902 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.903 I llm_load_print_meta: n_gqa            = 1
0.00.064.903 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.908 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.909 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.910 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.911 I llm_load_print_meta: n_ff             = 8192
0.00.064.911 I llm_load_print_meta: n_expert         = 0
0.00.064.911 I llm_load_print_meta: n_expert_used    = 0
0.00.064.911 I llm_load_print_meta: causal attn      = 1
0.00.064.911 I llm_load_print_meta: pooling type     = 0
0.00.064.911 I llm_load_print_meta: rope type        = 2
0.00.064.912 I llm_load_print_meta: rope scaling     = linear
0.00.064.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.912 I llm_load_print_meta: freq_scale_train = 1
0.00.064.912 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.913 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.913 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.924 I llm_load_print_meta: model type       = 1.4B
0.00.064.924 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.925 I llm_load_print_meta: model params     = 1.41 B
0.00.064.925 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.925 I llm_load_print_meta: general.name     = 1.4B
0.00.064.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.927 I llm_load_print_meta: max token length = 1024
0.00.067.399 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.399 I llm_load_tensors: offloading output layer to GPU
0.00.067.400 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.411 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.412 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.377 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.378 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.378 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.378 I llama_new_context_with_model: n_batch       = 2048
0.00.068.379 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.379 I llama_new_context_with_model: flash_attn    = 0
0.00.068.379 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.379 I llama_new_context_with_model: freq_scale    = 1
0.00.068.380 I ggml_metal_init: allocating
0.00.068.382 I ggml_metal_init: found device: Apple M4
0.00.068.385 I ggml_metal_init: picking default device: Apple M4
0.00.069.082 I ggml_metal_init: using embedded metal library
0.00.071.644 I ggml_metal_init: GPU name:   Apple M4
0.00.071.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.647 I ggml_metal_init: simdgroup reduction   = true
0.00.071.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.647 I ggml_metal_init: has bfloat            = true
0.00.071.647 I ggml_metal_init: use bfloat            = true
0.00.071.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.124 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.147 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.260 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.262 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.262 I llama_new_context_with_model: graph nodes  = 967
0.00.108.262 I llama_new_context_with_model: graph splits = 2
0.00.108.278 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.182.314 I main: llama threadpool init, n_threads = 4
0.01.182.346 I 
0.01.182.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.182.378 I 
0.01.182.526 I sampler seed: 1234
0.01.182.531 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.182.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.182.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.182.569 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.279.831 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.02.279.832 I llama_perf_context_print:        load time =    1172.30 ms
0.02.279.833 I llama_perf_context_print: prompt eval time =      39.85 ms /     7 tokens (    5.69 ms per token,   175.68 tokens per second)
0.02.279.833 I llama_perf_context_print:        eval time =    1054.50 ms /    63 runs   (   16.74 ms per token,    59.74 tokens per second)
0.02.279.837 I llama_perf_context_print:       total time =    1097.52 ms /    70 tokens
0.02.280.035 I ggml_metal_free: deallocating

real	0m2.298s
user	0m0.115s
sys	0m0.210s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.016.553 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.521 I llama_model_loader: - type  f32:  194 tensors
0.00.043.521 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.521 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.257 I llm_load_vocab: special tokens cache size = 25
0.00.078.755 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.758 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.758 I llm_load_print_meta: arch             = gptneox
0.00.078.759 I llm_load_print_meta: vocab type       = BPE
0.00.078.759 I llm_load_print_meta: n_vocab          = 50304
0.00.078.759 I llm_load_print_meta: n_merges         = 50009
0.00.078.759 I llm_load_print_meta: vocab_only       = 0
0.00.078.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.760 I llm_load_print_meta: n_embd           = 2048
0.00.078.760 I llm_load_print_meta: n_layer          = 24
0.00.078.764 I llm_load_print_meta: n_head           = 16
0.00.078.766 I llm_load_print_meta: n_head_kv        = 16
0.00.078.779 I llm_load_print_meta: n_rot            = 32
0.00.078.779 I llm_load_print_meta: n_swa            = 0
0.00.078.779 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.779 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.780 I llm_load_print_meta: n_gqa            = 1
0.00.078.781 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.781 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.782 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.784 I llm_load_print_meta: n_ff             = 8192
0.00.078.784 I llm_load_print_meta: n_expert         = 0
0.00.078.785 I llm_load_print_meta: n_expert_used    = 0
0.00.078.785 I llm_load_print_meta: causal attn      = 1
0.00.078.785 I llm_load_print_meta: pooling type     = 0
0.00.078.785 I llm_load_print_meta: rope type        = 2
0.00.078.785 I llm_load_print_meta: rope scaling     = linear
0.00.078.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.786 I llm_load_print_meta: freq_scale_train = 1
0.00.078.787 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.798 I llm_load_print_meta: model type       = 1.4B
0.00.078.798 I llm_load_print_meta: model ftype      = Q4_0
0.00.078.799 I llm_load_print_meta: model params     = 1.41 B
0.00.078.799 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.078.799 I llm_load_print_meta: general.name     = 1.4B
0.00.078.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.800 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.801 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.801 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.801 I llm_load_print_meta: max token length = 1024
0.00.081.269 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.269 I llm_load_tensors: offloading output layer to GPU
0.00.081.270 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.281 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.283 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.082.406 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.407 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.407 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.407 I llama_new_context_with_model: n_batch       = 2048
0.00.082.407 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.408 I llama_new_context_with_model: flash_attn    = 0
0.00.082.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.408 I llama_new_context_with_model: freq_scale    = 1
0.00.082.409 I ggml_metal_init: allocating
0.00.082.412 I ggml_metal_init: found device: Apple M4
0.00.082.414 I ggml_metal_init: picking default device: Apple M4
0.00.083.191 I ggml_metal_init: using embedded metal library
0.00.086.830 I ggml_metal_init: GPU name:   Apple M4
0.00.086.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.833 I ggml_metal_init: simdgroup reduction   = true
0.00.086.834 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.834 I ggml_metal_init: has bfloat            = true
0.00.086.834 I ggml_metal_init: use bfloat            = true
0.00.086.835 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.836 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.647 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.660 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.684 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.718 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.720 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.720 I llama_new_context_with_model: graph nodes  = 967
0.00.125.721 I llama_new_context_with_model: graph splits = 2
0.00.125.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.609 I main: llama threadpool init, n_threads = 4
0.00.681.646 I 
0.00.681.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.674 I 
0.00.681.915 I sampler seed: 1234
0.00.681.919 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.955 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.966 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.364.224 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.364.224 I llama_perf_context_print:        load time =     665.05 ms
0.01.364.225 I llama_perf_context_print: prompt eval time =      45.83 ms /     7 tokens (    6.55 ms per token,   152.73 tokens per second)
0.01.364.226 I llama_perf_context_print:        eval time =     633.43 ms /    63 runs   (   10.05 ms per token,    99.46 tokens per second)
0.01.364.226 I llama_perf_context_print:       total time =     682.62 ms /    70 tokens
0.01.364.417 I ggml_metal_free: deallocating

real	0m1.382s
user	0m0.125s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.017.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.475 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.479 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.481 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.317 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.043.317 I llama_model_loader: - type  f32:  194 tensors
0.00.043.317 I llama_model_loader: - type q4_1:   97 tensors
0.00.043.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.580 I llm_load_vocab: special tokens cache size = 25
0.00.076.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.500 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.500 I llm_load_print_meta: arch             = gptneox
0.00.076.501 I llm_load_print_meta: vocab type       = BPE
0.00.076.501 I llm_load_print_meta: n_vocab          = 50304
0.00.076.501 I llm_load_print_meta: n_merges         = 50009
0.00.076.501 I llm_load_print_meta: vocab_only       = 0
0.00.076.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.502 I llm_load_print_meta: n_embd           = 2048
0.00.076.502 I llm_load_print_meta: n_layer          = 24
0.00.076.505 I llm_load_print_meta: n_head           = 16
0.00.076.506 I llm_load_print_meta: n_head_kv        = 16
0.00.076.519 I llm_load_print_meta: n_rot            = 32
0.00.076.519 I llm_load_print_meta: n_swa            = 0
0.00.076.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.520 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.520 I llm_load_print_meta: n_gqa            = 1
0.00.076.521 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.522 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.522 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.522 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.523 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.523 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.523 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.523 I llm_load_print_meta: n_ff             = 8192
0.00.076.524 I llm_load_print_meta: n_expert         = 0
0.00.076.524 I llm_load_print_meta: n_expert_used    = 0
0.00.076.525 I llm_load_print_meta: causal attn      = 1
0.00.076.527 I llm_load_print_meta: pooling type     = 0
0.00.076.527 I llm_load_print_meta: rope type        = 2
0.00.076.527 I llm_load_print_meta: rope scaling     = linear
0.00.076.528 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.528 I llm_load_print_meta: freq_scale_train = 1
0.00.076.528 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.528 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.529 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.529 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.529 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.529 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.530 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.539 I llm_load_print_meta: model type       = 1.4B
0.00.076.540 I llm_load_print_meta: model ftype      = Q4_1
0.00.076.540 I llm_load_print_meta: model params     = 1.41 B
0.00.076.541 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.076.541 I llm_load_print_meta: general.name     = 1.4B
0.00.076.541 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.541 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.542 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.542 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.542 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.076.542 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.544 I llm_load_print_meta: max token length = 1024
0.00.078.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.732 I llm_load_tensors: offloading output layer to GPU
0.00.078.732 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.743 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.078.744 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.079.711 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.712 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.712 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.712 I llama_new_context_with_model: n_batch       = 2048
0.00.079.712 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.713 I llama_new_context_with_model: flash_attn    = 0
0.00.079.713 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.713 I llama_new_context_with_model: freq_scale    = 1
0.00.079.714 I ggml_metal_init: allocating
0.00.079.720 I ggml_metal_init: found device: Apple M4
0.00.079.722 I ggml_metal_init: picking default device: Apple M4
0.00.080.296 I ggml_metal_init: using embedded metal library
0.00.082.866 I ggml_metal_init: GPU name:   Apple M4
0.00.082.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.868 I ggml_metal_init: simdgroup reduction   = true
0.00.082.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.869 I ggml_metal_init: has bfloat            = true
0.00.082.871 I ggml_metal_init: use bfloat            = true
0.00.082.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.544 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.554 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.573 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.574 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.574 I llama_new_context_with_model: graph nodes  = 967
0.00.117.574 I llama_new_context_with_model: graph splits = 2
0.00.117.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.438 I main: llama threadpool init, n_threads = 4
0.00.710.485 I 
0.00.710.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.513 I 
0.00.710.746 I sampler seed: 1234
0.00.710.750 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.761 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.761 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.761 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.442.648 I llama_perf_sampler_print:    sampling time =       1.04 ms /    71 runs   (    0.01 ms per token, 67942.58 tokens per second)
0.01.442.649 I llama_perf_context_print:        load time =     692.79 ms
0.01.442.650 I llama_perf_context_print: prompt eval time =      43.43 ms /     7 tokens (    6.20 ms per token,   161.18 tokens per second)
0.01.442.651 I llama_perf_context_print:        eval time =     685.63 ms /    63 runs   (   10.88 ms per token,    91.89 tokens per second)
0.01.442.652 I llama_perf_context_print:       total time =     732.21 ms /    70 tokens
0.01.442.838 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.120s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.669 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.165 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.048 I llama_model_loader: - type  f32:  194 tensors
0.00.026.049 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.049 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.603 I llm_load_vocab: special tokens cache size = 25
0.00.052.569 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.572 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.572 I llm_load_print_meta: arch             = gptneox
0.00.052.573 I llm_load_print_meta: vocab type       = BPE
0.00.052.573 I llm_load_print_meta: n_vocab          = 50304
0.00.052.573 I llm_load_print_meta: n_merges         = 50009
0.00.052.573 I llm_load_print_meta: vocab_only       = 0
0.00.052.574 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.574 I llm_load_print_meta: n_embd           = 2048
0.00.052.574 I llm_load_print_meta: n_layer          = 24
0.00.052.577 I llm_load_print_meta: n_head           = 16
0.00.052.577 I llm_load_print_meta: n_head_kv        = 16
0.00.052.589 I llm_load_print_meta: n_rot            = 32
0.00.052.589 I llm_load_print_meta: n_swa            = 0
0.00.052.589 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.589 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.590 I llm_load_print_meta: n_gqa            = 1
0.00.052.591 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.592 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.592 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.593 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.593 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.595 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.596 I llm_load_print_meta: n_ff             = 8192
0.00.052.596 I llm_load_print_meta: n_expert         = 0
0.00.052.596 I llm_load_print_meta: n_expert_used    = 0
0.00.052.597 I llm_load_print_meta: causal attn      = 1
0.00.052.599 I llm_load_print_meta: pooling type     = 0
0.00.052.599 I llm_load_print_meta: rope type        = 2
0.00.052.599 I llm_load_print_meta: rope scaling     = linear
0.00.052.599 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.600 I llm_load_print_meta: freq_scale_train = 1
0.00.052.600 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.600 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.600 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.600 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.610 I llm_load_print_meta: model type       = 1.4B
0.00.052.610 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.610 I llm_load_print_meta: model params     = 1.41 B
0.00.052.611 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.611 I llm_load_print_meta: general.name     = 1.4B
0.00.052.611 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.612 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.612 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.612 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.612 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.613 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.613 I llm_load_print_meta: max token length = 1024
0.00.054.166 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.166 I llm_load_tensors: offloading output layer to GPU
0.00.054.166 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.176 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.177 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.055 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.055 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.055 I llama_new_context_with_model: n_batch       = 2048
0.00.055.056 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.056 I llama_new_context_with_model: flash_attn    = 0
0.00.055.056 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.057 I llama_new_context_with_model: freq_scale    = 1
0.00.055.057 I ggml_metal_init: allocating
0.00.055.063 I ggml_metal_init: found device: Apple M4
0.00.055.065 I ggml_metal_init: picking default device: Apple M4
0.00.055.629 I ggml_metal_init: using embedded metal library
0.00.057.959 I ggml_metal_init: GPU name:   Apple M4
0.00.057.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.961 I ggml_metal_init: simdgroup reduction   = true
0.00.057.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.962 I ggml_metal_init: has bfloat            = true
0.00.057.962 I ggml_metal_init: use bfloat            = true
0.00.057.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.845 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.853 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.874 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.923 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.924 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.924 I llama_new_context_with_model: graph nodes  = 967
0.00.087.924 I llama_new_context_with_model: graph splits = 2
0.00.087.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.888.353 I main: llama threadpool init, n_threads = 4
0.00.888.389 I 
0.00.888.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.888.422 I 
0.00.888.659 I sampler seed: 1234
0.00.888.664 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.888.675 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.888.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.888.675 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.679.226 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.679.227 I llama_perf_context_print:        load time =     879.68 ms
0.01.679.227 I llama_perf_context_print: prompt eval time =      43.04 ms /     7 tokens (    6.15 ms per token,   162.64 tokens per second)
0.01.679.228 I llama_perf_context_print:        eval time =     744.55 ms /    63 runs   (   11.82 ms per token,    84.61 tokens per second)
0.01.679.229 I llama_perf_context_print:       total time =     790.88 ms /    70 tokens
0.01.679.422 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.016.547 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.561 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.573 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.574 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.039.818 I llama_model_loader: - type  f32:  194 tensors
0.00.039.819 I llama_model_loader: - type q5_1:   97 tensors
0.00.039.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.236 I llm_load_vocab: special tokens cache size = 25
0.00.075.572 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.576 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.576 I llm_load_print_meta: arch             = gptneox
0.00.075.576 I llm_load_print_meta: vocab type       = BPE
0.00.075.577 I llm_load_print_meta: n_vocab          = 50304
0.00.075.577 I llm_load_print_meta: n_merges         = 50009
0.00.075.577 I llm_load_print_meta: vocab_only       = 0
0.00.075.577 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.577 I llm_load_print_meta: n_embd           = 2048
0.00.075.578 I llm_load_print_meta: n_layer          = 24
0.00.075.580 I llm_load_print_meta: n_head           = 16
0.00.075.581 I llm_load_print_meta: n_head_kv        = 16
0.00.075.594 I llm_load_print_meta: n_rot            = 32
0.00.075.594 I llm_load_print_meta: n_swa            = 0
0.00.075.594 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.594 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.595 I llm_load_print_meta: n_gqa            = 1
0.00.075.596 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.597 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.597 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.598 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.598 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.598 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.598 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.604 I llm_load_print_meta: n_ff             = 8192
0.00.075.605 I llm_load_print_meta: n_expert         = 0
0.00.075.605 I llm_load_print_meta: n_expert_used    = 0
0.00.075.605 I llm_load_print_meta: causal attn      = 1
0.00.075.605 I llm_load_print_meta: pooling type     = 0
0.00.075.605 I llm_load_print_meta: rope type        = 2
0.00.075.606 I llm_load_print_meta: rope scaling     = linear
0.00.075.606 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.607 I llm_load_print_meta: freq_scale_train = 1
0.00.075.607 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.607 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.607 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.609 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.610 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.610 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.610 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.620 I llm_load_print_meta: model type       = 1.4B
0.00.075.621 I llm_load_print_meta: model ftype      = Q5_1
0.00.075.621 I llm_load_print_meta: model params     = 1.41 B
0.00.075.622 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.075.622 I llm_load_print_meta: general.name     = 1.4B
0.00.075.622 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.623 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.623 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.623 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.624 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.075.624 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.624 I llm_load_print_meta: max token length = 1024
0.00.078.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.221 I llm_load_tensors: offloading output layer to GPU
0.00.078.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.233 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.078.234 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.079.353 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.354 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.354 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.355 I llama_new_context_with_model: n_batch       = 2048
0.00.079.355 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.355 I llama_new_context_with_model: flash_attn    = 0
0.00.079.356 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.356 I llama_new_context_with_model: freq_scale    = 1
0.00.079.356 I ggml_metal_init: allocating
0.00.079.360 I ggml_metal_init: found device: Apple M4
0.00.079.362 I ggml_metal_init: picking default device: Apple M4
0.00.080.017 I ggml_metal_init: using embedded metal library
0.00.083.138 I ggml_metal_init: GPU name:   Apple M4
0.00.083.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.141 I ggml_metal_init: simdgroup reduction   = true
0.00.083.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.142 I ggml_metal_init: has bfloat            = true
0.00.083.142 I ggml_metal_init: use bfloat            = true
0.00.083.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.143 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.469 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.490 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.525 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.527 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.527 I llama_new_context_with_model: graph nodes  = 967
0.00.118.528 I llama_new_context_with_model: graph splits = 2
0.00.118.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.187 I main: llama threadpool init, n_threads = 4
0.00.886.225 I 
0.00.886.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.257 I 
0.00.886.473 I sampler seed: 1234
0.00.886.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.886.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.886.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.886.492 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.733.320 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.733.321 I llama_perf_context_print:        load time =     869.64 ms
0.01.733.322 I llama_perf_context_print: prompt eval time =      46.18 ms /     7 tokens (    6.60 ms per token,   151.59 tokens per second)
0.01.733.322 I llama_perf_context_print:        eval time =     797.67 ms /    63 runs   (   12.66 ms per token,    78.98 tokens per second)
0.01.733.323 I llama_perf_context_print:       total time =     847.14 ms /    70 tokens
0.01.733.509 I ggml_metal_free: deallocating

real	0m1.753s
user	0m0.124s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.029 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.009.616 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.189 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.190 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.863 I llama_model_loader: - type  f32:  194 tensors
0.00.025.863 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.863 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.746 I llm_load_vocab: special tokens cache size = 25
0.00.051.629 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.632 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.632 I llm_load_print_meta: arch             = gptneox
0.00.051.633 I llm_load_print_meta: vocab type       = BPE
0.00.051.633 I llm_load_print_meta: n_vocab          = 50304
0.00.051.633 I llm_load_print_meta: n_merges         = 50009
0.00.051.633 I llm_load_print_meta: vocab_only       = 0
0.00.051.634 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.634 I llm_load_print_meta: n_embd           = 2048
0.00.051.634 I llm_load_print_meta: n_layer          = 24
0.00.051.637 I llm_load_print_meta: n_head           = 16
0.00.051.637 I llm_load_print_meta: n_head_kv        = 16
0.00.051.649 I llm_load_print_meta: n_rot            = 32
0.00.051.650 I llm_load_print_meta: n_swa            = 0
0.00.051.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.651 I llm_load_print_meta: n_gqa            = 1
0.00.051.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.652 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.653 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.653 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.653 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.654 I llm_load_print_meta: n_ff             = 8192
0.00.051.655 I llm_load_print_meta: n_expert         = 0
0.00.051.655 I llm_load_print_meta: n_expert_used    = 0
0.00.051.655 I llm_load_print_meta: causal attn      = 1
0.00.051.655 I llm_load_print_meta: pooling type     = 0
0.00.051.655 I llm_load_print_meta: rope type        = 2
0.00.051.657 I llm_load_print_meta: rope scaling     = linear
0.00.051.658 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.658 I llm_load_print_meta: freq_scale_train = 1
0.00.051.658 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.658 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.658 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.660 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.660 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.660 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.660 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.670 I llm_load_print_meta: model type       = 1.4B
0.00.051.670 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.670 I llm_load_print_meta: model params     = 1.41 B
0.00.051.671 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.672 I llm_load_print_meta: general.name     = 1.4B
0.00.051.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: max token length = 1024
0.00.053.458 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.458 I llm_load_tensors: offloading output layer to GPU
0.00.053.458 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.469 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.470 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.359 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.360 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.360 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.360 I llama_new_context_with_model: n_batch       = 2048
0.00.054.360 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.361 I llama_new_context_with_model: flash_attn    = 0
0.00.054.361 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.361 I llama_new_context_with_model: freq_scale    = 1
0.00.054.362 I ggml_metal_init: allocating
0.00.054.365 I ggml_metal_init: found device: Apple M4
0.00.054.367 I ggml_metal_init: picking default device: Apple M4
0.00.054.930 I ggml_metal_init: using embedded metal library
0.00.057.239 I ggml_metal_init: GPU name:   Apple M4
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.242 I ggml_metal_init: simdgroup reduction   = true
0.00.057.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.242 I ggml_metal_init: has bfloat            = true
0.00.057.242 I ggml_metal_init: use bfloat            = true
0.00.057.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.975 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.994 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.119 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.121 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.121 I llama_new_context_with_model: graph nodes  = 967
0.00.087.121 I llama_new_context_with_model: graph splits = 2
0.00.087.134 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.412 I main: llama threadpool init, n_threads = 4
0.00.549.456 I 
0.00.549.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.499 I 
0.00.549.747 I sampler seed: 1234
0.00.549.753 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.793 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.797 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.232.096 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63620.07 tokens per second)
0.01.232.097 I llama_perf_context_print:        load time =     539.79 ms
0.01.232.098 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.78 tokens per second)
0.01.232.098 I llama_perf_context_print:        eval time =     639.57 ms /    63 runs   (   10.15 ms per token,    98.50 tokens per second)
0.01.232.099 I llama_perf_context_print:       total time =     682.69 ms /    70 tokens
0.01.232.278 I ggml_metal_free: deallocating

real	0m1.251s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.067 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.070 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.070 I llama_model_loader: - type  f32:  194 tensors
0.00.025.070 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.071 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.071 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.071 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.935 I llm_load_vocab: special tokens cache size = 25
0.00.051.932 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.934 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.935 I llm_load_print_meta: arch             = gptneox
0.00.051.935 I llm_load_print_meta: vocab type       = BPE
0.00.051.935 I llm_load_print_meta: n_vocab          = 50304
0.00.051.935 I llm_load_print_meta: n_merges         = 50009
0.00.051.936 I llm_load_print_meta: vocab_only       = 0
0.00.051.936 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.936 I llm_load_print_meta: n_embd           = 2048
0.00.051.936 I llm_load_print_meta: n_layer          = 24
0.00.051.939 I llm_load_print_meta: n_head           = 16
0.00.051.939 I llm_load_print_meta: n_head_kv        = 16
0.00.051.951 I llm_load_print_meta: n_rot            = 32
0.00.051.952 I llm_load_print_meta: n_swa            = 0
0.00.051.952 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.952 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.953 I llm_load_print_meta: n_gqa            = 1
0.00.051.953 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.954 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.955 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.955 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.955 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.955 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.956 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.956 I llm_load_print_meta: n_ff             = 8192
0.00.051.957 I llm_load_print_meta: n_expert         = 0
0.00.051.957 I llm_load_print_meta: n_expert_used    = 0
0.00.051.957 I llm_load_print_meta: causal attn      = 1
0.00.051.957 I llm_load_print_meta: pooling type     = 0
0.00.051.957 I llm_load_print_meta: rope type        = 2
0.00.051.957 I llm_load_print_meta: rope scaling     = linear
0.00.051.960 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.960 I llm_load_print_meta: freq_scale_train = 1
0.00.051.960 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.961 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.961 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.961 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.961 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.962 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.962 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.972 I llm_load_print_meta: model type       = 1.4B
0.00.051.972 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.973 I llm_load_print_meta: model params     = 1.41 B
0.00.051.973 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.973 I llm_load_print_meta: general.name     = 1.4B
0.00.051.973 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.975 I llm_load_print_meta: max token length = 1024
0.00.053.920 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.921 I llm_load_tensors: offloading output layer to GPU
0.00.053.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.931 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.932 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.859 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.860 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.860 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.860 I llama_new_context_with_model: n_batch       = 2048
0.00.054.860 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.860 I llama_new_context_with_model: flash_attn    = 0
0.00.054.861 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.861 I llama_new_context_with_model: freq_scale    = 1
0.00.054.861 I ggml_metal_init: allocating
0.00.054.867 I ggml_metal_init: found device: Apple M4
0.00.054.869 I ggml_metal_init: picking default device: Apple M4
0.00.055.404 I ggml_metal_init: using embedded metal library
0.00.057.754 I ggml_metal_init: GPU name:   Apple M4
0.00.057.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.756 I ggml_metal_init: simdgroup reduction   = true
0.00.057.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.756 I ggml_metal_init: has bfloat            = true
0.00.057.756 I ggml_metal_init: use bfloat            = true
0.00.057.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.023 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.029 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.062 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.063 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.063 I llama_new_context_with_model: graph nodes  = 967
0.00.088.064 I llama_new_context_with_model: graph splits = 2
0.00.088.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.090 I main: llama threadpool init, n_threads = 4
0.00.543.128 I 
0.00.543.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.172 I 
0.00.543.400 I sampler seed: 1234
0.00.543.404 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.461 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.462 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.358 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.289.358 I llama_perf_context_print:        load time =     532.31 ms
0.01.289.359 I llama_perf_context_print: prompt eval time =      40.51 ms /     7 tokens (    5.79 ms per token,   172.81 tokens per second)
0.01.289.360 I llama_perf_context_print:        eval time =     702.28 ms /    63 runs   (   11.15 ms per token,    89.71 tokens per second)
0.01.289.360 I llama_perf_context_print:       total time =     746.27 ms /    70 tokens
0.01.289.549 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.108s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.253 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.280 I llama_model_loader: - type  f32:  194 tensors
0.00.024.280 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.280 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.281 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.131 I llm_load_vocab: special tokens cache size = 25
0.00.050.933 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.935 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.936 I llm_load_print_meta: arch             = gptneox
0.00.050.936 I llm_load_print_meta: vocab type       = BPE
0.00.050.936 I llm_load_print_meta: n_vocab          = 50304
0.00.050.936 I llm_load_print_meta: n_merges         = 50009
0.00.050.937 I llm_load_print_meta: vocab_only       = 0
0.00.050.937 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.937 I llm_load_print_meta: n_embd           = 2048
0.00.050.937 I llm_load_print_meta: n_layer          = 24
0.00.050.940 I llm_load_print_meta: n_head           = 16
0.00.050.941 I llm_load_print_meta: n_head_kv        = 16
0.00.050.952 I llm_load_print_meta: n_rot            = 32
0.00.050.952 I llm_load_print_meta: n_swa            = 0
0.00.050.953 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.953 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.953 I llm_load_print_meta: n_gqa            = 1
0.00.050.954 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.955 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.955 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.956 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.956 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.956 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.956 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.957 I llm_load_print_meta: n_ff             = 8192
0.00.050.957 I llm_load_print_meta: n_expert         = 0
0.00.050.957 I llm_load_print_meta: n_expert_used    = 0
0.00.050.958 I llm_load_print_meta: causal attn      = 1
0.00.050.958 I llm_load_print_meta: pooling type     = 0
0.00.050.958 I llm_load_print_meta: rope type        = 2
0.00.050.958 I llm_load_print_meta: rope scaling     = linear
0.00.050.958 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.959 I llm_load_print_meta: freq_scale_train = 1
0.00.050.959 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.959 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.959 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.959 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.960 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.960 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.960 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.969 I llm_load_print_meta: model type       = 1.4B
0.00.050.969 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.969 I llm_load_print_meta: model params     = 1.41 B
0.00.050.970 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.970 I llm_load_print_meta: general.name     = 1.4B
0.00.050.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: max token length = 1024
0.00.052.558 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.558 I llm_load_tensors: offloading output layer to GPU
0.00.052.558 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.568 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.569 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.411 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.411 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.412 I llama_new_context_with_model: n_batch       = 2048
0.00.053.412 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.412 I llama_new_context_with_model: flash_attn    = 0
0.00.053.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.413 I llama_new_context_with_model: freq_scale    = 1
0.00.053.413 I ggml_metal_init: allocating
0.00.053.419 I ggml_metal_init: found device: Apple M4
0.00.053.421 I ggml_metal_init: picking default device: Apple M4
0.00.053.946 I ggml_metal_init: using embedded metal library
0.00.056.284 I ggml_metal_init: GPU name:   Apple M4
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.286 I ggml_metal_init: simdgroup reduction   = true
0.00.056.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.286 I ggml_metal_init: has bfloat            = true
0.00.056.286 I ggml_metal_init: use bfloat            = true
0.00.056.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.491 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.497 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.513 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.542 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.542 I llama_new_context_with_model: graph nodes  = 967
0.00.085.543 I llama_new_context_with_model: graph splits = 2
0.00.085.555 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.823 I main: llama threadpool init, n_threads = 4
0.00.628.869 I 
0.00.628.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.909 I 
0.00.629.148 I sampler seed: 1234
0.00.629.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.165 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.167 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.935 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.392.935 I llama_perf_context_print:        load time =     619.56 ms
0.01.392.936 I llama_perf_context_print: prompt eval time =      51.05 ms /     7 tokens (    7.29 ms per token,   137.12 tokens per second)
0.01.392.938 I llama_perf_context_print:        eval time =     709.80 ms /    63 runs   (   11.27 ms per token,    88.76 tokens per second)
0.01.392.938 I llama_perf_context_print:       total time =     764.12 ms /    70 tokens
0.01.393.141 I ggml_metal_free: deallocating

real	0m1.410s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.916 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.843 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.843 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.845 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.846 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.595 I llama_model_loader: - type  f32:  194 tensors
0.00.024.595 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.595 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.490 I llm_load_vocab: special tokens cache size = 25
0.00.050.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.408 I llm_load_print_meta: arch             = gptneox
0.00.050.408 I llm_load_print_meta: vocab type       = BPE
0.00.050.408 I llm_load_print_meta: n_vocab          = 50304
0.00.050.409 I llm_load_print_meta: n_merges         = 50009
0.00.050.409 I llm_load_print_meta: vocab_only       = 0
0.00.050.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.409 I llm_load_print_meta: n_embd           = 2048
0.00.050.409 I llm_load_print_meta: n_layer          = 24
0.00.050.412 I llm_load_print_meta: n_head           = 16
0.00.050.413 I llm_load_print_meta: n_head_kv        = 16
0.00.050.424 I llm_load_print_meta: n_rot            = 32
0.00.050.425 I llm_load_print_meta: n_swa            = 0
0.00.050.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.428 I llm_load_print_meta: n_gqa            = 1
0.00.050.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.432 I llm_load_print_meta: n_ff             = 8192
0.00.050.432 I llm_load_print_meta: n_expert         = 0
0.00.050.432 I llm_load_print_meta: n_expert_used    = 0
0.00.050.432 I llm_load_print_meta: causal attn      = 1
0.00.050.433 I llm_load_print_meta: pooling type     = 0
0.00.050.434 I llm_load_print_meta: rope type        = 2
0.00.050.434 I llm_load_print_meta: rope scaling     = linear
0.00.050.434 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.434 I llm_load_print_meta: freq_scale_train = 1
0.00.050.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.435 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.435 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.435 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.438 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.438 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.438 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.448 I llm_load_print_meta: model type       = 1.4B
0.00.050.449 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.449 I llm_load_print_meta: model params     = 1.41 B
0.00.050.449 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.450 I llm_load_print_meta: general.name     = 1.4B
0.00.050.450 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.450 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.450 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.450 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.450 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.451 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.451 I llm_load_print_meta: max token length = 1024
0.00.052.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.371 I llm_load_tensors: offloading output layer to GPU
0.00.052.372 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.382 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.383 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.284 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.285 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.285 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.285 I llama_new_context_with_model: n_batch       = 2048
0.00.053.285 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.286 I llama_new_context_with_model: flash_attn    = 0
0.00.053.286 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.286 I llama_new_context_with_model: freq_scale    = 1
0.00.053.287 I ggml_metal_init: allocating
0.00.053.290 I ggml_metal_init: found device: Apple M4
0.00.053.292 I ggml_metal_init: picking default device: Apple M4
0.00.053.841 I ggml_metal_init: using embedded metal library
0.00.056.137 I ggml_metal_init: GPU name:   Apple M4
0.00.056.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.140 I ggml_metal_init: simdgroup reduction   = true
0.00.056.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.140 I ggml_metal_init: has bfloat            = true
0.00.056.140 I ggml_metal_init: use bfloat            = true
0.00.056.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.696 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.701 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.721 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.711 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.712 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.712 I llama_new_context_with_model: graph nodes  = 967
0.00.085.713 I llama_new_context_with_model: graph splits = 2
0.00.085.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.986 I main: llama threadpool init, n_threads = 4
0.00.706.026 I 
0.00.706.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.060 I 
0.00.706.292 I sampler seed: 1234
0.00.706.298 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.340 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.345 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.345 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.980 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.553.981 I llama_perf_context_print:        load time =     697.06 ms
0.01.553.983 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.62 tokens per second)
0.01.553.984 I llama_perf_context_print:        eval time =     792.98 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.553.984 I llama_perf_context_print:       total time =     848.00 ms /    70 tokens
0.01.554.151 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.295 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.323 I llama_model_loader: - type  f32:  194 tensors
0.00.025.323 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.479 I llm_load_vocab: special tokens cache size = 25
0.00.051.389 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.391 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.392 I llm_load_print_meta: arch             = gptneox
0.00.051.392 I llm_load_print_meta: vocab type       = BPE
0.00.051.392 I llm_load_print_meta: n_vocab          = 50304
0.00.051.392 I llm_load_print_meta: n_merges         = 50009
0.00.051.393 I llm_load_print_meta: vocab_only       = 0
0.00.051.393 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.393 I llm_load_print_meta: n_embd           = 2048
0.00.051.393 I llm_load_print_meta: n_layer          = 24
0.00.051.396 I llm_load_print_meta: n_head           = 16
0.00.051.397 I llm_load_print_meta: n_head_kv        = 16
0.00.051.409 I llm_load_print_meta: n_rot            = 32
0.00.051.409 I llm_load_print_meta: n_swa            = 0
0.00.051.409 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.410 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.410 I llm_load_print_meta: n_gqa            = 1
0.00.051.411 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.412 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.412 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.413 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.414 I llm_load_print_meta: n_ff             = 8192
0.00.051.414 I llm_load_print_meta: n_expert         = 0
0.00.051.414 I llm_load_print_meta: n_expert_used    = 0
0.00.051.414 I llm_load_print_meta: causal attn      = 1
0.00.051.416 I llm_load_print_meta: pooling type     = 0
0.00.051.417 I llm_load_print_meta: rope type        = 2
0.00.051.417 I llm_load_print_meta: rope scaling     = linear
0.00.051.417 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.417 I llm_load_print_meta: freq_scale_train = 1
0.00.051.418 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.418 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.418 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.418 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.419 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.419 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.419 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.429 I llm_load_print_meta: model type       = 1.4B
0.00.051.429 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.429 I llm_load_print_meta: model params     = 1.41 B
0.00.051.430 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.430 I llm_load_print_meta: general.name     = 1.4B
0.00.051.430 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.431 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.432 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: max token length = 1024
0.00.053.505 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.505 I llm_load_tensors: offloading output layer to GPU
0.00.053.505 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.515 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.517 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.475 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.475 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.475 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.476 I llama_new_context_with_model: n_batch       = 2048
0.00.054.476 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.476 I llama_new_context_with_model: flash_attn    = 0
0.00.054.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.477 I llama_new_context_with_model: freq_scale    = 1
0.00.054.477 I ggml_metal_init: allocating
0.00.054.480 I ggml_metal_init: found device: Apple M4
0.00.054.482 I ggml_metal_init: picking default device: Apple M4
0.00.055.056 I ggml_metal_init: using embedded metal library
0.00.057.330 I ggml_metal_init: GPU name:   Apple M4
0.00.057.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.332 I ggml_metal_init: simdgroup reduction   = true
0.00.057.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.334 I ggml_metal_init: has bfloat            = true
0.00.057.334 I ggml_metal_init: use bfloat            = true
0.00.057.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.597 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.603 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.623 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.644 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.645 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.645 I llama_new_context_with_model: graph nodes  = 967
0.00.087.646 I llama_new_context_with_model: graph splits = 2
0.00.087.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.159 I main: llama threadpool init, n_threads = 4
0.00.754.196 I 
0.00.754.221 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.222 I 
0.00.754.442 I sampler seed: 1234
0.00.754.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.475 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.476 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.476 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.636.877 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.636.878 I llama_perf_context_print:        load time =     743.86 ms
0.01.636.878 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.49 tokens per second)
0.01.636.880 I llama_perf_context_print:        eval time =     824.89 ms /    63 runs   (   13.09 ms per token,    76.37 tokens per second)
0.01.636.880 I llama_perf_context_print:       total time =     882.72 ms /    70 tokens
0.01.637.071 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.109s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.515 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.087 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.386 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.669 I llama_model_loader: - type  f32:  194 tensors
0.00.051.669 I llama_model_loader: - type  f16:   98 tensors
0.00.079.484 I llm_load_vocab: special tokens cache size = 25
0.00.085.937 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.940 I llm_load_print_meta: arch             = gptneox
0.00.085.940 I llm_load_print_meta: vocab type       = BPE
0.00.085.940 I llm_load_print_meta: n_vocab          = 50304
0.00.085.940 I llm_load_print_meta: n_merges         = 50009
0.00.085.941 I llm_load_print_meta: vocab_only       = 0
0.00.085.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.941 I llm_load_print_meta: n_embd           = 2048
0.00.085.941 I llm_load_print_meta: n_layer          = 24
0.00.085.944 I llm_load_print_meta: n_head           = 16
0.00.085.946 I llm_load_print_meta: n_head_kv        = 16
0.00.085.958 I llm_load_print_meta: n_rot            = 32
0.00.085.958 I llm_load_print_meta: n_swa            = 0
0.00.085.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.959 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.960 I llm_load_print_meta: n_gqa            = 1
0.00.085.960 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.962 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.963 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.963 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.964 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.964 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.964 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.965 I llm_load_print_meta: n_ff             = 8192
0.00.085.965 I llm_load_print_meta: n_expert         = 0
0.00.085.965 I llm_load_print_meta: n_expert_used    = 0
0.00.085.966 I llm_load_print_meta: causal attn      = 1
0.00.085.966 I llm_load_print_meta: pooling type     = 0
0.00.085.966 I llm_load_print_meta: rope type        = 2
0.00.085.967 I llm_load_print_meta: rope scaling     = linear
0.00.085.967 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.967 I llm_load_print_meta: freq_scale_train = 1
0.00.085.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.968 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.968 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.968 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.978 I llm_load_print_meta: model type       = 1.4B
0.00.085.979 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.979 I llm_load_print_meta: model params     = 1.41 B
0.00.085.979 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.980 I llm_load_print_meta: general.name     = 1.4B
0.00.085.980 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.980 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.980 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.980 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.980 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.981 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.981 I llm_load_print_meta: max token length = 1024
0.00.088.416 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.416 I llm_load_tensors: offloading output layer to GPU
0.00.088.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.427 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.429 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.347 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.348 I llama_new_context_with_model: n_ctx         = 128
0.00.089.348 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.348 I llama_new_context_with_model: n_batch       = 128
0.00.089.348 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.348 I llama_new_context_with_model: flash_attn    = 0
0.00.089.349 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.349 I llama_new_context_with_model: freq_scale    = 1
0.00.089.350 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.350 I ggml_metal_init: allocating
0.00.089.353 I ggml_metal_init: found device: Apple M4
0.00.089.355 I ggml_metal_init: picking default device: Apple M4
0.00.089.919 I ggml_metal_init: using embedded metal library
0.00.092.385 I ggml_metal_init: GPU name:   Apple M4
0.00.092.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.387 I ggml_metal_init: simdgroup reduction   = true
0.00.092.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.387 I ggml_metal_init: has bfloat            = true
0.00.092.388 I ggml_metal_init: use bfloat            = true
0.00.092.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.299 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.312 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.152 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.153 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.154 I llama_new_context_with_model: graph nodes  = 967
0.00.103.154 I llama_new_context_with_model: graph splits = 2
0.00.103.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.426 I 
0.00.912.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.564 I perplexity: tokenizing the input ..
0.00.925.950 I perplexity: tokenization took 13.383 ms
0.00.925.979 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.048.626 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.050.534 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.050.574 I llama_perf_context_print:        load time =     889.33 ms
0.01.050.576 I llama_perf_context_print: prompt eval time =     121.75 ms /   128 tokens (    0.95 ms per token,  1051.38 tokens per second)
0.01.050.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.050.578 I llama_perf_context_print:       total time =     138.15 ms /   129 tokens
0.01.051.403 I ggml_metal_free: deallocating

real	0m1.244s
user	0m0.122s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.155 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.430 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.375 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.390 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.324 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.324 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.325 I llama_model_loader: - type  f32:  194 tensors
0.00.034.325 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.097 I llm_load_vocab: special tokens cache size = 25
0.00.066.337 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.340 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.340 I llm_load_print_meta: arch             = gptneox
0.00.066.340 I llm_load_print_meta: vocab type       = BPE
0.00.066.340 I llm_load_print_meta: n_vocab          = 50304
0.00.066.341 I llm_load_print_meta: n_merges         = 50009
0.00.066.341 I llm_load_print_meta: vocab_only       = 0
0.00.066.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.341 I llm_load_print_meta: n_embd           = 2048
0.00.066.341 I llm_load_print_meta: n_layer          = 24
0.00.066.345 I llm_load_print_meta: n_head           = 16
0.00.066.346 I llm_load_print_meta: n_head_kv        = 16
0.00.066.359 I llm_load_print_meta: n_rot            = 32
0.00.066.359 I llm_load_print_meta: n_swa            = 0
0.00.066.359 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.359 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.360 I llm_load_print_meta: n_gqa            = 1
0.00.066.361 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.361 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.362 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.362 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.362 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.362 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.364 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.364 I llm_load_print_meta: n_ff             = 8192
0.00.066.365 I llm_load_print_meta: n_expert         = 0
0.00.066.365 I llm_load_print_meta: n_expert_used    = 0
0.00.066.365 I llm_load_print_meta: causal attn      = 1
0.00.066.365 I llm_load_print_meta: pooling type     = 0
0.00.066.365 I llm_load_print_meta: rope type        = 2
0.00.066.367 I llm_load_print_meta: rope scaling     = linear
0.00.066.367 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.367 I llm_load_print_meta: freq_scale_train = 1
0.00.066.368 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.368 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.368 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.369 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.369 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.379 I llm_load_print_meta: model type       = 1.4B
0.00.066.379 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.379 I llm_load_print_meta: model params     = 1.41 B
0.00.066.380 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.380 I llm_load_print_meta: general.name     = 1.4B
0.00.066.380 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.380 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.380 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.381 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.381 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.381 I llm_load_print_meta: max token length = 1024
0.00.068.800 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.800 I llm_load_tensors: offloading output layer to GPU
0.00.068.800 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.811 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.812 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.873 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.874 I llama_new_context_with_model: n_ctx         = 128
0.00.069.874 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.874 I llama_new_context_with_model: n_batch       = 128
0.00.069.874 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.874 I llama_new_context_with_model: flash_attn    = 0
0.00.069.875 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.875 I llama_new_context_with_model: freq_scale    = 1
0.00.069.876 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.876 I ggml_metal_init: allocating
0.00.069.883 I ggml_metal_init: found device: Apple M4
0.00.069.885 I ggml_metal_init: picking default device: Apple M4
0.00.070.594 I ggml_metal_init: using embedded metal library
0.00.073.303 I ggml_metal_init: GPU name:   Apple M4
0.00.073.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.306 I ggml_metal_init: simdgroup reduction   = true
0.00.073.306 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.306 I ggml_metal_init: has bfloat            = true
0.00.073.306 I ggml_metal_init: use bfloat            = true
0.00.073.307 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.192 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.219 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.118 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.120 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.120 I llama_new_context_with_model: graph nodes  = 967
0.00.086.120 I llama_new_context_with_model: graph splits = 2
0.00.086.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.043 I 
0.00.832.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.832.083 I perplexity: tokenizing the input ..
0.00.839.895 I perplexity: tokenization took 7.811 ms
0.00.839.906 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.963.622 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.964.951 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.964.962 I llama_perf_context_print:        load time =     819.61 ms
0.00.964.963 I llama_perf_context_print: prompt eval time =     123.46 ms /   128 tokens (    0.96 ms per token,  1036.76 tokens per second)
0.00.964.964 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.964.965 I llama_perf_context_print:       total time =     132.92 ms /   129 tokens
0.00.965.218 I ggml_metal_free: deallocating

real	0m0.983s
user	0m0.096s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.118 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.966 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.966 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.967 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.968 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.739 I llama_model_loader: - type  f32:  194 tensors
0.00.024.739 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.483 I llm_load_vocab: special tokens cache size = 25
0.00.051.531 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.534 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.535 I llm_load_print_meta: arch             = gptneox
0.00.051.535 I llm_load_print_meta: vocab type       = BPE
0.00.051.535 I llm_load_print_meta: n_vocab          = 50304
0.00.051.536 I llm_load_print_meta: n_merges         = 50009
0.00.051.536 I llm_load_print_meta: vocab_only       = 0
0.00.051.536 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.536 I llm_load_print_meta: n_embd           = 2048
0.00.051.536 I llm_load_print_meta: n_layer          = 24
0.00.051.539 I llm_load_print_meta: n_head           = 16
0.00.051.540 I llm_load_print_meta: n_head_kv        = 16
0.00.051.554 I llm_load_print_meta: n_rot            = 32
0.00.051.555 I llm_load_print_meta: n_swa            = 0
0.00.051.555 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.555 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.556 I llm_load_print_meta: n_gqa            = 1
0.00.051.557 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.557 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.558 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.558 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.558 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.559 I llm_load_print_meta: n_ff             = 8192
0.00.051.559 I llm_load_print_meta: n_expert         = 0
0.00.051.559 I llm_load_print_meta: n_expert_used    = 0
0.00.051.559 I llm_load_print_meta: causal attn      = 1
0.00.051.559 I llm_load_print_meta: pooling type     = 0
0.00.051.560 I llm_load_print_meta: rope type        = 2
0.00.051.560 I llm_load_print_meta: rope scaling     = linear
0.00.051.561 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.561 I llm_load_print_meta: freq_scale_train = 1
0.00.051.561 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.562 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.562 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.562 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.562 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.572 I llm_load_print_meta: model type       = 1.4B
0.00.051.572 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.572 I llm_load_print_meta: model params     = 1.41 B
0.00.051.573 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.573 I llm_load_print_meta: general.name     = 1.4B
0.00.051.573 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.573 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.573 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: max token length = 1024
0.00.053.487 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.487 I llm_load_tensors: offloading output layer to GPU
0.00.053.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.498 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.499 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.430 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.430 I llama_new_context_with_model: n_ctx         = 128
0.00.054.431 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.431 I llama_new_context_with_model: n_batch       = 128
0.00.054.431 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.431 I llama_new_context_with_model: flash_attn    = 0
0.00.054.432 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.432 I llama_new_context_with_model: freq_scale    = 1
0.00.054.432 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.433 I ggml_metal_init: allocating
0.00.054.436 I ggml_metal_init: found device: Apple M4
0.00.054.438 I ggml_metal_init: picking default device: Apple M4
0.00.054.987 I ggml_metal_init: using embedded metal library
0.00.057.298 I ggml_metal_init: GPU name:   Apple M4
0.00.057.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.300 I ggml_metal_init: simdgroup reduction   = true
0.00.057.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.301 I ggml_metal_init: has bfloat            = true
0.00.057.301 I ggml_metal_init: use bfloat            = true
0.00.057.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.220 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.223 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.236 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.187 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.188 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.188 I llama_new_context_with_model: graph nodes  = 967
0.00.069.188 I llama_new_context_with_model: graph splits = 2
0.00.069.200 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.735 I 
0.00.609.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.786 I perplexity: tokenizing the input ..
0.00.617.666 I perplexity: tokenization took 7.878 ms
0.00.617.677 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.300 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.741.679 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.697 I llama_perf_context_print:        load time =     599.61 ms
0.00.741.698 I llama_perf_context_print: prompt eval time =     122.38 ms /   128 tokens (    0.96 ms per token,  1045.94 tokens per second)
0.00.741.699 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.700 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.742.140 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.571 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.420 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.421 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.238 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.101 I llama_model_loader: - type  f32:  194 tensors
0.00.023.102 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.764 I llm_load_vocab: special tokens cache size = 25
0.00.049.806 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.809 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.809 I llm_load_print_meta: arch             = gptneox
0.00.049.810 I llm_load_print_meta: vocab type       = BPE
0.00.049.810 I llm_load_print_meta: n_vocab          = 50304
0.00.049.810 I llm_load_print_meta: n_merges         = 50009
0.00.049.810 I llm_load_print_meta: vocab_only       = 0
0.00.049.811 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.811 I llm_load_print_meta: n_embd           = 2048
0.00.049.811 I llm_load_print_meta: n_layer          = 24
0.00.049.815 I llm_load_print_meta: n_head           = 16
0.00.049.816 I llm_load_print_meta: n_head_kv        = 16
0.00.049.829 I llm_load_print_meta: n_rot            = 32
0.00.049.830 I llm_load_print_meta: n_swa            = 0
0.00.049.830 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.831 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.831 I llm_load_print_meta: n_gqa            = 1
0.00.049.832 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.833 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.834 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.834 I llm_load_print_meta: n_ff             = 8192
0.00.049.835 I llm_load_print_meta: n_expert         = 0
0.00.049.835 I llm_load_print_meta: n_expert_used    = 0
0.00.049.835 I llm_load_print_meta: causal attn      = 1
0.00.049.835 I llm_load_print_meta: pooling type     = 0
0.00.049.835 I llm_load_print_meta: rope type        = 2
0.00.049.835 I llm_load_print_meta: rope scaling     = linear
0.00.049.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.836 I llm_load_print_meta: freq_scale_train = 1
0.00.049.836 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.836 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.836 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.836 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.837 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.837 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.837 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.846 I llm_load_print_meta: model type       = 1.4B
0.00.049.847 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.847 I llm_load_print_meta: model params     = 1.41 B
0.00.049.847 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.848 I llm_load_print_meta: general.name     = 1.4B
0.00.049.848 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.848 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.848 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.848 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.849 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.849 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.849 I llm_load_print_meta: max token length = 1024
0.00.051.782 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.782 I llm_load_tensors: offloading output layer to GPU
0.00.051.783 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.793 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.794 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.692 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.693 I llama_new_context_with_model: n_ctx         = 128
0.00.052.693 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.693 I llama_new_context_with_model: n_batch       = 128
0.00.052.694 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.694 I llama_new_context_with_model: flash_attn    = 0
0.00.052.694 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.694 I llama_new_context_with_model: freq_scale    = 1
0.00.052.695 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.695 I ggml_metal_init: allocating
0.00.052.698 I ggml_metal_init: found device: Apple M4
0.00.052.700 I ggml_metal_init: picking default device: Apple M4
0.00.053.280 I ggml_metal_init: using embedded metal library
0.00.055.610 I ggml_metal_init: GPU name:   Apple M4
0.00.055.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.613 I ggml_metal_init: simdgroup reduction   = true
0.00.055.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.613 I ggml_metal_init: has bfloat            = true
0.00.055.613 I ggml_metal_init: use bfloat            = true
0.00.055.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.481 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.483 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.497 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.408 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.409 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.409 I llama_new_context_with_model: graph nodes  = 967
0.00.067.410 I llama_new_context_with_model: graph splits = 2
0.00.067.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.293 I 
0.00.677.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.334 I perplexity: tokenizing the input ..
0.00.684.751 I perplexity: tokenization took 7.416 ms
0.00.684.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.840 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.809.291 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.809.307 I llama_perf_context_print:        load time =     668.72 ms
0.00.809.308 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.27 tokens per second)
0.00.809.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.309 I llama_perf_context_print:       total time =     132.02 ms /   129 tokens
0.00.809.698 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.518 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.519 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.522 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.523 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.101 I llama_model_loader: - type  f32:  194 tensors
0.00.024.102 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.077 I llm_load_vocab: special tokens cache size = 25
0.00.049.856 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.859 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.859 I llm_load_print_meta: arch             = gptneox
0.00.049.860 I llm_load_print_meta: vocab type       = BPE
0.00.049.860 I llm_load_print_meta: n_vocab          = 50304
0.00.049.860 I llm_load_print_meta: n_merges         = 50009
0.00.049.860 I llm_load_print_meta: vocab_only       = 0
0.00.049.860 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.861 I llm_load_print_meta: n_embd           = 2048
0.00.049.861 I llm_load_print_meta: n_layer          = 24
0.00.049.863 I llm_load_print_meta: n_head           = 16
0.00.049.864 I llm_load_print_meta: n_head_kv        = 16
0.00.049.876 I llm_load_print_meta: n_rot            = 32
0.00.049.876 I llm_load_print_meta: n_swa            = 0
0.00.049.876 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.877 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.877 I llm_load_print_meta: n_gqa            = 1
0.00.049.878 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.881 I llm_load_print_meta: n_ff             = 8192
0.00.049.881 I llm_load_print_meta: n_expert         = 0
0.00.049.881 I llm_load_print_meta: n_expert_used    = 0
0.00.049.881 I llm_load_print_meta: causal attn      = 1
0.00.049.881 I llm_load_print_meta: pooling type     = 0
0.00.049.881 I llm_load_print_meta: rope type        = 2
0.00.049.882 I llm_load_print_meta: rope scaling     = linear
0.00.049.882 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.882 I llm_load_print_meta: freq_scale_train = 1
0.00.049.882 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.883 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.883 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.883 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.883 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.883 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.883 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.893 I llm_load_print_meta: model type       = 1.4B
0.00.049.893 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.893 I llm_load_print_meta: model params     = 1.41 B
0.00.049.894 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.894 I llm_load_print_meta: general.name     = 1.4B
0.00.049.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.894 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: max token length = 1024
0.00.051.833 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.833 I llm_load_tensors: offloading output layer to GPU
0.00.051.833 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.844 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.845 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.796 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.796 I llama_new_context_with_model: n_ctx         = 128
0.00.052.797 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.797 I llama_new_context_with_model: n_batch       = 128
0.00.052.797 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.797 I llama_new_context_with_model: flash_attn    = 0
0.00.052.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.798 I llama_new_context_with_model: freq_scale    = 1
0.00.052.798 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.799 I ggml_metal_init: allocating
0.00.052.802 I ggml_metal_init: found device: Apple M4
0.00.052.804 I ggml_metal_init: picking default device: Apple M4
0.00.053.332 I ggml_metal_init: using embedded metal library
0.00.055.638 I ggml_metal_init: GPU name:   Apple M4
0.00.055.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.640 I ggml_metal_init: simdgroup reduction   = true
0.00.055.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.641 I ggml_metal_init: has bfloat            = true
0.00.055.641 I ggml_metal_init: use bfloat            = true
0.00.055.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.222 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.226 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.241 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.143 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.144 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.145 I llama_new_context_with_model: graph nodes  = 967
0.00.067.145 I llama_new_context_with_model: graph splits = 2
0.00.067.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.826 I 
0.00.720.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.903 I perplexity: tokenizing the input ..
0.00.728.858 I perplexity: tokenization took 7.954 ms
0.00.728.869 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.459 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.864.831 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.851 I llama_perf_context_print:        load time =     711.04 ms
0.00.864.852 I llama_perf_context_print: prompt eval time =     134.36 ms /   128 tokens (    1.05 ms per token,   952.68 tokens per second)
0.00.864.853 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.854 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.00.865.328 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.604 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.617 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.342 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.342 I llama_model_loader: - type  f32:  194 tensors
0.00.023.343 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.343 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.914 I llm_load_vocab: special tokens cache size = 25
0.00.049.740 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.742 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.743 I llm_load_print_meta: arch             = gptneox
0.00.049.743 I llm_load_print_meta: vocab type       = BPE
0.00.049.743 I llm_load_print_meta: n_vocab          = 50304
0.00.049.743 I llm_load_print_meta: n_merges         = 50009
0.00.049.744 I llm_load_print_meta: vocab_only       = 0
0.00.049.744 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.744 I llm_load_print_meta: n_embd           = 2048
0.00.049.744 I llm_load_print_meta: n_layer          = 24
0.00.049.747 I llm_load_print_meta: n_head           = 16
0.00.049.747 I llm_load_print_meta: n_head_kv        = 16
0.00.049.759 I llm_load_print_meta: n_rot            = 32
0.00.049.760 I llm_load_print_meta: n_swa            = 0
0.00.049.760 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.760 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.761 I llm_load_print_meta: n_gqa            = 1
0.00.049.761 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.762 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.763 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.763 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.763 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.763 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.764 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.764 I llm_load_print_meta: n_ff             = 8192
0.00.049.764 I llm_load_print_meta: n_expert         = 0
0.00.049.765 I llm_load_print_meta: n_expert_used    = 0
0.00.049.765 I llm_load_print_meta: causal attn      = 1
0.00.049.765 I llm_load_print_meta: pooling type     = 0
0.00.049.765 I llm_load_print_meta: rope type        = 2
0.00.049.765 I llm_load_print_meta: rope scaling     = linear
0.00.049.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.766 I llm_load_print_meta: freq_scale_train = 1
0.00.049.766 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.766 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.766 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.766 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.766 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.767 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.767 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.776 I llm_load_print_meta: model type       = 1.4B
0.00.049.777 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.777 I llm_load_print_meta: model params     = 1.41 B
0.00.049.778 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.778 I llm_load_print_meta: general.name     = 1.4B
0.00.049.778 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.779 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.779 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.779 I llm_load_print_meta: max token length = 1024
0.00.051.755 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.755 I llm_load_tensors: offloading output layer to GPU
0.00.051.755 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.765 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.767 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.642 I llama_new_context_with_model: n_ctx         = 128
0.00.052.642 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.642 I llama_new_context_with_model: n_batch       = 128
0.00.052.643 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.643 I llama_new_context_with_model: flash_attn    = 0
0.00.052.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.643 I llama_new_context_with_model: freq_scale    = 1
0.00.052.644 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.644 I ggml_metal_init: allocating
0.00.052.647 I ggml_metal_init: found device: Apple M4
0.00.052.649 I ggml_metal_init: picking default device: Apple M4
0.00.053.199 I ggml_metal_init: using embedded metal library
0.00.055.541 I ggml_metal_init: GPU name:   Apple M4
0.00.055.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.543 I ggml_metal_init: simdgroup reduction   = true
0.00.055.544 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.544 I ggml_metal_init: has bfloat            = true
0.00.055.544 I ggml_metal_init: use bfloat            = true
0.00.055.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.328 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.256 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.257 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.257 I llama_new_context_with_model: graph nodes  = 967
0.00.067.257 I llama_new_context_with_model: graph splits = 2
0.00.067.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.115 I 
0.00.779.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.164 I perplexity: tokenizing the input ..
0.00.787.306 I perplexity: tokenization took 8.14 ms
0.00.787.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.047 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.923.389 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.923.402 I llama_perf_context_print:        load time =     770.51 ms
0.00.923.403 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.67 tokens per second)
0.00.923.404 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.404 I llama_perf_context_print:       total time =     144.29 ms /   129 tokens
0.00.923.735 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.630 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.838 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.840 I llama_model_loader: - type  f32:  194 tensors
0.00.023.840 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.840 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.394 I llm_load_vocab: special tokens cache size = 25
0.00.050.352 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.355 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.355 I llm_load_print_meta: arch             = gptneox
0.00.050.356 I llm_load_print_meta: vocab type       = BPE
0.00.050.356 I llm_load_print_meta: n_vocab          = 50304
0.00.050.356 I llm_load_print_meta: n_merges         = 50009
0.00.050.356 I llm_load_print_meta: vocab_only       = 0
0.00.050.356 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.357 I llm_load_print_meta: n_embd           = 2048
0.00.050.357 I llm_load_print_meta: n_layer          = 24
0.00.050.359 I llm_load_print_meta: n_head           = 16
0.00.050.360 I llm_load_print_meta: n_head_kv        = 16
0.00.050.372 I llm_load_print_meta: n_rot            = 32
0.00.050.372 I llm_load_print_meta: n_swa            = 0
0.00.050.372 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.372 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.373 I llm_load_print_meta: n_gqa            = 1
0.00.050.374 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.374 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.375 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.375 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.376 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.376 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.376 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.377 I llm_load_print_meta: n_ff             = 8192
0.00.050.377 I llm_load_print_meta: n_expert         = 0
0.00.050.377 I llm_load_print_meta: n_expert_used    = 0
0.00.050.377 I llm_load_print_meta: causal attn      = 1
0.00.050.377 I llm_load_print_meta: pooling type     = 0
0.00.050.377 I llm_load_print_meta: rope type        = 2
0.00.050.377 I llm_load_print_meta: rope scaling     = linear
0.00.050.378 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.378 I llm_load_print_meta: freq_scale_train = 1
0.00.050.378 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.378 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.378 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.379 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.379 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.389 I llm_load_print_meta: model type       = 1.4B
0.00.050.390 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.390 I llm_load_print_meta: model params     = 1.41 B
0.00.050.391 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.391 I llm_load_print_meta: general.name     = 1.4B
0.00.050.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.392 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: max token length = 1024
0.00.051.943 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.943 I llm_load_tensors: offloading output layer to GPU
0.00.051.944 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.954 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.955 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.801 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.802 I llama_new_context_with_model: n_ctx         = 128
0.00.052.802 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.802 I llama_new_context_with_model: n_batch       = 128
0.00.052.803 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.803 I llama_new_context_with_model: flash_attn    = 0
0.00.052.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.803 I llama_new_context_with_model: freq_scale    = 1
0.00.052.804 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.804 I ggml_metal_init: allocating
0.00.052.809 I ggml_metal_init: found device: Apple M4
0.00.052.812 I ggml_metal_init: picking default device: Apple M4
0.00.053.331 I ggml_metal_init: using embedded metal library
0.00.055.666 I ggml_metal_init: GPU name:   Apple M4
0.00.055.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.668 I ggml_metal_init: simdgroup reduction   = true
0.00.055.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.668 I ggml_metal_init: has bfloat            = true
0.00.055.668 I ggml_metal_init: use bfloat            = true
0.00.055.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.211 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.224 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.067 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.068 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.068 I llama_new_context_with_model: graph nodes  = 967
0.00.067.068 I llama_new_context_with_model: graph splits = 2
0.00.067.080 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.536 I 
0.00.454.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.454.590 I perplexity: tokenizing the input ..
0.00.462.820 I perplexity: tokenization took 8.227 ms
0.00.462.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.595.398 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.596.725 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.596.741 I llama_perf_context_print:        load time =     444.90 ms
0.00.596.742 I llama_perf_context_print: prompt eval time =     132.31 ms /   128 tokens (    1.03 ms per token,   967.40 tokens per second)
0.00.596.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.596.743 I llama_perf_context_print:       total time =     142.21 ms /   129 tokens
0.00.597.308 I ggml_metal_free: deallocating

real	0m0.612s
user	0m0.080s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.297 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.298 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.298 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.243 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.065 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.067 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.068 I llama_model_loader: - type  f32:  194 tensors
0.00.023.068 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.068 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.068 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.563 I llm_load_vocab: special tokens cache size = 25
0.00.049.358 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.361 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.361 I llm_load_print_meta: arch             = gptneox
0.00.049.362 I llm_load_print_meta: vocab type       = BPE
0.00.049.362 I llm_load_print_meta: n_vocab          = 50304
0.00.049.362 I llm_load_print_meta: n_merges         = 50009
0.00.049.362 I llm_load_print_meta: vocab_only       = 0
0.00.049.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.363 I llm_load_print_meta: n_embd           = 2048
0.00.049.363 I llm_load_print_meta: n_layer          = 24
0.00.049.365 I llm_load_print_meta: n_head           = 16
0.00.049.366 I llm_load_print_meta: n_head_kv        = 16
0.00.049.378 I llm_load_print_meta: n_rot            = 32
0.00.049.378 I llm_load_print_meta: n_swa            = 0
0.00.049.380 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.381 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.381 I llm_load_print_meta: n_gqa            = 1
0.00.049.382 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.383 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.383 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.384 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.384 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.384 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.384 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.385 I llm_load_print_meta: n_ff             = 8192
0.00.049.385 I llm_load_print_meta: n_expert         = 0
0.00.049.385 I llm_load_print_meta: n_expert_used    = 0
0.00.049.385 I llm_load_print_meta: causal attn      = 1
0.00.049.385 I llm_load_print_meta: pooling type     = 0
0.00.049.385 I llm_load_print_meta: rope type        = 2
0.00.049.386 I llm_load_print_meta: rope scaling     = linear
0.00.049.386 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.386 I llm_load_print_meta: freq_scale_train = 1
0.00.049.386 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.387 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.387 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.387 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.387 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.397 I llm_load_print_meta: model type       = 1.4B
0.00.049.397 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.397 I llm_load_print_meta: model params     = 1.41 B
0.00.049.398 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.398 I llm_load_print_meta: general.name     = 1.4B
0.00.049.398 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.398 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.398 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.400 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.400 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.400 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.400 I llm_load_print_meta: max token length = 1024
0.00.051.295 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.296 I llm_load_tensors: offloading output layer to GPU
0.00.051.296 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.306 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.307 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.176 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.177 I llama_new_context_with_model: n_ctx         = 128
0.00.052.177 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.177 I llama_new_context_with_model: n_batch       = 128
0.00.052.177 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.178 I llama_new_context_with_model: flash_attn    = 0
0.00.052.178 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.178 I llama_new_context_with_model: freq_scale    = 1
0.00.052.179 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.179 I ggml_metal_init: allocating
0.00.052.185 I ggml_metal_init: found device: Apple M4
0.00.052.187 I ggml_metal_init: picking default device: Apple M4
0.00.052.705 I ggml_metal_init: using embedded metal library
0.00.055.026 I ggml_metal_init: GPU name:   Apple M4
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.029 I ggml_metal_init: simdgroup reduction   = true
0.00.055.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.029 I ggml_metal_init: has bfloat            = true
0.00.055.029 I ggml_metal_init: use bfloat            = true
0.00.055.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.438 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.453 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.304 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.305 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.306 I llama_new_context_with_model: graph nodes  = 967
0.00.066.306 I llama_new_context_with_model: graph splits = 2
0.00.066.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.448 I 
0.00.490.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.494 I perplexity: tokenizing the input ..
0.00.498.124 I perplexity: tokenization took 7.628 ms
0.00.498.134 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.625 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.632.069 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.632.081 I llama_perf_context_print:        load time =     481.72 ms
0.00.632.082 I llama_perf_context_print: prompt eval time =     132.27 ms /   128 tokens (    1.03 ms per token,   967.72 tokens per second)
0.00.632.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.084 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.632.421 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.078s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.299 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.073 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.823 I llama_model_loader: - type  f32:  194 tensors
0.00.023.824 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.824 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.824 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.530 I llm_load_vocab: special tokens cache size = 25
0.00.050.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.616 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.617 I llm_load_print_meta: arch             = gptneox
0.00.050.617 I llm_load_print_meta: vocab type       = BPE
0.00.050.617 I llm_load_print_meta: n_vocab          = 50304
0.00.050.617 I llm_load_print_meta: n_merges         = 50009
0.00.050.618 I llm_load_print_meta: vocab_only       = 0
0.00.050.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.618 I llm_load_print_meta: n_embd           = 2048
0.00.050.618 I llm_load_print_meta: n_layer          = 24
0.00.050.621 I llm_load_print_meta: n_head           = 16
0.00.050.622 I llm_load_print_meta: n_head_kv        = 16
0.00.050.634 I llm_load_print_meta: n_rot            = 32
0.00.050.634 I llm_load_print_meta: n_swa            = 0
0.00.050.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.635 I llm_load_print_meta: n_gqa            = 1
0.00.050.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.638 I llm_load_print_meta: n_ff             = 8192
0.00.050.638 I llm_load_print_meta: n_expert         = 0
0.00.050.639 I llm_load_print_meta: n_expert_used    = 0
0.00.050.639 I llm_load_print_meta: causal attn      = 1
0.00.050.639 I llm_load_print_meta: pooling type     = 0
0.00.050.639 I llm_load_print_meta: rope type        = 2
0.00.050.639 I llm_load_print_meta: rope scaling     = linear
0.00.050.639 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.640 I llm_load_print_meta: freq_scale_train = 1
0.00.050.640 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.640 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.640 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.640 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.641 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.641 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.650 I llm_load_print_meta: model type       = 1.4B
0.00.050.651 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.651 I llm_load_print_meta: model params     = 1.41 B
0.00.050.651 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.652 I llm_load_print_meta: general.name     = 1.4B
0.00.050.652 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.653 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.653 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.653 I llm_load_print_meta: max token length = 1024
0.00.052.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.595 I llm_load_tensors: offloading output layer to GPU
0.00.052.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.605 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.606 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.544 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.545 I llama_new_context_with_model: n_ctx         = 128
0.00.053.545 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.545 I llama_new_context_with_model: n_batch       = 128
0.00.053.545 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.546 I llama_new_context_with_model: flash_attn    = 0
0.00.053.546 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.546 I llama_new_context_with_model: freq_scale    = 1
0.00.053.547 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.547 I ggml_metal_init: allocating
0.00.053.553 I ggml_metal_init: found device: Apple M4
0.00.053.556 I ggml_metal_init: picking default device: Apple M4
0.00.054.097 I ggml_metal_init: using embedded metal library
0.00.056.436 I ggml_metal_init: GPU name:   Apple M4
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.439 I ggml_metal_init: simdgroup reduction   = true
0.00.056.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.439 I ggml_metal_init: has bfloat            = true
0.00.056.439 I ggml_metal_init: use bfloat            = true
0.00.056.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.973 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.980 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.994 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.887 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.888 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.888 I llama_new_context_with_model: graph nodes  = 967
0.00.067.888 I llama_new_context_with_model: graph splits = 2
0.00.067.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.691 I 
0.00.565.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.732 I perplexity: tokenizing the input ..
0.00.573.501 I perplexity: tokenization took 7.767 ms
0.00.573.516 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.290 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.708.730 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.708.746 I llama_perf_context_print:        load time =     556.39 ms
0.00.708.747 I llama_perf_context_print: prompt eval time =     133.55 ms /   128 tokens (    1.04 ms per token,   958.46 tokens per second)
0.00.708.748 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.708.749 I llama_perf_context_print:       total time =     143.06 ms /   129 tokens
0.00.709.053 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.426 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.427 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.427 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.428 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.432 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.434 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.434 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.000 I llama_model_loader: - type  f32:  194 tensors
0.00.023.000 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.000 I llama_model_loader: - type q6_K:   37 tensors
0.00.042.572 I llm_load_vocab: special tokens cache size = 25
0.00.048.532 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.534 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.535 I llm_load_print_meta: arch             = gptneox
0.00.048.535 I llm_load_print_meta: vocab type       = BPE
0.00.048.535 I llm_load_print_meta: n_vocab          = 50304
0.00.048.535 I llm_load_print_meta: n_merges         = 50009
0.00.048.536 I llm_load_print_meta: vocab_only       = 0
0.00.048.536 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.536 I llm_load_print_meta: n_embd           = 2048
0.00.048.536 I llm_load_print_meta: n_layer          = 24
0.00.048.538 I llm_load_print_meta: n_head           = 16
0.00.048.539 I llm_load_print_meta: n_head_kv        = 16
0.00.048.550 I llm_load_print_meta: n_rot            = 32
0.00.048.552 I llm_load_print_meta: n_swa            = 0
0.00.048.553 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.553 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.554 I llm_load_print_meta: n_gqa            = 1
0.00.048.554 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.555 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.555 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.559 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.560 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.560 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.560 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.561 I llm_load_print_meta: n_ff             = 8192
0.00.048.561 I llm_load_print_meta: n_expert         = 0
0.00.048.561 I llm_load_print_meta: n_expert_used    = 0
0.00.048.561 I llm_load_print_meta: causal attn      = 1
0.00.048.561 I llm_load_print_meta: pooling type     = 0
0.00.048.562 I llm_load_print_meta: rope type        = 2
0.00.048.562 I llm_load_print_meta: rope scaling     = linear
0.00.048.562 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.564 I llm_load_print_meta: freq_scale_train = 1
0.00.048.564 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.564 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.564 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.564 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.581 I llm_load_print_meta: model type       = 1.4B
0.00.048.581 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.582 I llm_load_print_meta: model params     = 1.41 B
0.00.048.582 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.582 I llm_load_print_meta: general.name     = 1.4B
0.00.048.583 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.583 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.583 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.583 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.584 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.585 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.585 I llm_load_print_meta: max token length = 1024
0.00.050.123 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.123 I llm_load_tensors: offloading output layer to GPU
0.00.050.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.133 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.134 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.050.965 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.966 I llama_new_context_with_model: n_ctx         = 128
0.00.050.966 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.966 I llama_new_context_with_model: n_batch       = 128
0.00.050.966 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.967 I llama_new_context_with_model: flash_attn    = 0
0.00.050.967 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.967 I llama_new_context_with_model: freq_scale    = 1
0.00.050.968 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.968 I ggml_metal_init: allocating
0.00.050.971 I ggml_metal_init: found device: Apple M4
0.00.050.973 I ggml_metal_init: picking default device: Apple M4
0.00.051.498 I ggml_metal_init: using embedded metal library
0.00.053.792 I ggml_metal_init: GPU name:   Apple M4
0.00.053.794 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.795 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.795 I ggml_metal_init: simdgroup reduction   = true
0.00.053.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.795 I ggml_metal_init: has bfloat            = true
0.00.053.795 I ggml_metal_init: use bfloat            = true
0.00.053.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.464 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.466 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.499 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.394 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.395 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.396 I llama_new_context_with_model: graph nodes  = 967
0.00.065.396 I llama_new_context_with_model: graph splits = 2
0.00.065.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.795 I 
0.00.648.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.887 I perplexity: tokenizing the input ..
0.00.656.945 I perplexity: tokenization took 8.056 ms
0.00.656.955 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.293 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.662 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.685 I llama_perf_context_print:        load time =     640.19 ms
0.00.798.686 I llama_perf_context_print: prompt eval time =     140.11 ms /   128 tokens (    1.09 ms per token,   913.57 tokens per second)
0.00.798.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.689 I llama_perf_context_print:       total time =     149.89 ms /   129 tokens
0.00.799.167 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.078s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.339 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.340 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.340 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.979 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.980 I llama_model_loader: - type  f32:  194 tensors
0.00.023.980 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.823 I llm_load_vocab: special tokens cache size = 25
0.00.049.588 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.590 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.591 I llm_load_print_meta: arch             = gptneox
0.00.049.591 I llm_load_print_meta: vocab type       = BPE
0.00.049.591 I llm_load_print_meta: n_vocab          = 50304
0.00.049.592 I llm_load_print_meta: n_merges         = 50009
0.00.049.592 I llm_load_print_meta: vocab_only       = 0
0.00.049.592 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.592 I llm_load_print_meta: n_embd           = 2048
0.00.049.592 I llm_load_print_meta: n_layer          = 24
0.00.049.595 I llm_load_print_meta: n_head           = 16
0.00.049.596 I llm_load_print_meta: n_head_kv        = 16
0.00.049.608 I llm_load_print_meta: n_rot            = 32
0.00.049.608 I llm_load_print_meta: n_swa            = 0
0.00.049.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.611 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.611 I llm_load_print_meta: n_gqa            = 1
0.00.049.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.613 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.614 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.614 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.614 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.615 I llm_load_print_meta: n_ff             = 8192
0.00.049.615 I llm_load_print_meta: n_expert         = 0
0.00.049.615 I llm_load_print_meta: n_expert_used    = 0
0.00.049.615 I llm_load_print_meta: causal attn      = 1
0.00.049.615 I llm_load_print_meta: pooling type     = 0
0.00.049.615 I llm_load_print_meta: rope type        = 2
0.00.049.615 I llm_load_print_meta: rope scaling     = linear
0.00.049.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.617 I llm_load_print_meta: freq_scale_train = 1
0.00.049.617 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.617 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.618 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.618 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.618 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.619 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.619 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.628 I llm_load_print_meta: model type       = 1.4B
0.00.049.628 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.629 I llm_load_print_meta: model params     = 1.41 B
0.00.049.629 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.629 I llm_load_print_meta: general.name     = 1.4B
0.00.049.630 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.630 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: max token length = 1024
0.00.051.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.572 I llm_load_tensors: offloading output layer to GPU
0.00.051.572 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.582 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.584 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.488 I llama_new_context_with_model: n_ctx         = 128
0.00.052.488 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.488 I llama_new_context_with_model: n_batch       = 128
0.00.052.488 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.489 I llama_new_context_with_model: flash_attn    = 0
0.00.052.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.489 I llama_new_context_with_model: freq_scale    = 1
0.00.052.490 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.490 I ggml_metal_init: allocating
0.00.052.493 I ggml_metal_init: found device: Apple M4
0.00.052.495 I ggml_metal_init: picking default device: Apple M4
0.00.053.031 I ggml_metal_init: using embedded metal library
0.00.055.300 I ggml_metal_init: GPU name:   Apple M4
0.00.055.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.302 I ggml_metal_init: simdgroup reduction   = true
0.00.055.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.303 I ggml_metal_init: has bfloat            = true
0.00.055.303 I ggml_metal_init: use bfloat            = true
0.00.055.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.849 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.851 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.864 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.735 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.736 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.737 I llama_new_context_with_model: graph nodes  = 967
0.00.066.737 I llama_new_context_with_model: graph splits = 2
0.00.066.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.300.736 I 
0.00.300.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.300.782 I perplexity: tokenizing the input ..
0.00.308.797 I perplexity: tokenization took 8.013 ms
0.00.308.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.449.454 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.450.874 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.450.954 I llama_perf_context_print:        load time =     290.86 ms
0.00.450.955 I llama_perf_context_print: prompt eval time =     140.38 ms /   128 tokens (    1.10 ms per token,   911.79 tokens per second)
0.00.450.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.450.956 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.451.409 I ggml_metal_free: deallocating

real	0m0.467s
user	0m0.078s
sys	0m0.078s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.234 I build: 4274 (7736837d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.280 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.248 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.281 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.243 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.247 I llama_model_loader: - type  f32:  194 tensors
0.00.052.248 I llama_model_loader: - type  f16:   98 tensors
0.00.080.347 I llm_load_vocab: special tokens cache size = 25
0.00.086.590 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.593 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.593 I llm_load_print_meta: arch             = gptneox
0.00.086.594 I llm_load_print_meta: vocab type       = BPE
0.00.086.594 I llm_load_print_meta: n_vocab          = 50304
0.00.086.594 I llm_load_print_meta: n_merges         = 50009
0.00.086.594 I llm_load_print_meta: vocab_only       = 0
0.00.086.594 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.595 I llm_load_print_meta: n_embd           = 2048
0.00.086.595 I llm_load_print_meta: n_layer          = 24
0.00.086.597 I llm_load_print_meta: n_head           = 16
0.00.086.598 I llm_load_print_meta: n_head_kv        = 16
0.00.086.610 I llm_load_print_meta: n_rot            = 32
0.00.086.610 I llm_load_print_meta: n_swa            = 0
0.00.086.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.610 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.611 I llm_load_print_meta: n_gqa            = 1
0.00.086.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.613 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.614 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.614 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.614 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.615 I llm_load_print_meta: n_ff             = 8192
0.00.086.615 I llm_load_print_meta: n_expert         = 0
0.00.086.615 I llm_load_print_meta: n_expert_used    = 0
0.00.086.615 I llm_load_print_meta: causal attn      = 1
0.00.086.615 I llm_load_print_meta: pooling type     = 0
0.00.086.615 I llm_load_print_meta: rope type        = 2
0.00.086.615 I llm_load_print_meta: rope scaling     = linear
0.00.086.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.616 I llm_load_print_meta: freq_scale_train = 1
0.00.086.616 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.617 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.617 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.617 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.617 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.617 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.618 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.628 I llm_load_print_meta: model type       = 1.4B
0.00.086.628 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.630 I llm_load_print_meta: model params     = 1.41 B
0.00.086.630 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.630 I llm_load_print_meta: general.name     = 1.4B
0.00.086.631 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.631 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.633 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.633 I llm_load_print_meta: max token length = 1024
0.00.089.047 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.047 I llm_load_tensors: offloading output layer to GPU
0.00.089.047 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.058 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.059 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.013 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.014 I llama_new_context_with_model: n_ctx         = 128
0.00.090.014 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.014 I llama_new_context_with_model: n_batch       = 128
0.00.090.014 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.014 I llama_new_context_with_model: flash_attn    = 0
0.00.090.015 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.015 I llama_new_context_with_model: freq_scale    = 1
0.00.090.016 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.016 I ggml_metal_init: allocating
0.00.090.023 I ggml_metal_init: found device: Apple M4
0.00.090.026 I ggml_metal_init: picking default device: Apple M4
0.00.090.579 I ggml_metal_init: using embedded metal library
0.00.093.058 I ggml_metal_init: GPU name:   Apple M4
0.00.093.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.061 I ggml_metal_init: simdgroup reduction   = true
0.00.093.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.061 I ggml_metal_init: has bfloat            = true
0.00.093.061 I ggml_metal_init: use bfloat            = true
0.00.093.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.834 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.838 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.851 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.738 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.739 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.739 I llama_new_context_with_model: graph nodes  = 967
0.00.103.739 I llama_new_context_with_model: graph splits = 2
0.00.103.751 I 
0.00.103.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.784 I compute_imatrix: tokenizing the input ..
0.00.110.405 I compute_imatrix: tokenization took 6.62 ms
0.00.110.406 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.481.051 I compute_imatrix: 1.37 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.483.933 I llama_perf_context_print:        load time =    1458.77 ms
0.01.483.935 I llama_perf_context_print: prompt eval time =    1370.01 ms /   128 tokens (   10.70 ms per token,    93.43 tokens per second)
0.01.483.936 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.483.937 I llama_perf_context_print:       total time =    1461.64 ms /   129 tokens
0.01.484.846 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.168s
sys	0m0.244s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4274 (7736837d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be07a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be07fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be090e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be09690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be09c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be0a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be0a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be0bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be0c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be0cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be0e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be0f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be0f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be11e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be12a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be12fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be13720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be15850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be15cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be16190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be16ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be19e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be1c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be1d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be1e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be1ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be21740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be18900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be29d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be2ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be2b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be2c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be47370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be47980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be48ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be4b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be4c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be4dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be50220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be50770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be50cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be62dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be64320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be64dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be65090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be656a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf07b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf07f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf08860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf08cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf09a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf0ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf0b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf0c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf0d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf0de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf17ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf18d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf19640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf19bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf1c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf2a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf2c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf3c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf41260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf42420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf48180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf4ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf4d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf4ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf50b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf51cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf54da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf55680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf57a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf5ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf5ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf5b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf5bed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf08350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf08c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf09510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf09980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf09df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf0a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf0ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf11160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf11a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf13070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf35970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf40f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf42a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf44030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf45500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf45de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf46250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf46b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf47410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf47880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf48160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf49320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf4a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf4a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf4a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf4b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf4b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf4bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf4c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf4ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf4d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf4ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf4f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf4fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf50680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf52120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf52590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf53750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf54030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf56820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf59d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf5b620 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.763s
user	0m0.291s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4274 (7736837d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f14ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f1edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f20040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f23520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f26f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f27d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f29d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f2a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f2c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f2e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f2ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f32910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f40220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f41de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f4b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f4cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f4d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f57860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f5d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f5e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f5e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f5f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f62d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f65cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f66220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f66770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f66cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f67160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f67aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f68880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f69660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f69b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f69fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f6a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f6a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f6ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f6b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f6be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f6c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f6ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f6d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f6dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f6e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1266053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1266069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1266073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126607850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126607ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1266089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1266091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1266099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12660a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12660a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12660af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12660b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12660be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12660c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12660cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12660d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12660da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12660e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12660e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12660e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12660eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12660f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12660f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12660f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12660fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126610280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126610540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1266109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126610e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126611700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126612450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1266128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126612d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1266131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126613610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126613a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126613ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126614360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1266147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1266150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126615520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126615990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126615e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126616270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1266166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126616c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126617150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1266175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1266194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126619940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126619db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12661a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12661a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12661ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12661af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12661b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12661b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12661bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12661c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12661c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12661ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12661ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12661d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12661d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12661dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12661e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12661e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12661e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12661ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12661f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12661f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12661fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12661ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1266203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126620830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126620ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126621580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1266219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126621e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1266222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126622740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126622bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126623020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126623490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1266241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126624650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126624f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1266253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126625810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1266260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1266269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126626e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1266272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126628000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126628470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1266288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1266291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126629aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126629f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12662a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12662a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12662ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12662b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12662b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12662b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12662be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12662c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12662c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12662cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12662cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12662d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12662d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12662dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12662e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12662e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12662ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12662eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12662f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12662f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12662fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1266300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126630e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126631270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1266316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126631fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1266328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126633180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1266335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126633ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126634340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1266347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126634c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126635090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126635500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126635970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126635de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126636250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1266366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126636b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126636fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126637410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126637cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126638160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1266385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126638a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126638eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126639320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126639c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12663a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12663a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12663a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12663adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12663b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12663b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12663bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12663bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12663c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12663c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12663ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12663d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12663d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12663da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12663de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12663e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12663e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12663ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12663f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12663f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12663f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12663fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126640210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126640680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126640c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126641080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1266414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126642040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1266425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126642a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126642ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126643780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126643bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126644060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1266444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126644940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126644db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126645220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126645690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126645b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126645f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1266463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126646cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126647130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1266475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126647a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126647e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1266482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126648bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126649040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1266494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126649920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126649d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12664a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12664a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12664aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12664af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12664b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12664b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12664bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12664c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12664c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12664c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12664ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12664d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12664d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12664dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12664e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12664e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12664e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12664ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12664f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12664f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12664fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12664ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1266503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126650810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126650c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1266510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126651560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1266519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126651e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1266522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126652720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126652b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126653000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126653470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1266538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126653d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1266541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126654630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126654aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126654f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126655380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1266557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126655c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1266566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126657510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126657c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126657ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126658360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126658960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126658f70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1230044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1230056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1230063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1230092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12300a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12300a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12300af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12300b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12300be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12300c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12300cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12300d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12300dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12300dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12300e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12300e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12300e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12300edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12300f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12300f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12300fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12300fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1230102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1230114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1230133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1230149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1230152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1230177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1230180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1230189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1230196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12301a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12301a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12301ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12301b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12301b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12301ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12301bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12301c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12301c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12301cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12301d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12301d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12301d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12301ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12301e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12301e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12301eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12301efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12301f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12301f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12301fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1230205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1230217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1230224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1230236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123023b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1230255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123025ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123026780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123026bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1230274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123028690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123028f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1230293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123029850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12302a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12302a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12302aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12302ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12302b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12302b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12302bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12302c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12302c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12302c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12302cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12302d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12302d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12302dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12302df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12302e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12302e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12302eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12302f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12302f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12302f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12302fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1230302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123030740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123031020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123031900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123031d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1230321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123032650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123032ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1230333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123033810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1230340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123034560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1230349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1230352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123035b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1230368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123036d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1230371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123037630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123037aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123037f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123038380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1230387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123038c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1230390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123039540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1230399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12303a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12303a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12303ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12303afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12303b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12303b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12303bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12303c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12303c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12303ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12303cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12303d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12303d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12303dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12303e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12303e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12303e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12303ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12303f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12303f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12303fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12303ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123040550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1230409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123040e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123041980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123041c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123042370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1230427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123042c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1230430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123043530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1230439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123044280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1230446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123044b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123045440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1230458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123046190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123046600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123046a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123046ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1230477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1230480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123048510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123048980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123049260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1230496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123049b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123049fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12304a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12304a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12304b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12304b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12304b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12304bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12304c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12304c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12304caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12304cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12304d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12304d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12304dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12304e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12304e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12304ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12304ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12304f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12304f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12304fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123050030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1230504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123050910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123050d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1230511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123051660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123051ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123051f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1230523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123052820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123052c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123053100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123053570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1230539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123053e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1230542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123054ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123055010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123055480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1230558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123056360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123056a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1230571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1230578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123057b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123057ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1230585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123058c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.942s
user	0m0.243s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
