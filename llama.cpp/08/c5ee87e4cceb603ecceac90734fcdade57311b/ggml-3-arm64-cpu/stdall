Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.7.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.7.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.7.0-py3-none-any.whl size=3229 sha256=e77d3489e0beea138863799a05abb421092af60b67ffc67b376508ee3f0720bf
  Stored in directory: /tmp/pip-ephem-wheel-cache-waqpukfj/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.7.0
    Uninstalling gguf-0.7.0:
      Successfully uninstalled gguf-0.7.0
Successfully installed gguf-0.7.0
+ gg_run_ctest_debug
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_debug.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.764s
user	0m0.436s
sys	0m0.332s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Built target ggml
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Linking C static library libggml_static.a
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  8%] Built target build_info
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target gguf
[  9%] Built target llama
[ 11%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 19%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 20%] Linking CXX executable ../../bin/benchmark
[ 21%] Linking CXX executable ../../bin/quantize-stats
[ 21%] Built target llava
[ 22%] Linking CXX executable ../../bin/quantize
[ 23%] Linking CXX static library libcommon.a
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target test-c
[ 23%] Built target benchmark
[ 23%] Built target llava_static
[ 23%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-quantize-perf
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-quantize-fns
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 42%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 43%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Linking CXX executable ../bin/test-chat-template
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 48%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 50%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 50%] Linking CXX executable ../bin/test-grad0
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 55%] Linking CXX executable ../../bin/baby-llama
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-llama-grammar
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 60%] Linking CXX executable ../../bin/batched
[ 61%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 61%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-autorelease
[ 65%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 67%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../bin/test-model-load-cancel
[ 67%] Built target quantize
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 68%] Linking CXX executable ../../bin/finetune
[ 69%] Linking CXX executable ../../bin/beam-search
[ 69%] Linking CXX executable ../../bin/batched-bench
[ 70%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 71%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 73%] Linking CXX executable ../../bin/llava-cli
[ 72%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Linking CXX executable ../../bin/embedding
[ 74%] Built target test-quantize-fns
[ 75%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 76%] Linking CXX executable ../../bin/main
[ 77%] Linking CXX executable ../../bin/infill
[ 78%] Linking CXX executable ../../bin/parallel
[ 79%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 80%] Linking CXX executable ../../bin/perplexity
[ 80%] Linking CXX executable ../../bin/tokenize
[ 81%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 81%] Built target test-grad0
[ 81%] Linking CXX executable ../../bin/save-load-state
[ 82%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 83%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 84%] Built target test-grammar-parser
[ 84%] Built target test-quantize-perf
[ 85%] Linking CXX executable ../../bin/simple
[ 85%] Built target test-rope
[ 86%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 87%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 87%] Linking CXX executable ../../bin/passkey
[ 88%] Linking CXX executable ../../bin/lookup
[ 89%] Linking CXX executable ../../bin/lookahead
[ 90%] Linking CXX executable ../../bin/speculative
[ 91%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 92%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/imatrix
[ 96%] Linking CXX executable ../../bin/q8dot
[ 97%] Linking CXX executable ../../bin/train-text-from-scratch
[ 97%] Built target test-backend-ops
[ 97%] Built target quantize-stats
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target convert-llama2c-to-ggml
[100%] Linking CXX executable ../../bin/server
[100%] Built target q8dot
[100%] Built target vdot
[100%] Built target export-lora
[100%] Built target test-chat-template
[100%] Built target test-model-load-cancel
[100%] Built target test-llama-grammar
[100%] Built target test-autorelease
[100%] Built target test-sampling
[100%] Built target test-tokenizer-1-llama
[100%] Built target test-tokenizer-0-llama
[100%] Built target batched
[100%] Built target tokenize
[100%] Built target test-tokenizer-1-bpe
[100%] Built target batched-bench
[100%] Built target embedding
[100%] Built target test-tokenizer-0-falcon
[100%] Built target save-load-state
[100%] Built target llava-cli
[100%] Built target speculative
[100%] Built target baby-llama
[100%] Built target beam-search
[100%] Built target lookahead
[100%] Built target llama-bench
[100%] Built target simple
[100%] Built target parallel
[100%] Built target passkey
[100%] Built target infill
[100%] Built target main
[100%] Built target perplexity
[100%] Built target finetune
[100%] Built target train-text-from-scratch
[100%] Built target lookup
[100%] Built target imatrix
[100%] Built target server

real	0m3.788s
user	0m18.830s
sys	0m5.940s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed   14.51 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed   10.94 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.04 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.01 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.18 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    1.05 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    2.79 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    3.20 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    6.20 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed    9.19 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    5.07 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    5.15 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    5.08 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    4.84 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    4.84 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    5.41 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.01 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    3.07 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed  134.86 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.08 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    = 216.50 sec*proc (21 tests)

Total Test time (real) = 216.51 sec

real	3m36.524s
user	6m18.477s
sys	0m15.457s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_release.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.761s
user	0m0.428s
sys	0m0.336s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_release-make.log
+ make -j
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target ggml
[  5%] Built target build_info
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  6%] Linking C static library libggml_static.a
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Built target gguf
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 14%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 14%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 18%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 20%] Linking CXX executable ../../bin/benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 22%] Linking CXX static library libcommon.a
[ 23%] Linking CXX executable ../../bin/quantize-stats
[ 23%] Built target llava
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target test-c
[ 23%] Built target common
[ 23%] Built target benchmark
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 26%] Built target llava_static
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-quantize-fns
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 31%] Built target quantize
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 32%] Linking CXX executable ../bin/test-quantize-perf
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Built target quantize-stats
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-sampling
[ 39%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 44%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 44%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 47%] Linking CXX executable ../bin/test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 52%] Linking CXX executable ../bin/test-llama-grammar
[ 52%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 52%] Linking CXX executable ../bin/test-grad0
[ 52%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../../bin/baby-llama
[ 59%] Linking CXX executable ../../bin/batched-bench
[ 63%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 64%] Linking CXX executable ../../bin/batched
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Built target test-quantize-perf
[ 67%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 68%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 69%] Linking CXX executable ../../bin/beam-search
[ 69%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 69%] Built target test-sampling
[ 70%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 70%] Built target test-grad0
[ 70%] Built target test-grammar-parser
[ 72%] Linking CXX executable ../../bin/infill
[ 72%] Linking CXX executable ../../bin/finetune
[ 72%] Built target test-llama-grammar
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/embedding
[ 73%] Linking CXX executable ../../bin/llava-cli
[ 73%] Built target test-tokenizer-1-llama
[ 74%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 74%] Built target test-backend-ops
[ 74%] Built target test-chat-template
[ 75%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 78%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 76%] Linking CXX executable ../../bin/main
[ 78%] Built target test-tokenizer-1-bpe
[ 79%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 79%] Built target test-tokenizer-0-llama
[ 80%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/tokenize
[ 80%] Built target test-model-load-cancel
[ 81%] Linking CXX executable ../../bin/parallel
[ 81%] Built target test-autorelease
[ 81%] Linking CXX executable ../../bin/save-load-state
[ 81%] Built target test-tokenizer-0-falcon
[ 81%] Linking CXX executable ../../bin/passkey
[ 82%] Linking CXX executable ../../bin/perplexity
[ 82%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Linking CXX executable ../../bin/simple
[ 83%] Built target test-rope
[ 83%] Built target baby-llama
[ 84%] Linking CXX executable ../../bin/speculative
[ 85%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 85%] Built target finetune
[ 85%] Built target batched-bench
[ 85%] Built target convert-llama2c-to-ggml
[ 85%] Built target batched
[ 85%] Built target llama-bench
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 87%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 88%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 89%] Linking CXX executable ../../bin/lookahead
[ 90%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 90%] Built target beam-search
[ 91%] Linking CXX executable ../../bin/lookup
[ 91%] Built target embedding
[ 92%] Linking CXX executable ../../bin/train-text-from-scratch
[ 92%] Built target infill
[ 93%] Built target llava-cli
[ 94%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/imatrix
[ 94%] Built target main
[ 95%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Built target save-load-state
[ 97%] Linking CXX executable ../../bin/export-lora
[ 97%] Built target parallel
[ 97%] Built target passkey
[ 98%] Linking CXX executable ../../bin/server
[ 98%] Built target tokenize
[ 98%] Built target perplexity
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target lookahead
[ 99%] Built target speculative
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target simple
[100%] Built target lookup
[100%] Built target train-text-from-scratch
[100%] Built target imatrix
[100%] Built target export-lora
[100%] Built target vdot
[100%] Built target q8dot
[100%] Built target server

real	0m1.503s
user	0m5.563s
sys	0m4.310s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed    8.27 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed    5.60 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.01 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.00 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.05 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    0.27 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    0.39 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    0.47 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    0.97 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed    1.55 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    0.80 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    0.78 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    0.82 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    0.77 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    0.77 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    0.79 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.00 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    3.12 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed   33.25 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.05 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    =  58.75 sec*proc (21 tests)

Total Test time (real) =  58.76 sec

real	0m58.771s
user	1m7.719s
sys	0m10.335s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/embd_bge_small.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model:
2024-02-28 16:50:41 ERROR 404: Not Found.
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:41 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.767s
user	0m0.453s
sys	0m0.317s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/embd_bge_small-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target build_info
[  5%] Built target ggml
[  6%] Linking C static library libggml_static.a
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Built target gguf
[ 11%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Linking CXX executable ../../bin/benchmark
[ 20%] Linking CXX executable ../../bin/quantize
[ 21%] Linking CXX executable ../bin/test-c
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 23%] Linking CXX static library libcommon.a
[ 23%] Built target llava
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target test-c
[ 23%] Built target common
[ 23%] Built target benchmark
[ 23%] Built target quantize
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 24%] Built target llava_static
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 29%] Built target quantize-stats
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Linking CXX executable ../bin/test-sampling
[ 35%] Linking CXX executable ../bin/test-chat-template
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-quantize-fns
[ 39%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 46%] Linking CXX executable ../bin/test-quantize-perf
[ 47%] Linking CXX executable ../bin/test-backend-ops
[ 47%] Linking CXX executable ../bin/test-grad0
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 51%] Linking CXX executable ../bin/test-grammar-parser
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-llama-grammar
[ 55%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../../bin/batched
[ 64%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 65%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 65%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/beam-search
[ 67%] Linking CXX executable ../../bin/baby-llama
[ 67%] Built target test-sampling
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/finetune
[ 69%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 69%] Built target test-tokenizer-0-llama
[ 70%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 70%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 70%] Built target test-grad0
[ 71%] Linking CXX executable ../../bin/infill
[ 72%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 73%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 73%] Built target test-quantize-perf
[ 74%] Linking CXX executable ../../bin/embedding
[ 74%] Built target test-chat-template
[ 75%] Linking CXX executable ../../bin/parallel
[ 76%] Linking CXX executable ../../bin/llava-cli
[ 76%] Built target test-llama-grammar
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 77%] Linking CXX executable ../../bin/main
[ 77%] Built target test-backend-ops
[ 77%] Built target test-model-load-cancel
[ 77%] Built target test-grammar-parser
[ 77%] Built target test-tokenizer-0-falcon
[ 77%] Linking CXX executable ../../bin/tokenize
[ 78%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 78%] Built target test-rope
[ 78%] Built target test-tokenizer-1-bpe
[ 78%] Built target test-tokenizer-1-llama
[ 79%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 80%] Built target batched
[ 81%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Built target beam-search
[ 81%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 82%] Linking CXX executable ../../bin/perplexity
[ 82%] Linking CXX executable ../../bin/passkey
[ 82%] Built target batched-bench
[ 82%] Built target infill
[ 82%] Built target test-autorelease
[ 82%] Built target baby-llama
[ 82%] Linking CXX executable ../../bin/save-load-state
[ 82%] Built target convert-llama2c-to-ggml
[ 84%] Linking CXX executable ../../bin/speculative
[ 83%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 85%] Linking CXX executable ../../bin/simple
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 86%] Built target finetune
[ 86%] Built target main
[ 86%] Built target llava-cli
[ 87%] Linking CXX executable ../../bin/lookahead
[ 88%] Linking CXX executable ../../bin/lookup
[ 88%] Built target llama-bench
[ 89%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 89%] Built target embedding
[ 90%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 90%] Built target parallel
[ 90%] Built target tokenize
[ 90%] Built target perplexity
[ 91%] Linking CXX executable ../../bin/train-text-from-scratch
[ 92%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/imatrix
[ 94%] Built target passkey
[ 95%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 95%] Built target speculative
[ 95%] Built target save-load-state
[ 96%] Linking CXX executable ../../bin/vdot
[ 97%] Linking CXX executable ../../bin/export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Built target simple
[ 98%] Built target lookahead
[ 99%] Linking CXX executable ../../bin/q8dot
[100%] Linking CXX executable ../../bin/server
[100%] Built target lookup
[100%] Built target train-text-from-scratch
[100%] Built target vdot
[100%] Built target export-lora
[100%] Built target imatrix
[100%] Built target q8dot
[100%] Built target server

real	0m1.511s
user	0m5.341s
sys	0m4.457s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
Set model tokenizer
fname_tokenizer: ../models-mnt/bge-small
gguf: Setting special token type pad to 0
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llama_model_quantize_internal ============ Strange model: n_attention_wv = 12, n_ffn_down = 24, hparams.n_layer = 12
llama_model_quantize_internal: meta size = 760800 bytes
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.35 MiB ->    11.88 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f16, size =    0.375 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.086 0.108 0.230 0.109 0.088 0.066 0.046 0.031 0.020 0.027 
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.046 0.067 0.089 0.107 0.227 0.107 0.087 0.068 0.047 0.031 0.019 0.027 
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.018 0.030 0.045 0.066 0.086 0.109 0.237 0.109 0.088 0.065 0.046 0.030 0.018 0.026 
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.032 0.048 0.066 0.089 0.106 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.028 
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.030 0.021 0.033 0.049 0.069 0.091 0.110 0.232 0.105 0.084 0.063 0.044 0.028 0.017 0.024 
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.061 0.087 0.114 0.259 0.115 0.087 0.061 0.041 0.026 0.016 0.025 
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.047 0.067 0.089 0.108 0.230 0.108 0.087 0.066 0.046 0.030 0.019 0.026 
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.240 0.108 0.088 0.065 0.046 0.029 0.018 0.026 
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.087 0.109 0.239 0.109 0.087 0.064 0.045 0.030 0.019 0.027 
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.090 0.107 0.226 0.106 0.089 0.066 0.047 0.031 0.020 0.027 
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.028 0.019 0.030 0.046 0.066 0.089 0.111 0.241 0.108 0.085 0.062 0.043 0.028 0.017 0.025 
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.025 0.039 0.059 0.085 0.116 0.269 0.115 0.086 0.060 0.040 0.025 0.015 0.025 
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.089 0.107 0.231 0.107 0.087 0.065 0.046 0.031 0.019 0.027 
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.046 0.065 0.088 0.110 0.237 0.109 0.088 0.064 0.045 0.029 0.018 0.027 
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.066 0.087 0.109 0.237 0.110 0.087 0.066 0.046 0.029 0.019 0.027 
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.021 0.032 0.048 0.066 0.087 0.107 0.224 0.104 0.089 0.067 0.049 0.032 0.020 0.027 
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.240 0.108 0.087 0.064 0.044 0.029 0.018 0.026 
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.058 0.085 0.116 0.273 0.116 0.086 0.059 0.039 0.024 0.016 0.025 
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.066 0.088 0.109 0.234 0.108 0.089 0.066 0.046 0.029 0.019 0.027 
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.044 0.063 0.087 0.111 0.247 0.114 0.088 0.062 0.043 0.028 0.017 0.026 
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.064 0.088 0.110 0.243 0.111 0.088 0.064 0.044 0.029 0.018 0.026 
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.047 0.068 0.087 0.106 0.225 0.106 0.088 0.068 0.046 0.031 0.020 0.027 
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.239 0.108 0.087 0.065 0.045 0.029 0.018 0.027 
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.059 0.086 0.115 0.269 0.115 0.086 0.060 0.040 0.025 0.015 0.025 
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.046 0.064 0.088 0.109 0.235 0.110 0.088 0.065 0.045 0.031 0.018 0.027 
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.027 0.043 0.064 0.088 0.111 0.245 0.112 0.087 0.064 0.043 0.028 0.017 0.026 
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.027 0.042 0.062 0.085 0.113 0.256 0.112 0.087 0.061 0.042 0.027 0.017 0.026 
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.048 0.067 0.089 0.105 0.226 0.107 0.088 0.066 0.047 0.032 0.020 0.027 
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.018 0.029 0.044 0.063 0.087 0.108 0.242 0.110 0.088 0.065 0.045 0.030 0.019 0.027 
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.024 0.038 0.059 0.085 0.116 0.275 0.116 0.086 0.059 0.038 0.024 0.015 0.025 
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.089 0.107 0.233 0.107 0.087 0.066 0.046 0.030 0.019 0.027 
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.044 0.064 0.088 0.111 0.240 0.110 0.087 0.064 0.045 0.029 0.018 0.026 
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.027 0.040 0.061 0.085 0.112 0.262 0.114 0.086 0.061 0.042 0.027 0.016 0.026 
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.086 0.105 0.225 0.105 0.089 0.067 0.048 0.032 0.020 0.027 
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.017 0.028 0.042 0.062 0.086 0.109 0.243 0.111 0.089 0.065 0.046 0.030 0.019 0.028 
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.040 0.060 0.087 0.115 0.267 0.116 0.086 0.060 0.040 0.025 0.015 0.025 
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.067 0.088 0.106 0.236 0.108 0.087 0.066 0.046 0.030 0.018 0.028 
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.017 0.029 0.044 0.064 0.088 0.110 0.244 0.112 0.087 0.064 0.045 0.028 0.018 0.026 
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.040 0.059 0.085 0.115 0.267 0.114 0.085 0.060 0.041 0.025 0.016 0.025 
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.105 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.064 0.087 0.110 0.240 0.110 0.088 0.065 0.045 0.029 0.019 0.027 
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.017 0.027 0.042 0.062 0.086 0.113 0.255 0.113 0.087 0.062 0.042 0.027 0.017 0.026 
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.047 0.064 0.088 0.110 0.235 0.109 0.089 0.065 0.045 0.029 0.018 0.027 
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.086 0.107 0.236 0.111 0.088 0.065 0.045 0.029 0.019 0.027 
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.041 0.060 0.086 0.113 0.263 0.113 0.086 0.061 0.041 0.027 0.017 0.025 
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.021 0.031 0.047 0.067 0.088 0.105 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.110 0.242 0.109 0.088 0.064 0.045 0.029 0.018 0.027 
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.016 0.027 0.041 0.062 0.087 0.113 0.256 0.113 0.087 0.062 0.042 0.027 0.016 0.025 
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.018 0.028 0.044 0.065 0.088 0.112 0.239 0.111 0.087 0.065 0.044 0.028 0.018 0.026 
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.062 0.089 0.113 0.247 0.113 0.087 0.063 0.042 0.027 0.017 0.026 
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.041 0.059 0.086 0.113 0.265 0.113 0.086 0.061 0.041 0.026 0.016 0.026 
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.089 0.104 0.225 0.105 0.087 0.067 0.049 0.032 0.020 0.027 
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.028 0.019 0.030 0.046 0.066 0.088 0.109 0.238 0.109 0.087 0.064 0.044 0.029 0.018 0.025 
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.017 0.027 0.042 0.062 0.087 0.112 0.253 0.113 0.087 0.063 0.042 0.027 0.017 0.025 
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.044 0.065 0.088 0.108 0.236 0.109 0.087 0.066 0.047 0.030 0.019 0.027 
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.044 0.065 0.088 0.109 0.237 0.111 0.087 0.065 0.045 0.029 0.019 0.026 
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.086 0.110 0.242 0.110 0.087 0.064 0.045 0.029 0.017 0.027 
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.021 0.031 0.047 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.031 0.020 0.028 
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.108 0.234 0.110 0.088 0.066 0.046 0.030 0.019 0.027 
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.017 0.028 0.044 0.064 0.088 0.111 0.245 0.112 0.087 0.064 0.043 0.028 0.017 0.026 
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.045 0.065 0.088 0.109 0.233 0.111 0.088 0.065 0.047 0.031 0.019 0.027 
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.066 0.086 0.108 0.233 0.109 0.088 0.066 0.046 0.030 0.019 0.027 
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.030 0.045 0.065 0.087 0.109 0.244 0.107 0.086 0.064 0.044 0.029 0.019 0.027 
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.068 0.088 0.107 0.223 0.105 0.089 0.068 0.047 0.032 0.020 0.027 
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.060 0.086 0.117 0.267 0.117 0.087 0.060 0.039 0.024 0.015 0.025 
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.047 0.066 0.089 0.108 0.233 0.108 0.089 0.065 0.046 0.030 0.018 0.027 
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.030 0.046 0.066 0.088 0.109 0.233 0.109 0.087 0.066 0.045 0.031 0.019 0.026 
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.025 0.017 0.028 0.043 0.064 0.088 0.111 0.245 0.110 0.087 0.064 0.044 0.027 0.018 0.027 
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.068 0.088 0.105 0.225 0.106 0.086 0.067 0.048 0.032 0.020 0.028 
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.061 0.085 0.112 0.257 0.115 0.088 0.063 0.043 0.027 0.017 0.026 
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.021 0.009 0.014 0.023 0.038 0.064 0.117 0.415 0.120 0.067 0.040 0.025 0.016 0.009 0.022 
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.46 MB
llama_model_quantize_internal: quant size  =    34.00 MB
llama_model_quantize_internal: hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.241 0.109 0.087 0.064 0.045 0.029 0.018 0.026 

main: quantize time =   162.85 ms
main:    total time =   162.85 ms
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139047
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU input buffer size   =     2.76 MiB
llama_new_context_with_model:        CPU compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1
embedding 0: -0.043995 -0.019933 0.007650 -0.000821 0.001375 -0.037048 0.109458 0.042581 0.092053 -0.015919 0.006731 -0.035685 -0.017900 0.015026 0.018126 0.015892 -0.011297 0.010435 -0.085234 -0.008461 0.091370 -0.017056 -0.060353 -0.024474 0.027577 0.076067 0.027992 -0.014567 0.017634 -0.033296 -0.037881 -0.019005 0.068671 -0.009861 -0.025038 0.072344 -0.046537 0.011001 -0.050245 0.047706 0.032395 -0.011769 0.022055 0.049634 0.010469 0.005772 -0.028886 0.008943 -0.018494 -0.051476 -0.046042 0.030489 -0.035401 0.054227 -0.069654 0.044238 0.029789 0.046299 0.073398 -0.042574 0.076084 0.038840 -0.181151 0.082467 0.042228 -0.064558 -0.060119 -0.017845 0.006460 0.005841 0.017179 -0.026620 0.064580 0.112602 0.035121 -0.067452 0.027106 -0.067323 -0.033518 -0.033213 0.033260 0.013514 -0.003310 -0.037496 -0.052052 0.055116 -0.001944 -0.038289 0.064392 0.028807 -0.043321 -0.029226 -0.039454 0.036305 0.008370 -0.015473 -0.036599 0.018126 0.028594 0.342802 -0.044460 0.056124 0.017632 -0.020892 -0.066748 0.000137 -0.037908 -0.030056 -0.008555 -0.021588 0.000544 -0.003241 0.003986 0.018943 -0.008539 0.025855 0.049382 0.000091 0.050952 -0.042475 -0.031913 0.023598 0.030654 -0.023169 -0.046256 -0.079271 0.115208 0.046723 0.027822 -0.040714 0.067770 -0.022936 0.010353 -0.032976 -0.018325 0.043832 0.024307 0.052399 0.007502 0.008923 0.011246 -0.074644 -0.065568 -0.026736 -0.041188 -0.023902 0.026700 0.006932 0.027760 0.052904 -0.036687 0.057717 -0.000162 0.031771 -0.019793 -0.022095 0.041053 -0.058918 0.019631 0.043146 0.043553 0.041586 -0.022511 0.027036 -0.021845 0.005466 -0.041300 -0.001272 0.024442 0.002073 0.044347 -0.022775 0.043659 0.064768 0.055422 0.037091 -0.000909 0.046159 0.045790 -0.008488 0.063064 -0.073225 -0.011955 0.032094 0.023934 0.014743 -0.033685 0.001109 -0.015789 -0.019008 0.047875 0.110827 0.028402 0.031333 -0.013265 -0.057491 0.006659 0.005141 -0.012298 -0.051440 -0.000975 -0.017679 -0.019427 -0.040954 0.009161 -0.057951 0.050929 0.052360 -0.009598 -0.040269 -0.014092 -0.024873 -0.017238 0.006311 0.006574 -0.026906 0.015610 0.030769 0.002563 0.023213 -0.022206 -0.098558 -0.051089 -0.278018 -0.014959 -0.061551 -0.027237 0.017694 -0.010925 -0.017099 0.035076 0.046989 -0.015413 0.015218 -0.025475 0.047835 -0.005982 -0.000731 -0.061026 -0.068926 -0.060352 -0.035965 0.043334 -0.055004 0.015064 0.000555 -0.058198 -0.010421 0.012639 0.151491 0.127108 -0.013574 0.041983 -0.025677 0.014046 -0.001049 -0.150491 0.044877 0.005313 -0.036290 -0.029791 -0.020228 -0.034879 0.010236 0.033565 -0.048163 -0.051816 -0.017452 -0.023475 0.047373 0.052068 -0.016805 -0.055464 0.025852 -0.005687 0.010743 0.038719 0.008187 -0.009729 -0.105754 -0.027403 -0.096116 0.025068 -0.011241 0.092388 0.056098 0.003761 0.027800 0.002087 -0.051138 -0.039886 -0.013538 -0.044974 -0.015340 0.002899 -0.043468 -0.077964 0.065219 -0.006851 -0.001666 -0.014646 0.071589 0.023727 -0.037187 0.009186 0.001551 -0.032272 0.015463 0.037888 0.000353 -0.053211 0.021333 -0.039820 0.000059 0.013388 0.019818 -0.057886 0.006480 -0.049547 -0.267863 0.039150 -0.067986 0.038246 -0.012344 0.041488 -0.016125 0.052420 -0.071345 0.011351 0.024763 -0.007186 0.082081 0.028537 -0.021519 0.040530 -0.004555 -0.074596 -0.014751 0.020040 0.002297 0.023129 0.197207 -0.043236 -0.025994 -0.004933 -0.019270 0.074258 0.001735 -0.031985 -0.036583 -0.045067 0.000554 -0.011569 0.018112 -0.029462 -0.008452 0.006454 0.050804 -0.014958 0.006202 0.026099 -0.030812 0.048044 0.114084 -0.040837 -0.011479 0.005396 -0.003589 0.025159 -0.059153 0.013757 -0.010376 0.038696 0.051459 0.035447 0.035003 -0.017027 0.026406 -0.014518 -0.050038 0.003227 0.054119 0.039730 -0.039126 



llama_print_timings:        load time =      39.77 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =      19.46 ms /     9 tokens (    2.16 ms per token,   462.44 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =      87.09 ms /    10 tokens

real	0m0.213s
user	0m0.229s
sys	0m0.056s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139047
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    34.00 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU input buffer size   =     2.76 MiB
llama_new_context_with_model:        CPU compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1
embedding 0: -0.044648 -0.020095 0.008152 -0.001860 0.002470 -0.037214 0.108845 0.043202 0.092183 -0.015342 0.005511 -0.036683 -0.018470 0.014659 0.017568 0.014399 -0.013224 0.011013 -0.084987 -0.006778 0.092496 -0.016748 -0.061849 -0.024172 0.027359 0.076665 0.027677 -0.014943 0.018285 -0.035276 -0.037496 -0.018876 0.068323 -0.011468 -0.023479 0.073357 -0.046146 0.010780 -0.051561 0.049270 0.033132 -0.012643 0.021644 0.049657 0.010751 0.005783 -0.028955 0.008475 -0.017766 -0.053982 -0.045658 0.029641 -0.035478 0.052504 -0.068460 0.042431 0.030335 0.046764 0.074249 -0.041989 0.074279 0.038535 -0.182798 0.081687 0.042359 -0.066400 -0.059340 -0.016805 0.007074 0.004832 0.017859 -0.026695 0.065355 0.112591 0.035083 -0.067538 0.027127 -0.067649 -0.033503 -0.034298 0.033397 0.015049 -0.004254 -0.036707 -0.051673 0.054244 -0.002444 -0.036797 0.063096 0.029624 -0.041910 -0.029575 -0.039074 0.036742 0.007421 -0.014835 -0.036313 0.017477 0.030399 0.345781 -0.043871 0.056811 0.017294 -0.021348 -0.063377 -0.000226 -0.038198 -0.029706 -0.008972 -0.020432 0.001054 -0.002517 0.004050 0.017994 -0.010269 0.025246 0.048338 -0.000304 0.049710 -0.042534 -0.030876 0.023882 0.031295 -0.023389 -0.043948 -0.079686 0.114883 0.047352 0.026544 -0.040920 0.067479 -0.022999 0.010547 -0.033894 -0.016468 0.044188 0.022437 0.051941 0.007785 0.006787 0.009840 -0.076438 -0.064074 -0.026401 -0.040498 -0.023375 0.027311 0.006782 0.027203 0.051464 -0.037989 0.058173 0.001505 0.033037 -0.019329 -0.021747 0.041788 -0.058934 0.019653 0.042307 0.043142 0.040860 -0.021880 0.028127 -0.022752 0.007200 -0.041423 0.000233 0.024596 0.001730 0.043834 -0.023233 0.044126 0.065598 0.056472 0.037635 0.000427 0.048192 0.046118 -0.008891 0.060391 -0.073346 -0.011842 0.032637 0.022743 0.014332 -0.033426 0.000625 -0.016279 -0.017402 0.047914 0.110760 0.028683 0.031709 -0.012036 -0.056578 0.005100 0.004289 -0.011746 -0.052193 -0.002581 -0.017480 -0.020004 -0.041182 0.010436 -0.059200 0.050859 0.052596 -0.009710 -0.040669 -0.014335 -0.023963 -0.014847 0.005597 0.006783 -0.027083 0.016261 0.030640 0.001371 0.023169 -0.021865 -0.096388 -0.051354 -0.276986 -0.013240 -0.060379 -0.026195 0.016195 -0.009994 -0.016038 0.035308 0.048235 -0.017112 0.014628 -0.024547 0.049780 -0.005602 -0.000434 -0.059480 -0.068910 -0.060569 -0.034893 0.044262 -0.055245 0.013913 -0.000779 -0.058107 -0.010925 0.010939 0.151242 0.125644 -0.013335 0.041996 -0.025357 0.014158 -0.000322 -0.150425 0.043735 0.006315 -0.036158 -0.030101 -0.019286 -0.034935 0.009344 0.034358 -0.048924 -0.052856 -0.015598 -0.024822 0.048607 0.050189 -0.017854 -0.057621 0.024894 -0.006350 0.012244 0.038675 0.006855 -0.009344 -0.106711 -0.027576 -0.096615 0.024724 -0.010574 0.093018 0.055489 0.003812 0.027871 0.001308 -0.050768 -0.039646 -0.013383 -0.045922 -0.014930 0.002046 -0.044317 -0.076845 0.065247 -0.006665 -0.001394 -0.015551 0.071043 0.024314 -0.036174 0.008053 0.001738 -0.032680 0.016803 0.038860 0.000293 -0.051371 0.021334 -0.038418 -0.000554 0.012652 0.020263 -0.058353 0.006411 -0.049726 -0.268843 0.038928 -0.067277 0.037000 -0.011681 0.042094 -0.015772 0.050464 -0.071982 0.012641 0.024267 -0.007312 0.081738 0.029433 -0.021097 0.041088 -0.003933 -0.074237 -0.015101 0.018650 0.002402 0.022366 0.196279 -0.042875 -0.024678 -0.004525 -0.018832 0.073611 0.002230 -0.032109 -0.037091 -0.045308 -0.000571 -0.010465 0.018075 -0.027516 -0.009616 0.005757 0.049919 -0.014695 0.006332 0.026321 -0.031619 0.048354 0.112807 -0.040569 -0.011565 0.005411 -0.003452 0.025768 -0.060478 0.014234 -0.010107 0.037471 0.050044 0.035656 0.036415 -0.016698 0.026776 -0.014915 -0.050522 0.003945 0.053370 0.040635 -0.039820 



llama_print_timings:        load time =      24.49 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       9.43 ms /     9 tokens (    1.05 ms per token,   954.50 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =      75.64 ms /    10 tokens

real	0m0.188s
user	0m0.177s
sys	0m0.047s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_3b_v2
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:47 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json [506/506] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:47 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:47 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 16:50:47 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json [137/137] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/open-llama/3B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_QKK_64=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.768s
user	0m0.393s
sys	0m0.377s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-make.log
+ make -j
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target ggml
[  5%] Built target build_info
[  7%] Linking C static library libggml_static.a
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Built target gguf
[  9%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 10%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 12%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 18%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 20%] Linking CXX executable ../../bin/quantize
[ 21%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Linking CXX executable ../../bin/benchmark
[ 23%] Linking CXX static library libcommon.a
[ 23%] Built target llava
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target test-c
[ 23%] Built target common
[ 23%] Built target quantize
[ 23%] Built target benchmark
[ 23%] Built target llava_static
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 26%] Built target quantize-stats
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 38%] Linking CXX executable ../bin/test-chat-template
[ 37%] Linking CXX executable ../bin/test-quantize-fns
[ 38%] Linking CXX executable ../bin/test-quantize-perf
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Linking CXX executable ../bin/test-sampling
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 48%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 52%] Linking CXX executable ../bin/test-grad0
[ 52%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-grammar-parser
[ 55%] Linking CXX executable ../bin/test-llama-grammar
[ 56%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 57%] Linking CXX executable ../bin/test-rope
[ 58%] Linking CXX executable ../../bin/baby-llama
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 63%] Linking CXX executable ../bin/test-backend-ops
[ 63%] Built target test-grad0
[ 64%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 65%] Linking CXX executable ../../bin/batched
[ 66%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 66%] Linking CXX executable ../../bin/batched-bench
[ 65%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 66%] Built target test-quantize-fns
[ 67%] Linking CXX executable ../../bin/beam-search
[ 67%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/embedding
[ 70%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 70%] Built target test-chat-template
[ 70%] Built target test-tokenizer-0-falcon
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 71%] Linking CXX executable ../../bin/finetune
[ 71%] Built target test-tokenizer-0-llama
[ 72%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 72%] Linking CXX executable ../../bin/infill
[ 72%] Built target test-sampling
[ 72%] Built target test-tokenizer-1-llama
[ 72%] Built target test-grammar-parser
[ 73%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 73%] Built target baby-llama
[ 74%] Linking CXX executable ../../bin/llava-cli
[ 75%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/main
[ 76%] Built target test-llama-grammar
[ 76%] Built target test-tokenizer-1-bpe
[ 76%] Built target test-rope
[ 76%] Linking CXX executable ../../bin/tokenize
[ 76%] Built target test-autorelease
[ 76%] Built target test-backend-ops
[ 77%] Linking CXX executable ../../bin/parallel
[ 77%] Built target batched-bench
[ 77%] Built target test-model-load-cancel
[ 77%] Built target batched
[ 77%] Built target beam-search
[ 77%] Built target embedding
[ 78%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 79%] Built target convert-llama2c-to-ggml
[ 80%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 80%] Built target infill
[ 80%] Linking CXX executable ../../bin/save-load-state
[ 80%] Built target llama-bench
[ 81%] Linking CXX executable ../../bin/perplexity
[ 81%] Built target finetune
[ 82%] Linking CXX executable ../../bin/simple
[ 82%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 84%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 84%] Built target llava-cli
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 86%] Linking CXX executable ../../bin/speculative
[ 87%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 87%] Linking CXX executable ../../bin/passkey
[ 87%] Built target main
[ 88%] Linking CXX executable ../../bin/lookahead
[ 88%] Built target tokenize
[ 89%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 90%] Linking CXX executable ../../bin/lookup
[ 92%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 92%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 93%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 95%] Linking CXX executable ../../bin/train-text-from-scratch
[ 95%] Built target parallel
[ 96%] Linking CXX executable ../../bin/vdot
[ 96%] Built target save-load-state
[ 97%] Linking CXX executable ../../bin/q8dot
[ 99%] Linking CXX executable ../../bin/server
[ 99%] Linking CXX executable ../../bin/export-lora
[100%] Linking CXX executable ../../bin/imatrix
[100%] Built target perplexity
[100%] Built target simple
[100%] Built target speculative
[100%] Built target passkey
[100%] Built target lookahead
[100%] Built target vdot
[100%] Built target q8dot
[100%] Built target export-lora
[100%] Built target lookup
[100%] Built target train-text-from-scratch
[100%] Built target imatrix
[100%] Built target server

real	0m1.509s
user	0m5.305s
sys	0m4.646s
+ python3 ../convert.py ../models-mnt/open-llama/3B-v2
Loading model file ../models-mnt/open-llama/3B-v2/pytorch_model.bin
params = Params(n_vocab=32000, n_embd=3200, n_layer=26, n_ctx=2048, n_ff=8640, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/3B-v2'))
Found vocab files: {'tokenizer.model': PosixPath('../models-mnt/open-llama/3B-v2/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}
Loading vocab file '../models-mnt/open-llama/3B-v2/tokenizer.model', type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 3200]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [3200]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [3200]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [3200]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [3200]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [3200]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [3200]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [3200]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [3200]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [3200]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [3200]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [3200]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [3200]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [3200]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [3200]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [3200]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [3200]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [3200]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [3200]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [3200]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [3200]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [3200]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [3200]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [3200]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [3200]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [3200]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [3200]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [3200]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [3200]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [3200]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [3200]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [3200]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [3200]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [3200]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [3200]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [3200]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [3200]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [3200]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [3200]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [3200]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [3200]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [3200]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [3200]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [3200]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [3200]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [3200]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [3200]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [3200]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [3200]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [3200]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [3200]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [3200]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [3200]
model.norm.weight                                -> output_norm.weight                       | F16    | [3200]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 3200]
Writing ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/237] Writing tensor token_embd.weight                      | size  32000 x   3200  | type F16  | T+   0
[  2/237] Writing tensor blk.0.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[  3/237] Writing tensor blk.0.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[  4/237] Writing tensor blk.0.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[  5/237] Writing tensor blk.0.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[  6/237] Writing tensor blk.0.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[  7/237] Writing tensor blk.0.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[  8/237] Writing tensor blk.0.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[  9/237] Writing tensor blk.0.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 10/237] Writing tensor blk.0.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 11/237] Writing tensor blk.1.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 12/237] Writing tensor blk.1.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 13/237] Writing tensor blk.1.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 14/237] Writing tensor blk.1.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 15/237] Writing tensor blk.1.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 16/237] Writing tensor blk.1.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 17/237] Writing tensor blk.1.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 18/237] Writing tensor blk.1.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 19/237] Writing tensor blk.1.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 20/237] Writing tensor blk.2.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 21/237] Writing tensor blk.2.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 22/237] Writing tensor blk.2.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 23/237] Writing tensor blk.2.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 24/237] Writing tensor blk.2.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 25/237] Writing tensor blk.2.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 26/237] Writing tensor blk.2.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 27/237] Writing tensor blk.2.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 28/237] Writing tensor blk.2.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 29/237] Writing tensor blk.3.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 30/237] Writing tensor blk.3.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 31/237] Writing tensor blk.3.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 32/237] Writing tensor blk.3.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 33/237] Writing tensor blk.3.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 34/237] Writing tensor blk.3.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 35/237] Writing tensor blk.3.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 36/237] Writing tensor blk.3.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 37/237] Writing tensor blk.3.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 38/237] Writing tensor blk.4.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 39/237] Writing tensor blk.4.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 40/237] Writing tensor blk.4.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 41/237] Writing tensor blk.4.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 42/237] Writing tensor blk.4.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 43/237] Writing tensor blk.4.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 44/237] Writing tensor blk.4.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 45/237] Writing tensor blk.4.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 46/237] Writing tensor blk.4.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 47/237] Writing tensor blk.5.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 48/237] Writing tensor blk.5.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 49/237] Writing tensor blk.5.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 50/237] Writing tensor blk.5.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 51/237] Writing tensor blk.5.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 52/237] Writing tensor blk.5.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 53/237] Writing tensor blk.5.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 54/237] Writing tensor blk.5.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 55/237] Writing tensor blk.5.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 56/237] Writing tensor blk.6.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 57/237] Writing tensor blk.6.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 58/237] Writing tensor blk.6.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 59/237] Writing tensor blk.6.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 60/237] Writing tensor blk.6.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 61/237] Writing tensor blk.6.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 62/237] Writing tensor blk.6.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 63/237] Writing tensor blk.6.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 64/237] Writing tensor blk.6.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 65/237] Writing tensor blk.7.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 66/237] Writing tensor blk.7.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 67/237] Writing tensor blk.7.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 68/237] Writing tensor blk.7.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 69/237] Writing tensor blk.7.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 70/237] Writing tensor blk.7.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 71/237] Writing tensor blk.7.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 72/237] Writing tensor blk.7.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 73/237] Writing tensor blk.7.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 74/237] Writing tensor blk.8.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 75/237] Writing tensor blk.8.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 76/237] Writing tensor blk.8.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 77/237] Writing tensor blk.8.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 78/237] Writing tensor blk.8.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 79/237] Writing tensor blk.8.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 80/237] Writing tensor blk.8.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 81/237] Writing tensor blk.8.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 82/237] Writing tensor blk.8.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 83/237] Writing tensor blk.9.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 84/237] Writing tensor blk.9.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 85/237] Writing tensor blk.9.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 86/237] Writing tensor blk.9.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 87/237] Writing tensor blk.9.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 88/237] Writing tensor blk.9.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 89/237] Writing tensor blk.9.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   2
[ 90/237] Writing tensor blk.9.attn_norm.weight                 | size   3200           | type F32  | T+   2
[ 91/237] Writing tensor blk.9.ffn_norm.weight                  | size   3200           | type F32  | T+   2
[ 92/237] Writing tensor blk.10.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 93/237] Writing tensor blk.10.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 94/237] Writing tensor blk.10.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 95/237] Writing tensor blk.10.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[ 96/237] Writing tensor blk.10.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[ 97/237] Writing tensor blk.10.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[ 98/237] Writing tensor blk.10.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[ 99/237] Writing tensor blk.10.attn_norm.weight                | size   3200           | type F32  | T+   2
[100/237] Writing tensor blk.10.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[101/237] Writing tensor blk.11.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[102/237] Writing tensor blk.11.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[103/237] Writing tensor blk.11.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[104/237] Writing tensor blk.11.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[105/237] Writing tensor blk.11.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[106/237] Writing tensor blk.11.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[107/237] Writing tensor blk.11.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[108/237] Writing tensor blk.11.attn_norm.weight                | size   3200           | type F32  | T+   2
[109/237] Writing tensor blk.11.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[110/237] Writing tensor blk.12.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[111/237] Writing tensor blk.12.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[112/237] Writing tensor blk.12.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[113/237] Writing tensor blk.12.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[114/237] Writing tensor blk.12.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[115/237] Writing tensor blk.12.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[116/237] Writing tensor blk.12.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[117/237] Writing tensor blk.12.attn_norm.weight                | size   3200           | type F32  | T+   2
[118/237] Writing tensor blk.12.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[119/237] Writing tensor blk.13.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[120/237] Writing tensor blk.13.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[121/237] Writing tensor blk.13.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[122/237] Writing tensor blk.13.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[123/237] Writing tensor blk.13.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[124/237] Writing tensor blk.13.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[125/237] Writing tensor blk.13.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[126/237] Writing tensor blk.13.attn_norm.weight                | size   3200           | type F32  | T+   2
[127/237] Writing tensor blk.13.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[128/237] Writing tensor blk.14.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[129/237] Writing tensor blk.14.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[130/237] Writing tensor blk.14.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[131/237] Writing tensor blk.14.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[132/237] Writing tensor blk.14.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[133/237] Writing tensor blk.14.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[134/237] Writing tensor blk.14.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[135/237] Writing tensor blk.14.attn_norm.weight                | size   3200           | type F32  | T+   2
[136/237] Writing tensor blk.14.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[137/237] Writing tensor blk.15.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[138/237] Writing tensor blk.15.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[139/237] Writing tensor blk.15.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[140/237] Writing tensor blk.15.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[141/237] Writing tensor blk.15.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[142/237] Writing tensor blk.15.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[143/237] Writing tensor blk.15.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[144/237] Writing tensor blk.15.attn_norm.weight                | size   3200           | type F32  | T+   3
[145/237] Writing tensor blk.15.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[146/237] Writing tensor blk.16.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[147/237] Writing tensor blk.16.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[148/237] Writing tensor blk.16.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[149/237] Writing tensor blk.16.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[150/237] Writing tensor blk.16.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[151/237] Writing tensor blk.16.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[152/237] Writing tensor blk.16.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[153/237] Writing tensor blk.16.attn_norm.weight                | size   3200           | type F32  | T+   3
[154/237] Writing tensor blk.16.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[155/237] Writing tensor blk.17.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[156/237] Writing tensor blk.17.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[157/237] Writing tensor blk.17.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[158/237] Writing tensor blk.17.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[159/237] Writing tensor blk.17.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[160/237] Writing tensor blk.17.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[161/237] Writing tensor blk.17.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[162/237] Writing tensor blk.17.attn_norm.weight                | size   3200           | type F32  | T+   3
[163/237] Writing tensor blk.17.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[164/237] Writing tensor blk.18.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[165/237] Writing tensor blk.18.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[166/237] Writing tensor blk.18.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[167/237] Writing tensor blk.18.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[168/237] Writing tensor blk.18.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[169/237] Writing tensor blk.18.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[170/237] Writing tensor blk.18.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[171/237] Writing tensor blk.18.attn_norm.weight                | size   3200           | type F32  | T+   3
[172/237] Writing tensor blk.18.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[173/237] Writing tensor blk.19.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[174/237] Writing tensor blk.19.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[175/237] Writing tensor blk.19.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[176/237] Writing tensor blk.19.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[177/237] Writing tensor blk.19.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[178/237] Writing tensor blk.19.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[179/237] Writing tensor blk.19.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[180/237] Writing tensor blk.19.attn_norm.weight                | size   3200           | type F32  | T+   3
[181/237] Writing tensor blk.19.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[182/237] Writing tensor blk.20.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[183/237] Writing tensor blk.20.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[184/237] Writing tensor blk.20.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[185/237] Writing tensor blk.20.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[186/237] Writing tensor blk.20.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[187/237] Writing tensor blk.20.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[188/237] Writing tensor blk.20.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[189/237] Writing tensor blk.20.attn_norm.weight                | size   3200           | type F32  | T+   3
[190/237] Writing tensor blk.20.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[191/237] Writing tensor blk.21.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[192/237] Writing tensor blk.21.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[193/237] Writing tensor blk.21.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[194/237] Writing tensor blk.21.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[195/237] Writing tensor blk.21.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[196/237] Writing tensor blk.21.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[197/237] Writing tensor blk.21.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[198/237] Writing tensor blk.21.attn_norm.weight                | size   3200           | type F32  | T+   3
[199/237] Writing tensor blk.21.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[200/237] Writing tensor blk.22.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[201/237] Writing tensor blk.22.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[202/237] Writing tensor blk.22.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[203/237] Writing tensor blk.22.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[204/237] Writing tensor blk.22.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[205/237] Writing tensor blk.22.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[206/237] Writing tensor blk.22.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[207/237] Writing tensor blk.22.attn_norm.weight                | size   3200           | type F32  | T+   4
[208/237] Writing tensor blk.22.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[209/237] Writing tensor blk.23.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[210/237] Writing tensor blk.23.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[211/237] Writing tensor blk.23.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[212/237] Writing tensor blk.23.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[213/237] Writing tensor blk.23.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[214/237] Writing tensor blk.23.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[215/237] Writing tensor blk.23.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[216/237] Writing tensor blk.23.attn_norm.weight                | size   3200           | type F32  | T+   4
[217/237] Writing tensor blk.23.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[218/237] Writing tensor blk.24.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[219/237] Writing tensor blk.24.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[220/237] Writing tensor blk.24.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[221/237] Writing tensor blk.24.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[222/237] Writing tensor blk.24.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[223/237] Writing tensor blk.24.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[224/237] Writing tensor blk.24.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[225/237] Writing tensor blk.24.attn_norm.weight                | size   3200           | type F32  | T+   4
[226/237] Writing tensor blk.24.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[227/237] Writing tensor blk.25.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[228/237] Writing tensor blk.25.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[229/237] Writing tensor blk.25.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[230/237] Writing tensor blk.25.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[231/237] Writing tensor blk.25.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[232/237] Writing tensor blk.25.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[233/237] Writing tensor blk.25.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[234/237] Writing tensor blk.25.attn_norm.weight                | size   3200           | type F32  | T+   4
[235/237] Writing tensor blk.25.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[236/237] Writing tensor output_norm.weight                     | size   3200           | type F32  | T+   4
[237/237] Writing tensor output.weight                          | size  32000 x   3200  | type F16  | T+   4
Wrote ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf q8_0
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   195.31 MiB ->   103.76 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.063 0.087 0.110 0.246 0.110 0.087 0.063 0.044 0.028 0.018 0.026 
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.025 0.017 0.027 0.042 0.062 0.087 0.113 0.255 0.112 0.087 0.062 0.042 0.027 0.017 0.025 
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.238 0.108 0.087 0.065 0.046 0.030 0.019 0.027 
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.088 0.111 0.246 0.111 0.088 0.063 0.044 0.028 0.017 0.026 
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.028 0.020 0.032 0.049 0.067 0.088 0.105 0.223 0.105 0.088 0.067 0.049 0.032 0.020 0.028 
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.028 0.020 0.032 0.049 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.111 0.242 0.110 0.088 0.064 0.044 0.029 0.018 0.026 
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.087 0.067 0.048 0.032 0.020 0.027 
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.087 0.106 0.228 0.106 0.088 0.066 0.048 0.032 0.020 0.027 
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.087 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.087 0.066 0.048 0.031 0.020 0.027 
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.066 0.048 0.032 0.020 0.027 
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.228 0.105 0.088 0.066 0.048 0.032 0.020 0.027 
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.228 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.228 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.087 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    19.53 MiB ->    10.38 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q8_0 .. size =    52.73 MiB ->    28.02 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   195.31 MiB ->   103.76 MiB | hist: 0.000 0.027 0.020 0.033 0.049 0.068 0.088 0.103 0.217 0.104 0.088 0.069 0.050 0.033 0.021 0.028 
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  3472.45 MB
llama_model_quantize_internal: hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 

main: quantize time =  9445.54 ms
main:    total time =  9445.54 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf q4_0
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   195.31 MiB ->    54.93 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.014 0.023 0.035 0.053 0.074 0.098 0.119 0.129 0.119 0.098 0.074 0.053 0.035 0.023 0.019 
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.134 0.123 0.099 0.073 0.051 0.033 0.021 0.017 
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.014 0.022 0.035 0.052 0.074 0.099 0.120 0.128 0.119 0.099 0.075 0.052 0.035 0.022 0.018 
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.024 0.020 
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.014 0.023 0.035 0.053 0.075 0.098 0.118 0.126 0.118 0.098 0.075 0.053 0.035 0.023 0.019 
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.057 0.039 0.025 0.021 
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.120 0.112 0.096 0.076 0.056 0.038 0.025 0.020 
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.119 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    19.53 MiB ->     5.49 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.098 0.076 0.055 0.037 0.024 0.020 
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_0 .. size =    52.73 MiB ->    14.83 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1866.13 MB
llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 

main: quantize time =  6629.51 ms
main:    total time =  6629.51 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf q4_1
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q4_1 .. size =   195.31 MiB ->    61.04 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.068 0.051 0.037 0.025 0.040 
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.024 0.037 0.052 0.068 0.083 0.095 0.101 0.102 0.095 0.083 0.068 0.052 0.037 0.024 0.040 
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.024 0.036 0.050 0.066 0.083 0.097 0.105 0.105 0.097 0.082 0.066 0.050 0.036 0.024 0.040 
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.024 0.035 0.050 0.067 0.083 0.097 0.104 0.104 0.097 0.083 0.067 0.050 0.036 0.024 0.040 
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.051 0.037 0.025 0.040 
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.094 0.082 0.067 0.051 0.037 0.025 0.040 
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.036 0.050 0.067 0.083 0.096 0.104 0.104 0.096 0.083 0.067 0.050 0.036 0.024 0.040 
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.024 0.035 0.050 0.067 0.083 0.097 0.104 0.104 0.097 0.083 0.067 0.050 0.036 0.024 0.040 
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.083 0.067 0.052 0.037 0.025 0.040 
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.094 0.083 0.067 0.051 0.037 0.025 0.040 
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.102 0.101 0.095 0.083 0.067 0.052 0.037 0.025 0.040 
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.051 0.037 0.025 0.040 
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.068 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.025 0.040 
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.082 0.095 0.103 0.103 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.103 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    19.53 MiB ->     6.10 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.096 0.103 0.103 0.096 0.083 0.067 0.050 0.036 0.025 0.040 
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_1 .. size =    52.73 MiB ->    16.48 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2064.25 MB
llama_model_quantize_internal: hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 

main: quantize time =  7216.49 ms
main:    total time =  7216.49 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf q5_0
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q5_0 .. size =   195.31 MiB ->    67.14 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.016 0.025 0.039 0.058 0.080 0.103 0.123 0.128 0.114 0.092 0.069 0.048 0.032 0.020 0.014 
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.038 0.015 0.024 0.037 0.056 0.079 0.105 0.127 0.133 0.117 0.092 0.067 0.046 0.030 0.019 0.013 
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.017 0.027 0.041 0.059 0.080 0.102 0.119 0.124 0.111 0.091 0.070 0.050 0.033 0.021 0.016 
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.015 0.025 0.039 0.057 0.081 0.105 0.123 0.128 0.115 0.093 0.069 0.047 0.031 0.020 0.014 
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.029 0.044 0.062 0.082 0.100 0.112 0.115 0.107 0.092 0.072 0.052 0.036 0.023 0.017 
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.036 0.023 0.016 
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.034 0.022 0.016 
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.017 0.028 0.042 0.061 0.082 0.102 0.116 0.119 0.110 0.092 0.071 0.051 0.034 0.022 0.016 
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.017 0.027 0.041 0.060 0.081 0.102 0.117 0.121 0.111 0.092 0.071 0.050 0.034 0.021 0.016 
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.039 0.016 0.026 0.039 0.058 0.081 0.104 0.122 0.126 0.114 0.093 0.069 0.048 0.032 0.020 0.015 
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.115 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.114 0.118 0.108 0.091 0.072 0.052 0.035 0.022 0.016 
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.119 0.109 0.091 0.071 0.051 0.035 0.022 0.016 
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.119 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.091 0.071 0.051 0.035 0.022 0.016 
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.091 0.071 0.052 0.035 0.022 0.016 
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.091 0.072 0.052 0.035 0.022 0.016 
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    19.53 MiB ->     6.71 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.039 0.017 0.027 0.041 0.060 0.081 0.102 0.117 0.120 0.111 0.092 0.071 0.050 0.034 0.021 0.016 
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_0 .. size =    52.73 MiB ->    18.13 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2262.37 MB
llama_model_quantize_internal: hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 

main: quantize time =  7761.51 ms
main:    total time =  7761.51 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf q5_1
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q5_1 .. size =   195.31 MiB ->    73.24 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.044 
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.094 0.102 0.102 0.094 0.081 0.066 0.050 0.037 0.026 0.045 
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.044 0.025 0.037 0.051 0.066 0.082 0.094 0.101 0.101 0.094 0.082 0.066 0.051 0.037 0.025 0.044 
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.094 0.100 0.101 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.044 0.025 0.037 0.051 0.066 0.082 0.094 0.101 0.101 0.094 0.082 0.066 0.051 0.037 0.025 0.044 
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.039 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.051 0.038 0.026 0.045 
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.091 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    19.53 MiB ->     7.32 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_1 .. size =    52.73 MiB ->    19.78 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2460.49 MB
llama_model_quantize_internal: hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 

main: quantize time =  8415.41 ms
main:    total time =  8415.41 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf q2_k
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q2_K .. size =   195.31 MiB ->    36.62 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1346.35 MB

main: quantize time = 25638.87 ms
main:    total time = 25638.87 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf q3_k
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q3_K .. size =   195.31 MiB ->    42.72 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1662.08 MB

main: quantize time = 23181.12 ms
main:    total time = 23181.12 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf q4_k
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   195.31 MiB ->    57.98 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2082.62 MB

main: quantize time = 40696.61 ms
main:    total time = 40696.61 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf q5_k
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q5_K .. size =   195.31 MiB ->    70.19 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2420.14 MB

main: quantize time = 27598.51 ms
main:    total time = 27598.51 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf q6_k
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749792 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, quantizing to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2757.67 MB

main: quantize time = 26877.14 ms
main:    total time = 26877.14 ms
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love with everything you have and then some more!
It’s not easy, but it should be everyones goal in this lifetime because that means your soul has a purpose on Earth or at least deserves an adventure; especially if we are given so much freedom as humans do today (even though many still
llama_print_timings:        load time =    3210.75 ms
llama_print_timings:      sample time =      11.68 ms /    64 runs   (    0.18 ms per token,  5479.92 tokens per second)
llama_print_timings: prompt eval time =    2387.20 ms /     8 tokens (  298.40 ms per token,     3.35 tokens per second)
llama_print_timings:        eval time =   19807.81 ms /    63 runs   (  314.41 ms per token,     3.18 tokens per second)
llama_print_timings:       total time =   22218.62 ms /    71 tokens
Log end

real	0m25.721s
user	1m31.027s
sys	0m2.977s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love unconditionally, without expectations and with a deep sense of gratitude for what you have been given on this day in your life; every single one that God has created so far!
To be able to walk down from childhood years into adulthood feeling complete contentment about yourself as well as the world
llama_print_timings:        load time =    1624.62 ms
llama_print_timings:      sample time =      11.83 ms /    64 runs   (    0.18 ms per token,  5409.52 tokens per second)
llama_print_timings: prompt eval time =     923.12 ms /     8 tokens (  115.39 ms per token,     8.67 tokens per second)
llama_print_timings:        eval time =    8388.15 ms /    63 runs   (  133.15 ms per token,     7.51 tokens per second)
llama_print_timings:       total time =    9334.92 ms /    71 tokens
Log end

real	0m11.246s
user	0m38.083s
sys	0m1.714s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to experience all that this world has in store for us. To take risks, and make mistakes so we can grow as people who have been given a chance on earth with another opportunity at life after death comes knocking your door….
I love living here because there are no boundaries or limitations when it come s to the
llama_print_timings:        load time =    1018.89 ms
llama_print_timings:      sample time =      11.85 ms /    64 runs   (    0.19 ms per token,  5401.30 tokens per second)
llama_print_timings: prompt eval time =     963.96 ms /     8 tokens (  120.50 ms per token,     8.30 tokens per second)
llama_print_timings:        eval time =    7963.42 ms /    63 runs   (  126.40 ms per token,     7.91 tokens per second)
llama_print_timings:       total time =    8951.09 ms /    71 tokens
Log end

real	0m10.164s
user	0m36.461s
sys	0m1.099s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make a difference, and that all people are created equal. My goal in this class will be for us to figure out how we can help each other as well as our community improve by doing what you love best: writing stories!
Mission Statement 2016-present day; I am the proud
llama_print_timings:        load time =    1087.07 ms
llama_print_timings:      sample time =      11.66 ms /    64 runs   (    0.18 ms per token,  5490.73 tokens per second)
llama_print_timings: prompt eval time =     919.81 ms /     8 tokens (  114.98 ms per token,     8.70 tokens per second)
llama_print_timings:        eval time =    7709.55 ms /    63 runs   (  122.37 ms per token,     8.17 tokens per second)
llama_print_timings:       total time =    8652.87 ms /    71 tokens
Log end

real	0m9.946s
user	0m35.283s
sys	0m1.135s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find and do your calling.
If you are not sure what that means, or if it’s a myth about how we all know who we were meant for in this world – then keep reading… because once upon an elvish time (I can see my family rolling their eyes at me) I was
llama_print_timings:        load time =    1253.05 ms
llama_print_timings:      sample time =      11.72 ms /    64 runs   (    0.18 ms per token,  5459.82 tokens per second)
llama_print_timings: prompt eval time =    1296.11 ms /     8 tokens (  162.01 ms per token,     6.17 tokens per second)
llama_print_timings:        eval time =   10680.33 ms /    63 runs   (  169.53 ms per token,     5.90 tokens per second)
llama_print_timings:       total time =   11999.27 ms /    71 tokens
Log end

real	0m13.470s
user	0m48.964s
sys	0m1.296s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find joy and share it with others.
I am a writer, photographer & graphic designer who loves adventure and getting lost in nature. My main focus for my personal work has been capturing moments through travel photography from around Asia: Singapore, Malaysia, Hong Kong, Thailand (Bangkok), Bali, Vietnam,
llama_print_timings:        load time =    1337.45 ms
llama_print_timings:      sample time =      12.22 ms /    64 runs   (    0.19 ms per token,  5238.60 tokens per second)
llama_print_timings: prompt eval time =    1234.28 ms /     8 tokens (  154.28 ms per token,     6.48 tokens per second)
llama_print_timings:        eval time =   10581.04 ms /    63 runs   (  167.95 ms per token,     5.95 tokens per second)
llama_print_timings:       total time =   11839.64 ms /    71 tokens
Log end

real	0m13.403s
user	0m48.252s
sys	0m1.437s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make it happen and not just for you but also with your children.
I am a 21st century woman, an urbanite who loves going out in nature even if only by walking up my neighbourhood hillock or taking public transport instead of driving around town on weekends! This blog aims at helping women like
llama_print_timings:        load time =     880.32 ms
llama_print_timings:      sample time =      12.14 ms /    64 runs   (    0.19 ms per token,  5271.39 tokens per second)
llama_print_timings: prompt eval time =    1212.81 ms /     8 tokens (  151.60 ms per token,     6.60 tokens per second)
llama_print_timings:        eval time =    9817.79 ms /    63 runs   (  155.84 ms per token,     6.42 tokens per second)
llama_print_timings:       total time =   11054.84 ms /    71 tokens
Log end

real	0m12.096s
user	0m45.053s
sys	0m0.928s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to help others, but it’s so hard when you have a kid with autism.
I was asked what advice would I give my younger self and this came out – “It will get better!” This may be cliche in hindsight (and also very vague), because there are many different ways that things could
llama_print_timings:        load time =     980.89 ms
llama_print_timings:      sample time =      11.97 ms /    64 runs   (    0.19 ms per token,  5344.91 tokens per second)
llama_print_timings: prompt eval time =    1143.47 ms /     8 tokens (  142.93 ms per token,     7.00 tokens per second)
llama_print_timings:        eval time =    9381.44 ms /    63 runs   (  148.91 ms per token,     6.72 tokens per second)
llama_print_timings:       total time =   10548.88 ms /    71 tokens
Log end

real	0m11.709s
user	0m42.871s
sys	0m1.159s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love, live and serve.
I am a wife who loves my husband with all her heart! We are best friends…we have been together since we were 14 years old and married for almost ten years now!!! He supports me in every area; he’s an awesome cook (that I don’
llama_print_timings:        load time =    1120.97 ms
llama_print_timings:      sample time =      11.92 ms /    64 runs   (    0.19 ms per token,  5369.13 tokens per second)
llama_print_timings: prompt eval time =    1016.49 ms /     8 tokens (  127.06 ms per token,     7.87 tokens per second)
llama_print_timings:        eval time =    8388.69 ms /    63 runs   (  133.15 ms per token,     7.51 tokens per second)
llama_print_timings:       total time =    9428.37 ms /    71 tokens
Log end

real	0m10.755s
user	0m38.261s
sys	0m1.358s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find and follow your bliss.
Monday, November 25,: #143697) | Contact Support
Please refer to our Terms of Use for further information regarding our privacy policy.
- Type in an item number or a part name (i.e., "A0
llama_print_timings:        load time =    1335.40 ms
llama_print_timings:      sample time =      12.21 ms /    64 runs   (    0.19 ms per token,  5242.46 tokens per second)
llama_print_timings: prompt eval time =    1378.70 ms /     8 tokens (  172.34 ms per token,     5.80 tokens per second)
llama_print_timings:        eval time =   11210.41 ms /    63 runs   (  177.94 ms per token,     5.62 tokens per second)
llama_print_timings:       total time =   12613.57 ms /    71 tokens
Log end

real	0m14.169s
user	0m51.371s
sys	0m1.491s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love with everything you have and then some more!
My name's Alexandra, but my friends just call me Lexi or L.E.! :3 My nickname came from one day when in school everyone had trouble saying "Lex-" so they all started calling me that instead of
llama_print_timings:        load time =    1465.70 ms
llama_print_timings:      sample time =      11.77 ms /    64 runs   (    0.18 ms per token,  5439.86 tokens per second)
llama_print_timings: prompt eval time =    1413.51 ms /     8 tokens (  176.69 ms per token,     5.66 tokens per second)
llama_print_timings:        eval time =   11526.90 ms /    63 runs   (  182.97 ms per token,     5.47 tokens per second)
llama_print_timings:       total time =   12963.96 ms /    71 tokens
Log end

real	0m14.670s
user	0m52.794s
sys	0m1.654s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139395
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.91 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 37.57 seconds per pass - ETA 1.25 minutes
[1]4.2442,[2]7.2529,
llama_print_timings:        load time =    3202.63 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   75135.90 ms /   256 tokens (  293.50 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   75149.10 ms /   257 tokens

Final estimate: PPL = 7.2529 +/- 1.61716

real	1m18.634s
user	5m2.509s
sys	0m3.329s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139473
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.959 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 14.74 seconds per pass - ETA 0.48 minutes
[1]4.2615,[2]7.2942,
llama_print_timings:        load time =    1605.41 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   29475.06 ms /   256 tokens (  115.14 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   29489.33 ms /   257 tokens

Final estimate: PPL = 7.2942 +/- 1.62752

real	0m31.245s
user	1m58.388s
sys	0m2.008s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139505
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.92 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 15.48 seconds per pass - ETA 0.50 minutes
[1]4.1750,[2]7.8608,
llama_print_timings:        load time =     992.75 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   30964.36 ms /   256 tokens (  120.95 ms per token,     8.27 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   30977.97 ms /   257 tokens

Final estimate: PPL = 7.8608 +/- 1.78065

real	0m32.087s
user	2m4.355s
sys	0m1.352s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139537
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.559 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 14.75 seconds per pass - ETA 0.48 minutes
[1]4.3872,[2]8.0708,
llama_print_timings:        load time =     994.69 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   29496.47 ms /   256 tokens (  115.22 ms per token,     8.68 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   29509.68 ms /   257 tokens

Final estimate: PPL = 8.0708 +/- 1.79778

real	0m30.621s
user	1m58.475s
sys	0m1.336s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139567
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.501 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 20.66 seconds per pass - ETA 0.68 minutes
[1]4.3466,[2]7.5875,
llama_print_timings:        load time =    1153.22 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   41319.04 ms /   256 tokens (  161.40 ms per token,     6.20 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   41332.02 ms /   257 tokens

Final estimate: PPL = 7.5875 +/- 1.71970

real	0m42.605s
user	2m46.203s
sys	0m1.340s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139610
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.577 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 19.86 seconds per pass - ETA 0.65 minutes
[1]4.2515,[2]7.4058,
llama_print_timings:        load time =    1220.86 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   39711.09 ms /   256 tokens (  155.12 ms per token,     6.45 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   39724.43 ms /   257 tokens

Final estimate: PPL = 7.4058 +/- 1.64051

real	0m41.066s
user	2m39.635s
sys	0m1.516s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139651
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.868 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 19.35 seconds per pass - ETA 0.63 minutes
[1]5.6188,[2]12.0836,
llama_print_timings:        load time =     832.35 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   38689.01 ms /   256 tokens (  151.13 ms per token,     6.62 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   38702.33 ms /   257 tokens

Final estimate: PPL = 12.0836 +/- 2.70688

real	0m39.636s
user	2m34.896s
sys	0m1.713s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139691
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.83 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 18.37 seconds per pass - ETA 0.60 minutes
[1]4.4258,[2]7.9340,
llama_print_timings:        load time =     953.25 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   36737.28 ms /   256 tokens (  143.50 ms per token,     6.97 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   36751.10 ms /   257 tokens

Final estimate: PPL = 7.9340 +/- 1.75275

real	0m37.814s
user	2m27.082s
sys	0m1.808s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139728
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.894 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 16.23 seconds per pass - ETA 0.53 minutes
[1]4.1929,[2]7.2814,
llama_print_timings:        load time =    1084.50 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   32451.44 ms /   256 tokens (  126.76 ms per token,     7.89 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   32465.38 ms /   257 tokens

Final estimate: PPL = 7.2814 +/- 1.60026

real	0m33.669s
user	2m9.808s
sys	0m1.981s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139762
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.548 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 22.10 seconds per pass - ETA 0.73 minutes
[1]4.2316,[2]7.3382,
llama_print_timings:        load time =    1225.42 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   44201.46 ms /   256 tokens (  172.66 ms per token,     5.79 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   44214.83 ms /   257 tokens

Final estimate: PPL = 7.3382 +/- 1.63982

real	0m45.559s
user	2m57.327s
sys	0m1.875s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139808
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.587 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 22.65 seconds per pass - ETA 0.75 minutes
[1]4.2451,[2]7.2711,
llama_print_timings:        load time =    1357.31 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   45302.28 ms /   256 tokens (  176.96 ms per token,     5.65 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   45315.70 ms /   257 tokens

Final estimate: PPL = 7.2711 +/- 1.62073

real	0m46.800s
user	3m1.686s
sys	0m2.092s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139854
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 9.625 ms
compute_imatrix: computing over 2 chunks with batch_size 128
compute_imatrix: 37.64 seconds per pass - ETA 1.25 minutes
[1]4.2442,[2]7.2529,
save_imatrix: stored collected data after 2 chunks in imatrix.dat

llama_print_timings:        load time =   40022.61 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   75265.96 ms /   256 tokens (  294.01 ms per token,     3.40 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   77658.02 ms /   257 tokens

Final estimate: PPL = 7.2529 +/- 1.61716

real	1m17.930s
user	5m0.346s
sys	0m3.079s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1
main : serialized state into 1804908 out of a maximum of 235995204 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1
main : deserialized state from 1804908 out of a maximum of 235995204 bytes

main : success

first run: The quick brown fox jumped over the narrow the on the table; and, although if was being more


second run: The quick brown fox jumped over the narrow the on the table; and, although if was being more

real	0m6.063s
user	0m19.374s
sys	0m1.194s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]4.2442,[2]7.2529,'
+ qnt=f16
++ echo '[1]4.2442,[2]7.2529,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.2529
++ echo '7.2529 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 7.2529
+ return 0
  - f16 @ 7.2529 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]4.2615,[2]7.2942,'
+ qnt=q8_0
++ echo '[1]4.2615,[2]7.2942,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.2942
++ echo '7.2942 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 7.2942
+ return 0
  - q8_0 @ 7.2942 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]4.1750,[2]7.8608,'
+ qnt=q4_0
++ echo '[1]4.1750,[2]7.8608,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.8608
++ echo '7.8608 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 7.8608
+ return 0
  - q4_0 @ 7.8608 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]4.3872,[2]8.0708,'
+ qnt=q4_1
++ echo '[1]4.3872,[2]8.0708,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=8.0708
++ echo '8.0708 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 8.0708
+ return 0
  - q4_1 @ 8.0708 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]4.3466,[2]7.5875,'
+ qnt=q5_0
++ echo '[1]4.3466,[2]7.5875,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.5875
++ echo '7.5875 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 7.5875
+ return 0
  - q5_0 @ 7.5875 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]4.2515,[2]7.4058,'
+ qnt=q5_1
++ echo '[1]4.2515,[2]7.4058,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.4058
++ echo '7.4058 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 7.4058
+ return 0
  - q5_1 @ 7.4058 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q2_k.log
++ grep '^\[1\]'
+ check_ppl q2_k '[1]5.6188,[2]12.0836,'
+ qnt=q2_k
++ echo '[1]5.6188,[2]12.0836,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0836
++ echo '12.0836 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 12.0836
+ return 0
  - q2_k @ 12.0836 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]4.4258,[2]7.9340,'
+ qnt=q3_k
++ echo '[1]4.4258,[2]7.9340,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.9340
++ echo '7.9340 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 7.9340
+ return 0
  - q3_k @ 7.9340 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]4.1929,[2]7.2814,'
+ qnt=q4_k
++ echo '[1]4.1929,[2]7.2814,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.2814
++ echo '7.2814 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 7.2814
+ return 0
  - q4_k @ 7.2814 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]4.2316,[2]7.3382,'
+ qnt=q5_k
++ echo '[1]4.2316,[2]7.3382,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.3382
++ echo '7.3382 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 7.3382
+ return 0
  - q5_k @ 7.3382 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]4.2451,[2]7.2711,'
+ qnt=q6_k
++ echo '[1]4.2451,[2]7.2711,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.2711
++ echo '7.2711 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 7.2711
+ return 0
  - q6_k @ 7.2711 OK
+ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-imatrix.log
+ grep Final
+ path_lora=../models-mnt/open-llama/3B-v2/lora
+ path_shakespeare=../models-mnt/shakespeare
+ shakespeare=../models-mnt/shakespeare/shakespeare.txt
+ lora_shakespeare=../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin
+ gg_wget ../models-mnt/open-llama/3B-v2/lora https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
+ local out=../models-mnt/open-llama/3B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/3B-v2/lora
+ cd ../models-mnt/open-llama/3B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-28 17:05:39 URL:https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json [457/457] -> "adapter_config.json" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/open-llama/3B-v2/lora https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ local out=../models-mnt/open-llama/3B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/3B-v2/lora
+ cd ../models-mnt/open-llama/3B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/shakespeare https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
+ local out=../models-mnt/shakespeare
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/shakespeare
+ cd ../models-mnt/shakespeare
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
Last-modified header missing -- time-stamps turned off.
2024-02-28 17:05:39 URL:https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt [94275/94275] -> "shakespeare.txt" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ python3 ../convert-lora-to-ggml.py ../models-mnt/open-llama/3B-v2/lora
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (3200, 64) float32 0.78MB
Converted ../models-mnt/open-llama/3B-v2/lora/adapter_config.json and ../models-mnt/open-llama/3B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709139941
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 58.992 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 37.58 seconds per pass - ETA 1.25 minutes
[1]9.1017,[2]8.0185,
llama_print_timings:        load time =    3001.81 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   75145.37 ms /   256 tokens (  293.54 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   75208.38 ms /   257 tokens

Final estimate: PPL = 8.0185 +/- 1.46636

real	1m18.483s
user	5m2.818s
sys	0m2.912s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709140019
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
............. done (2908.87 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 55.82 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 37.58 seconds per pass - ETA 1.25 minutes
[1]7.0171,[2]4.4077,
llama_print_timings:        load time =    6232.71 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   75142.05 ms /   256 tokens (  293.52 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   75201.22 ms /   257 tokens

Final estimate: PPL = 4.4077 +/- 0.76561

real	1m21.819s
user	5m11.392s
sys	0m4.916s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-f16.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-f16.log
++ grep '^\[1\]'
+ compare_ppl 'f16 shakespeare' '[1]9.1017,[2]8.0185,' '[1]7.0171,[2]4.4077,'
+ qnt='f16 shakespeare'
++ echo '[1]9.1017,[2]8.0185,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=8.0185
++ echo '[1]7.0171,[2]4.4077,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=4.4077
++ echo '8.0185 < 4.4077'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'f16 shakespeare' 8.0185 4.4077
+ return 0
  - f16 shakespeare @ 8.0185 4.4077 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709140101
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 58.858 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 14.73 seconds per pass - ETA 0.48 minutes
[1]9.1296,[2]8.0574,
llama_print_timings:        load time =    1489.18 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   29460.17 ms /   256 tokens (  115.08 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   29522.50 ms /   257 tokens

Final estimate: PPL = 8.0574 +/- 1.47781

real	0m31.158s
user	1m58.375s
sys	0m1.896s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709140132
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base
............. done (2910.64 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 58.056 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 14.74 seconds per pass - ETA 0.48 minutes
[1]6.9856,[2]4.4043,
llama_print_timings:        load time =    4517.55 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   29472.68 ms /   256 tokens (  115.13 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   29534.87 ms /   257 tokens

Final estimate: PPL = 4.4043 +/- 0.76375

real	0m34.291s
user	2m7.092s
sys	0m3.809s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0.log
++ grep '^\[1\]'
+ compare_ppl 'q8_0 shakespeare' '[1]9.1296,[2]8.0574,' '[1]6.9856,[2]4.4043,'
+ qnt='q8_0 shakespeare'
++ echo '[1]9.1296,[2]8.0574,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=8.0574
++ echo '[1]6.9856,[2]4.4043,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=4.4043
++ echo '8.0574 < 4.4043'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'q8_0 shakespeare' 8.0574 4.4043
+ return 0
  - q8_0 shakespeare @ 8.0574 4.4043 OK
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin --lora-base ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -c 128 -b 128 --chunks 2
main: build = 2293 (08c5ee87)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709140167
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: loading base model from '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf'
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
............. done (3439.23 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 59.33 ms
perplexity: calculating perplexity over 2 chunks, batch_size=128
perplexity: 14.73 seconds per pass - ETA 0.48 minutes
[1]6.9967,[2]4.4155,
llama_print_timings:        load time =    5036.65 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   29458.65 ms /   256 tokens (  115.07 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   29521.57 ms /   257 tokens

Final estimate: PPL = 4.4155 +/- 0.76532

real	0m34.796s
user	2m7.278s
sys	0m4.348s
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0-f16.log
++ grep '^\[1\]'
+ compare_ppl 'q8_0 / f16 base shakespeare' '[1]9.1296,[2]8.0574,' '[1]6.9967,[2]4.4155,'
+ qnt='q8_0 / f16 base shakespeare'
++ echo '[1]9.1296,[2]8.0574,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=8.0574
++ echo '[1]6.9967,[2]4.4155,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=4.4155
++ echo '8.0574 < 4.4155'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'q8_0 / f16 base shakespeare' 8.0574 4.4155
+ return 0
  - q8_0 / f16 base shakespeare @ 8.0574 4.4155 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_with_model_debug.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.18 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    2.83 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   3.01 sec*proc (2 tests)

Total Test time (real) =   3.01 sec
0.36user 2.66system 0:03.02elapsed 100%CPU (0avgtext+0avgdata 6876152maxresident)k
0inputs+40outputs (0major+153224minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ tee /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_with_model_release.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/08/c5ee87e4cceb603ecceac90734fcdade57311b/ggml-3-arm64-cpu/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.06 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    2.71 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.76 sec*proc (2 tests)

Total Test time (real) =   2.77 sec
0.10user 2.67system 0:02.77elapsed 100%CPU (0avgtext+0avgdata 6875492maxresident)k
0inputs+40outputs (0major+153178minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
