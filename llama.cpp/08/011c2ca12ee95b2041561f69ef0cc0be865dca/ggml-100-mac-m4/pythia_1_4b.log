Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.493s
user	0m0.816s
sys	0m1.208s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Built target sha256
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 36%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-sampling
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Built target test-log
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-arg-parser
[ 62%] Built target test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Built target llama-retrieval
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative
[ 90%] Built target llama-run
[ 90%] Built target llama-speculative-simple
[ 90%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.189s
user	0m6.642s
sys	0m10.629s

main: quantize time =  6158.00 ms
main:    total time =  6158.00 ms

main: quantize time =  3665.79 ms
main:    total time =  3665.79 ms

main: quantize time =  3564.40 ms
main:    total time =  3564.40 ms

main: quantize time =  3673.46 ms
main:    total time =  3673.46 ms

main: quantize time =  3038.33 ms
main:    total time =  3038.33 ms

main: quantize time =  6055.31 ms
main:    total time =  6055.31 ms

main: quantize time =  5919.21 ms
main:    total time =  5919.21 ms

main: quantize time =  6764.49 ms
main:    total time =  6764.49 ms

main: quantize time =  5871.55 ms
main:    total time =  5871.55 ms

main: quantize time =  4384.88 ms
main:    total time =  4384.88 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.352 I main: llama backend init
0.00.000.359 I main: load the model and apply lora adapter, if any
0.00.047.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.482 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.323 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.326 I llama_model_loader: - type  f32:  194 tensors
0.00.079.327 I llama_model_loader: - type  f16:   98 tensors
0.00.079.328 I print_info: file format = GGUF V3 (latest)
0.00.079.349 I print_info: file type   = all F32 (guessed)
0.00.079.351 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.990 I load: special tokens cache size = 25
0.00.103.934 I load: token to piece cache size = 0.2984 MB
0.00.103.962 I print_info: arch             = gptneox
0.00.103.963 I print_info: vocab_only       = 0
0.00.103.964 I print_info: n_ctx_train      = 2048
0.00.103.964 I print_info: n_embd           = 2048
0.00.103.964 I print_info: n_layer          = 24
0.00.103.968 I print_info: n_head           = 16
0.00.103.969 I print_info: n_head_kv        = 16
0.00.103.969 I print_info: n_rot            = 32
0.00.103.970 I print_info: n_swa            = 0
0.00.103.970 I print_info: n_embd_head_k    = 128
0.00.103.970 I print_info: n_embd_head_v    = 128
0.00.103.971 I print_info: n_gqa            = 1
0.00.103.972 I print_info: n_embd_k_gqa     = 2048
0.00.103.973 I print_info: n_embd_v_gqa     = 2048
0.00.103.974 I print_info: f_norm_eps       = 1.0e-05
0.00.103.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.975 I print_info: f_logit_scale    = 0.0e+00
0.00.103.976 I print_info: n_ff             = 8192
0.00.103.976 I print_info: n_expert         = 0
0.00.103.976 I print_info: n_expert_used    = 0
0.00.103.977 I print_info: causal attn      = 1
0.00.103.977 I print_info: pooling type     = 0
0.00.103.977 I print_info: rope type        = 2
0.00.103.978 I print_info: rope scaling     = linear
0.00.103.978 I print_info: freq_base_train  = 10000.0
0.00.103.978 I print_info: freq_scale_train = 1
0.00.103.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.979 I print_info: rope_finetuned   = unknown
0.00.103.979 I print_info: ssm_d_conv       = 0
0.00.103.979 I print_info: ssm_d_inner      = 0
0.00.103.979 I print_info: ssm_d_state      = 0
0.00.103.979 I print_info: ssm_dt_rank      = 0
0.00.103.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.980 I print_info: model type       = 1.4B
0.00.103.980 I print_info: model params     = 1.41 B
0.00.103.981 I print_info: general.name     = 1.4B
0.00.103.981 I print_info: vocab type       = BPE
0.00.103.982 I print_info: n_vocab          = 50304
0.00.103.982 I print_info: n_merges         = 50009
0.00.103.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.983 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.983 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.983 I print_info: LF token         = 187 'Ċ'
0.00.103.984 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.984 I print_info: max token length = 1024
0.00.103.984 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.140.453 I load_tensors: offloading 24 repeating layers to GPU
0.00.140.456 I load_tensors: offloading output layer to GPU
0.00.140.456 I load_tensors: offloaded 25/25 layers to GPU
0.00.140.477 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.140.478 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.140.866 I llama_context: constructing llama_context
0.00.140.867 I llama_context: n_seq_max     = 1
0.00.140.867 I llama_context: n_ctx         = 2048
0.00.140.867 I llama_context: n_ctx_per_seq = 2048
0.00.140.867 I llama_context: n_batch       = 2048
0.00.140.867 I llama_context: n_ubatch      = 512
0.00.140.868 I llama_context: flash_attn    = 0
0.00.140.868 I llama_context: freq_base     = 10000.0
0.00.140.868 I llama_context: freq_scale    = 1
0.00.140.869 I ggml_metal_init: allocating
0.00.140.888 I ggml_metal_init: found device: Apple M4
0.00.140.894 I ggml_metal_init: picking default device: Apple M4
0.00.141.513 I ggml_metal_init: using embedded metal library
0.00.368.595 I ggml_metal_init: GPU name:   Apple M4
0.00.368.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.368.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.368.610 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.368.611 I ggml_metal_init: simdgroup reduction   = true
0.00.368.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.368.612 I ggml_metal_init: has residency sets    = true
0.00.368.612 I ggml_metal_init: has bfloat            = true
0.00.368.612 I ggml_metal_init: use bfloat            = true
0.00.368.615 I ggml_metal_init: hasUnifiedMemory      = true
0.00.368.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.404.472 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.404.474 I llama_context_kv_self: constructing llama_context_kv_self
0.00.404.477 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.132 I init:      Metal KV buffer size =   384.00 MiB
0.00.444.147 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.448.497 I init:      Metal compute buffer size =   102.25 MiB
0.00.448.500 I init:        CPU compute buffer size =     8.01 MiB
0.00.448.500 I init: graph nodes  = 991
0.00.448.500 I init: graph splits = 2
0.00.448.504 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.448.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.448.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.544 I main: llama threadpool init, n_threads = 4
0.00.516.590 I 
0.00.516.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.606 I 
0.00.516.654 I sampler seed: 1234
0.00.516.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.516.682 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.516.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.516.684 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.346.320 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.346.321 I llama_perf_context_print:        load time =     467.73 ms
0.02.346.323 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.43 tokens per second)
0.02.346.324 I llama_perf_context_print:        eval time =    1783.05 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.346.324 I llama_perf_context_print:       total time =    1830.68 ms /    70 tokens
0.02.350.342 I ggml_metal_free: deallocating

real	0m2.634s
user	0m0.142s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.012.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.376 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.394 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.810 I llama_model_loader: - type  f32:  194 tensors
0.00.038.811 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.811 I print_info: file format = GGUF V3 (latest)
0.00.038.825 I print_info: file type   = Q8_0
0.00.038.827 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.754 I load: special tokens cache size = 25
0.00.054.814 I load: token to piece cache size = 0.2984 MB
0.00.054.830 I print_info: arch             = gptneox
0.00.054.831 I print_info: vocab_only       = 0
0.00.054.832 I print_info: n_ctx_train      = 2048
0.00.054.832 I print_info: n_embd           = 2048
0.00.054.832 I print_info: n_layer          = 24
0.00.054.838 I print_info: n_head           = 16
0.00.054.839 I print_info: n_head_kv        = 16
0.00.054.839 I print_info: n_rot            = 32
0.00.054.839 I print_info: n_swa            = 0
0.00.054.839 I print_info: n_embd_head_k    = 128
0.00.054.841 I print_info: n_embd_head_v    = 128
0.00.054.842 I print_info: n_gqa            = 1
0.00.054.843 I print_info: n_embd_k_gqa     = 2048
0.00.054.843 I print_info: n_embd_v_gqa     = 2048
0.00.054.844 I print_info: f_norm_eps       = 1.0e-05
0.00.054.844 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.845 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.845 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.845 I print_info: f_logit_scale    = 0.0e+00
0.00.054.846 I print_info: n_ff             = 8192
0.00.054.846 I print_info: n_expert         = 0
0.00.054.846 I print_info: n_expert_used    = 0
0.00.054.846 I print_info: causal attn      = 1
0.00.054.846 I print_info: pooling type     = 0
0.00.054.846 I print_info: rope type        = 2
0.00.054.847 I print_info: rope scaling     = linear
0.00.054.847 I print_info: freq_base_train  = 10000.0
0.00.054.847 I print_info: freq_scale_train = 1
0.00.054.848 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.848 I print_info: rope_finetuned   = unknown
0.00.054.848 I print_info: ssm_d_conv       = 0
0.00.054.848 I print_info: ssm_d_inner      = 0
0.00.054.848 I print_info: ssm_d_state      = 0
0.00.054.849 I print_info: ssm_dt_rank      = 0
0.00.054.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.849 I print_info: model type       = 1.4B
0.00.054.850 I print_info: model params     = 1.41 B
0.00.054.850 I print_info: general.name     = 1.4B
0.00.054.850 I print_info: vocab type       = BPE
0.00.054.851 I print_info: n_vocab          = 50304
0.00.054.853 I print_info: n_merges         = 50009
0.00.054.853 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.853 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.853 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.853 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.854 I print_info: LF token         = 187 'Ċ'
0.00.054.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.854 I print_info: max token length = 1024
0.00.054.856 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.088.642 I load_tensors: offloading 24 repeating layers to GPU
0.01.088.647 I load_tensors: offloading output layer to GPU
0.01.088.648 I load_tensors: offloaded 25/25 layers to GPU
0.01.088.671 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.088.672 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.089.987 I llama_context: constructing llama_context
0.01.089.989 I llama_context: n_seq_max     = 1
0.01.089.989 I llama_context: n_ctx         = 2048
0.01.089.990 I llama_context: n_ctx_per_seq = 2048
0.01.089.990 I llama_context: n_batch       = 2048
0.01.089.991 I llama_context: n_ubatch      = 512
0.01.089.991 I llama_context: flash_attn    = 0
0.01.089.991 I llama_context: freq_base     = 10000.0
0.01.089.992 I llama_context: freq_scale    = 1
0.01.089.993 I ggml_metal_init: allocating
0.01.090.000 I ggml_metal_init: found device: Apple M4
0.01.090.006 I ggml_metal_init: picking default device: Apple M4
0.01.091.251 I ggml_metal_init: using embedded metal library
0.01.096.539 I ggml_metal_init: GPU name:   Apple M4
0.01.096.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.096.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.096.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.096.543 I ggml_metal_init: simdgroup reduction   = true
0.01.096.544 I ggml_metal_init: simdgroup matrix mul. = true
0.01.096.544 I ggml_metal_init: has residency sets    = true
0.01.096.544 I ggml_metal_init: has bfloat            = true
0.01.096.544 I ggml_metal_init: use bfloat            = true
0.01.096.545 I ggml_metal_init: hasUnifiedMemory      = true
0.01.096.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.111.698 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.111.700 I llama_context_kv_self: constructing llama_context_kv_self
0.01.111.702 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.167.576 I init:      Metal KV buffer size =   384.00 MiB
0.01.167.581 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.171.987 I init:      Metal compute buffer size =   102.25 MiB
0.01.171.989 I init:        CPU compute buffer size =     8.01 MiB
0.01.171.990 I init: graph nodes  = 991
0.01.171.990 I init: graph splits = 2
0.01.171.996 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.172.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.172.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.226.941 I main: llama threadpool init, n_threads = 4
0.01.226.981 I 
0.01.226.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.226.996 I 
0.01.227.165 I sampler seed: 1234
0.01.227.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.227.180 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.227.181 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.227.181 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.301.436 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.02.301.437 I llama_perf_context_print:        load time =    1214.17 ms
0.02.301.438 I llama_perf_context_print: prompt eval time =      39.52 ms /     7 tokens (    5.65 ms per token,   177.13 tokens per second)
0.02.301.439 I llama_perf_context_print:        eval time =    1031.81 ms /    63 runs   (   16.38 ms per token,    61.06 tokens per second)
0.02.301.439 I llama_perf_context_print:       total time =    1075.21 ms /    70 tokens
0.02.305.460 I ggml_metal_free: deallocating

real	0m2.323s
user	0m0.109s
sys	0m0.287s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.010.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.231 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.232 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.235 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.069 I llama_model_loader: - type  f32:  194 tensors
0.00.027.069 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.070 I print_info: file format = GGUF V3 (latest)
0.00.027.082 I print_info: file type   = Q4_0
0.00.027.083 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.393 I load: special tokens cache size = 25
0.00.041.466 I load: token to piece cache size = 0.2984 MB
0.00.041.480 I print_info: arch             = gptneox
0.00.041.481 I print_info: vocab_only       = 0
0.00.041.482 I print_info: n_ctx_train      = 2048
0.00.041.482 I print_info: n_embd           = 2048
0.00.041.482 I print_info: n_layer          = 24
0.00.041.486 I print_info: n_head           = 16
0.00.041.487 I print_info: n_head_kv        = 16
0.00.041.487 I print_info: n_rot            = 32
0.00.041.487 I print_info: n_swa            = 0
0.00.041.488 I print_info: n_embd_head_k    = 128
0.00.041.488 I print_info: n_embd_head_v    = 128
0.00.041.489 I print_info: n_gqa            = 1
0.00.041.490 I print_info: n_embd_k_gqa     = 2048
0.00.041.490 I print_info: n_embd_v_gqa     = 2048
0.00.041.491 I print_info: f_norm_eps       = 1.0e-05
0.00.041.492 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.493 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.493 I print_info: f_logit_scale    = 0.0e+00
0.00.041.494 I print_info: n_ff             = 8192
0.00.041.494 I print_info: n_expert         = 0
0.00.041.494 I print_info: n_expert_used    = 0
0.00.041.494 I print_info: causal attn      = 1
0.00.041.494 I print_info: pooling type     = 0
0.00.041.494 I print_info: rope type        = 2
0.00.041.495 I print_info: rope scaling     = linear
0.00.041.495 I print_info: freq_base_train  = 10000.0
0.00.041.495 I print_info: freq_scale_train = 1
0.00.041.495 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.495 I print_info: rope_finetuned   = unknown
0.00.041.496 I print_info: ssm_d_conv       = 0
0.00.041.496 I print_info: ssm_d_inner      = 0
0.00.041.496 I print_info: ssm_d_state      = 0
0.00.041.496 I print_info: ssm_dt_rank      = 0
0.00.041.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.496 I print_info: model type       = 1.4B
0.00.041.497 I print_info: model params     = 1.41 B
0.00.041.497 I print_info: general.name     = 1.4B
0.00.041.497 I print_info: vocab type       = BPE
0.00.041.498 I print_info: n_vocab          = 50304
0.00.041.498 I print_info: n_merges         = 50009
0.00.041.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.499 I print_info: LF token         = 187 'Ċ'
0.00.041.499 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.499 I print_info: max token length = 1024
0.00.041.499 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.189 I load_tensors: offloading output layer to GPU
0.00.611.189 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.223 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.611.224 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.612.760 I llama_context: constructing llama_context
0.00.612.763 I llama_context: n_seq_max     = 1
0.00.612.764 I llama_context: n_ctx         = 2048
0.00.612.765 I llama_context: n_ctx_per_seq = 2048
0.00.612.765 I llama_context: n_batch       = 2048
0.00.612.766 I llama_context: n_ubatch      = 512
0.00.612.766 I llama_context: flash_attn    = 0
0.00.612.768 I llama_context: freq_base     = 10000.0
0.00.612.769 I llama_context: freq_scale    = 1
0.00.612.771 I ggml_metal_init: allocating
0.00.612.840 I ggml_metal_init: found device: Apple M4
0.00.612.854 I ggml_metal_init: picking default device: Apple M4
0.00.614.690 I ggml_metal_init: using embedded metal library
0.00.620.302 I ggml_metal_init: GPU name:   Apple M4
0.00.620.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.309 I ggml_metal_init: simdgroup reduction   = true
0.00.620.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.310 I ggml_metal_init: has residency sets    = true
0.00.620.310 I ggml_metal_init: has bfloat            = true
0.00.620.310 I ggml_metal_init: use bfloat            = true
0.00.620.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.131 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.640.134 I llama_context_kv_self: constructing llama_context_kv_self
0.00.640.136 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.402 I init:      Metal KV buffer size =   384.00 MiB
0.00.697.412 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.653 I init:      Metal compute buffer size =   102.25 MiB
0.00.701.655 I init:        CPU compute buffer size =     8.01 MiB
0.00.701.656 I init: graph nodes  = 991
0.00.701.656 I init: graph splits = 2
0.00.701.663 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.701.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.060 I main: llama threadpool init, n_threads = 4
0.00.760.107 I 
0.00.760.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.123 I 
0.00.760.307 I sampler seed: 1234
0.00.760.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.332 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.333 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.333 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.434.517 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.434.518 I llama_perf_context_print:        load time =     748.71 ms
0.01.434.519 I llama_perf_context_print: prompt eval time =      45.26 ms /     7 tokens (    6.47 ms per token,   154.66 tokens per second)
0.01.434.520 I llama_perf_context_print:        eval time =     626.03 ms /    63 runs   (    9.94 ms per token,   100.63 tokens per second)
0.01.434.520 I llama_perf_context_print:       total time =     675.18 ms /    70 tokens
0.01.438.285 I ggml_metal_free: deallocating

real	0m1.455s
user	0m0.111s
sys	0m0.224s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.096 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.097 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.098 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.099 I llama_model_loader: - type  f32:  194 tensors
0.00.026.099 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.100 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.100 I print_info: file format = GGUF V3 (latest)
0.00.026.107 I print_info: file type   = Q4_1
0.00.026.108 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.986 I load: special tokens cache size = 25
0.00.040.041 I load: token to piece cache size = 0.2984 MB
0.00.040.055 I print_info: arch             = gptneox
0.00.040.056 I print_info: vocab_only       = 0
0.00.040.057 I print_info: n_ctx_train      = 2048
0.00.040.057 I print_info: n_embd           = 2048
0.00.040.057 I print_info: n_layer          = 24
0.00.040.060 I print_info: n_head           = 16
0.00.040.061 I print_info: n_head_kv        = 16
0.00.040.061 I print_info: n_rot            = 32
0.00.040.061 I print_info: n_swa            = 0
0.00.040.061 I print_info: n_embd_head_k    = 128
0.00.040.061 I print_info: n_embd_head_v    = 128
0.00.040.062 I print_info: n_gqa            = 1
0.00.040.063 I print_info: n_embd_k_gqa     = 2048
0.00.040.064 I print_info: n_embd_v_gqa     = 2048
0.00.040.064 I print_info: f_norm_eps       = 1.0e-05
0.00.040.065 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.071 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.071 I print_info: f_logit_scale    = 0.0e+00
0.00.040.078 I print_info: n_ff             = 8192
0.00.040.078 I print_info: n_expert         = 0
0.00.040.078 I print_info: n_expert_used    = 0
0.00.040.078 I print_info: causal attn      = 1
0.00.040.078 I print_info: pooling type     = 0
0.00.040.080 I print_info: rope type        = 2
0.00.040.081 I print_info: rope scaling     = linear
0.00.040.082 I print_info: freq_base_train  = 10000.0
0.00.040.082 I print_info: freq_scale_train = 1
0.00.040.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.082 I print_info: rope_finetuned   = unknown
0.00.040.082 I print_info: ssm_d_conv       = 0
0.00.040.082 I print_info: ssm_d_inner      = 0
0.00.040.083 I print_info: ssm_d_state      = 0
0.00.040.084 I print_info: ssm_dt_rank      = 0
0.00.040.084 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.085 I print_info: model type       = 1.4B
0.00.040.085 I print_info: model params     = 1.41 B
0.00.040.085 I print_info: general.name     = 1.4B
0.00.040.086 I print_info: vocab type       = BPE
0.00.040.086 I print_info: n_vocab          = 50304
0.00.040.086 I print_info: n_merges         = 50009
0.00.040.086 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.087 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.087 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.087 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.087 I print_info: LF token         = 187 'Ċ'
0.00.040.087 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: max token length = 1024
0.00.040.088 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.001 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.017 I load_tensors: offloading output layer to GPU
0.00.612.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.051 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.612.052 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.613.606 I llama_context: constructing llama_context
0.00.613.608 I llama_context: n_seq_max     = 1
0.00.613.609 I llama_context: n_ctx         = 2048
0.00.613.610 I llama_context: n_ctx_per_seq = 2048
0.00.613.611 I llama_context: n_batch       = 2048
0.00.613.611 I llama_context: n_ubatch      = 512
0.00.613.611 I llama_context: flash_attn    = 0
0.00.613.614 I llama_context: freq_base     = 10000.0
0.00.613.614 I llama_context: freq_scale    = 1
0.00.613.616 I ggml_metal_init: allocating
0.00.613.686 I ggml_metal_init: found device: Apple M4
0.00.613.699 I ggml_metal_init: picking default device: Apple M4
0.00.615.712 I ggml_metal_init: using embedded metal library
0.00.622.222 I ggml_metal_init: GPU name:   Apple M4
0.00.622.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.227 I ggml_metal_init: simdgroup reduction   = true
0.00.622.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.228 I ggml_metal_init: has residency sets    = true
0.00.622.228 I ggml_metal_init: has bfloat            = true
0.00.622.228 I ggml_metal_init: use bfloat            = true
0.00.622.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.462 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.639.465 I llama_context_kv_self: constructing llama_context_kv_self
0.00.639.467 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.881 I init:      Metal KV buffer size =   384.00 MiB
0.00.692.890 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.103 I init:      Metal compute buffer size =   102.25 MiB
0.00.697.105 I init:        CPU compute buffer size =     8.01 MiB
0.00.697.106 I init: graph nodes  = 991
0.00.697.106 I init: graph splits = 2
0.00.697.111 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.832 I main: llama threadpool init, n_threads = 4
0.00.745.874 I 
0.00.745.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.889 I 
0.00.746.001 I sampler seed: 1234
0.00.746.006 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.040 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.042 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.042 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.483.020 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.483.021 I llama_perf_context_print:        load time =     735.36 ms
0.01.483.022 I llama_perf_context_print: prompt eval time =      49.25 ms /     7 tokens (    7.04 ms per token,   142.12 tokens per second)
0.01.483.022 I llama_perf_context_print:        eval time =     684.92 ms /    63 runs   (   10.87 ms per token,    91.98 tokens per second)
0.01.483.023 I llama_perf_context_print:       total time =     737.89 ms /    70 tokens
0.01.487.049 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.251 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.252 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.253 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.052 I llama_model_loader: - type  f32:  194 tensors
0.00.025.053 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.053 I print_info: file format = GGUF V3 (latest)
0.00.025.060 I print_info: file type   = Q5_0
0.00.025.061 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.263 I load: special tokens cache size = 25
0.00.039.415 I load: token to piece cache size = 0.2984 MB
0.00.039.430 I print_info: arch             = gptneox
0.00.039.431 I print_info: vocab_only       = 0
0.00.039.431 I print_info: n_ctx_train      = 2048
0.00.039.431 I print_info: n_embd           = 2048
0.00.039.431 I print_info: n_layer          = 24
0.00.039.434 I print_info: n_head           = 16
0.00.039.435 I print_info: n_head_kv        = 16
0.00.039.435 I print_info: n_rot            = 32
0.00.039.435 I print_info: n_swa            = 0
0.00.039.435 I print_info: n_embd_head_k    = 128
0.00.039.435 I print_info: n_embd_head_v    = 128
0.00.039.436 I print_info: n_gqa            = 1
0.00.039.437 I print_info: n_embd_k_gqa     = 2048
0.00.039.438 I print_info: n_embd_v_gqa     = 2048
0.00.039.438 I print_info: f_norm_eps       = 1.0e-05
0.00.039.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.439 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.439 I print_info: f_logit_scale    = 0.0e+00
0.00.039.440 I print_info: n_ff             = 8192
0.00.039.440 I print_info: n_expert         = 0
0.00.039.440 I print_info: n_expert_used    = 0
0.00.039.440 I print_info: causal attn      = 1
0.00.039.440 I print_info: pooling type     = 0
0.00.039.442 I print_info: rope type        = 2
0.00.039.443 I print_info: rope scaling     = linear
0.00.039.444 I print_info: freq_base_train  = 10000.0
0.00.039.444 I print_info: freq_scale_train = 1
0.00.039.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.444 I print_info: rope_finetuned   = unknown
0.00.039.444 I print_info: ssm_d_conv       = 0
0.00.039.445 I print_info: ssm_d_inner      = 0
0.00.039.445 I print_info: ssm_d_state      = 0
0.00.039.445 I print_info: ssm_dt_rank      = 0
0.00.039.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.445 I print_info: model type       = 1.4B
0.00.039.446 I print_info: model params     = 1.41 B
0.00.039.446 I print_info: general.name     = 1.4B
0.00.039.446 I print_info: vocab type       = BPE
0.00.039.448 I print_info: n_vocab          = 50304
0.00.039.448 I print_info: n_merges         = 50009
0.00.039.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.449 I print_info: LF token         = 187 'Ċ'
0.00.039.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.449 I print_info: max token length = 1024
0.00.039.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.005 I load_tensors: offloading output layer to GPU
0.00.644.006 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.041 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.644.042 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.645.452 I llama_context: constructing llama_context
0.00.645.455 I llama_context: n_seq_max     = 1
0.00.645.456 I llama_context: n_ctx         = 2048
0.00.645.456 I llama_context: n_ctx_per_seq = 2048
0.00.645.457 I llama_context: n_batch       = 2048
0.00.645.457 I llama_context: n_ubatch      = 512
0.00.645.457 I llama_context: flash_attn    = 0
0.00.645.460 I llama_context: freq_base     = 10000.0
0.00.645.461 I llama_context: freq_scale    = 1
0.00.645.464 I ggml_metal_init: allocating
0.00.645.539 I ggml_metal_init: found device: Apple M4
0.00.645.552 I ggml_metal_init: picking default device: Apple M4
0.00.647.377 I ggml_metal_init: using embedded metal library
0.00.653.801 I ggml_metal_init: GPU name:   Apple M4
0.00.653.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.806 I ggml_metal_init: simdgroup reduction   = true
0.00.653.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.807 I ggml_metal_init: has residency sets    = true
0.00.653.807 I ggml_metal_init: has bfloat            = true
0.00.653.807 I ggml_metal_init: use bfloat            = true
0.00.653.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.862 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.670.864 I llama_context_kv_self: constructing llama_context_kv_self
0.00.670.866 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.728.786 I init:      Metal KV buffer size =   384.00 MiB
0.00.728.792 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.266 I init:      Metal compute buffer size =   102.25 MiB
0.00.733.268 I init:        CPU compute buffer size =     8.01 MiB
0.00.733.268 I init: graph nodes  = 991
0.00.733.268 I init: graph splits = 2
0.00.733.275 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.733.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.329 I main: llama threadpool init, n_threads = 4
0.00.791.375 I 
0.00.791.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.390 I 
0.00.791.532 I sampler seed: 1234
0.00.791.537 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.791.582 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.791.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.791.591 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.591.270 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.591.271 I llama_perf_context_print:        load time =     781.95 ms
0.01.591.271 I llama_perf_context_print: prompt eval time =      49.03 ms /     7 tokens (    7.00 ms per token,   142.76 tokens per second)
0.01.591.272 I llama_perf_context_print:        eval time =     747.99 ms /    63 runs   (   11.87 ms per token,    84.23 tokens per second)
0.01.591.272 I llama_perf_context_print:       total time =     800.66 ms /    70 tokens
0.01.594.524 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.108s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.717 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.512 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.717 I llama_model_loader: - type  f32:  194 tensors
0.00.026.718 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.718 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.719 I print_info: file format = GGUF V3 (latest)
0.00.026.731 I print_info: file type   = Q5_1
0.00.026.732 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.936 I load: special tokens cache size = 25
0.00.041.094 I load: token to piece cache size = 0.2984 MB
0.00.041.108 I print_info: arch             = gptneox
0.00.041.109 I print_info: vocab_only       = 0
0.00.041.109 I print_info: n_ctx_train      = 2048
0.00.041.109 I print_info: n_embd           = 2048
0.00.041.110 I print_info: n_layer          = 24
0.00.041.113 I print_info: n_head           = 16
0.00.041.113 I print_info: n_head_kv        = 16
0.00.041.114 I print_info: n_rot            = 32
0.00.041.114 I print_info: n_swa            = 0
0.00.041.114 I print_info: n_embd_head_k    = 128
0.00.041.114 I print_info: n_embd_head_v    = 128
0.00.041.115 I print_info: n_gqa            = 1
0.00.041.116 I print_info: n_embd_k_gqa     = 2048
0.00.041.118 I print_info: n_embd_v_gqa     = 2048
0.00.041.118 I print_info: f_norm_eps       = 1.0e-05
0.00.041.119 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.119 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.119 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.119 I print_info: f_logit_scale    = 0.0e+00
0.00.041.120 I print_info: n_ff             = 8192
0.00.041.120 I print_info: n_expert         = 0
0.00.041.121 I print_info: n_expert_used    = 0
0.00.041.121 I print_info: causal attn      = 1
0.00.041.122 I print_info: pooling type     = 0
0.00.041.123 I print_info: rope type        = 2
0.00.041.123 I print_info: rope scaling     = linear
0.00.041.124 I print_info: freq_base_train  = 10000.0
0.00.041.124 I print_info: freq_scale_train = 1
0.00.041.127 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.128 I print_info: rope_finetuned   = unknown
0.00.041.128 I print_info: ssm_d_conv       = 0
0.00.041.128 I print_info: ssm_d_inner      = 0
0.00.041.128 I print_info: ssm_d_state      = 0
0.00.041.128 I print_info: ssm_dt_rank      = 0
0.00.041.128 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.128 I print_info: model type       = 1.4B
0.00.041.129 I print_info: model params     = 1.41 B
0.00.041.129 I print_info: general.name     = 1.4B
0.00.041.129 I print_info: vocab type       = BPE
0.00.041.129 I print_info: n_vocab          = 50304
0.00.041.130 I print_info: n_merges         = 50009
0.00.041.130 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.131 I print_info: LF token         = 187 'Ċ'
0.00.041.132 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.132 I print_info: max token length = 1024
0.00.041.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.417 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.434 I load_tensors: offloading output layer to GPU
0.00.693.435 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.470 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.693.471 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.694.908 I llama_context: constructing llama_context
0.00.694.910 I llama_context: n_seq_max     = 1
0.00.694.911 I llama_context: n_ctx         = 2048
0.00.694.911 I llama_context: n_ctx_per_seq = 2048
0.00.694.912 I llama_context: n_batch       = 2048
0.00.694.912 I llama_context: n_ubatch      = 512
0.00.694.912 I llama_context: flash_attn    = 0
0.00.694.914 I llama_context: freq_base     = 10000.0
0.00.694.914 I llama_context: freq_scale    = 1
0.00.694.915 I ggml_metal_init: allocating
0.00.694.935 I ggml_metal_init: found device: Apple M4
0.00.694.946 I ggml_metal_init: picking default device: Apple M4
0.00.696.471 I ggml_metal_init: using embedded metal library
0.00.702.709 I ggml_metal_init: GPU name:   Apple M4
0.00.702.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.716 I ggml_metal_init: simdgroup reduction   = true
0.00.702.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.716 I ggml_metal_init: has residency sets    = true
0.00.702.717 I ggml_metal_init: has bfloat            = true
0.00.702.717 I ggml_metal_init: use bfloat            = true
0.00.702.718 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.173 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.175 I llama_context_kv_self: constructing llama_context_kv_self
0.00.719.177 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.771.701 I init:      Metal KV buffer size =   384.00 MiB
0.00.771.712 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.775.806 I init:      Metal compute buffer size =   102.25 MiB
0.00.775.808 I init:        CPU compute buffer size =     8.01 MiB
0.00.775.808 I init: graph nodes  = 991
0.00.775.809 I init: graph splits = 2
0.00.775.814 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.775.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.775.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.193 I main: llama threadpool init, n_threads = 4
0.00.832.236 I 
0.00.832.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.832.252 I 
0.00.832.396 I sampler seed: 1234
0.00.832.401 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.832.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.832.439 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.832.440 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.672.253 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.672.254 I llama_perf_context_print:        load time =     821.75 ms
0.01.672.255 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.57 tokens per second)
0.01.672.256 I llama_perf_context_print:        eval time =     794.58 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.672.256 I llama_perf_context_print:       total time =     840.78 ms /    70 tokens
0.01.676.350 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.631 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.631 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.634 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.408 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.408 I llama_model_loader: - type  f32:  194 tensors
0.00.024.409 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.409 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.410 I print_info: file format = GGUF V3 (latest)
0.00.024.417 I print_info: file type   = Q2_K - Medium
0.00.024.418 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.541 I load: special tokens cache size = 25
0.00.038.573 I load: token to piece cache size = 0.2984 MB
0.00.038.587 I print_info: arch             = gptneox
0.00.038.588 I print_info: vocab_only       = 0
0.00.038.589 I print_info: n_ctx_train      = 2048
0.00.038.589 I print_info: n_embd           = 2048
0.00.038.589 I print_info: n_layer          = 24
0.00.038.592 I print_info: n_head           = 16
0.00.038.592 I print_info: n_head_kv        = 16
0.00.038.593 I print_info: n_rot            = 32
0.00.038.593 I print_info: n_swa            = 0
0.00.038.593 I print_info: n_embd_head_k    = 128
0.00.038.593 I print_info: n_embd_head_v    = 128
0.00.038.594 I print_info: n_gqa            = 1
0.00.038.595 I print_info: n_embd_k_gqa     = 2048
0.00.038.595 I print_info: n_embd_v_gqa     = 2048
0.00.038.596 I print_info: f_norm_eps       = 1.0e-05
0.00.038.598 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.598 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.598 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.598 I print_info: f_logit_scale    = 0.0e+00
0.00.038.601 I print_info: n_ff             = 8192
0.00.038.601 I print_info: n_expert         = 0
0.00.038.601 I print_info: n_expert_used    = 0
0.00.038.601 I print_info: causal attn      = 1
0.00.038.601 I print_info: pooling type     = 0
0.00.038.601 I print_info: rope type        = 2
0.00.038.601 I print_info: rope scaling     = linear
0.00.038.602 I print_info: freq_base_train  = 10000.0
0.00.038.602 I print_info: freq_scale_train = 1
0.00.038.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.602 I print_info: rope_finetuned   = unknown
0.00.038.603 I print_info: ssm_d_conv       = 0
0.00.038.603 I print_info: ssm_d_inner      = 0
0.00.038.603 I print_info: ssm_d_state      = 0
0.00.038.603 I print_info: ssm_dt_rank      = 0
0.00.038.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.605 I print_info: model type       = 1.4B
0.00.038.605 I print_info: model params     = 1.41 B
0.00.038.605 I print_info: general.name     = 1.4B
0.00.038.610 I print_info: vocab type       = BPE
0.00.038.611 I print_info: n_vocab          = 50304
0.00.038.611 I print_info: n_merges         = 50009
0.00.038.611 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: LF token         = 187 'Ċ'
0.00.038.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.614 I print_info: max token length = 1024
0.00.038.614 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.363.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.114 I load_tensors: offloading output layer to GPU
0.00.363.114 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.148 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.150 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.364.884 I llama_context: constructing llama_context
0.00.364.888 I llama_context: n_seq_max     = 1
0.00.364.889 I llama_context: n_ctx         = 2048
0.00.364.889 I llama_context: n_ctx_per_seq = 2048
0.00.364.889 I llama_context: n_batch       = 2048
0.00.364.890 I llama_context: n_ubatch      = 512
0.00.364.890 I llama_context: flash_attn    = 0
0.00.364.892 I llama_context: freq_base     = 10000.0
0.00.364.893 I llama_context: freq_scale    = 1
0.00.364.894 I ggml_metal_init: allocating
0.00.364.979 I ggml_metal_init: found device: Apple M4
0.00.364.991 I ggml_metal_init: picking default device: Apple M4
0.00.366.902 I ggml_metal_init: using embedded metal library
0.00.372.495 I ggml_metal_init: GPU name:   Apple M4
0.00.372.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.515 I ggml_metal_init: simdgroup reduction   = true
0.00.372.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.516 I ggml_metal_init: has residency sets    = true
0.00.372.516 I ggml_metal_init: has bfloat            = true
0.00.372.516 I ggml_metal_init: use bfloat            = true
0.00.372.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.375 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.393.378 I llama_context_kv_self: constructing llama_context_kv_self
0.00.393.380 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.455.980 I init:      Metal KV buffer size =   384.00 MiB
0.00.455.990 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.460.239 I init:      Metal compute buffer size =   102.25 MiB
0.00.460.242 I init:        CPU compute buffer size =     8.01 MiB
0.00.460.242 I init: graph nodes  = 991
0.00.460.243 I init: graph splits = 2
0.00.460.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.460.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.460.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.781 I main: llama threadpool init, n_threads = 4
0.00.519.839 I 
0.00.519.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.855 I 
0.00.520.021 I sampler seed: 1234
0.00.520.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.036 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.037 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.037 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.198.985 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.198.985 I llama_perf_context_print:        load time =     510.33 ms
0.01.198.986 I llama_perf_context_print: prompt eval time =      43.27 ms /     7 tokens (    6.18 ms per token,   161.78 tokens per second)
0.01.198.987 I llama_perf_context_print:        eval time =     632.90 ms /    63 runs   (   10.05 ms per token,    99.54 tokens per second)
0.01.198.987 I llama_perf_context_print:       total time =     679.95 ms /    70 tokens
0.01.202.860 I ggml_metal_free: deallocating

real	0m1.218s
user	0m0.112s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.328 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.995 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.995 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.996 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.996 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.996 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.998 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.746 I llama_model_loader: - type  f32:  194 tensors
0.00.024.746 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.746 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.747 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.747 I print_info: file format = GGUF V3 (latest)
0.00.024.754 I print_info: file type   = Q3_K - Medium
0.00.024.754 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.545 I load: special tokens cache size = 25
0.00.038.602 I load: token to piece cache size = 0.2984 MB
0.00.038.616 I print_info: arch             = gptneox
0.00.038.617 I print_info: vocab_only       = 0
0.00.038.617 I print_info: n_ctx_train      = 2048
0.00.038.617 I print_info: n_embd           = 2048
0.00.038.617 I print_info: n_layer          = 24
0.00.038.620 I print_info: n_head           = 16
0.00.038.621 I print_info: n_head_kv        = 16
0.00.038.621 I print_info: n_rot            = 32
0.00.038.621 I print_info: n_swa            = 0
0.00.038.621 I print_info: n_embd_head_k    = 128
0.00.038.621 I print_info: n_embd_head_v    = 128
0.00.038.622 I print_info: n_gqa            = 1
0.00.038.623 I print_info: n_embd_k_gqa     = 2048
0.00.038.624 I print_info: n_embd_v_gqa     = 2048
0.00.038.624 I print_info: f_norm_eps       = 1.0e-05
0.00.038.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.625 I print_info: f_logit_scale    = 0.0e+00
0.00.038.626 I print_info: n_ff             = 8192
0.00.038.626 I print_info: n_expert         = 0
0.00.038.626 I print_info: n_expert_used    = 0
0.00.038.627 I print_info: causal attn      = 1
0.00.038.627 I print_info: pooling type     = 0
0.00.038.628 I print_info: rope type        = 2
0.00.038.629 I print_info: rope scaling     = linear
0.00.038.629 I print_info: freq_base_train  = 10000.0
0.00.038.629 I print_info: freq_scale_train = 1
0.00.038.629 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.630 I print_info: rope_finetuned   = unknown
0.00.038.631 I print_info: ssm_d_conv       = 0
0.00.038.631 I print_info: ssm_d_inner      = 0
0.00.038.631 I print_info: ssm_d_state      = 0
0.00.038.631 I print_info: ssm_dt_rank      = 0
0.00.038.631 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.632 I print_info: model type       = 1.4B
0.00.038.632 I print_info: model params     = 1.41 B
0.00.038.632 I print_info: general.name     = 1.4B
0.00.038.633 I print_info: vocab type       = BPE
0.00.038.633 I print_info: n_vocab          = 50304
0.00.038.633 I print_info: n_merges         = 50009
0.00.038.633 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: LF token         = 187 'Ċ'
0.00.038.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: max token length = 1024
0.00.038.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.454.528 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.546 I load_tensors: offloading output layer to GPU
0.00.454.547 I load_tensors: offloaded 25/25 layers to GPU
0.00.454.579 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.454.580 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.456.183 I llama_context: constructing llama_context
0.00.456.188 I llama_context: n_seq_max     = 1
0.00.456.189 I llama_context: n_ctx         = 2048
0.00.456.190 I llama_context: n_ctx_per_seq = 2048
0.00.456.190 I llama_context: n_batch       = 2048
0.00.456.190 I llama_context: n_ubatch      = 512
0.00.456.190 I llama_context: flash_attn    = 0
0.00.456.193 I llama_context: freq_base     = 10000.0
0.00.456.193 I llama_context: freq_scale    = 1
0.00.456.195 I ggml_metal_init: allocating
0.00.456.254 I ggml_metal_init: found device: Apple M4
0.00.456.265 I ggml_metal_init: picking default device: Apple M4
0.00.458.365 I ggml_metal_init: using embedded metal library
0.00.464.484 I ggml_metal_init: GPU name:   Apple M4
0.00.464.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.493 I ggml_metal_init: simdgroup reduction   = true
0.00.464.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.493 I ggml_metal_init: has residency sets    = true
0.00.464.493 I ggml_metal_init: has bfloat            = true
0.00.464.494 I ggml_metal_init: use bfloat            = true
0.00.464.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.500 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.484.283 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.484.285 I llama_context_kv_self: constructing llama_context_kv_self
0.00.484.287 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.668 I init:      Metal KV buffer size =   384.00 MiB
0.00.542.674 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.547.477 I init:      Metal compute buffer size =   102.25 MiB
0.00.547.479 I init:        CPU compute buffer size =     8.01 MiB
0.00.547.479 I init: graph nodes  = 991
0.00.547.479 I init: graph splits = 2
0.00.547.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.467 I main: llama threadpool init, n_threads = 4
0.00.602.512 I 
0.00.602.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.528 I 
0.00.602.687 I sampler seed: 1234
0.00.602.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.703 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.704 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.704 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.335.728 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.335.728 I llama_perf_context_print:        load time =     593.39 ms
0.01.335.730 I llama_perf_context_print: prompt eval time =      40.13 ms /     7 tokens (    5.73 ms per token,   174.41 tokens per second)
0.01.335.730 I llama_perf_context_print:        eval time =     689.99 ms /    63 runs   (   10.95 ms per token,    91.31 tokens per second)
0.01.335.731 I llama_perf_context_print:       total time =     734.01 ms /    70 tokens
0.01.339.632 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.110s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.149 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.150 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.156 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.686 I llama_model_loader: - type  f32:  194 tensors
0.00.025.686 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.686 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.686 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.687 I print_info: file format = GGUF V3 (latest)
0.00.025.694 I print_info: file type   = Q4_K - Medium
0.00.025.694 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.478 I load: special tokens cache size = 25
0.00.039.479 I load: token to piece cache size = 0.2984 MB
0.00.039.493 I print_info: arch             = gptneox
0.00.039.494 I print_info: vocab_only       = 0
0.00.039.494 I print_info: n_ctx_train      = 2048
0.00.039.495 I print_info: n_embd           = 2048
0.00.039.495 I print_info: n_layer          = 24
0.00.039.497 I print_info: n_head           = 16
0.00.039.498 I print_info: n_head_kv        = 16
0.00.039.498 I print_info: n_rot            = 32
0.00.039.498 I print_info: n_swa            = 0
0.00.039.499 I print_info: n_embd_head_k    = 128
0.00.039.499 I print_info: n_embd_head_v    = 128
0.00.039.499 I print_info: n_gqa            = 1
0.00.039.500 I print_info: n_embd_k_gqa     = 2048
0.00.039.501 I print_info: n_embd_v_gqa     = 2048
0.00.039.502 I print_info: f_norm_eps       = 1.0e-05
0.00.039.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.502 I print_info: f_logit_scale    = 0.0e+00
0.00.039.503 I print_info: n_ff             = 8192
0.00.039.503 I print_info: n_expert         = 0
0.00.039.503 I print_info: n_expert_used    = 0
0.00.039.504 I print_info: causal attn      = 1
0.00.039.504 I print_info: pooling type     = 0
0.00.039.504 I print_info: rope type        = 2
0.00.039.504 I print_info: rope scaling     = linear
0.00.039.504 I print_info: freq_base_train  = 10000.0
0.00.039.505 I print_info: freq_scale_train = 1
0.00.039.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.505 I print_info: rope_finetuned   = unknown
0.00.039.505 I print_info: ssm_d_conv       = 0
0.00.039.505 I print_info: ssm_d_inner      = 0
0.00.039.505 I print_info: ssm_d_state      = 0
0.00.039.506 I print_info: ssm_dt_rank      = 0
0.00.039.506 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.506 I print_info: model type       = 1.4B
0.00.039.506 I print_info: model params     = 1.41 B
0.00.039.506 I print_info: general.name     = 1.4B
0.00.039.507 I print_info: vocab type       = BPE
0.00.039.507 I print_info: n_vocab          = 50304
0.00.039.507 I print_info: n_merges         = 50009
0.00.039.507 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: LF token         = 187 'Ċ'
0.00.039.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: max token length = 1024
0.00.039.509 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.521 I load_tensors: offloading output layer to GPU
0.00.521.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.554 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.555 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.523.285 I llama_context: constructing llama_context
0.00.523.288 I llama_context: n_seq_max     = 1
0.00.523.288 I llama_context: n_ctx         = 2048
0.00.523.289 I llama_context: n_ctx_per_seq = 2048
0.00.523.289 I llama_context: n_batch       = 2048
0.00.523.289 I llama_context: n_ubatch      = 512
0.00.523.290 I llama_context: flash_attn    = 0
0.00.523.292 I llama_context: freq_base     = 10000.0
0.00.523.293 I llama_context: freq_scale    = 1
0.00.523.295 I ggml_metal_init: allocating
0.00.523.358 I ggml_metal_init: found device: Apple M4
0.00.523.371 I ggml_metal_init: picking default device: Apple M4
0.00.525.300 I ggml_metal_init: using embedded metal library
0.00.531.881 I ggml_metal_init: GPU name:   Apple M4
0.00.531.885 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.887 I ggml_metal_init: simdgroup reduction   = true
0.00.531.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.887 I ggml_metal_init: has residency sets    = true
0.00.531.888 I ggml_metal_init: has bfloat            = true
0.00.531.888 I ggml_metal_init: use bfloat            = true
0.00.531.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.394 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.549.397 I llama_context_kv_self: constructing llama_context_kv_self
0.00.549.399 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.782 I init:      Metal KV buffer size =   384.00 MiB
0.00.608.788 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.613.201 I init:      Metal compute buffer size =   102.25 MiB
0.00.613.203 I init:        CPU compute buffer size =     8.01 MiB
0.00.613.203 I init: graph nodes  = 991
0.00.613.203 I init: graph splits = 2
0.00.613.210 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.845 I main: llama threadpool init, n_threads = 4
0.00.663.888 I 
0.00.663.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.901 I 
0.00.664.035 I sampler seed: 1234
0.00.664.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.073 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.076 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.436.315 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.436.316 I llama_perf_context_print:        load time =     653.38 ms
0.01.436.316 I llama_perf_context_print: prompt eval time =      59.22 ms /     7 tokens (    8.46 ms per token,   118.21 tokens per second)
0.01.436.317 I llama_perf_context_print:        eval time =     709.96 ms /    63 runs   (   11.27 ms per token,    88.74 tokens per second)
0.01.436.317 I llama_perf_context_print:       total time =     773.20 ms /    70 tokens
0.01.440.057 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.108s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.597 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.999 I llama_model_loader: - type  f32:  194 tensors
0.00.024.000 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.000 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.000 I print_info: file format = GGUF V3 (latest)
0.00.024.007 I print_info: file type   = Q5_K - Medium
0.00.024.008 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.888 I load: special tokens cache size = 25
0.00.037.709 I load: token to piece cache size = 0.2984 MB
0.00.037.723 I print_info: arch             = gptneox
0.00.037.724 I print_info: vocab_only       = 0
0.00.037.725 I print_info: n_ctx_train      = 2048
0.00.037.725 I print_info: n_embd           = 2048
0.00.037.725 I print_info: n_layer          = 24
0.00.037.727 I print_info: n_head           = 16
0.00.037.728 I print_info: n_head_kv        = 16
0.00.037.728 I print_info: n_rot            = 32
0.00.037.728 I print_info: n_swa            = 0
0.00.037.729 I print_info: n_embd_head_k    = 128
0.00.037.729 I print_info: n_embd_head_v    = 128
0.00.037.729 I print_info: n_gqa            = 1
0.00.037.730 I print_info: n_embd_k_gqa     = 2048
0.00.037.731 I print_info: n_embd_v_gqa     = 2048
0.00.037.731 I print_info: f_norm_eps       = 1.0e-05
0.00.037.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.732 I print_info: f_logit_scale    = 0.0e+00
0.00.037.733 I print_info: n_ff             = 8192
0.00.037.733 I print_info: n_expert         = 0
0.00.037.733 I print_info: n_expert_used    = 0
0.00.037.734 I print_info: causal attn      = 1
0.00.037.734 I print_info: pooling type     = 0
0.00.037.734 I print_info: rope type        = 2
0.00.037.734 I print_info: rope scaling     = linear
0.00.037.735 I print_info: freq_base_train  = 10000.0
0.00.037.735 I print_info: freq_scale_train = 1
0.00.037.735 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.735 I print_info: rope_finetuned   = unknown
0.00.037.735 I print_info: ssm_d_conv       = 0
0.00.037.735 I print_info: ssm_d_inner      = 0
0.00.037.735 I print_info: ssm_d_state      = 0
0.00.037.736 I print_info: ssm_dt_rank      = 0
0.00.037.736 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.736 I print_info: model type       = 1.4B
0.00.037.737 I print_info: model params     = 1.41 B
0.00.037.737 I print_info: general.name     = 1.4B
0.00.037.738 I print_info: vocab type       = BPE
0.00.037.739 I print_info: n_vocab          = 50304
0.00.037.739 I print_info: n_merges         = 50009
0.00.037.739 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: LF token         = 187 'Ċ'
0.00.037.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: max token length = 1024
0.00.037.740 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.878 I load_tensors: offloading output layer to GPU
0.00.608.879 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.914 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.608.916 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.526 I llama_context: constructing llama_context
0.00.610.529 I llama_context: n_seq_max     = 1
0.00.610.530 I llama_context: n_ctx         = 2048
0.00.610.530 I llama_context: n_ctx_per_seq = 2048
0.00.610.530 I llama_context: n_batch       = 2048
0.00.610.531 I llama_context: n_ubatch      = 512
0.00.610.531 I llama_context: flash_attn    = 0
0.00.610.533 I llama_context: freq_base     = 10000.0
0.00.610.534 I llama_context: freq_scale    = 1
0.00.610.535 I ggml_metal_init: allocating
0.00.610.567 I ggml_metal_init: found device: Apple M4
0.00.610.579 I ggml_metal_init: picking default device: Apple M4
0.00.612.147 I ggml_metal_init: using embedded metal library
0.00.618.304 I ggml_metal_init: GPU name:   Apple M4
0.00.618.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.310 I ggml_metal_init: simdgroup reduction   = true
0.00.618.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.311 I ggml_metal_init: has residency sets    = true
0.00.618.311 I ggml_metal_init: has bfloat            = true
0.00.618.311 I ggml_metal_init: use bfloat            = true
0.00.618.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.596 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.635.598 I llama_context_kv_self: constructing llama_context_kv_self
0.00.635.600 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.946 I init:      Metal KV buffer size =   384.00 MiB
0.00.689.953 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.695.678 I init:      Metal compute buffer size =   102.25 MiB
0.00.695.680 I init:        CPU compute buffer size =     8.01 MiB
0.00.695.681 I init: graph nodes  = 991
0.00.695.681 I init: graph splits = 2
0.00.695.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.695.822 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.695.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.683 I main: llama threadpool init, n_threads = 4
0.00.758.729 I 
0.00.758.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.744 I 
0.00.758.910 I sampler seed: 1234
0.00.758.915 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.965 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.968 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.968 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.609.228 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.609.229 I llama_perf_context_print:        load time =     749.36 ms
0.01.609.230 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.29 tokens per second)
0.01.609.230 I llama_perf_context_print:        eval time =     794.60 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.609.231 I llama_perf_context_print:       total time =     851.27 ms /    70 tokens
0.01.611.975 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.108s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.030 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.809 I llama_model_loader: - type  f32:  194 tensors
0.00.024.809 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.810 I print_info: file format = GGUF V3 (latest)
0.00.024.822 I print_info: file type   = Q6_K
0.00.024.823 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.127 I load: special tokens cache size = 25
0.00.039.406 I load: token to piece cache size = 0.2984 MB
0.00.039.420 I print_info: arch             = gptneox
0.00.039.421 I print_info: vocab_only       = 0
0.00.039.421 I print_info: n_ctx_train      = 2048
0.00.039.422 I print_info: n_embd           = 2048
0.00.039.422 I print_info: n_layer          = 24
0.00.039.424 I print_info: n_head           = 16
0.00.039.425 I print_info: n_head_kv        = 16
0.00.039.425 I print_info: n_rot            = 32
0.00.039.425 I print_info: n_swa            = 0
0.00.039.425 I print_info: n_embd_head_k    = 128
0.00.039.425 I print_info: n_embd_head_v    = 128
0.00.039.426 I print_info: n_gqa            = 1
0.00.039.427 I print_info: n_embd_k_gqa     = 2048
0.00.039.428 I print_info: n_embd_v_gqa     = 2048
0.00.039.428 I print_info: f_norm_eps       = 1.0e-05
0.00.039.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.431 I print_info: f_logit_scale    = 0.0e+00
0.00.039.431 I print_info: n_ff             = 8192
0.00.039.432 I print_info: n_expert         = 0
0.00.039.432 I print_info: n_expert_used    = 0
0.00.039.432 I print_info: causal attn      = 1
0.00.039.432 I print_info: pooling type     = 0
0.00.039.432 I print_info: rope type        = 2
0.00.039.432 I print_info: rope scaling     = linear
0.00.039.434 I print_info: freq_base_train  = 10000.0
0.00.039.434 I print_info: freq_scale_train = 1
0.00.039.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.434 I print_info: rope_finetuned   = unknown
0.00.039.434 I print_info: ssm_d_conv       = 0
0.00.039.435 I print_info: ssm_d_inner      = 0
0.00.039.435 I print_info: ssm_d_state      = 0
0.00.039.435 I print_info: ssm_dt_rank      = 0
0.00.039.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.435 I print_info: model type       = 1.4B
0.00.039.435 I print_info: model params     = 1.41 B
0.00.039.436 I print_info: general.name     = 1.4B
0.00.039.436 I print_info: vocab type       = BPE
0.00.039.436 I print_info: n_vocab          = 50304
0.00.039.437 I print_info: n_merges         = 50009
0.00.039.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: LF token         = 187 'Ċ'
0.00.039.440 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: max token length = 1024
0.00.039.440 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.958 I load_tensors: offloading output layer to GPU
0.00.617.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.982 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.617.985 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.618.868 I llama_context: constructing llama_context
0.00.618.874 I llama_context: n_seq_max     = 1
0.00.618.874 I llama_context: n_ctx         = 2048
0.00.618.874 I llama_context: n_ctx_per_seq = 2048
0.00.618.875 I llama_context: n_batch       = 2048
0.00.618.875 I llama_context: n_ubatch      = 512
0.00.618.875 I llama_context: flash_attn    = 0
0.00.618.876 I llama_context: freq_base     = 10000.0
0.00.618.877 I llama_context: freq_scale    = 1
0.00.618.878 I ggml_metal_init: allocating
0.00.618.923 I ggml_metal_init: found device: Apple M4
0.00.618.934 I ggml_metal_init: picking default device: Apple M4
0.00.620.063 I ggml_metal_init: using embedded metal library
0.00.624.280 I ggml_metal_init: GPU name:   Apple M4
0.00.624.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.291 I ggml_metal_init: simdgroup reduction   = true
0.00.624.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.292 I ggml_metal_init: has residency sets    = true
0.00.624.292 I ggml_metal_init: has bfloat            = true
0.00.624.293 I ggml_metal_init: use bfloat            = true
0.00.624.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.416 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.638.417 I llama_context_kv_self: constructing llama_context_kv_self
0.00.638.419 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.803 I init:      Metal KV buffer size =   384.00 MiB
0.00.669.812 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.674.377 I init:      Metal compute buffer size =   102.25 MiB
0.00.674.379 I init:        CPU compute buffer size =     8.01 MiB
0.00.674.379 I init: graph nodes  = 991
0.00.674.380 I init: graph splits = 2
0.00.674.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.674.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.674.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.061 I main: llama threadpool init, n_threads = 4
0.00.735.105 I 
0.00.735.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.118 I 
0.00.735.258 I sampler seed: 1234
0.00.735.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.274 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.619.118 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.619.119 I llama_perf_context_print:        load time =     725.31 ms
0.01.619.120 I llama_perf_context_print: prompt eval time =      57.90 ms /     7 tokens (    8.27 ms per token,   120.91 tokens per second)
0.01.619.121 I llama_perf_context_print:        eval time =     823.05 ms /    63 runs   (   13.06 ms per token,    76.54 tokens per second)
0.01.619.122 I llama_perf_context_print:       total time =     884.78 ms /    70 tokens
0.01.623.011 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.104s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.566 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.162 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.413 I llama_model_loader: - type  f32:  194 tensors
0.00.051.413 I llama_model_loader: - type  f16:   98 tensors
0.00.051.414 I print_info: file format = GGUF V3 (latest)
0.00.051.427 I print_info: file type   = all F32 (guessed)
0.00.051.428 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.103 I load: special tokens cache size = 25
0.00.070.897 I load: token to piece cache size = 0.2984 MB
0.00.070.912 I print_info: arch             = gptneox
0.00.070.913 I print_info: vocab_only       = 0
0.00.070.914 I print_info: n_ctx_train      = 2048
0.00.070.914 I print_info: n_embd           = 2048
0.00.070.914 I print_info: n_layer          = 24
0.00.070.917 I print_info: n_head           = 16
0.00.070.918 I print_info: n_head_kv        = 16
0.00.070.918 I print_info: n_rot            = 32
0.00.070.919 I print_info: n_swa            = 0
0.00.070.919 I print_info: n_embd_head_k    = 128
0.00.070.919 I print_info: n_embd_head_v    = 128
0.00.070.920 I print_info: n_gqa            = 1
0.00.070.921 I print_info: n_embd_k_gqa     = 2048
0.00.070.921 I print_info: n_embd_v_gqa     = 2048
0.00.070.922 I print_info: f_norm_eps       = 1.0e-05
0.00.070.922 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.924 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.933 I print_info: f_logit_scale    = 0.0e+00
0.00.070.940 I print_info: n_ff             = 8192
0.00.070.941 I print_info: n_expert         = 0
0.00.070.941 I print_info: n_expert_used    = 0
0.00.070.941 I print_info: causal attn      = 1
0.00.070.941 I print_info: pooling type     = 0
0.00.070.941 I print_info: rope type        = 2
0.00.070.942 I print_info: rope scaling     = linear
0.00.070.943 I print_info: freq_base_train  = 10000.0
0.00.070.943 I print_info: freq_scale_train = 1
0.00.070.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.944 I print_info: rope_finetuned   = unknown
0.00.070.944 I print_info: ssm_d_conv       = 0
0.00.070.944 I print_info: ssm_d_inner      = 0
0.00.070.944 I print_info: ssm_d_state      = 0
0.00.070.945 I print_info: ssm_dt_rank      = 0
0.00.070.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.945 I print_info: model type       = 1.4B
0.00.070.945 I print_info: model params     = 1.41 B
0.00.070.946 I print_info: general.name     = 1.4B
0.00.070.947 I print_info: vocab type       = BPE
0.00.070.947 I print_info: n_vocab          = 50304
0.00.070.948 I print_info: n_merges         = 50009
0.00.070.948 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.949 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.949 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.950 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.950 I print_info: LF token         = 187 'Ċ'
0.00.070.950 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.950 I print_info: max token length = 1024
0.00.070.951 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.402.039 I load_tensors: offloading 24 repeating layers to GPU
0.01.402.045 I load_tensors: offloading output layer to GPU
0.01.402.047 I load_tensors: offloaded 25/25 layers to GPU
0.01.402.070 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.402.072 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.403.119 I llama_context: constructing llama_context
0.01.403.121 I llama_context: n_seq_max     = 1
0.01.403.121 I llama_context: n_ctx         = 128
0.01.403.121 I llama_context: n_ctx_per_seq = 128
0.01.403.121 I llama_context: n_batch       = 128
0.01.403.121 I llama_context: n_ubatch      = 128
0.01.403.122 I llama_context: flash_attn    = 0
0.01.403.122 I llama_context: freq_base     = 10000.0
0.01.403.122 I llama_context: freq_scale    = 1
0.01.403.123 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.403.123 I ggml_metal_init: allocating
0.01.403.144 I ggml_metal_init: found device: Apple M4
0.01.403.152 I ggml_metal_init: picking default device: Apple M4
0.01.404.234 I ggml_metal_init: using embedded metal library
0.01.408.248 I ggml_metal_init: GPU name:   Apple M4
0.01.408.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.408.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.408.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.408.252 I ggml_metal_init: simdgroup reduction   = true
0.01.408.252 I ggml_metal_init: simdgroup matrix mul. = true
0.01.408.252 I ggml_metal_init: has residency sets    = true
0.01.408.252 I ggml_metal_init: has bfloat            = true
0.01.408.252 I ggml_metal_init: use bfloat            = true
0.01.408.253 I ggml_metal_init: hasUnifiedMemory      = true
0.01.408.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.419.157 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.419.158 I llama_context_kv_self: constructing llama_context_kv_self
0.01.419.160 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.420.931 I init:      Metal KV buffer size =    24.00 MiB
0.01.420.935 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.422.719 I init:      Metal compute buffer size =    25.56 MiB
0.01.422.720 I init:        CPU compute buffer size =     1.06 MiB
0.01.422.720 I init: graph nodes  = 991
0.01.422.721 I init: graph splits = 2
0.01.422.722 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.422.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.459.708 I 
0.01.459.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.459.739 I perplexity: tokenizing the input ..
0.01.464.985 I perplexity: tokenization took 5.244 ms
0.01.464.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.583.473 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.584.827 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.584.856 I llama_perf_context_print:        load time =    1439.79 ms
0.01.584.857 I llama_perf_context_print: prompt eval time =     118.20 ms /   128 tokens (    0.92 ms per token,  1082.86 tokens per second)
0.01.584.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.584.858 I llama_perf_context_print:       total time =     125.15 ms /   129 tokens
0.01.585.393 I ggml_metal_free: deallocating

real	0m1.771s
user	0m0.095s
sys	0m0.254s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.378 I llama_model_loader: - type  f32:  194 tensors
0.00.025.378 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.379 I print_info: file format = GGUF V3 (latest)
0.00.025.387 I print_info: file type   = Q8_0
0.00.025.388 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.807 I load: special tokens cache size = 25
0.00.040.084 I load: token to piece cache size = 0.2984 MB
0.00.040.102 I print_info: arch             = gptneox
0.00.040.103 I print_info: vocab_only       = 0
0.00.040.103 I print_info: n_ctx_train      = 2048
0.00.040.103 I print_info: n_embd           = 2048
0.00.040.103 I print_info: n_layer          = 24
0.00.040.108 I print_info: n_head           = 16
0.00.040.108 I print_info: n_head_kv        = 16
0.00.040.109 I print_info: n_rot            = 32
0.00.040.109 I print_info: n_swa            = 0
0.00.040.109 I print_info: n_embd_head_k    = 128
0.00.040.109 I print_info: n_embd_head_v    = 128
0.00.040.110 I print_info: n_gqa            = 1
0.00.040.110 I print_info: n_embd_k_gqa     = 2048
0.00.040.111 I print_info: n_embd_v_gqa     = 2048
0.00.040.112 I print_info: f_norm_eps       = 1.0e-05
0.00.040.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.112 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.112 I print_info: f_logit_scale    = 0.0e+00
0.00.040.113 I print_info: n_ff             = 8192
0.00.040.113 I print_info: n_expert         = 0
0.00.040.113 I print_info: n_expert_used    = 0
0.00.040.115 I print_info: causal attn      = 1
0.00.040.115 I print_info: pooling type     = 0
0.00.040.115 I print_info: rope type        = 2
0.00.040.115 I print_info: rope scaling     = linear
0.00.040.116 I print_info: freq_base_train  = 10000.0
0.00.040.116 I print_info: freq_scale_train = 1
0.00.040.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.116 I print_info: rope_finetuned   = unknown
0.00.040.116 I print_info: ssm_d_conv       = 0
0.00.040.117 I print_info: ssm_d_inner      = 0
0.00.040.117 I print_info: ssm_d_state      = 0
0.00.040.117 I print_info: ssm_dt_rank      = 0
0.00.040.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.117 I print_info: model type       = 1.4B
0.00.040.118 I print_info: model params     = 1.41 B
0.00.040.118 I print_info: general.name     = 1.4B
0.00.040.118 I print_info: vocab type       = BPE
0.00.040.120 I print_info: n_vocab          = 50304
0.00.040.120 I print_info: n_merges         = 50009
0.00.040.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: LF token         = 187 'Ċ'
0.00.040.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: max token length = 1024
0.00.040.124 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.856.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.856.081 I load_tensors: offloading output layer to GPU
0.00.856.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.856.110 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.856.113 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.857.570 I llama_context: constructing llama_context
0.00.857.571 I llama_context: n_seq_max     = 1
0.00.857.572 I llama_context: n_ctx         = 128
0.00.857.572 I llama_context: n_ctx_per_seq = 128
0.00.857.572 I llama_context: n_batch       = 128
0.00.857.573 I llama_context: n_ubatch      = 128
0.00.857.573 I llama_context: flash_attn    = 0
0.00.857.574 I llama_context: freq_base     = 10000.0
0.00.857.575 I llama_context: freq_scale    = 1
0.00.857.575 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.857.576 I ggml_metal_init: allocating
0.00.857.664 I ggml_metal_init: found device: Apple M4
0.00.857.673 I ggml_metal_init: picking default device: Apple M4
0.00.859.104 I ggml_metal_init: using embedded metal library
0.00.864.343 I ggml_metal_init: GPU name:   Apple M4
0.00.864.347 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.864.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.864.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.864.349 I ggml_metal_init: simdgroup reduction   = true
0.00.864.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.864.349 I ggml_metal_init: has residency sets    = true
0.00.864.349 I ggml_metal_init: has bfloat            = true
0.00.864.350 I ggml_metal_init: use bfloat            = true
0.00.864.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.864.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.879.399 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.879.401 I llama_context_kv_self: constructing llama_context_kv_self
0.00.879.404 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.882.713 I init:      Metal KV buffer size =    24.00 MiB
0.00.882.717 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.885.705 I init:      Metal compute buffer size =    25.56 MiB
0.00.885.707 I init:        CPU compute buffer size =     1.06 MiB
0.00.885.707 I init: graph nodes  = 991
0.00.885.707 I init: graph splits = 2
0.00.885.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.885.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.537 I 
0.00.912.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.590 I perplexity: tokenizing the input ..
0.00.919.757 I perplexity: tokenization took 7.164 ms
0.00.919.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.058.396 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.059.706 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.059.729 I llama_perf_context_print:        load time =     903.27 ms
0.01.059.731 I llama_perf_context_print: prompt eval time =     137.71 ms /   128 tokens (    1.08 ms per token,   929.52 tokens per second)
0.01.059.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.059.732 I llama_perf_context_print:       total time =     147.20 ms /   129 tokens
0.01.060.316 I ggml_metal_free: deallocating

real	0m1.074s
user	0m0.077s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.382 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.383 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.385 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.298 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.300 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.300 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.301 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.301 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.302 I llama_model_loader: - type  f32:  194 tensors
0.00.026.302 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.302 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.303 I print_info: file format = GGUF V3 (latest)
0.00.026.312 I print_info: file type   = Q4_0
0.00.026.313 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.730 I load: special tokens cache size = 25
0.00.040.992 I load: token to piece cache size = 0.2984 MB
0.00.041.009 I print_info: arch             = gptneox
0.00.041.010 I print_info: vocab_only       = 0
0.00.041.010 I print_info: n_ctx_train      = 2048
0.00.041.010 I print_info: n_embd           = 2048
0.00.041.011 I print_info: n_layer          = 24
0.00.041.014 I print_info: n_head           = 16
0.00.041.015 I print_info: n_head_kv        = 16
0.00.041.015 I print_info: n_rot            = 32
0.00.041.015 I print_info: n_swa            = 0
0.00.041.015 I print_info: n_embd_head_k    = 128
0.00.041.015 I print_info: n_embd_head_v    = 128
0.00.041.016 I print_info: n_gqa            = 1
0.00.041.017 I print_info: n_embd_k_gqa     = 2048
0.00.041.017 I print_info: n_embd_v_gqa     = 2048
0.00.041.019 I print_info: f_norm_eps       = 1.0e-05
0.00.041.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.023 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.023 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.023 I print_info: f_logit_scale    = 0.0e+00
0.00.041.024 I print_info: n_ff             = 8192
0.00.041.024 I print_info: n_expert         = 0
0.00.041.024 I print_info: n_expert_used    = 0
0.00.041.025 I print_info: causal attn      = 1
0.00.041.025 I print_info: pooling type     = 0
0.00.041.025 I print_info: rope type        = 2
0.00.041.025 I print_info: rope scaling     = linear
0.00.041.025 I print_info: freq_base_train  = 10000.0
0.00.041.027 I print_info: freq_scale_train = 1
0.00.041.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.027 I print_info: rope_finetuned   = unknown
0.00.041.027 I print_info: ssm_d_conv       = 0
0.00.041.027 I print_info: ssm_d_inner      = 0
0.00.041.027 I print_info: ssm_d_state      = 0
0.00.041.027 I print_info: ssm_dt_rank      = 0
0.00.041.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.028 I print_info: model type       = 1.4B
0.00.041.028 I print_info: model params     = 1.41 B
0.00.041.028 I print_info: general.name     = 1.4B
0.00.041.030 I print_info: vocab type       = BPE
0.00.041.030 I print_info: n_vocab          = 50304
0.00.041.031 I print_info: n_merges         = 50009
0.00.041.031 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.031 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.040 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: LF token         = 187 'Ċ'
0.00.041.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.043 I print_info: max token length = 1024
0.00.041.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.694 I load_tensors: offloading output layer to GPU
0.00.583.695 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.735 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.583.737 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.585.460 I llama_context: constructing llama_context
0.00.585.466 I llama_context: n_seq_max     = 1
0.00.585.467 I llama_context: n_ctx         = 128
0.00.585.467 I llama_context: n_ctx_per_seq = 128
0.00.585.467 I llama_context: n_batch       = 128
0.00.585.468 I llama_context: n_ubatch      = 128
0.00.585.468 I llama_context: flash_attn    = 0
0.00.585.470 I llama_context: freq_base     = 10000.0
0.00.585.471 I llama_context: freq_scale    = 1
0.00.585.472 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.474 I ggml_metal_init: allocating
0.00.585.617 I ggml_metal_init: found device: Apple M4
0.00.585.632 I ggml_metal_init: picking default device: Apple M4
0.00.587.590 I ggml_metal_init: using embedded metal library
0.00.593.517 I ggml_metal_init: GPU name:   Apple M4
0.00.593.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.528 I ggml_metal_init: simdgroup reduction   = true
0.00.593.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.528 I ggml_metal_init: has residency sets    = true
0.00.593.529 I ggml_metal_init: has bfloat            = true
0.00.593.529 I ggml_metal_init: use bfloat            = true
0.00.593.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.535 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.221 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.612.223 I llama_context_kv_self: constructing llama_context_kv_self
0.00.612.225 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.761 I init:      Metal KV buffer size =    24.00 MiB
0.00.615.765 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.063 I init:      Metal compute buffer size =    25.56 MiB
0.00.619.065 I init:        CPU compute buffer size =     1.06 MiB
0.00.619.065 I init: graph nodes  = 991
0.00.619.066 I init: graph splits = 2
0.00.619.069 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.099 I 
0.00.648.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.167 I perplexity: tokenizing the input ..
0.00.655.408 I perplexity: tokenization took 7.237 ms
0.00.655.417 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.198 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.789.623 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.789.645 I llama_perf_context_print:        load time =     637.99 ms
0.00.789.647 I llama_perf_context_print: prompt eval time =     131.88 ms /   128 tokens (    1.03 ms per token,   970.62 tokens per second)
0.00.789.647 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.648 I llama_perf_context_print:       total time =     141.55 ms /   129 tokens
0.00.790.146 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.240 I llama_model_loader: - type  f32:  194 tensors
0.00.027.240 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.243 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.243 I print_info: file format = GGUF V3 (latest)
0.00.027.252 I print_info: file type   = Q4_1
0.00.027.253 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.644 I load: special tokens cache size = 25
0.00.041.877 I load: token to piece cache size = 0.2984 MB
0.00.041.891 I print_info: arch             = gptneox
0.00.041.892 I print_info: vocab_only       = 0
0.00.041.892 I print_info: n_ctx_train      = 2048
0.00.041.892 I print_info: n_embd           = 2048
0.00.041.892 I print_info: n_layer          = 24
0.00.041.895 I print_info: n_head           = 16
0.00.041.896 I print_info: n_head_kv        = 16
0.00.041.896 I print_info: n_rot            = 32
0.00.041.896 I print_info: n_swa            = 0
0.00.041.898 I print_info: n_embd_head_k    = 128
0.00.041.898 I print_info: n_embd_head_v    = 128
0.00.041.899 I print_info: n_gqa            = 1
0.00.041.900 I print_info: n_embd_k_gqa     = 2048
0.00.041.900 I print_info: n_embd_v_gqa     = 2048
0.00.041.901 I print_info: f_norm_eps       = 1.0e-05
0.00.041.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.902 I print_info: f_logit_scale    = 0.0e+00
0.00.041.902 I print_info: n_ff             = 8192
0.00.041.902 I print_info: n_expert         = 0
0.00.041.902 I print_info: n_expert_used    = 0
0.00.041.903 I print_info: causal attn      = 1
0.00.041.903 I print_info: pooling type     = 0
0.00.041.903 I print_info: rope type        = 2
0.00.041.903 I print_info: rope scaling     = linear
0.00.041.903 I print_info: freq_base_train  = 10000.0
0.00.041.904 I print_info: freq_scale_train = 1
0.00.041.904 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.904 I print_info: rope_finetuned   = unknown
0.00.041.904 I print_info: ssm_d_conv       = 0
0.00.041.905 I print_info: ssm_d_inner      = 0
0.00.041.905 I print_info: ssm_d_state      = 0
0.00.041.905 I print_info: ssm_dt_rank      = 0
0.00.041.905 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.905 I print_info: model type       = 1.4B
0.00.041.906 I print_info: model params     = 1.41 B
0.00.041.906 I print_info: general.name     = 1.4B
0.00.041.906 I print_info: vocab type       = BPE
0.00.041.909 I print_info: n_vocab          = 50304
0.00.041.909 I print_info: n_merges         = 50009
0.00.041.909 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.909 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.909 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.910 I print_info: LF token         = 187 'Ċ'
0.00.041.910 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.910 I print_info: max token length = 1024
0.00.041.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.336 I load_tensors: offloading output layer to GPU
0.00.635.336 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.373 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.635.374 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.636.561 I llama_context: constructing llama_context
0.00.636.564 I llama_context: n_seq_max     = 1
0.00.636.565 I llama_context: n_ctx         = 128
0.00.636.566 I llama_context: n_ctx_per_seq = 128
0.00.636.566 I llama_context: n_batch       = 128
0.00.636.566 I llama_context: n_ubatch      = 128
0.00.636.567 I llama_context: flash_attn    = 0
0.00.636.569 I llama_context: freq_base     = 10000.0
0.00.636.569 I llama_context: freq_scale    = 1
0.00.636.570 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.636.572 I ggml_metal_init: allocating
0.00.636.687 I ggml_metal_init: found device: Apple M4
0.00.636.701 I ggml_metal_init: picking default device: Apple M4
0.00.638.670 I ggml_metal_init: using embedded metal library
0.00.645.566 I ggml_metal_init: GPU name:   Apple M4
0.00.645.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.576 I ggml_metal_init: simdgroup reduction   = true
0.00.645.576 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.576 I ggml_metal_init: has residency sets    = true
0.00.645.576 I ggml_metal_init: has bfloat            = true
0.00.645.577 I ggml_metal_init: use bfloat            = true
0.00.645.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.993 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.662.995 I llama_context_kv_self: constructing llama_context_kv_self
0.00.662.998 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.442 I init:      Metal KV buffer size =    24.00 MiB
0.00.666.445 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.815 I init:      Metal compute buffer size =    25.56 MiB
0.00.669.817 I init:        CPU compute buffer size =     1.06 MiB
0.00.669.817 I init: graph nodes  = 991
0.00.669.818 I init: graph splits = 2
0.00.669.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.995 I 
0.00.698.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.064 I perplexity: tokenizing the input ..
0.00.705.161 I perplexity: tokenization took 7.095 ms
0.00.705.166 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.674 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.840.021 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.840.043 I llama_perf_context_print:        load time =     686.93 ms
0.00.840.044 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.41 tokens per second)
0.00.840.045 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.045 I llama_perf_context_print:       total time =     142.05 ms /   129 tokens
0.00.840.627 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.946 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.150 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.153 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.062 I llama_model_loader: - type  f32:  194 tensors
0.00.025.062 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.063 I print_info: file format = GGUF V3 (latest)
0.00.025.072 I print_info: file type   = Q5_0
0.00.025.073 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.829 I load: special tokens cache size = 25
0.00.040.095 I load: token to piece cache size = 0.2984 MB
0.00.040.112 I print_info: arch             = gptneox
0.00.040.113 I print_info: vocab_only       = 0
0.00.040.113 I print_info: n_ctx_train      = 2048
0.00.040.114 I print_info: n_embd           = 2048
0.00.040.114 I print_info: n_layer          = 24
0.00.040.118 I print_info: n_head           = 16
0.00.040.118 I print_info: n_head_kv        = 16
0.00.040.118 I print_info: n_rot            = 32
0.00.040.119 I print_info: n_swa            = 0
0.00.040.119 I print_info: n_embd_head_k    = 128
0.00.040.119 I print_info: n_embd_head_v    = 128
0.00.040.119 I print_info: n_gqa            = 1
0.00.040.120 I print_info: n_embd_k_gqa     = 2048
0.00.040.121 I print_info: n_embd_v_gqa     = 2048
0.00.040.123 I print_info: f_norm_eps       = 1.0e-05
0.00.040.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.123 I print_info: f_logit_scale    = 0.0e+00
0.00.040.124 I print_info: n_ff             = 8192
0.00.040.125 I print_info: n_expert         = 0
0.00.040.125 I print_info: n_expert_used    = 0
0.00.040.125 I print_info: causal attn      = 1
0.00.040.125 I print_info: pooling type     = 0
0.00.040.125 I print_info: rope type        = 2
0.00.040.125 I print_info: rope scaling     = linear
0.00.040.126 I print_info: freq_base_train  = 10000.0
0.00.040.126 I print_info: freq_scale_train = 1
0.00.040.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.126 I print_info: rope_finetuned   = unknown
0.00.040.126 I print_info: ssm_d_conv       = 0
0.00.040.127 I print_info: ssm_d_inner      = 0
0.00.040.127 I print_info: ssm_d_state      = 0
0.00.040.127 I print_info: ssm_dt_rank      = 0
0.00.040.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.127 I print_info: model type       = 1.4B
0.00.040.127 I print_info: model params     = 1.41 B
0.00.040.128 I print_info: general.name     = 1.4B
0.00.040.128 I print_info: vocab type       = BPE
0.00.040.128 I print_info: n_vocab          = 50304
0.00.040.128 I print_info: n_merges         = 50009
0.00.040.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: LF token         = 187 'Ċ'
0.00.040.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.132 I print_info: max token length = 1024
0.00.040.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.973 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.994 I load_tensors: offloading output layer to GPU
0.00.640.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.026 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.641.027 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.642.731 I llama_context: constructing llama_context
0.00.642.735 I llama_context: n_seq_max     = 1
0.00.642.736 I llama_context: n_ctx         = 128
0.00.642.736 I llama_context: n_ctx_per_seq = 128
0.00.642.737 I llama_context: n_batch       = 128
0.00.642.737 I llama_context: n_ubatch      = 128
0.00.642.737 I llama_context: flash_attn    = 0
0.00.642.740 I llama_context: freq_base     = 10000.0
0.00.642.740 I llama_context: freq_scale    = 1
0.00.642.741 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.744 I ggml_metal_init: allocating
0.00.642.831 I ggml_metal_init: found device: Apple M4
0.00.642.847 I ggml_metal_init: picking default device: Apple M4
0.00.644.748 I ggml_metal_init: using embedded metal library
0.00.651.261 I ggml_metal_init: GPU name:   Apple M4
0.00.651.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.268 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.269 I ggml_metal_init: simdgroup reduction   = true
0.00.651.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.270 I ggml_metal_init: has residency sets    = true
0.00.651.270 I ggml_metal_init: has bfloat            = true
0.00.651.270 I ggml_metal_init: use bfloat            = true
0.00.651.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.913 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.668.916 I llama_context_kv_self: constructing llama_context_kv_self
0.00.668.919 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.444 I init:      Metal KV buffer size =    24.00 MiB
0.00.672.451 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.750 I init:      Metal compute buffer size =    25.56 MiB
0.00.675.752 I init:        CPU compute buffer size =     1.06 MiB
0.00.675.752 I init: graph nodes  = 991
0.00.675.752 I init: graph splits = 2
0.00.675.756 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.675.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.041 I 
0.00.708.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.113 I perplexity: tokenizing the input ..
0.00.713.740 I perplexity: tokenization took 5.626 ms
0.00.713.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.914 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.849.241 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.849.266 I llama_perf_context_print:        load time =     699.09 ms
0.00.849.267 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.67 tokens per second)
0.00.849.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.268 I llama_perf_context_print:       total time =     141.23 ms /   129 tokens
0.00.849.813 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.939 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.689 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.689 I llama_model_loader: - type  f32:  194 tensors
0.00.025.690 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.691 I print_info: file format = GGUF V3 (latest)
0.00.025.699 I print_info: file type   = Q5_1
0.00.025.701 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.448 I load: special tokens cache size = 25
0.00.040.789 I load: token to piece cache size = 0.2984 MB
0.00.040.806 I print_info: arch             = gptneox
0.00.040.807 I print_info: vocab_only       = 0
0.00.040.807 I print_info: n_ctx_train      = 2048
0.00.040.807 I print_info: n_embd           = 2048
0.00.040.808 I print_info: n_layer          = 24
0.00.040.812 I print_info: n_head           = 16
0.00.040.813 I print_info: n_head_kv        = 16
0.00.040.813 I print_info: n_rot            = 32
0.00.040.813 I print_info: n_swa            = 0
0.00.040.814 I print_info: n_embd_head_k    = 128
0.00.040.815 I print_info: n_embd_head_v    = 128
0.00.040.815 I print_info: n_gqa            = 1
0.00.040.816 I print_info: n_embd_k_gqa     = 2048
0.00.040.818 I print_info: n_embd_v_gqa     = 2048
0.00.040.819 I print_info: f_norm_eps       = 1.0e-05
0.00.040.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.821 I print_info: f_logit_scale    = 0.0e+00
0.00.040.822 I print_info: n_ff             = 8192
0.00.040.822 I print_info: n_expert         = 0
0.00.040.822 I print_info: n_expert_used    = 0
0.00.040.822 I print_info: causal attn      = 1
0.00.040.822 I print_info: pooling type     = 0
0.00.040.822 I print_info: rope type        = 2
0.00.040.822 I print_info: rope scaling     = linear
0.00.040.823 I print_info: freq_base_train  = 10000.0
0.00.040.823 I print_info: freq_scale_train = 1
0.00.040.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.824 I print_info: rope_finetuned   = unknown
0.00.040.832 I print_info: ssm_d_conv       = 0
0.00.040.833 I print_info: ssm_d_inner      = 0
0.00.040.833 I print_info: ssm_d_state      = 0
0.00.040.833 I print_info: ssm_dt_rank      = 0
0.00.040.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.834 I print_info: model type       = 1.4B
0.00.040.834 I print_info: model params     = 1.41 B
0.00.040.834 I print_info: general.name     = 1.4B
0.00.040.835 I print_info: vocab type       = BPE
0.00.040.835 I print_info: n_vocab          = 50304
0.00.040.835 I print_info: n_merges         = 50009
0.00.040.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: LF token         = 187 'Ċ'
0.00.040.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: max token length = 1024
0.00.040.837 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.688.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.688.947 I load_tensors: offloading output layer to GPU
0.00.688.947 I load_tensors: offloaded 25/25 layers to GPU
0.00.688.982 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.688.984 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.690.554 I llama_context: constructing llama_context
0.00.690.557 I llama_context: n_seq_max     = 1
0.00.690.557 I llama_context: n_ctx         = 128
0.00.690.558 I llama_context: n_ctx_per_seq = 128
0.00.690.558 I llama_context: n_batch       = 128
0.00.690.558 I llama_context: n_ubatch      = 128
0.00.690.558 I llama_context: flash_attn    = 0
0.00.690.560 I llama_context: freq_base     = 10000.0
0.00.690.561 I llama_context: freq_scale    = 1
0.00.690.561 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.690.566 I ggml_metal_init: allocating
0.00.690.631 I ggml_metal_init: found device: Apple M4
0.00.690.643 I ggml_metal_init: picking default device: Apple M4
0.00.692.178 I ggml_metal_init: using embedded metal library
0.00.698.338 I ggml_metal_init: GPU name:   Apple M4
0.00.698.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.698.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.698.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.698.344 I ggml_metal_init: simdgroup reduction   = true
0.00.698.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.698.345 I ggml_metal_init: has residency sets    = true
0.00.698.345 I ggml_metal_init: has bfloat            = true
0.00.698.345 I ggml_metal_init: use bfloat            = true
0.00.698.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.698.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.915 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.714.917 I llama_context_kv_self: constructing llama_context_kv_self
0.00.714.919 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.340 I init:      Metal KV buffer size =    24.00 MiB
0.00.718.344 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.470 I init:      Metal compute buffer size =    25.56 MiB
0.00.721.472 I init:        CPU compute buffer size =     1.06 MiB
0.00.721.473 I init: graph nodes  = 991
0.00.721.473 I init: graph splits = 2
0.00.721.476 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.662 I 
0.00.749.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.730 I perplexity: tokenizing the input ..
0.00.756.733 I perplexity: tokenization took 7 ms
0.00.756.739 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.767 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.893.200 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.893.220 I llama_perf_context_print:        load time =     739.78 ms
0.00.893.221 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.86 tokens per second)
0.00.893.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.222 I llama_perf_context_print:       total time =     143.56 ms /   129 tokens
0.00.893.811 I ggml_metal_free: deallocating

real	0m0.909s
user	0m0.078s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.294 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.429 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.440 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.658 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.489 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.492 I llama_model_loader: - type  f32:  194 tensors
0.00.025.493 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.493 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.493 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.494 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q2_K - Medium
0.00.025.504 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.795 I load: special tokens cache size = 25
0.00.039.798 I load: token to piece cache size = 0.2984 MB
0.00.039.815 I print_info: arch             = gptneox
0.00.039.816 I print_info: vocab_only       = 0
0.00.039.816 I print_info: n_ctx_train      = 2048
0.00.039.817 I print_info: n_embd           = 2048
0.00.039.817 I print_info: n_layer          = 24
0.00.039.820 I print_info: n_head           = 16
0.00.039.821 I print_info: n_head_kv        = 16
0.00.039.821 I print_info: n_rot            = 32
0.00.039.821 I print_info: n_swa            = 0
0.00.039.821 I print_info: n_embd_head_k    = 128
0.00.039.821 I print_info: n_embd_head_v    = 128
0.00.039.822 I print_info: n_gqa            = 1
0.00.039.822 I print_info: n_embd_k_gqa     = 2048
0.00.039.823 I print_info: n_embd_v_gqa     = 2048
0.00.039.824 I print_info: f_norm_eps       = 1.0e-05
0.00.039.824 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.824 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.824 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.824 I print_info: f_logit_scale    = 0.0e+00
0.00.039.825 I print_info: n_ff             = 8192
0.00.039.825 I print_info: n_expert         = 0
0.00.039.825 I print_info: n_expert_used    = 0
0.00.039.825 I print_info: causal attn      = 1
0.00.039.826 I print_info: pooling type     = 0
0.00.039.826 I print_info: rope type        = 2
0.00.039.826 I print_info: rope scaling     = linear
0.00.039.826 I print_info: freq_base_train  = 10000.0
0.00.039.827 I print_info: freq_scale_train = 1
0.00.039.827 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.827 I print_info: rope_finetuned   = unknown
0.00.039.827 I print_info: ssm_d_conv       = 0
0.00.039.827 I print_info: ssm_d_inner      = 0
0.00.039.827 I print_info: ssm_d_state      = 0
0.00.039.828 I print_info: ssm_dt_rank      = 0
0.00.039.828 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.828 I print_info: model type       = 1.4B
0.00.039.828 I print_info: model params     = 1.41 B
0.00.039.828 I print_info: general.name     = 1.4B
0.00.039.829 I print_info: vocab type       = BPE
0.00.039.829 I print_info: n_vocab          = 50304
0.00.039.829 I print_info: n_merges         = 50009
0.00.039.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: LF token         = 187 'Ċ'
0.00.039.830 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.831 I print_info: max token length = 1024
0.00.039.831 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.362.609 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.624 I load_tensors: offloading output layer to GPU
0.00.362.625 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.655 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.362.657 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.364.254 I llama_context: constructing llama_context
0.00.364.259 I llama_context: n_seq_max     = 1
0.00.364.259 I llama_context: n_ctx         = 128
0.00.364.260 I llama_context: n_ctx_per_seq = 128
0.00.364.260 I llama_context: n_batch       = 128
0.00.364.261 I llama_context: n_ubatch      = 128
0.00.364.261 I llama_context: flash_attn    = 0
0.00.364.263 I llama_context: freq_base     = 10000.0
0.00.364.263 I llama_context: freq_scale    = 1
0.00.364.264 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.266 I ggml_metal_init: allocating
0.00.364.355 I ggml_metal_init: found device: Apple M4
0.00.364.370 I ggml_metal_init: picking default device: Apple M4
0.00.366.137 I ggml_metal_init: using embedded metal library
0.00.371.375 I ggml_metal_init: GPU name:   Apple M4
0.00.371.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.371.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.371.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.371.399 I ggml_metal_init: simdgroup reduction   = true
0.00.371.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.371.399 I ggml_metal_init: has residency sets    = true
0.00.371.399 I ggml_metal_init: has bfloat            = true
0.00.371.400 I ggml_metal_init: use bfloat            = true
0.00.371.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.371.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.471 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.393.474 I llama_context_kv_self: constructing llama_context_kv_self
0.00.393.477 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.397.184 I init:      Metal KV buffer size =    24.00 MiB
0.00.397.192 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.400.704 I init:      Metal compute buffer size =    25.56 MiB
0.00.400.707 I init:        CPU compute buffer size =     1.06 MiB
0.00.400.708 I init: graph nodes  = 991
0.00.400.708 I init: graph splits = 2
0.00.400.712 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.400.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.632 I 
0.00.435.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.435.719 I perplexity: tokenizing the input ..
0.00.442.796 I perplexity: tokenization took 7.074 ms
0.00.442.805 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.587.338 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.588.686 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.588.713 I llama_perf_context_print:        load time =     426.33 ms
0.00.588.714 I llama_perf_context_print: prompt eval time =     143.62 ms /   128 tokens (    1.12 ms per token,   891.21 tokens per second)
0.00.588.715 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.588.716 I llama_perf_context_print:       total time =     153.09 ms /   129 tokens
0.00.589.252 I ggml_metal_free: deallocating

real	0m0.603s
user	0m0.082s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.232 I llama_model_loader: - type  f32:  194 tensors
0.00.025.232 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.233 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.233 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.233 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.239 I print_info: file format = GGUF V3 (latest)
0.00.025.245 I print_info: file type   = Q3_K - Medium
0.00.025.246 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.896 I load: special tokens cache size = 25
0.00.040.073 I load: token to piece cache size = 0.2984 MB
0.00.040.085 I print_info: arch             = gptneox
0.00.040.086 I print_info: vocab_only       = 0
0.00.040.086 I print_info: n_ctx_train      = 2048
0.00.040.086 I print_info: n_embd           = 2048
0.00.040.087 I print_info: n_layer          = 24
0.00.040.091 I print_info: n_head           = 16
0.00.040.094 I print_info: n_head_kv        = 16
0.00.040.095 I print_info: n_rot            = 32
0.00.040.095 I print_info: n_swa            = 0
0.00.040.095 I print_info: n_embd_head_k    = 128
0.00.040.095 I print_info: n_embd_head_v    = 128
0.00.040.097 I print_info: n_gqa            = 1
0.00.040.098 I print_info: n_embd_k_gqa     = 2048
0.00.040.099 I print_info: n_embd_v_gqa     = 2048
0.00.040.099 I print_info: f_norm_eps       = 1.0e-05
0.00.040.099 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.100 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.100 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.101 I print_info: f_logit_scale    = 0.0e+00
0.00.040.101 I print_info: n_ff             = 8192
0.00.040.101 I print_info: n_expert         = 0
0.00.040.102 I print_info: n_expert_used    = 0
0.00.040.102 I print_info: causal attn      = 1
0.00.040.102 I print_info: pooling type     = 0
0.00.040.102 I print_info: rope type        = 2
0.00.040.102 I print_info: rope scaling     = linear
0.00.040.102 I print_info: freq_base_train  = 10000.0
0.00.040.103 I print_info: freq_scale_train = 1
0.00.040.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.103 I print_info: rope_finetuned   = unknown
0.00.040.103 I print_info: ssm_d_conv       = 0
0.00.040.103 I print_info: ssm_d_inner      = 0
0.00.040.103 I print_info: ssm_d_state      = 0
0.00.040.103 I print_info: ssm_dt_rank      = 0
0.00.040.104 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.107 I print_info: model type       = 1.4B
0.00.040.107 I print_info: model params     = 1.41 B
0.00.040.108 I print_info: general.name     = 1.4B
0.00.040.108 I print_info: vocab type       = BPE
0.00.040.108 I print_info: n_vocab          = 50304
0.00.040.108 I print_info: n_merges         = 50009
0.00.040.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.110 I print_info: LF token         = 187 'Ċ'
0.00.040.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.110 I print_info: max token length = 1024
0.00.040.111 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.839 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.851 I load_tensors: offloading output layer to GPU
0.00.436.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.883 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.884 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.939 I llama_context: constructing llama_context
0.00.437.943 I llama_context: n_seq_max     = 1
0.00.437.944 I llama_context: n_ctx         = 128
0.00.437.944 I llama_context: n_ctx_per_seq = 128
0.00.437.944 I llama_context: n_batch       = 128
0.00.437.945 I llama_context: n_ubatch      = 128
0.00.437.945 I llama_context: flash_attn    = 0
0.00.437.947 I llama_context: freq_base     = 10000.0
0.00.437.947 I llama_context: freq_scale    = 1
0.00.437.948 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.950 I ggml_metal_init: allocating
0.00.438.013 I ggml_metal_init: found device: Apple M4
0.00.438.029 I ggml_metal_init: picking default device: Apple M4
0.00.439.845 I ggml_metal_init: using embedded metal library
0.00.445.289 I ggml_metal_init: GPU name:   Apple M4
0.00.445.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.297 I ggml_metal_init: simdgroup reduction   = true
0.00.445.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.297 I ggml_metal_init: has residency sets    = true
0.00.445.298 I ggml_metal_init: has bfloat            = true
0.00.445.298 I ggml_metal_init: use bfloat            = true
0.00.445.300 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.765 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.464.767 I llama_context_kv_self: constructing llama_context_kv_self
0.00.464.769 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.468.413 I init:      Metal KV buffer size =    24.00 MiB
0.00.468.417 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.471.647 I init:      Metal compute buffer size =    25.56 MiB
0.00.471.649 I init:        CPU compute buffer size =     1.06 MiB
0.00.471.649 I init: graph nodes  = 991
0.00.471.649 I init: graph splits = 2
0.00.471.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.471.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.913 I 
0.00.499.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.975 I perplexity: tokenizing the input ..
0.00.505.787 I perplexity: tokenization took 5.809 ms
0.00.505.795 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.637.739 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.639.070 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.639.092 I llama_perf_context_print:        load time =     490.94 ms
0.00.639.093 I llama_perf_context_print: prompt eval time =     131.12 ms /   128 tokens (    1.02 ms per token,   976.22 tokens per second)
0.00.639.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.094 I llama_perf_context_print:       total time =     139.18 ms /   129 tokens
0.00.639.633 I ggml_metal_free: deallocating

real	0m0.653s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.948 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.835 I llama_model_loader: - type  f32:  194 tensors
0.00.025.835 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.836 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.836 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.837 I print_info: file format = GGUF V3 (latest)
0.00.025.846 I print_info: file type   = Q4_K - Medium
0.00.025.847 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.247 I load: special tokens cache size = 25
0.00.040.476 I load: token to piece cache size = 0.2984 MB
0.00.040.494 I print_info: arch             = gptneox
0.00.040.495 I print_info: vocab_only       = 0
0.00.040.495 I print_info: n_ctx_train      = 2048
0.00.040.495 I print_info: n_embd           = 2048
0.00.040.496 I print_info: n_layer          = 24
0.00.040.500 I print_info: n_head           = 16
0.00.040.500 I print_info: n_head_kv        = 16
0.00.040.500 I print_info: n_rot            = 32
0.00.040.501 I print_info: n_swa            = 0
0.00.040.501 I print_info: n_embd_head_k    = 128
0.00.040.501 I print_info: n_embd_head_v    = 128
0.00.040.501 I print_info: n_gqa            = 1
0.00.040.502 I print_info: n_embd_k_gqa     = 2048
0.00.040.503 I print_info: n_embd_v_gqa     = 2048
0.00.040.503 I print_info: f_norm_eps       = 1.0e-05
0.00.040.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.504 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.504 I print_info: f_logit_scale    = 0.0e+00
0.00.040.506 I print_info: n_ff             = 8192
0.00.040.506 I print_info: n_expert         = 0
0.00.040.506 I print_info: n_expert_used    = 0
0.00.040.507 I print_info: causal attn      = 1
0.00.040.507 I print_info: pooling type     = 0
0.00.040.507 I print_info: rope type        = 2
0.00.040.507 I print_info: rope scaling     = linear
0.00.040.507 I print_info: freq_base_train  = 10000.0
0.00.040.508 I print_info: freq_scale_train = 1
0.00.040.508 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.508 I print_info: rope_finetuned   = unknown
0.00.040.508 I print_info: ssm_d_conv       = 0
0.00.040.508 I print_info: ssm_d_inner      = 0
0.00.040.508 I print_info: ssm_d_state      = 0
0.00.040.508 I print_info: ssm_dt_rank      = 0
0.00.040.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.509 I print_info: model type       = 1.4B
0.00.040.510 I print_info: model params     = 1.41 B
0.00.040.510 I print_info: general.name     = 1.4B
0.00.040.511 I print_info: vocab type       = BPE
0.00.040.511 I print_info: n_vocab          = 50304
0.00.040.511 I print_info: n_merges         = 50009
0.00.040.513 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: LF token         = 187 'Ċ'
0.00.040.514 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.514 I print_info: max token length = 1024
0.00.040.514 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.522.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.313 I load_tensors: offloading output layer to GPU
0.00.522.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.348 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.522.349 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.523.469 I llama_context: constructing llama_context
0.00.523.472 I llama_context: n_seq_max     = 1
0.00.523.472 I llama_context: n_ctx         = 128
0.00.523.473 I llama_context: n_ctx_per_seq = 128
0.00.523.473 I llama_context: n_batch       = 128
0.00.523.474 I llama_context: n_ubatch      = 128
0.00.523.474 I llama_context: flash_attn    = 0
0.00.523.476 I llama_context: freq_base     = 10000.0
0.00.523.477 I llama_context: freq_scale    = 1
0.00.523.477 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.523.479 I ggml_metal_init: allocating
0.00.523.591 I ggml_metal_init: found device: Apple M4
0.00.523.604 I ggml_metal_init: picking default device: Apple M4
0.00.525.541 I ggml_metal_init: using embedded metal library
0.00.532.238 I ggml_metal_init: GPU name:   Apple M4
0.00.532.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.532.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.532.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.532.248 I ggml_metal_init: simdgroup reduction   = true
0.00.532.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.532.248 I ggml_metal_init: has residency sets    = true
0.00.532.249 I ggml_metal_init: has bfloat            = true
0.00.532.249 I ggml_metal_init: use bfloat            = true
0.00.532.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.532.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.552.002 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.552.004 I llama_context_kv_self: constructing llama_context_kv_self
0.00.552.007 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.757 I init:      Metal KV buffer size =    24.00 MiB
0.00.555.762 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.559.299 I init:      Metal compute buffer size =    25.56 MiB
0.00.559.301 I init:        CPU compute buffer size =     1.06 MiB
0.00.559.302 I init: graph nodes  = 991
0.00.559.302 I init: graph splits = 2
0.00.559.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.559.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.807 I 
0.00.589.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.871 I perplexity: tokenizing the input ..
0.00.596.259 I perplexity: tokenization took 6.384 ms
0.00.596.264 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.458 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.738.000 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.738.029 I llama_perf_context_print:        load time =     580.05 ms
0.00.738.031 I llama_perf_context_print: prompt eval time =     139.24 ms /   128 tokens (    1.09 ms per token,   919.30 tokens per second)
0.00.738.032 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.032 I llama_perf_context_print:       total time =     148.23 ms /   129 tokens
0.00.738.593 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.432 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.256 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.258 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.263 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.263 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.263 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.173 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.174 I llama_model_loader: - type  f32:  194 tensors
0.00.025.174 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.174 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.175 I print_info: file format = GGUF V3 (latest)
0.00.025.184 I print_info: file type   = Q5_K - Medium
0.00.025.185 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.210 I load: special tokens cache size = 25
0.00.039.358 I load: token to piece cache size = 0.2984 MB
0.00.039.375 I print_info: arch             = gptneox
0.00.039.376 I print_info: vocab_only       = 0
0.00.039.376 I print_info: n_ctx_train      = 2048
0.00.039.376 I print_info: n_embd           = 2048
0.00.039.376 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.381 I print_info: n_head_kv        = 16
0.00.039.381 I print_info: n_rot            = 32
0.00.039.381 I print_info: n_swa            = 0
0.00.039.381 I print_info: n_embd_head_k    = 128
0.00.039.381 I print_info: n_embd_head_v    = 128
0.00.039.384 I print_info: n_gqa            = 1
0.00.039.385 I print_info: n_embd_k_gqa     = 2048
0.00.039.386 I print_info: n_embd_v_gqa     = 2048
0.00.039.386 I print_info: f_norm_eps       = 1.0e-05
0.00.039.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.387 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.387 I print_info: f_logit_scale    = 0.0e+00
0.00.039.389 I print_info: n_ff             = 8192
0.00.039.391 I print_info: n_expert         = 0
0.00.039.391 I print_info: n_expert_used    = 0
0.00.039.391 I print_info: causal attn      = 1
0.00.039.391 I print_info: pooling type     = 0
0.00.039.391 I print_info: rope type        = 2
0.00.039.392 I print_info: rope scaling     = linear
0.00.039.392 I print_info: freq_base_train  = 10000.0
0.00.039.392 I print_info: freq_scale_train = 1
0.00.039.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.392 I print_info: rope_finetuned   = unknown
0.00.039.392 I print_info: ssm_d_conv       = 0
0.00.039.393 I print_info: ssm_d_inner      = 0
0.00.039.393 I print_info: ssm_d_state      = 0
0.00.039.393 I print_info: ssm_dt_rank      = 0
0.00.039.393 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.398 I print_info: model type       = 1.4B
0.00.039.399 I print_info: model params     = 1.41 B
0.00.039.399 I print_info: general.name     = 1.4B
0.00.039.400 I print_info: vocab type       = BPE
0.00.039.400 I print_info: n_vocab          = 50304
0.00.039.400 I print_info: n_merges         = 50009
0.00.039.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.400 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.400 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: LF token         = 187 'Ċ'
0.00.039.401 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: max token length = 1024
0.00.039.402 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.689 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.704 I load_tensors: offloading output layer to GPU
0.00.612.705 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.739 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.612.740 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.613.941 I llama_context: constructing llama_context
0.00.613.943 I llama_context: n_seq_max     = 1
0.00.613.944 I llama_context: n_ctx         = 128
0.00.613.945 I llama_context: n_ctx_per_seq = 128
0.00.613.945 I llama_context: n_batch       = 128
0.00.613.945 I llama_context: n_ubatch      = 128
0.00.613.946 I llama_context: flash_attn    = 0
0.00.613.948 I llama_context: freq_base     = 10000.0
0.00.613.948 I llama_context: freq_scale    = 1
0.00.613.949 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.951 I ggml_metal_init: allocating
0.00.614.045 I ggml_metal_init: found device: Apple M4
0.00.614.058 I ggml_metal_init: picking default device: Apple M4
0.00.616.035 I ggml_metal_init: using embedded metal library
0.00.622.608 I ggml_metal_init: GPU name:   Apple M4
0.00.622.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.616 I ggml_metal_init: simdgroup reduction   = true
0.00.622.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.617 I ggml_metal_init: has residency sets    = true
0.00.622.617 I ggml_metal_init: has bfloat            = true
0.00.622.617 I ggml_metal_init: use bfloat            = true
0.00.622.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.152 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.640.155 I llama_context_kv_self: constructing llama_context_kv_self
0.00.640.157 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.643.643 I init:      Metal KV buffer size =    24.00 MiB
0.00.643.651 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.844 I init:      Metal compute buffer size =    25.56 MiB
0.00.646.846 I init:        CPU compute buffer size =     1.06 MiB
0.00.646.846 I init: graph nodes  = 991
0.00.646.847 I init: graph splits = 2
0.00.646.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.400 I 
0.00.681.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.468 I perplexity: tokenizing the input ..
0.00.688.478 I perplexity: tokenization took 7.005 ms
0.00.688.484 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.939 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.827.287 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.827.307 I llama_perf_context_print:        load time =     671.96 ms
0.00.827.308 I llama_perf_context_print: prompt eval time =     136.50 ms /   128 tokens (    1.07 ms per token,   937.70 tokens per second)
0.00.827.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.309 I llama_perf_context_print:       total time =     145.91 ms /   129 tokens
0.00.827.909 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.079s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.464 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.701 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.702 I llama_model_loader: - type  f32:  194 tensors
0.00.024.702 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.703 I print_info: file format = GGUF V3 (latest)
0.00.024.711 I print_info: file type   = Q6_K
0.00.024.712 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.791 I load: special tokens cache size = 25
0.00.039.021 I load: token to piece cache size = 0.2984 MB
0.00.039.038 I print_info: arch             = gptneox
0.00.039.039 I print_info: vocab_only       = 0
0.00.039.039 I print_info: n_ctx_train      = 2048
0.00.039.039 I print_info: n_embd           = 2048
0.00.039.040 I print_info: n_layer          = 24
0.00.039.044 I print_info: n_head           = 16
0.00.039.044 I print_info: n_head_kv        = 16
0.00.039.044 I print_info: n_rot            = 32
0.00.039.044 I print_info: n_swa            = 0
0.00.039.045 I print_info: n_embd_head_k    = 128
0.00.039.045 I print_info: n_embd_head_v    = 128
0.00.039.045 I print_info: n_gqa            = 1
0.00.039.046 I print_info: n_embd_k_gqa     = 2048
0.00.039.046 I print_info: n_embd_v_gqa     = 2048
0.00.039.047 I print_info: f_norm_eps       = 1.0e-05
0.00.039.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.048 I print_info: f_logit_scale    = 0.0e+00
0.00.039.048 I print_info: n_ff             = 8192
0.00.039.048 I print_info: n_expert         = 0
0.00.039.049 I print_info: n_expert_used    = 0
0.00.039.049 I print_info: causal attn      = 1
0.00.039.049 I print_info: pooling type     = 0
0.00.039.050 I print_info: rope type        = 2
0.00.039.051 I print_info: rope scaling     = linear
0.00.039.051 I print_info: freq_base_train  = 10000.0
0.00.039.051 I print_info: freq_scale_train = 1
0.00.039.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.052 I print_info: rope_finetuned   = unknown
0.00.039.056 I print_info: ssm_d_conv       = 0
0.00.039.056 I print_info: ssm_d_inner      = 0
0.00.039.056 I print_info: ssm_d_state      = 0
0.00.039.056 I print_info: ssm_dt_rank      = 0
0.00.039.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.057 I print_info: model type       = 1.4B
0.00.039.057 I print_info: model params     = 1.41 B
0.00.039.057 I print_info: general.name     = 1.4B
0.00.039.058 I print_info: vocab type       = BPE
0.00.039.058 I print_info: n_vocab          = 50304
0.00.039.058 I print_info: n_merges         = 50009
0.00.039.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.058 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: LF token         = 187 'Ċ'
0.00.039.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.061 I print_info: max token length = 1024
0.00.039.061 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.567.182 I load_tensors: offloading 24 repeating layers to GPU
0.00.567.198 I load_tensors: offloading output layer to GPU
0.00.567.199 I load_tensors: offloaded 25/25 layers to GPU
0.00.567.262 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.567.267 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.568.829 I llama_context: constructing llama_context
0.00.568.831 I llama_context: n_seq_max     = 1
0.00.568.832 I llama_context: n_ctx         = 128
0.00.568.832 I llama_context: n_ctx_per_seq = 128
0.00.568.833 I llama_context: n_batch       = 128
0.00.568.833 I llama_context: n_ubatch      = 128
0.00.568.834 I llama_context: flash_attn    = 0
0.00.568.835 I llama_context: freq_base     = 10000.0
0.00.568.835 I llama_context: freq_scale    = 1
0.00.568.836 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.568.840 I ggml_metal_init: allocating
0.00.568.891 I ggml_metal_init: found device: Apple M4
0.00.568.905 I ggml_metal_init: picking default device: Apple M4
0.00.570.438 I ggml_metal_init: using embedded metal library
0.00.576.713 I ggml_metal_init: GPU name:   Apple M4
0.00.576.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.576.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.576.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.576.723 I ggml_metal_init: simdgroup reduction   = true
0.00.576.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.576.724 I ggml_metal_init: has residency sets    = true
0.00.576.724 I ggml_metal_init: has bfloat            = true
0.00.576.725 I ggml_metal_init: use bfloat            = true
0.00.576.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.576.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.593.446 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.593.448 I llama_context_kv_self: constructing llama_context_kv_self
0.00.593.451 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.916 I init:      Metal KV buffer size =    24.00 MiB
0.00.596.920 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.600.102 I init:      Metal compute buffer size =    25.56 MiB
0.00.600.104 I init:        CPU compute buffer size =     1.06 MiB
0.00.600.104 I init: graph nodes  = 991
0.00.600.105 I init: graph splits = 2
0.00.600.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.600.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.151 I 
0.00.637.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.226 I perplexity: tokenizing the input ..
0.00.644.239 I perplexity: tokenization took 7.01 ms
0.00.644.245 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.977 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.778.284 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.778.319 I llama_perf_context_print:        load time =     628.20 ms
0.00.778.320 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.73 tokens per second)
0.00.778.321 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.322 I llama_perf_context_print:       total time =     141.17 ms /   129 tokens
0.00.778.926 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.079s
sys	0m0.133s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.287 I build: 4816 (08011c2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.333 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.974 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.983 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.985 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.986 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.986 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.987 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.990 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.799 I llama_model_loader: - type  f32:  194 tensors
0.00.054.799 I llama_model_loader: - type  f16:   98 tensors
0.00.054.800 I print_info: file format = GGUF V3 (latest)
0.00.054.816 I print_info: file type   = all F32 (guessed)
0.00.054.817 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.229 I load: special tokens cache size = 25
0.00.075.396 I load: token to piece cache size = 0.2984 MB
0.00.075.411 I print_info: arch             = gptneox
0.00.075.412 I print_info: vocab_only       = 0
0.00.075.412 I print_info: n_ctx_train      = 2048
0.00.075.412 I print_info: n_embd           = 2048
0.00.075.413 I print_info: n_layer          = 24
0.00.075.415 I print_info: n_head           = 16
0.00.075.416 I print_info: n_head_kv        = 16
0.00.075.416 I print_info: n_rot            = 32
0.00.075.417 I print_info: n_swa            = 0
0.00.075.417 I print_info: n_embd_head_k    = 128
0.00.075.417 I print_info: n_embd_head_v    = 128
0.00.075.418 I print_info: n_gqa            = 1
0.00.075.418 I print_info: n_embd_k_gqa     = 2048
0.00.075.419 I print_info: n_embd_v_gqa     = 2048
0.00.075.420 I print_info: f_norm_eps       = 1.0e-05
0.00.075.420 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.421 I print_info: f_logit_scale    = 0.0e+00
0.00.075.421 I print_info: n_ff             = 8192
0.00.075.422 I print_info: n_expert         = 0
0.00.075.422 I print_info: n_expert_used    = 0
0.00.075.422 I print_info: causal attn      = 1
0.00.075.422 I print_info: pooling type     = 0
0.00.075.422 I print_info: rope type        = 2
0.00.075.422 I print_info: rope scaling     = linear
0.00.075.423 I print_info: freq_base_train  = 10000.0
0.00.075.423 I print_info: freq_scale_train = 1
0.00.075.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.423 I print_info: rope_finetuned   = unknown
0.00.075.424 I print_info: ssm_d_conv       = 0
0.00.075.424 I print_info: ssm_d_inner      = 0
0.00.075.424 I print_info: ssm_d_state      = 0
0.00.075.425 I print_info: ssm_dt_rank      = 0
0.00.075.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.426 I print_info: model type       = 1.4B
0.00.075.426 I print_info: model params     = 1.41 B
0.00.075.426 I print_info: general.name     = 1.4B
0.00.075.427 I print_info: vocab type       = BPE
0.00.075.427 I print_info: n_vocab          = 50304
0.00.075.429 I print_info: n_merges         = 50009
0.00.075.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.430 I print_info: LF token         = 187 'Ċ'
0.00.075.430 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.430 I print_info: max token length = 1024
0.00.075.431 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.322.829 I load_tensors: offloading 24 repeating layers to GPU
0.01.322.833 I load_tensors: offloading output layer to GPU
0.01.322.833 I load_tensors: offloaded 25/25 layers to GPU
0.01.322.865 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.322.867 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.323.782 I llama_context: constructing llama_context
0.01.323.783 I llama_context: n_seq_max     = 1
0.01.323.783 I llama_context: n_ctx         = 128
0.01.323.784 I llama_context: n_ctx_per_seq = 128
0.01.323.784 I llama_context: n_batch       = 128
0.01.323.784 I llama_context: n_ubatch      = 128
0.01.323.784 I llama_context: flash_attn    = 0
0.01.323.785 I llama_context: freq_base     = 10000.0
0.01.323.785 I llama_context: freq_scale    = 1
0.01.323.785 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.323.789 I ggml_metal_init: allocating
0.01.323.880 I ggml_metal_init: found device: Apple M4
0.01.323.887 I ggml_metal_init: picking default device: Apple M4
0.01.325.173 I ggml_metal_init: using embedded metal library
0.01.329.034 I ggml_metal_init: GPU name:   Apple M4
0.01.329.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.329.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.329.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.329.038 I ggml_metal_init: simdgroup reduction   = true
0.01.329.039 I ggml_metal_init: simdgroup matrix mul. = true
0.01.329.039 I ggml_metal_init: has residency sets    = true
0.01.329.039 I ggml_metal_init: has bfloat            = true
0.01.329.039 I ggml_metal_init: use bfloat            = true
0.01.329.040 I ggml_metal_init: hasUnifiedMemory      = true
0.01.329.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.339.569 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.339.570 I llama_context_kv_self: constructing llama_context_kv_self
0.01.339.572 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.341.265 I init:      Metal KV buffer size =    24.00 MiB
0.01.341.267 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.342.882 I init:      Metal compute buffer size =    25.56 MiB
0.01.342.883 I init:        CPU compute buffer size =     1.06 MiB
0.01.342.884 I init: graph nodes  = 991
0.01.342.884 I init: graph splits = 2
0.01.342.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.342.886 I 
0.01.342.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.342.916 I compute_imatrix: tokenizing the input ..
0.01.347.007 I compute_imatrix: tokenization took 4.091 ms
0.01.347.009 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.610.402 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.612.804 I llama_perf_context_print:        load time =    1587.06 ms
0.01.612.804 I llama_perf_context_print: prompt eval time =     261.64 ms /   128 tokens (    2.04 ms per token,   489.21 tokens per second)
0.01.612.805 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.612.806 I llama_perf_context_print:       total time =    1589.46 ms /   129 tokens
0.01.613.500 I ggml_metal_free: deallocating

real	0m1.798s
user	0m0.125s
sys	0m0.257s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4816 (08011c2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101708280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1017088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101708ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101709450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101709a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101709fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10170a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10170ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10170b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10170b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10170bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10170bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10170cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10170d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10170daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10170e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10170e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10170f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10170f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10170fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101710610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101710d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101711450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101711cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101712410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1017126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101712ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101713950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101713e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1017145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1017148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101715140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101715680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101715940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101715de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101716720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101716bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101717060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101717500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1017179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101717e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1017182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1017185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101718bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1017191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101719ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10171a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10171a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10171ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10171b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10171b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10171bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10171c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10171cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10171d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10171d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10171d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10171e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10171e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10171e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10171ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10171f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10171f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10171fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10171ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101720450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1017208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101720d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1017216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1017220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101722610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101722b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1017230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101723600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101723b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1017240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1017245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101724b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101725090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1017255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101725b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1017265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101726b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101727070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1017275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101727b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101728060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1017285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101729050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1017295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101729af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1017197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101729f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10172a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10172ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10172b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10172b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10172bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10172c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10172c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10172cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10172d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10172d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10172dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10172e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10172e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10172ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10172f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10172f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10172fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10172fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101730340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1017307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101730c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101731120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1017315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101731a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101731f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1017323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101732840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101733ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101733f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101734400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1017348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101734d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1017351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101735680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101735fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101736900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101736da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101737240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1017376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101737b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101738020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1017384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101738960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1017392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101739be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10173a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10173a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10173a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10173ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10173b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10173b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10173bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10173c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10173c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10173ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10173cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10173d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10173d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10173dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10173e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10173e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10173ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10173ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10173f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10173f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10173fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1017401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101740640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101740ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101740f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101741420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1017418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101741d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101742200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1017426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101742b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101742fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101743480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101743920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101743dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101744260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101744700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101744ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101745040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1017454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101745980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101745e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101746370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1017468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101746e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101747360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101747620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101747c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101748240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101749040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1017494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1017497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101749db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10174a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10174abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10174b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10174b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10174b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10174c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10174c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10174cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10174d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10174d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10174dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10174e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10174e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10174ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10174f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10174f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10174fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101750100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101750650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101750ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1017510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101751b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1017520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101752630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101752b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1017530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101753620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1017540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101754610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101754b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1017550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101755600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101755b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1017560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1017565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101757090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1017575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101757b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1017585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101758b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101759070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1017595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101759b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10175a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10175a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10175ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10175b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10175b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10175baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10175c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10175c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10175cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10175d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10175d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10175dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10175e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10175e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10175eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10175ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10175f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10175f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10175fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1017601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101760680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101760b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101760fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101761460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101761900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101761da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101762240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1017626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101762b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101763020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101763570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101763c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1017643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101764ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1017651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1017654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101765ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101765f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101766570 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
0.00.729.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e007e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e008920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e0090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e0098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e00a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e00a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e00ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e00b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e00bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e00c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e00cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e00d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e00d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e00e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e00e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e00e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e00eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e00ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e00f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e00f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e00fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e0101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e010470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e0108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e010d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e0111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e011630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e011f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e012380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e0127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e012c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e0130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e013540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e0139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e013e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e014290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e014700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e014b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e014fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e015450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e0158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e015d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e0161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e016610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e016b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e017080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e0174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e017960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e018240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e0186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e018b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e018f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e019400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e019870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e019ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e01a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e01a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e01aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e01aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e01b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e01b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e01bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e01c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e01c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e01c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e01cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e01d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e01d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e01db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e01df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e01e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e01e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e01ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e01f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e01f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e01fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e01fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e0202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e020760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e020bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e021040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e0214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e021920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e021d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e022200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e022ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e022f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e0233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e023830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e023ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e024110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e024580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e0249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e024e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e0252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e025740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e025bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e026020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e026490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e026900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e026d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e0271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e027650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e027ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e027f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e0283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e028810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e028c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e0290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e029560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e0299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e029e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e02a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e02a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e02ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e02b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e02b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e02b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e02bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e02c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e02c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e02caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e02cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e02d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e02d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e02dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e02e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e02e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e02e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e02ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e02f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e02f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e02fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e02ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e030450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e0308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e030d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e0311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e031610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e031a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e031ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e032360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e0327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e032c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e0330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e033520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e033990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e033e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e034270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e0346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e034b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e034fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e035bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e035eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e036170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e0365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e036a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e036ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e037330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e0377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e037c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e038080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e0384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e038960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e038dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e039240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e0396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e039b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e039f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e03a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e03a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e03ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e03b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e03b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e03ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e03bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e03c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e03c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e03cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e03d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e03d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e03d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e03ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e03e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e03e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e03eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e03ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e03f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e03f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e03fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e0402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e040730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e040ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e041010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e041530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e041a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e0425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e042870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e042e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e0433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e0439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e043f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e044530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e044af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e0450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e045670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e0461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e0467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e046d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e047330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e0478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e047eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e048470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e048a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e048ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e0495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e049b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e04a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e04acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e04b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e04b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e04bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e04c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e04c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e04cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e04d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e04dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e04e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e04e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e04ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e04f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e04f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e04fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e0502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e0508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e050e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e051430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e0519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e051fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e052570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e052b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e0530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e0536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e053c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e054230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e0547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e054db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e055370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e055930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e055ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e0564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e056a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e056f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e057470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e057970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e057e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e058370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e058870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e058d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e059270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e059770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e059c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e05a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e05a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e05ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e05b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e05b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e05bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e05c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e05cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e05d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e05d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e05df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e05e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e05e860 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1057086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105708b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105708e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105709290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105709700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1057099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105709e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10570a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10570a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10570ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10570aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10570b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10570c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10570c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10570d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10570d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10570dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10570e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10570ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10570f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10570fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105710420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105710b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105711260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105711980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105711c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105711f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105712370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1057127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105712c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1057130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1057135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105713a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105714190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105714600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105714ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105715350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1057157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105715c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1057160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105716510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105716980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105716df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105717260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1057176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105717b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105717fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105718420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105718890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105718d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105719170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1057195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105719a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105719ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10571a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10571a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10571ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10571b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10571b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10571baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10571bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10571c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10571c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10571ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10571d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10571d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10571da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10571de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10571e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10571e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10571ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10571f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10571f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10571f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10571fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1057201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105720660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105720ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105720f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1057213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105721820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105721c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105722100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105722570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1057229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105722e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1057232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105723730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105724010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105724480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1057248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105724d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1057251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105725640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105725ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105725f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105726390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105726800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105726c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1057270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105727550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105727ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105728180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1057285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105728a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105728ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105729340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1057297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105729c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10572a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10572a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10572a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10572ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10572b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10572b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10572bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10572bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10572c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10572c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10572ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10572d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10572d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10572da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10572deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10572e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10572e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10572ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10572f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10572f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10572f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10572fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105730230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1057306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105730b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105730f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1057313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105731860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105731cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105732140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1057325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105732a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105732e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105733300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105733770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105733be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105734050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1057344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105734930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105734da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105735210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105735680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105735f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1057363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105736840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105736cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105737120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105737590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105737a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105737e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1057382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105738750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105738bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105739030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1057394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105739910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105739d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10573a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10573a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10573aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10573af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10573b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10573b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10573bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10573c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10573c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10573c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10573ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10573d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10573d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10573dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10573e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10573e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10573e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10573ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10573f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10573f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10573fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10573ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105740390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105740800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105740c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1057410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105741550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1057419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1057422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105742710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105742b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105743460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1057438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105743d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1057441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105744740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105745020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105745b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1057460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105746560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1057469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1057472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105747720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105748470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1057488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105748d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1057491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105749aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105749f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10574a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10574a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10574ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10574b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10574b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10574b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10574be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10574c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10574c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10574cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10574cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10574d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10574d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10574dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10574e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10574e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10574ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10574eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10574f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10574f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10574fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1057500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105750520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105750e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105751270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1057516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105751b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105751fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105752430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1057528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105752d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105753180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1057535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105753a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105753ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1057547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105754c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105755090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105755500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105755970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105755de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105756250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1057566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105756b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105756fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105757410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105757880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105757cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105758160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1057585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105758a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105758eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105759320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105759790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10575a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10575a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10575b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10575b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10575ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10575be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10575c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10575caa0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.783s
user	0m0.279s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4816 (08011c2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13260f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13260f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13260fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132610ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132611a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132611ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1326124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1326129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132612ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132613a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1326141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1326149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1326150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132615810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132616650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132616e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132617c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132618380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132618c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132619340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132619600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132619c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13261a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13261adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13261b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13261b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13261b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13261c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13261c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13261c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13261cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13261d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13261d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13261daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13261df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13261e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13261e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13261ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13261f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13261f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13261fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1326200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132621020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132621630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132622250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132622e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132623660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132623b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132623fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132624870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132625060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132625320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1326257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132625c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132626100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1326265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132626a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132626ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132627380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132627820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132627cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132628160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132628aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132628ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132629540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132629a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132629fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13262a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13262aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13262afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13262b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13262ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13262bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13262c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13262ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13262cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13262d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13262da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13262dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13262e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13262ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13262ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13262f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13262fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13262ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1326304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132630e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132631640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132631b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1326320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132632630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132632b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1326330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132633620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132633b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1326340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132634610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1326350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132635600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132635b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132635ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132636490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132636dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132637270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132637bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132638050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1326384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132638990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132638e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1326392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132639770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132639c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13263a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13263a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13263a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13263ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13263b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13263b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13263bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13263c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13263c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13263ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13263cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13263d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13263d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13263dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13263e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13263e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13263eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13263ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13263f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13263f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13263fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1326401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132640670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132640b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132641450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1326418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132641d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1326426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132642b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1326434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132643950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132643df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132644290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132644730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132644bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132645070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132645510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1326459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132645e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1326462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132646790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1326470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132647570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132647a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132648350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1326487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132648c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132649130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1326495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13264a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13264a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13264acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13264b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13264b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13264bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13264bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13264c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13264c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13264cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13264d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13264d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13264dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13264e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13264e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13264eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13264f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13264f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13264ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132650410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1326506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132650ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1326512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132651ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132651f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132652420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1326528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132653070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1326535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132653b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132654060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1326545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132654b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132655050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1326555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132655af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132656040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132656ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132658020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132658570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132658ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132659010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132659560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13265a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13265a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13265aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13265aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13265b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13265ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13265bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13265c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13265ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13265cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13265d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13265da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13265dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13265e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13265ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13265efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13265f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13265fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13265ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1326604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132660a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1326614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132661a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132661f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1326624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132662a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132662f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1326634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132663a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132663f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1326644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132664a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132664f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1326654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1326659f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132665e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132666330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1326667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132666c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132667110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1326675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132667a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132667ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132668390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132668830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132668cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132669170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132669610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132669ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132669f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13266a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13266abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13266b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13266ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13266c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13266c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13266cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13266ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13266d4a0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
0.00.099.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1338053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1338069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1338072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1338090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13380a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13380a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13380ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13380b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13380bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13380c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13380cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13380d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13380d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13380e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13380e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13380e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13380eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13380ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13380f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13380f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13380fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1338101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1338111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1338123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1338130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1338139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1338142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1338158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1338161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1338170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1338186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13381a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13381a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13381aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13381aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13381b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13381b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13381bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13381c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13381c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13381c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13381cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13381d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13381d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13381db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13381df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13381e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13381e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13381ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13381f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13381f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13381fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13381fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1338214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1338233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1338245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1338252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1338264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1338283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1338299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13382a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13382a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13382abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13382b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13382b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13382b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13382bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13382c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13382c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13382cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13382cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13382d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13382d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13382dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13382e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13382e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13382e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13382ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13382f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13382f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13382fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1338308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1338311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1338327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1338330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1338339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1338377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1338380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1338396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13383a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13383a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13383ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13383b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13383b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13383ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13383bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13383c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13383c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13383cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13383d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13383d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13383d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13383ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13383e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13383e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13383eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13383ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13383f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13383f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13383fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1338402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1338425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1338439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1338450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1338467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1338495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13384a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13384a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13384acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13384b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13384b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13384be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13384c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13384c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13384cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13384d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13384dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13384e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13384e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13384ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13384f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13384f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13384fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1338508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1338536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1338564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13385a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13385a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13385ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13385b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13385b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13385bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13385c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13385cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13385d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13385d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13385dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13385e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13385e880 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132706bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132707040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1327074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132707920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132707d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132708200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132708670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132708ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132708f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1327093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132709830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132709f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13270aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13270b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13270ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13270c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13270c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13270cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13270d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13270ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13270e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13270ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13270f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13270fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132710190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132710450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132711960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132711e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1327122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1327125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132712a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132712e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1327133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1327138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1327142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1327147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1327151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1327156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132715be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132716050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1327164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132716930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132716da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132717210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132717680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132717af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132717f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1327183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132718840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132719010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1327194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132719770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132719d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13271a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13271aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13271aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13271b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13271b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13271bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13271c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13271c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13271ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13271cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13271d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13271d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13271dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13271e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13271e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13271ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13271f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13271f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13271fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132720170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1327206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132720c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132721160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1327216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132721c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132722150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1327226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132722bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132723140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132723690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132724130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132724680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132724bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132725120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132725670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132725bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132726110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132726660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132726bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132727100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132727650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132727ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1327280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132728640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1327290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132729b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13272a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13272a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13272ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13272b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13272b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13272bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13272bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13272c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13272c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13272cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13272d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13272d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13272db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13272dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13272e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13272e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13272ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13272f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13272f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13272fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132730010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1327304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132730df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132731290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132731730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132731bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132732510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1327329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132732e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1327332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132733790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132733c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1327340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132734570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132734a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132734eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132735350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1327357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132735c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132736130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1327365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132736f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1327373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132737850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132737cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132738190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132738630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132738ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132738f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132739410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1327398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132739d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13273a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13273a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13273ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13273afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13273b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13273b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13273bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13273c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13273c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13273cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13273d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13273d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13273d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13273de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13273e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13273e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13273ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13273f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13273f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13273f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13273fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132740310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1327407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132740c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1327410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132741590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132741a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132741ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132742370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132742810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1327432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132743800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132743d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132744010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132744620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132744c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132745240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132745a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132746190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1327467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132746db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1327475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132747a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132747ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132748380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132748b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132749080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1327495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132749b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13274a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13274a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13274ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13274b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13274b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13274bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13274c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13274c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13274caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13274d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13274d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13274dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13274e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13274e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13274ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13274f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13274f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13274fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132750010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132750560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132750ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132751000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132751550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132751aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132751ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132752540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132752a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132752fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132753530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132753a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132753fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132754520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132754a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132754fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132755510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132755a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132755fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132756500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132756a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132756fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1327574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132757f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1327584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132758a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132758f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1327594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132759a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132759f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13275a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13275aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13275af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13275b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13275b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13275bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13275c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13275c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13275cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13275d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13275d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13275d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13275de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13275e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13275e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13275ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13275f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13275f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13275fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13275ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132760680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132760da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1327614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132761be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132761ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132762690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132762f60 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.230s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
