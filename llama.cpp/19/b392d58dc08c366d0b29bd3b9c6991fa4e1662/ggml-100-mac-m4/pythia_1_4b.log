Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.693s
user	0m0.912s
sys	0m1.294s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target sha256
[  4%] Built target build_info
[  4%] Built target xxhash
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Built target llama-batched-bench
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookup-merge
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-parallel
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.168s
user	0m6.562s
sys	0m10.013s

main: quantize time =  3291.65 ms
main:    total time =  3291.65 ms

main: quantize time =  2209.16 ms
main:    total time =  2209.16 ms

main: quantize time =  2061.08 ms
main:    total time =  2061.08 ms

main: quantize time =  1855.38 ms
main:    total time =  1855.38 ms

main: quantize time =  1934.51 ms
main:    total time =  1934.51 ms

main: quantize time =  5901.88 ms
main:    total time =  5901.88 ms

main: quantize time =  5806.01 ms
main:    total time =  5806.01 ms

main: quantize time =  7358.59 ms
main:    total time =  7358.59 ms

main: quantize time =  6241.70 ms
main:    total time =  6241.70 ms

main: quantize time =  4796.59 ms
main:    total time =  4796.59 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.221 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.380 I main: llama backend init
0.00.000.386 I main: load the model and apply lora adapter, if any
0.00.055.249 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.067.941 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.964 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.966 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.966 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.971 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.084.486 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.487 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.487 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.490 I llama_model_loader: - type  f32:  194 tensors
0.00.084.490 I llama_model_loader: - type  f16:   98 tensors
0.00.084.492 I print_info: file format = GGUF V3 (latest)
0.00.084.493 I print_info: file type   = all F32 (guessed)
0.00.084.497 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.103.618 I load: special tokens cache size = 25
0.00.114.190 I load: token to piece cache size = 0.2984 MB
0.00.114.197 I print_info: arch             = gptneox
0.00.114.197 I print_info: vocab_only       = 0
0.00.114.198 I print_info: n_ctx_train      = 2048
0.00.114.198 I print_info: n_embd           = 2048
0.00.114.198 I print_info: n_layer          = 24
0.00.114.205 I print_info: n_head           = 16
0.00.114.206 I print_info: n_head_kv        = 16
0.00.114.206 I print_info: n_rot            = 32
0.00.114.210 I print_info: n_swa            = 0
0.00.114.210 I print_info: n_embd_head_k    = 128
0.00.114.210 I print_info: n_embd_head_v    = 128
0.00.114.211 I print_info: n_gqa            = 1
0.00.114.212 I print_info: n_embd_k_gqa     = 2048
0.00.114.213 I print_info: n_embd_v_gqa     = 2048
0.00.114.216 I print_info: f_norm_eps       = 1.0e-05
0.00.114.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.114.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.114.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.114.217 I print_info: f_logit_scale    = 0.0e+00
0.00.114.218 I print_info: n_ff             = 8192
0.00.114.219 I print_info: n_expert         = 0
0.00.114.219 I print_info: n_expert_used    = 0
0.00.114.219 I print_info: causal attn      = 1
0.00.114.219 I print_info: pooling type     = 0
0.00.114.219 I print_info: rope type        = 2
0.00.114.220 I print_info: rope scaling     = linear
0.00.114.220 I print_info: freq_base_train  = 10000.0
0.00.114.221 I print_info: freq_scale_train = 1
0.00.114.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.114.221 I print_info: rope_finetuned   = unknown
0.00.114.221 I print_info: ssm_d_conv       = 0
0.00.114.221 I print_info: ssm_d_inner      = 0
0.00.114.222 I print_info: ssm_d_state      = 0
0.00.114.222 I print_info: ssm_dt_rank      = 0
0.00.114.222 I print_info: ssm_dt_b_c_rms   = 0
0.00.114.222 I print_info: model type       = 1.4B
0.00.114.223 I print_info: model params     = 1.41 B
0.00.114.223 I print_info: general.name     = 1.4B
0.00.114.224 I print_info: vocab type       = BPE
0.00.114.224 I print_info: n_vocab          = 50304
0.00.114.224 I print_info: n_merges         = 50009
0.00.114.225 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.114.225 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.114.225 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.114.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.114.226 I print_info: LF token         = 187 'Ċ'
0.00.114.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.114.227 I print_info: max token length = 1024
0.00.114.227 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.185.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.185.134 I load_tensors: offloading output layer to GPU
0.00.185.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.185.162 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.185.164 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.185.785 I llama_init_from_model: n_seq_max     = 1
0.00.185.786 I llama_init_from_model: n_ctx         = 2048
0.00.185.786 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.185.786 I llama_init_from_model: n_batch       = 2048
0.00.185.786 I llama_init_from_model: n_ubatch      = 512
0.00.185.786 I llama_init_from_model: flash_attn    = 0
0.00.185.787 I llama_init_from_model: freq_base     = 10000.0
0.00.185.787 I llama_init_from_model: freq_scale    = 1
0.00.185.789 I ggml_metal_init: allocating
0.00.185.845 I ggml_metal_init: found device: Apple M4
0.00.185.852 I ggml_metal_init: picking default device: Apple M4
0.00.186.499 I ggml_metal_init: using embedded metal library
0.00.196.152 I ggml_metal_init: GPU name:   Apple M4
0.00.196.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.196.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.196.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.196.154 I ggml_metal_init: simdgroup reduction   = true
0.00.196.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.196.155 I ggml_metal_init: has residency sets    = true
0.00.196.155 I ggml_metal_init: has bfloat            = true
0.00.196.155 I ggml_metal_init: use bfloat            = true
0.00.196.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.196.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.220.682 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.251.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.251.164 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.251.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.254.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.254.852 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.254.853 I llama_init_from_model: graph nodes  = 967
0.00.254.853 I llama_init_from_model: graph splits = 2
0.00.254.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.254.988 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.254.988 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.319.913 I main: llama threadpool init, n_threads = 4
0.00.319.951 I 
0.00.319.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.319.980 I 
0.00.320.113 I sampler seed: 1234
0.00.320.117 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.320.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.320.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.320.142 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.169.781 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.02.169.782 I llama_perf_context_print:        load time =     263.83 ms
0.02.169.782 I llama_perf_context_print: prompt eval time =      53.53 ms /     7 tokens (    7.65 ms per token,   130.76 tokens per second)
0.02.169.783 I llama_perf_context_print:        eval time =    1793.23 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.169.784 I llama_perf_context_print:       total time =    1850.69 ms /    70 tokens
0.02.170.024 I ggml_metal_free: deallocating

real	0m2.511s
user	0m0.135s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.068 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.103 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.323 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.184 I llama_model_loader: - type  f32:  194 tensors
0.00.038.184 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.185 I print_info: file format = GGUF V3 (latest)
0.00.038.185 I print_info: file type   = Q8_0
0.00.038.186 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.106 I load: special tokens cache size = 25
0.00.054.037 I load: token to piece cache size = 0.2984 MB
0.00.054.042 I print_info: arch             = gptneox
0.00.054.042 I print_info: vocab_only       = 0
0.00.054.042 I print_info: n_ctx_train      = 2048
0.00.054.044 I print_info: n_embd           = 2048
0.00.054.044 I print_info: n_layer          = 24
0.00.054.050 I print_info: n_head           = 16
0.00.054.051 I print_info: n_head_kv        = 16
0.00.054.051 I print_info: n_rot            = 32
0.00.054.051 I print_info: n_swa            = 0
0.00.054.051 I print_info: n_embd_head_k    = 128
0.00.054.051 I print_info: n_embd_head_v    = 128
0.00.054.052 I print_info: n_gqa            = 1
0.00.054.053 I print_info: n_embd_k_gqa     = 2048
0.00.054.053 I print_info: n_embd_v_gqa     = 2048
0.00.054.054 I print_info: f_norm_eps       = 1.0e-05
0.00.054.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.055 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.055 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.055 I print_info: f_logit_scale    = 0.0e+00
0.00.054.056 I print_info: n_ff             = 8192
0.00.054.056 I print_info: n_expert         = 0
0.00.054.057 I print_info: n_expert_used    = 0
0.00.054.057 I print_info: causal attn      = 1
0.00.054.057 I print_info: pooling type     = 0
0.00.054.057 I print_info: rope type        = 2
0.00.054.057 I print_info: rope scaling     = linear
0.00.054.058 I print_info: freq_base_train  = 10000.0
0.00.054.058 I print_info: freq_scale_train = 1
0.00.054.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.058 I print_info: rope_finetuned   = unknown
0.00.054.058 I print_info: ssm_d_conv       = 0
0.00.054.059 I print_info: ssm_d_inner      = 0
0.00.054.059 I print_info: ssm_d_state      = 0
0.00.054.059 I print_info: ssm_dt_rank      = 0
0.00.054.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.060 I print_info: model type       = 1.4B
0.00.054.060 I print_info: model params     = 1.41 B
0.00.054.060 I print_info: general.name     = 1.4B
0.00.054.061 I print_info: vocab type       = BPE
0.00.054.061 I print_info: n_vocab          = 50304
0.00.054.061 I print_info: n_merges         = 50009
0.00.054.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.062 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.062 I print_info: LF token         = 187 'Ċ'
0.00.054.062 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.063 I print_info: max token length = 1024
0.00.054.063 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.088.393 I load_tensors: offloading 24 repeating layers to GPU
0.01.088.398 I load_tensors: offloading output layer to GPU
0.01.088.399 I load_tensors: offloaded 25/25 layers to GPU
0.01.088.423 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.088.425 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.089.453 I llama_init_from_model: n_seq_max     = 1
0.01.089.454 I llama_init_from_model: n_ctx         = 2048
0.01.089.455 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.089.455 I llama_init_from_model: n_batch       = 2048
0.01.089.455 I llama_init_from_model: n_ubatch      = 512
0.01.089.456 I llama_init_from_model: flash_attn    = 0
0.01.089.456 I llama_init_from_model: freq_base     = 10000.0
0.01.089.457 I llama_init_from_model: freq_scale    = 1
0.01.089.458 I ggml_metal_init: allocating
0.01.089.470 I ggml_metal_init: found device: Apple M4
0.01.089.476 I ggml_metal_init: picking default device: Apple M4
0.01.090.807 I ggml_metal_init: using embedded metal library
0.01.096.665 I ggml_metal_init: GPU name:   Apple M4
0.01.096.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.096.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.096.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.096.670 I ggml_metal_init: simdgroup reduction   = true
0.01.096.670 I ggml_metal_init: simdgroup matrix mul. = true
0.01.096.670 I ggml_metal_init: has residency sets    = true
0.01.096.671 I ggml_metal_init: has bfloat            = true
0.01.096.671 I ggml_metal_init: use bfloat            = true
0.01.096.671 I ggml_metal_init: hasUnifiedMemory      = true
0.01.096.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.112.551 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.165.525 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.165.531 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.165.553 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.170.555 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.170.557 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.170.557 I llama_init_from_model: graph nodes  = 967
0.01.170.557 I llama_init_from_model: graph splits = 2
0.01.170.563 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.170.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.170.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.227.964 I main: llama threadpool init, n_threads = 4
0.01.228.008 I 
0.01.228.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.228.031 I 
0.01.228.210 I sampler seed: 1234
0.01.228.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.228.252 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.228.256 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.228.256 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.323.130 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.02.323.131 I llama_perf_context_print:        load time =    1217.38 ms
0.02.323.131 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.11 tokens per second)
0.02.323.133 I llama_perf_context_print:        eval time =    1042.97 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.323.133 I llama_perf_context_print:       total time =    1095.87 ms /    70 tokens
0.02.323.380 I ggml_metal_free: deallocating

real	0m2.342s
user	0m0.110s
sys	0m0.257s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.015.033 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.445 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.450 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.459 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.460 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.084 I llama_model_loader: - type  f32:  194 tensors
0.00.035.085 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.086 I print_info: file format = GGUF V3 (latest)
0.00.035.087 I print_info: file type   = Q4_0
0.00.035.088 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.633 I load: special tokens cache size = 25
0.00.052.088 I load: token to piece cache size = 0.2984 MB
0.00.052.091 I print_info: arch             = gptneox
0.00.052.092 I print_info: vocab_only       = 0
0.00.052.092 I print_info: n_ctx_train      = 2048
0.00.052.092 I print_info: n_embd           = 2048
0.00.052.092 I print_info: n_layer          = 24
0.00.052.096 I print_info: n_head           = 16
0.00.052.099 I print_info: n_head_kv        = 16
0.00.052.099 I print_info: n_rot            = 32
0.00.052.099 I print_info: n_swa            = 0
0.00.052.099 I print_info: n_embd_head_k    = 128
0.00.052.099 I print_info: n_embd_head_v    = 128
0.00.052.100 I print_info: n_gqa            = 1
0.00.052.101 I print_info: n_embd_k_gqa     = 2048
0.00.052.101 I print_info: n_embd_v_gqa     = 2048
0.00.052.102 I print_info: f_norm_eps       = 1.0e-05
0.00.052.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.103 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.105 I print_info: f_logit_scale    = 0.0e+00
0.00.052.106 I print_info: n_ff             = 8192
0.00.052.106 I print_info: n_expert         = 0
0.00.052.106 I print_info: n_expert_used    = 0
0.00.052.106 I print_info: causal attn      = 1
0.00.052.106 I print_info: pooling type     = 0
0.00.052.107 I print_info: rope type        = 2
0.00.052.107 I print_info: rope scaling     = linear
0.00.052.107 I print_info: freq_base_train  = 10000.0
0.00.052.107 I print_info: freq_scale_train = 1
0.00.052.108 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.108 I print_info: rope_finetuned   = unknown
0.00.052.108 I print_info: ssm_d_conv       = 0
0.00.052.108 I print_info: ssm_d_inner      = 0
0.00.052.108 I print_info: ssm_d_state      = 0
0.00.052.108 I print_info: ssm_dt_rank      = 0
0.00.052.112 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.112 I print_info: model type       = 1.4B
0.00.052.113 I print_info: model params     = 1.41 B
0.00.052.115 I print_info: general.name     = 1.4B
0.00.052.115 I print_info: vocab type       = BPE
0.00.052.116 I print_info: n_vocab          = 50304
0.00.052.116 I print_info: n_merges         = 50009
0.00.052.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.116 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.117 I print_info: LF token         = 187 'Ċ'
0.00.052.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.117 I print_info: max token length = 1024
0.00.052.118 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.641 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.656 I load_tensors: offloading output layer to GPU
0.00.704.658 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.690 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.704.692 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.706.342 I llama_init_from_model: n_seq_max     = 1
0.00.706.345 I llama_init_from_model: n_ctx         = 2048
0.00.706.345 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.706.346 I llama_init_from_model: n_batch       = 2048
0.00.706.346 I llama_init_from_model: n_ubatch      = 512
0.00.706.347 I llama_init_from_model: flash_attn    = 0
0.00.706.349 I llama_init_from_model: freq_base     = 10000.0
0.00.706.349 I llama_init_from_model: freq_scale    = 1
0.00.706.358 I ggml_metal_init: allocating
0.00.706.450 I ggml_metal_init: found device: Apple M4
0.00.706.464 I ggml_metal_init: picking default device: Apple M4
0.00.708.391 I ggml_metal_init: using embedded metal library
0.00.714.862 I ggml_metal_init: GPU name:   Apple M4
0.00.714.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.872 I ggml_metal_init: simdgroup reduction   = true
0.00.714.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.872 I ggml_metal_init: has residency sets    = true
0.00.714.873 I ggml_metal_init: has bfloat            = true
0.00.714.873 I ggml_metal_init: use bfloat            = true
0.00.714.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.879 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.733.483 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.064 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.795.069 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.795.094 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.343 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.799.345 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.799.345 I llama_init_from_model: graph nodes  = 967
0.00.799.346 I llama_init_from_model: graph splits = 2
0.00.799.352 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.799.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.799.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.966 I main: llama threadpool init, n_threads = 4
0.00.858.008 I 
0.00.858.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.030 I 
0.00.858.209 I sampler seed: 1234
0.00.858.214 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.241 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.550.877 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.550.878 I llama_perf_context_print:        load time =     842.23 ms
0.01.550.878 I llama_perf_context_print: prompt eval time =      49.48 ms /     7 tokens (    7.07 ms per token,   141.47 tokens per second)
0.01.550.880 I llama_perf_context_print:        eval time =     640.25 ms /    63 runs   (   10.16 ms per token,    98.40 tokens per second)
0.01.550.881 I llama_perf_context_print:       total time =     693.61 ms /    70 tokens
0.01.551.151 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.115s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.007 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.014 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.021 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.024 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.025 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.592 I llama_model_loader: - type  f32:  194 tensors
0.00.026.592 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.593 I print_info: file format = GGUF V3 (latest)
0.00.026.593 I print_info: file type   = Q4_1
0.00.026.594 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.361 I load: special tokens cache size = 25
0.00.040.360 I load: token to piece cache size = 0.2984 MB
0.00.040.363 I print_info: arch             = gptneox
0.00.040.363 I print_info: vocab_only       = 0
0.00.040.363 I print_info: n_ctx_train      = 2048
0.00.040.363 I print_info: n_embd           = 2048
0.00.040.364 I print_info: n_layer          = 24
0.00.040.367 I print_info: n_head           = 16
0.00.040.367 I print_info: n_head_kv        = 16
0.00.040.368 I print_info: n_rot            = 32
0.00.040.368 I print_info: n_swa            = 0
0.00.040.368 I print_info: n_embd_head_k    = 128
0.00.040.368 I print_info: n_embd_head_v    = 128
0.00.040.369 I print_info: n_gqa            = 1
0.00.040.370 I print_info: n_embd_k_gqa     = 2048
0.00.040.370 I print_info: n_embd_v_gqa     = 2048
0.00.040.371 I print_info: f_norm_eps       = 1.0e-05
0.00.040.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.372 I print_info: f_logit_scale    = 0.0e+00
0.00.040.373 I print_info: n_ff             = 8192
0.00.040.373 I print_info: n_expert         = 0
0.00.040.373 I print_info: n_expert_used    = 0
0.00.040.373 I print_info: causal attn      = 1
0.00.040.373 I print_info: pooling type     = 0
0.00.040.375 I print_info: rope type        = 2
0.00.040.377 I print_info: rope scaling     = linear
0.00.040.377 I print_info: freq_base_train  = 10000.0
0.00.040.377 I print_info: freq_scale_train = 1
0.00.040.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.378 I print_info: rope_finetuned   = unknown
0.00.040.378 I print_info: ssm_d_conv       = 0
0.00.040.378 I print_info: ssm_d_inner      = 0
0.00.040.378 I print_info: ssm_d_state      = 0
0.00.040.378 I print_info: ssm_dt_rank      = 0
0.00.040.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.379 I print_info: model type       = 1.4B
0.00.040.379 I print_info: model params     = 1.41 B
0.00.040.379 I print_info: general.name     = 1.4B
0.00.040.380 I print_info: vocab type       = BPE
0.00.040.380 I print_info: n_vocab          = 50304
0.00.040.380 I print_info: n_merges         = 50009
0.00.040.382 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: LF token         = 187 'Ċ'
0.00.040.383 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.384 I print_info: max token length = 1024
0.00.040.384 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.379 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.394 I load_tensors: offloading output layer to GPU
0.00.641.395 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.429 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.641.431 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.643.057 I llama_init_from_model: n_seq_max     = 1
0.00.643.060 I llama_init_from_model: n_ctx         = 2048
0.00.643.060 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.061 I llama_init_from_model: n_batch       = 2048
0.00.643.062 I llama_init_from_model: n_ubatch      = 512
0.00.643.062 I llama_init_from_model: flash_attn    = 0
0.00.643.064 I llama_init_from_model: freq_base     = 10000.0
0.00.643.065 I llama_init_from_model: freq_scale    = 1
0.00.643.067 I ggml_metal_init: allocating
0.00.643.158 I ggml_metal_init: found device: Apple M4
0.00.643.172 I ggml_metal_init: picking default device: Apple M4
0.00.645.102 I ggml_metal_init: using embedded metal library
0.00.651.710 I ggml_metal_init: GPU name:   Apple M4
0.00.651.714 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.716 I ggml_metal_init: simdgroup reduction   = true
0.00.651.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.716 I ggml_metal_init: has residency sets    = true
0.00.651.717 I ggml_metal_init: has bfloat            = true
0.00.651.717 I ggml_metal_init: use bfloat            = true
0.00.651.718 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.735 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.189 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.397 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.399 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.399 I llama_init_from_model: graph nodes  = 967
0.00.731.400 I llama_init_from_model: graph splits = 2
0.00.731.406 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.282 I main: llama threadpool init, n_threads = 4
0.00.789.323 I 
0.00.789.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.347 I 
0.00.789.520 I sampler seed: 1234
0.00.789.524 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.535 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.535 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.535 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.519.925 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.519.925 I llama_perf_context_print:        load time =     779.79 ms
0.01.519.926 I llama_perf_context_print: prompt eval time =      49.13 ms /     7 tokens (    7.02 ms per token,   142.47 tokens per second)
0.01.519.927 I llama_perf_context_print:        eval time =     678.42 ms /    63 runs   (   10.77 ms per token,    92.86 tokens per second)
0.01.519.929 I llama_perf_context_print:       total time =     731.34 ms /    70 tokens
0.01.520.214 I ggml_metal_free: deallocating

real	0m1.537s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.401 I llama_model_loader: - type  f32:  194 tensors
0.00.026.401 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.401 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.402 I print_info: file format = GGUF V3 (latest)
0.00.026.402 I print_info: file type   = Q5_0
0.00.026.403 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.211 I load: special tokens cache size = 25
0.00.040.281 I load: token to piece cache size = 0.2984 MB
0.00.040.284 I print_info: arch             = gptneox
0.00.040.284 I print_info: vocab_only       = 0
0.00.040.285 I print_info: n_ctx_train      = 2048
0.00.040.285 I print_info: n_embd           = 2048
0.00.040.285 I print_info: n_layer          = 24
0.00.040.288 I print_info: n_head           = 16
0.00.040.289 I print_info: n_head_kv        = 16
0.00.040.289 I print_info: n_rot            = 32
0.00.040.291 I print_info: n_swa            = 0
0.00.040.291 I print_info: n_embd_head_k    = 128
0.00.040.291 I print_info: n_embd_head_v    = 128
0.00.040.292 I print_info: n_gqa            = 1
0.00.040.293 I print_info: n_embd_k_gqa     = 2048
0.00.040.294 I print_info: n_embd_v_gqa     = 2048
0.00.040.294 I print_info: f_norm_eps       = 1.0e-05
0.00.040.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.295 I print_info: f_logit_scale    = 0.0e+00
0.00.040.296 I print_info: n_ff             = 8192
0.00.040.296 I print_info: n_expert         = 0
0.00.040.296 I print_info: n_expert_used    = 0
0.00.040.296 I print_info: causal attn      = 1
0.00.040.296 I print_info: pooling type     = 0
0.00.040.298 I print_info: rope type        = 2
0.00.040.299 I print_info: rope scaling     = linear
0.00.040.299 I print_info: freq_base_train  = 10000.0
0.00.040.300 I print_info: freq_scale_train = 1
0.00.040.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.300 I print_info: rope_finetuned   = unknown
0.00.040.300 I print_info: ssm_d_conv       = 0
0.00.040.300 I print_info: ssm_d_inner      = 0
0.00.040.301 I print_info: ssm_d_state      = 0
0.00.040.301 I print_info: ssm_dt_rank      = 0
0.00.040.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.301 I print_info: model type       = 1.4B
0.00.040.301 I print_info: model params     = 1.41 B
0.00.040.305 I print_info: general.name     = 1.4B
0.00.040.306 I print_info: vocab type       = BPE
0.00.040.306 I print_info: n_vocab          = 50304
0.00.040.306 I print_info: n_merges         = 50009
0.00.040.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: LF token         = 187 'Ċ'
0.00.040.307 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.308 I print_info: max token length = 1024
0.00.040.308 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.791 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.806 I load_tensors: offloading output layer to GPU
0.00.704.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.842 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.704.844 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.706.362 I llama_init_from_model: n_seq_max     = 1
0.00.706.365 I llama_init_from_model: n_ctx         = 2048
0.00.706.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.706.367 I llama_init_from_model: n_batch       = 2048
0.00.706.367 I llama_init_from_model: n_ubatch      = 512
0.00.706.367 I llama_init_from_model: flash_attn    = 0
0.00.706.369 I llama_init_from_model: freq_base     = 10000.0
0.00.706.370 I llama_init_from_model: freq_scale    = 1
0.00.706.373 I ggml_metal_init: allocating
0.00.706.453 I ggml_metal_init: found device: Apple M4
0.00.706.467 I ggml_metal_init: picking default device: Apple M4
0.00.708.343 I ggml_metal_init: using embedded metal library
0.00.714.714 I ggml_metal_init: GPU name:   Apple M4
0.00.714.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.721 I ggml_metal_init: simdgroup reduction   = true
0.00.714.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.721 I ggml_metal_init: has residency sets    = true
0.00.714.722 I ggml_metal_init: has bfloat            = true
0.00.714.722 I ggml_metal_init: use bfloat            = true
0.00.714.723 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.732.094 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.791.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.791.810 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.791.834 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.796.028 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.796.030 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.796.030 I llama_init_from_model: graph nodes  = 967
0.00.796.030 I llama_init_from_model: graph splits = 2
0.00.796.036 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.796.156 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.796.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.745 I main: llama threadpool init, n_threads = 4
0.00.855.791 I 
0.00.855.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.816 I 
0.00.855.974 I sampler seed: 1234
0.00.855.979 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.989 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.644.973 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.644.973 I llama_perf_context_print:        load time =     845.12 ms
0.01.644.974 I llama_perf_context_print: prompt eval time =      52.97 ms /     7 tokens (    7.57 ms per token,   132.14 tokens per second)
0.01.644.975 I llama_perf_context_print:        eval time =     733.16 ms /    63 runs   (   11.64 ms per token,    85.93 tokens per second)
0.01.644.975 I llama_perf_context_print:       total time =     789.92 ms /    70 tokens
0.01.645.275 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.108s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.851 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.626 I llama_model_loader: - type  f32:  194 tensors
0.00.024.626 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.627 I print_info: file format = GGUF V3 (latest)
0.00.024.627 I print_info: file type   = Q5_1
0.00.024.628 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.409 I load: special tokens cache size = 25
0.00.038.477 I load: token to piece cache size = 0.2984 MB
0.00.038.480 I print_info: arch             = gptneox
0.00.038.480 I print_info: vocab_only       = 0
0.00.038.480 I print_info: n_ctx_train      = 2048
0.00.038.480 I print_info: n_embd           = 2048
0.00.038.480 I print_info: n_layer          = 24
0.00.038.483 I print_info: n_head           = 16
0.00.038.484 I print_info: n_head_kv        = 16
0.00.038.484 I print_info: n_rot            = 32
0.00.038.485 I print_info: n_swa            = 0
0.00.038.485 I print_info: n_embd_head_k    = 128
0.00.038.485 I print_info: n_embd_head_v    = 128
0.00.038.486 I print_info: n_gqa            = 1
0.00.038.486 I print_info: n_embd_k_gqa     = 2048
0.00.038.487 I print_info: n_embd_v_gqa     = 2048
0.00.038.488 I print_info: f_norm_eps       = 1.0e-05
0.00.038.488 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.488 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.489 I print_info: f_logit_scale    = 0.0e+00
0.00.038.490 I print_info: n_ff             = 8192
0.00.038.490 I print_info: n_expert         = 0
0.00.038.491 I print_info: n_expert_used    = 0
0.00.038.491 I print_info: causal attn      = 1
0.00.038.493 I print_info: pooling type     = 0
0.00.038.493 I print_info: rope type        = 2
0.00.038.493 I print_info: rope scaling     = linear
0.00.038.494 I print_info: freq_base_train  = 10000.0
0.00.038.494 I print_info: freq_scale_train = 1
0.00.038.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.495 I print_info: rope_finetuned   = unknown
0.00.038.495 I print_info: ssm_d_conv       = 0
0.00.038.495 I print_info: ssm_d_inner      = 0
0.00.038.495 I print_info: ssm_d_state      = 0
0.00.038.495 I print_info: ssm_dt_rank      = 0
0.00.038.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.495 I print_info: model type       = 1.4B
0.00.038.496 I print_info: model params     = 1.41 B
0.00.038.496 I print_info: general.name     = 1.4B
0.00.038.496 I print_info: vocab type       = BPE
0.00.038.496 I print_info: n_vocab          = 50304
0.00.038.497 I print_info: n_merges         = 50009
0.00.038.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.497 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: LF token         = 187 'Ċ'
0.00.038.503 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.503 I print_info: max token length = 1024
0.00.038.503 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.247 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.251 I load_tensors: offloading output layer to GPU
0.00.608.252 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.275 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.276 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.609.718 I llama_init_from_model: n_seq_max     = 1
0.00.609.720 I llama_init_from_model: n_ctx         = 2048
0.00.609.721 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.609.721 I llama_init_from_model: n_batch       = 2048
0.00.609.722 I llama_init_from_model: n_ubatch      = 512
0.00.609.723 I llama_init_from_model: flash_attn    = 0
0.00.609.724 I llama_init_from_model: freq_base     = 10000.0
0.00.609.724 I llama_init_from_model: freq_scale    = 1
0.00.609.725 I ggml_metal_init: allocating
0.00.609.779 I ggml_metal_init: found device: Apple M4
0.00.609.791 I ggml_metal_init: picking default device: Apple M4
0.00.611.341 I ggml_metal_init: using embedded metal library
0.00.617.402 I ggml_metal_init: GPU name:   Apple M4
0.00.617.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.408 I ggml_metal_init: simdgroup reduction   = true
0.00.617.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.409 I ggml_metal_init: has residency sets    = true
0.00.617.409 I ggml_metal_init: has bfloat            = true
0.00.617.409 I ggml_metal_init: use bfloat            = true
0.00.617.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.788 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.028 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.034 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.102 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.580 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.691.582 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.691.582 I llama_init_from_model: graph nodes  = 967
0.00.691.582 I llama_init_from_model: graph splits = 2
0.00.691.588 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.952 I main: llama threadpool init, n_threads = 4
0.00.748.004 I 
0.00.748.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.026 I 
0.00.748.210 I sampler seed: 1234
0.00.748.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.225 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.226 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.574.931 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.574.932 I llama_perf_context_print:        load time =     738.50 ms
0.01.574.933 I llama_perf_context_print: prompt eval time =      41.89 ms /     7 tokens (    5.98 ms per token,   167.12 tokens per second)
0.01.574.933 I llama_perf_context_print:        eval time =     781.85 ms /    63 runs   (   12.41 ms per token,    80.58 tokens per second)
0.01.574.934 I llama_perf_context_print:       total time =     827.68 ms /    70 tokens
0.01.575.188 I ggml_metal_free: deallocating

real	0m1.592s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.790 I llama_model_loader: - type  f32:  194 tensors
0.00.024.790 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.791 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.791 I print_info: file format = GGUF V3 (latest)
0.00.024.792 I print_info: file type   = Q2_K - Medium
0.00.024.793 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.401 I load: special tokens cache size = 25
0.00.038.498 I load: token to piece cache size = 0.2984 MB
0.00.038.501 I print_info: arch             = gptneox
0.00.038.501 I print_info: vocab_only       = 0
0.00.038.501 I print_info: n_ctx_train      = 2048
0.00.038.501 I print_info: n_embd           = 2048
0.00.038.501 I print_info: n_layer          = 24
0.00.038.504 I print_info: n_head           = 16
0.00.038.505 I print_info: n_head_kv        = 16
0.00.038.505 I print_info: n_rot            = 32
0.00.038.505 I print_info: n_swa            = 0
0.00.038.505 I print_info: n_embd_head_k    = 128
0.00.038.505 I print_info: n_embd_head_v    = 128
0.00.038.506 I print_info: n_gqa            = 1
0.00.038.507 I print_info: n_embd_k_gqa     = 2048
0.00.038.508 I print_info: n_embd_v_gqa     = 2048
0.00.038.508 I print_info: f_norm_eps       = 1.0e-05
0.00.038.508 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.509 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.509 I print_info: f_logit_scale    = 0.0e+00
0.00.038.510 I print_info: n_ff             = 8192
0.00.038.510 I print_info: n_expert         = 0
0.00.038.510 I print_info: n_expert_used    = 0
0.00.038.510 I print_info: causal attn      = 1
0.00.038.510 I print_info: pooling type     = 0
0.00.038.510 I print_info: rope type        = 2
0.00.038.511 I print_info: rope scaling     = linear
0.00.038.511 I print_info: freq_base_train  = 10000.0
0.00.038.511 I print_info: freq_scale_train = 1
0.00.038.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.512 I print_info: rope_finetuned   = unknown
0.00.038.512 I print_info: ssm_d_conv       = 0
0.00.038.512 I print_info: ssm_d_inner      = 0
0.00.038.512 I print_info: ssm_d_state      = 0
0.00.038.512 I print_info: ssm_dt_rank      = 0
0.00.038.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.513 I print_info: model type       = 1.4B
0.00.038.513 I print_info: model params     = 1.41 B
0.00.038.513 I print_info: general.name     = 1.4B
0.00.038.514 I print_info: vocab type       = BPE
0.00.038.514 I print_info: n_vocab          = 50304
0.00.038.514 I print_info: n_merges         = 50009
0.00.038.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: LF token         = 187 'Ċ'
0.00.038.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.516 I print_info: max token length = 1024
0.00.038.516 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.350.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.350.816 I load_tensors: offloading output layer to GPU
0.00.350.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.350.860 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.350.864 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.457 I llama_init_from_model: n_seq_max     = 1
0.00.352.461 I llama_init_from_model: n_ctx         = 2048
0.00.352.461 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.352.462 I llama_init_from_model: n_batch       = 2048
0.00.352.462 I llama_init_from_model: n_ubatch      = 512
0.00.352.462 I llama_init_from_model: flash_attn    = 0
0.00.352.464 I llama_init_from_model: freq_base     = 10000.0
0.00.352.464 I llama_init_from_model: freq_scale    = 1
0.00.352.467 I ggml_metal_init: allocating
0.00.352.514 I ggml_metal_init: found device: Apple M4
0.00.352.528 I ggml_metal_init: picking default device: Apple M4
0.00.354.213 I ggml_metal_init: using embedded metal library
0.00.359.961 I ggml_metal_init: GPU name:   Apple M4
0.00.359.972 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.973 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.974 I ggml_metal_init: simdgroup reduction   = true
0.00.359.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.974 I ggml_metal_init: has residency sets    = true
0.00.359.975 I ggml_metal_init: has bfloat            = true
0.00.359.975 I ggml_metal_init: use bfloat            = true
0.00.359.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.442 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.033 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.446.039 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.446.076 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.450.314 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.450.316 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.450.317 I llama_init_from_model: graph nodes  = 967
0.00.450.317 I llama_init_from_model: graph splits = 2
0.00.450.322 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.450.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.450.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.980 I main: llama threadpool init, n_threads = 4
0.00.514.032 I 
0.00.514.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.514.055 I 
0.00.514.241 I sampler seed: 1234
0.00.514.245 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.514.257 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.514.258 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.514.258 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.189.670 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.189.670 I llama_perf_context_print:        load time =     503.41 ms
0.01.189.671 I llama_perf_context_print: prompt eval time =      40.21 ms /     7 tokens (    5.74 ms per token,   174.09 tokens per second)
0.01.189.671 I llama_perf_context_print:        eval time =     632.35 ms /    63 runs   (   10.04 ms per token,    99.63 tokens per second)
0.01.189.672 I llama_perf_context_print:       total time =     676.40 ms /    70 tokens
0.01.189.923 I ggml_metal_free: deallocating

real	0m1.208s
user	0m0.112s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.107 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.858 I llama_model_loader: - type  f32:  194 tensors
0.00.024.858 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.858 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.859 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.859 I print_info: file format = GGUF V3 (latest)
0.00.024.860 I print_info: file type   = Q3_K - Medium
0.00.024.861 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.598 I load: special tokens cache size = 25
0.00.038.632 I load: token to piece cache size = 0.2984 MB
0.00.038.635 I print_info: arch             = gptneox
0.00.038.635 I print_info: vocab_only       = 0
0.00.038.636 I print_info: n_ctx_train      = 2048
0.00.038.636 I print_info: n_embd           = 2048
0.00.038.636 I print_info: n_layer          = 24
0.00.038.638 I print_info: n_head           = 16
0.00.038.639 I print_info: n_head_kv        = 16
0.00.038.639 I print_info: n_rot            = 32
0.00.038.639 I print_info: n_swa            = 0
0.00.038.639 I print_info: n_embd_head_k    = 128
0.00.038.640 I print_info: n_embd_head_v    = 128
0.00.038.640 I print_info: n_gqa            = 1
0.00.038.641 I print_info: n_embd_k_gqa     = 2048
0.00.038.642 I print_info: n_embd_v_gqa     = 2048
0.00.038.643 I print_info: f_norm_eps       = 1.0e-05
0.00.038.643 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.646 I print_info: f_logit_scale    = 0.0e+00
0.00.038.647 I print_info: n_ff             = 8192
0.00.038.647 I print_info: n_expert         = 0
0.00.038.647 I print_info: n_expert_used    = 0
0.00.038.649 I print_info: causal attn      = 1
0.00.038.650 I print_info: pooling type     = 0
0.00.038.650 I print_info: rope type        = 2
0.00.038.650 I print_info: rope scaling     = linear
0.00.038.651 I print_info: freq_base_train  = 10000.0
0.00.038.651 I print_info: freq_scale_train = 1
0.00.038.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.651 I print_info: rope_finetuned   = unknown
0.00.038.651 I print_info: ssm_d_conv       = 0
0.00.038.651 I print_info: ssm_d_inner      = 0
0.00.038.652 I print_info: ssm_d_state      = 0
0.00.038.652 I print_info: ssm_dt_rank      = 0
0.00.038.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.654 I print_info: model type       = 1.4B
0.00.038.654 I print_info: model params     = 1.41 B
0.00.038.654 I print_info: general.name     = 1.4B
0.00.038.655 I print_info: vocab type       = BPE
0.00.038.655 I print_info: n_vocab          = 50304
0.00.038.655 I print_info: n_merges         = 50009
0.00.038.655 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.656 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.657 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.657 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.657 I print_info: LF token         = 187 'Ċ'
0.00.038.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.658 I print_info: max token length = 1024
0.00.038.658 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.434.585 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.594 I load_tensors: offloading output layer to GPU
0.00.434.595 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.634 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.638 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.335 I llama_init_from_model: n_seq_max     = 1
0.00.436.339 I llama_init_from_model: n_ctx         = 2048
0.00.436.339 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.436.340 I llama_init_from_model: n_batch       = 2048
0.00.436.340 I llama_init_from_model: n_ubatch      = 512
0.00.436.341 I llama_init_from_model: flash_attn    = 0
0.00.436.343 I llama_init_from_model: freq_base     = 10000.0
0.00.436.344 I llama_init_from_model: freq_scale    = 1
0.00.436.359 I ggml_metal_init: allocating
0.00.436.424 I ggml_metal_init: found device: Apple M4
0.00.436.467 I ggml_metal_init: picking default device: Apple M4
0.00.438.341 I ggml_metal_init: using embedded metal library
0.00.443.881 I ggml_metal_init: GPU name:   Apple M4
0.00.443.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.889 I ggml_metal_init: simdgroup reduction   = true
0.00.443.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.889 I ggml_metal_init: has residency sets    = true
0.00.443.890 I ggml_metal_init: has bfloat            = true
0.00.443.890 I ggml_metal_init: use bfloat            = true
0.00.443.891 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.033 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.023 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.522.032 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.526.505 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.526.508 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.526.508 I llama_init_from_model: graph nodes  = 967
0.00.526.509 I llama_init_from_model: graph splits = 2
0.00.526.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.526.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.526.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.834 I main: llama threadpool init, n_threads = 4
0.00.577.875 I 
0.00.577.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.898 I 
0.00.578.029 I sampler seed: 1234
0.00.578.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.578.067 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.578.071 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.578.071 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.320.150 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46802.90 tokens per second)
0.01.320.151 I llama_perf_context_print:        load time =     568.40 ms
0.01.320.152 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.88 tokens per second)
0.01.320.153 I llama_perf_context_print:        eval time =     699.05 ms /    63 runs   (   11.10 ms per token,    90.12 tokens per second)
0.01.320.153 I llama_perf_context_print:       total time =     743.00 ms /    70 tokens
0.01.320.361 I ggml_metal_free: deallocating

real	0m1.335s
user	0m0.111s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.739 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.025.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.034.658 I llama_model_loader: - type  f32:  194 tensors
0.00.034.658 I llama_model_loader: - type q4_K:   61 tensors
0.00.034.658 I llama_model_loader: - type q5_K:   24 tensors
0.00.034.658 I llama_model_loader: - type q6_K:   13 tensors
0.00.034.659 I print_info: file format = GGUF V3 (latest)
0.00.034.659 I print_info: file type   = Q4_K - Medium
0.00.034.663 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.862 I load: special tokens cache size = 25
0.00.049.033 I load: token to piece cache size = 0.2984 MB
0.00.049.036 I print_info: arch             = gptneox
0.00.049.037 I print_info: vocab_only       = 0
0.00.049.037 I print_info: n_ctx_train      = 2048
0.00.049.037 I print_info: n_embd           = 2048
0.00.049.037 I print_info: n_layer          = 24
0.00.049.040 I print_info: n_head           = 16
0.00.049.041 I print_info: n_head_kv        = 16
0.00.049.041 I print_info: n_rot            = 32
0.00.049.041 I print_info: n_swa            = 0
0.00.049.042 I print_info: n_embd_head_k    = 128
0.00.049.042 I print_info: n_embd_head_v    = 128
0.00.049.045 I print_info: n_gqa            = 1
0.00.049.046 I print_info: n_embd_k_gqa     = 2048
0.00.049.046 I print_info: n_embd_v_gqa     = 2048
0.00.049.047 I print_info: f_norm_eps       = 1.0e-05
0.00.049.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.048 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.048 I print_info: f_logit_scale    = 0.0e+00
0.00.049.048 I print_info: n_ff             = 8192
0.00.049.048 I print_info: n_expert         = 0
0.00.049.049 I print_info: n_expert_used    = 0
0.00.049.049 I print_info: causal attn      = 1
0.00.049.049 I print_info: pooling type     = 0
0.00.049.049 I print_info: rope type        = 2
0.00.049.049 I print_info: rope scaling     = linear
0.00.049.050 I print_info: freq_base_train  = 10000.0
0.00.049.051 I print_info: freq_scale_train = 1
0.00.049.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.051 I print_info: rope_finetuned   = unknown
0.00.049.051 I print_info: ssm_d_conv       = 0
0.00.049.051 I print_info: ssm_d_inner      = 0
0.00.049.051 I print_info: ssm_d_state      = 0
0.00.049.052 I print_info: ssm_dt_rank      = 0
0.00.049.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.052 I print_info: model type       = 1.4B
0.00.049.053 I print_info: model params     = 1.41 B
0.00.049.053 I print_info: general.name     = 1.4B
0.00.049.053 I print_info: vocab type       = BPE
0.00.049.053 I print_info: n_vocab          = 50304
0.00.049.054 I print_info: n_merges         = 50009
0.00.049.054 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.054 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.055 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.055 I print_info: LF token         = 187 'Ċ'
0.00.049.056 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.056 I print_info: max token length = 1024
0.00.049.056 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.062.382 I load_tensors: offloading 24 repeating layers to GPU
0.01.062.389 I load_tensors: offloading output layer to GPU
0.01.062.390 I load_tensors: offloaded 25/25 layers to GPU
0.01.062.407 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.01.062.408 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.01.063.305 I llama_init_from_model: n_seq_max     = 1
0.01.063.309 I llama_init_from_model: n_ctx         = 2048
0.01.063.309 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.063.309 I llama_init_from_model: n_batch       = 2048
0.01.063.310 I llama_init_from_model: n_ubatch      = 512
0.01.063.310 I llama_init_from_model: flash_attn    = 0
0.01.063.312 I llama_init_from_model: freq_base     = 10000.0
0.01.063.312 I llama_init_from_model: freq_scale    = 1
0.01.063.313 I ggml_metal_init: allocating
0.01.063.352 I ggml_metal_init: found device: Apple M4
0.01.063.365 I ggml_metal_init: picking default device: Apple M4
0.01.064.423 I ggml_metal_init: using embedded metal library
0.01.075.873 I ggml_metal_init: GPU name:   Apple M4
0.01.075.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.075.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.075.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.075.881 I ggml_metal_init: simdgroup reduction   = true
0.01.075.881 I ggml_metal_init: simdgroup matrix mul. = true
0.01.075.882 I ggml_metal_init: has residency sets    = true
0.01.075.882 I ggml_metal_init: has bfloat            = true
0.01.075.882 I ggml_metal_init: use bfloat            = true
0.01.075.884 I ggml_metal_init: hasUnifiedMemory      = true
0.01.075.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.095.290 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.125.937 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.125.944 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.125.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.130.549 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.130.552 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.130.552 I llama_init_from_model: graph nodes  = 967
0.01.130.552 I llama_init_from_model: graph splits = 2
0.01.130.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.130.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.130.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.186.175 I main: llama threadpool init, n_threads = 4
0.01.186.218 I 
0.01.186.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.186.240 I 
0.01.186.396 I sampler seed: 1234
0.01.186.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.186.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.186.437 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.186.437 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.940.171 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.01.940.172 I llama_perf_context_print:        load time =    1176.75 ms
0.01.940.172 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.44 tokens per second)
0.01.940.173 I llama_perf_context_print:        eval time =     703.61 ms /    63 runs   (   11.17 ms per token,    89.54 tokens per second)
0.01.940.173 I llama_perf_context_print:       total time =     754.68 ms /    70 tokens
0.01.940.402 I ggml_metal_free: deallocating

real	0m1.964s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.512 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.983 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.984 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.660 I llama_model_loader: - type  f32:  194 tensors
0.00.025.660 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.661 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.661 I print_info: file format = GGUF V3 (latest)
0.00.025.661 I print_info: file type   = Q5_K - Medium
0.00.025.662 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.363 I load: special tokens cache size = 25
0.00.039.423 I load: token to piece cache size = 0.2984 MB
0.00.039.426 I print_info: arch             = gptneox
0.00.039.426 I print_info: vocab_only       = 0
0.00.039.426 I print_info: n_ctx_train      = 2048
0.00.039.426 I print_info: n_embd           = 2048
0.00.039.427 I print_info: n_layer          = 24
0.00.039.429 I print_info: n_head           = 16
0.00.039.430 I print_info: n_head_kv        = 16
0.00.039.430 I print_info: n_rot            = 32
0.00.039.430 I print_info: n_swa            = 0
0.00.039.433 I print_info: n_embd_head_k    = 128
0.00.039.433 I print_info: n_embd_head_v    = 128
0.00.039.433 I print_info: n_gqa            = 1
0.00.039.434 I print_info: n_embd_k_gqa     = 2048
0.00.039.435 I print_info: n_embd_v_gqa     = 2048
0.00.039.436 I print_info: f_norm_eps       = 1.0e-05
0.00.039.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.437 I print_info: f_logit_scale    = 0.0e+00
0.00.039.437 I print_info: n_ff             = 8192
0.00.039.438 I print_info: n_expert         = 0
0.00.039.438 I print_info: n_expert_used    = 0
0.00.039.438 I print_info: causal attn      = 1
0.00.039.438 I print_info: pooling type     = 0
0.00.039.438 I print_info: rope type        = 2
0.00.039.438 I print_info: rope scaling     = linear
0.00.039.439 I print_info: freq_base_train  = 10000.0
0.00.039.439 I print_info: freq_scale_train = 1
0.00.039.439 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.439 I print_info: rope_finetuned   = unknown
0.00.039.439 I print_info: ssm_d_conv       = 0
0.00.039.440 I print_info: ssm_d_inner      = 0
0.00.039.440 I print_info: ssm_d_state      = 0
0.00.039.440 I print_info: ssm_dt_rank      = 0
0.00.039.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.441 I print_info: model type       = 1.4B
0.00.039.441 I print_info: model params     = 1.41 B
0.00.039.441 I print_info: general.name     = 1.4B
0.00.039.442 I print_info: vocab type       = BPE
0.00.039.443 I print_info: n_vocab          = 50304
0.00.039.443 I print_info: n_merges         = 50009
0.00.039.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: LF token         = 187 'Ċ'
0.00.039.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.445 I print_info: max token length = 1024
0.00.039.445 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.185 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.194 I load_tensors: offloading output layer to GPU
0.00.653.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.216 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.653.217 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.654.209 I llama_init_from_model: n_seq_max     = 1
0.00.654.212 I llama_init_from_model: n_ctx         = 2048
0.00.654.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.213 I llama_init_from_model: n_batch       = 2048
0.00.654.213 I llama_init_from_model: n_ubatch      = 512
0.00.654.214 I llama_init_from_model: flash_attn    = 0
0.00.654.215 I llama_init_from_model: freq_base     = 10000.0
0.00.654.215 I llama_init_from_model: freq_scale    = 1
0.00.654.217 I ggml_metal_init: allocating
0.00.654.276 I ggml_metal_init: found device: Apple M4
0.00.654.288 I ggml_metal_init: picking default device: Apple M4
0.00.655.366 I ggml_metal_init: using embedded metal library
0.00.659.204 I ggml_metal_init: GPU name:   Apple M4
0.00.659.207 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.208 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.209 I ggml_metal_init: simdgroup reduction   = true
0.00.659.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.209 I ggml_metal_init: has residency sets    = true
0.00.659.209 I ggml_metal_init: has bfloat            = true
0.00.659.209 I ggml_metal_init: use bfloat            = true
0.00.659.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.601 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.107 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.700.115 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.457 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.459 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.460 I llama_init_from_model: graph nodes  = 967
0.00.704.460 I llama_init_from_model: graph splits = 2
0.00.704.466 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.947 I main: llama threadpool init, n_threads = 4
0.00.764.995 I 
0.00.765.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.018 I 
0.00.765.186 I sampler seed: 1234
0.00.765.191 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.244 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.248 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.248 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.605.943 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.605.943 I llama_perf_context_print:        load time =     754.74 ms
0.01.605.944 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.92 tokens per second)
0.01.605.945 I llama_perf_context_print:        eval time =     786.18 ms /    63 runs   (   12.48 ms per token,    80.13 tokens per second)
0.01.605.945 I llama_perf_context_print:       total time =     841.69 ms /    70 tokens
0.01.606.170 I ggml_metal_free: deallocating

real	0m1.623s
user	0m0.099s
sys	0m0.221s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.770 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.579 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.234 I llama_model_loader: - type  f32:  194 tensors
0.00.025.234 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.234 I print_info: file format = GGUF V3 (latest)
0.00.025.235 I print_info: file type   = Q6_K
0.00.025.236 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.947 I load: special tokens cache size = 25
0.00.039.107 I load: token to piece cache size = 0.2984 MB
0.00.039.110 I print_info: arch             = gptneox
0.00.039.110 I print_info: vocab_only       = 0
0.00.039.110 I print_info: n_ctx_train      = 2048
0.00.039.110 I print_info: n_embd           = 2048
0.00.039.111 I print_info: n_layer          = 24
0.00.039.113 I print_info: n_head           = 16
0.00.039.114 I print_info: n_head_kv        = 16
0.00.039.114 I print_info: n_rot            = 32
0.00.039.114 I print_info: n_swa            = 0
0.00.039.115 I print_info: n_embd_head_k    = 128
0.00.039.115 I print_info: n_embd_head_v    = 128
0.00.039.115 I print_info: n_gqa            = 1
0.00.039.116 I print_info: n_embd_k_gqa     = 2048
0.00.039.117 I print_info: n_embd_v_gqa     = 2048
0.00.039.117 I print_info: f_norm_eps       = 1.0e-05
0.00.039.118 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.118 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.118 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.118 I print_info: f_logit_scale    = 0.0e+00
0.00.039.121 I print_info: n_ff             = 8192
0.00.039.121 I print_info: n_expert         = 0
0.00.039.121 I print_info: n_expert_used    = 0
0.00.039.121 I print_info: causal attn      = 1
0.00.039.122 I print_info: pooling type     = 0
0.00.039.122 I print_info: rope type        = 2
0.00.039.122 I print_info: rope scaling     = linear
0.00.039.122 I print_info: freq_base_train  = 10000.0
0.00.039.123 I print_info: freq_scale_train = 1
0.00.039.123 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.123 I print_info: rope_finetuned   = unknown
0.00.039.124 I print_info: ssm_d_conv       = 0
0.00.039.125 I print_info: ssm_d_inner      = 0
0.00.039.126 I print_info: ssm_d_state      = 0
0.00.039.126 I print_info: ssm_dt_rank      = 0
0.00.039.126 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.126 I print_info: model type       = 1.4B
0.00.039.126 I print_info: model params     = 1.41 B
0.00.039.126 I print_info: general.name     = 1.4B
0.00.039.127 I print_info: vocab type       = BPE
0.00.039.127 I print_info: n_vocab          = 50304
0.00.039.127 I print_info: n_merges         = 50009
0.00.039.128 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: LF token         = 187 'Ċ'
0.00.039.129 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: max token length = 1024
0.00.039.129 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.275 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.281 I load_tensors: offloading output layer to GPU
0.00.668.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.308 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.668.311 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.669.595 I llama_init_from_model: n_seq_max     = 1
0.00.669.597 I llama_init_from_model: n_ctx         = 2048
0.00.669.598 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.669.598 I llama_init_from_model: n_batch       = 2048
0.00.669.598 I llama_init_from_model: n_ubatch      = 512
0.00.669.599 I llama_init_from_model: flash_attn    = 0
0.00.669.600 I llama_init_from_model: freq_base     = 10000.0
0.00.669.600 I llama_init_from_model: freq_scale    = 1
0.00.669.602 I ggml_metal_init: allocating
0.00.669.618 I ggml_metal_init: found device: Apple M4
0.00.669.628 I ggml_metal_init: picking default device: Apple M4
0.00.671.084 I ggml_metal_init: using embedded metal library
0.00.677.161 I ggml_metal_init: GPU name:   Apple M4
0.00.677.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.166 I ggml_metal_init: simdgroup reduction   = true
0.00.677.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.166 I ggml_metal_init: has residency sets    = true
0.00.677.166 I ggml_metal_init: has bfloat            = true
0.00.677.167 I ggml_metal_init: use bfloat            = true
0.00.677.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.602 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.748.002 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.748.010 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.748.036 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.752.565 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.752.567 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.752.567 I llama_init_from_model: graph nodes  = 967
0.00.752.567 I llama_init_from_model: graph splits = 2
0.00.752.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.752.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.271 I main: llama threadpool init, n_threads = 4
0.00.822.316 I 
0.00.822.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.341 I 
0.00.822.492 I sampler seed: 1234
0.00.822.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.522 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.523 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.692.083 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.692.084 I llama_perf_context_print:        load time =     812.74 ms
0.01.692.085 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.69 tokens per second)
0.01.692.086 I llama_perf_context_print:        eval time =     812.25 ms /    63 runs   (   12.89 ms per token,    77.56 tokens per second)
0.01.692.086 I llama_perf_context_print:       total time =     870.50 ms /    70 tokens
0.01.692.334 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.108s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.687 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.967 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.080 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.088 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.156 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.158 I llama_model_loader: - type  f32:  194 tensors
0.00.057.158 I llama_model_loader: - type  f16:   98 tensors
0.00.057.159 I print_info: file format = GGUF V3 (latest)
0.00.057.160 I print_info: file type   = all F32 (guessed)
0.00.057.161 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.464 I load: special tokens cache size = 25
0.00.077.504 I load: token to piece cache size = 0.2984 MB
0.00.077.508 I print_info: arch             = gptneox
0.00.077.508 I print_info: vocab_only       = 0
0.00.077.508 I print_info: n_ctx_train      = 2048
0.00.077.508 I print_info: n_embd           = 2048
0.00.077.509 I print_info: n_layer          = 24
0.00.077.512 I print_info: n_head           = 16
0.00.077.513 I print_info: n_head_kv        = 16
0.00.077.513 I print_info: n_rot            = 32
0.00.077.513 I print_info: n_swa            = 0
0.00.077.513 I print_info: n_embd_head_k    = 128
0.00.077.513 I print_info: n_embd_head_v    = 128
0.00.077.514 I print_info: n_gqa            = 1
0.00.077.515 I print_info: n_embd_k_gqa     = 2048
0.00.077.518 I print_info: n_embd_v_gqa     = 2048
0.00.077.518 I print_info: f_norm_eps       = 1.0e-05
0.00.077.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.519 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.519 I print_info: f_logit_scale    = 0.0e+00
0.00.077.520 I print_info: n_ff             = 8192
0.00.077.520 I print_info: n_expert         = 0
0.00.077.520 I print_info: n_expert_used    = 0
0.00.077.521 I print_info: causal attn      = 1
0.00.077.523 I print_info: pooling type     = 0
0.00.077.523 I print_info: rope type        = 2
0.00.077.523 I print_info: rope scaling     = linear
0.00.077.524 I print_info: freq_base_train  = 10000.0
0.00.077.524 I print_info: freq_scale_train = 1
0.00.077.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.524 I print_info: rope_finetuned   = unknown
0.00.077.525 I print_info: ssm_d_conv       = 0
0.00.077.525 I print_info: ssm_d_inner      = 0
0.00.077.525 I print_info: ssm_d_state      = 0
0.00.077.525 I print_info: ssm_dt_rank      = 0
0.00.077.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.525 I print_info: model type       = 1.4B
0.00.077.526 I print_info: model params     = 1.41 B
0.00.077.526 I print_info: general.name     = 1.4B
0.00.077.526 I print_info: vocab type       = BPE
0.00.077.527 I print_info: n_vocab          = 50304
0.00.077.527 I print_info: n_merges         = 50009
0.00.077.527 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.528 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.528 I print_info: LF token         = 187 'Ċ'
0.00.077.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.528 I print_info: max token length = 1024
0.00.077.529 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.328.250 I load_tensors: offloading 24 repeating layers to GPU
0.01.328.259 I load_tensors: offloading output layer to GPU
0.01.328.260 I load_tensors: offloaded 25/25 layers to GPU
0.01.328.286 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.328.288 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.328.872 I llama_init_from_model: n_seq_max     = 1
0.01.328.874 I llama_init_from_model: n_ctx         = 128
0.01.328.874 I llama_init_from_model: n_ctx_per_seq = 128
0.01.328.874 I llama_init_from_model: n_batch       = 128
0.01.328.874 I llama_init_from_model: n_ubatch      = 128
0.01.328.875 I llama_init_from_model: flash_attn    = 0
0.01.328.875 I llama_init_from_model: freq_base     = 10000.0
0.01.328.875 I llama_init_from_model: freq_scale    = 1
0.01.328.876 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.328.877 I ggml_metal_init: allocating
0.01.328.929 I ggml_metal_init: found device: Apple M4
0.01.328.939 I ggml_metal_init: picking default device: Apple M4
0.01.330.025 I ggml_metal_init: using embedded metal library
0.01.333.878 I ggml_metal_init: GPU name:   Apple M4
0.01.333.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.333.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.333.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.333.881 I ggml_metal_init: simdgroup reduction   = true
0.01.333.881 I ggml_metal_init: simdgroup matrix mul. = true
0.01.333.882 I ggml_metal_init: has residency sets    = true
0.01.333.882 I ggml_metal_init: has bfloat            = true
0.01.333.882 I ggml_metal_init: use bfloat            = true
0.01.333.883 I ggml_metal_init: hasUnifiedMemory      = true
0.01.333.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.344.340 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.346.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.346.052 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.346.066 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.347.676 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.347.677 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.347.678 I llama_init_from_model: graph nodes  = 967
0.01.347.678 I llama_init_from_model: graph splits = 2
0.01.347.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.347.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.382.660 I 
0.01.382.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.382.721 I perplexity: tokenizing the input ..
0.01.387.562 I perplexity: tokenization took 4.839 ms
0.01.387.581 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.505.978 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.507.313 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.507.326 I llama_perf_context_print:        load time =    1357.68 ms
0.01.507.327 I llama_perf_context_print: prompt eval time =     118.13 ms /   128 tokens (    0.92 ms per token,  1083.52 tokens per second)
0.01.507.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.507.328 I llama_perf_context_print:       total time =     124.67 ms /   129 tokens
0.01.507.712 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.097s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.275 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.379 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.380 I print_info: file format = GGUF V3 (latest)
0.00.025.381 I print_info: file type   = Q8_0
0.00.025.382 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.442 I load: special tokens cache size = 25
0.00.039.463 I load: token to piece cache size = 0.2984 MB
0.00.039.470 I print_info: arch             = gptneox
0.00.039.471 I print_info: vocab_only       = 0
0.00.039.471 I print_info: n_ctx_train      = 2048
0.00.039.476 I print_info: n_embd           = 2048
0.00.039.477 I print_info: n_layer          = 24
0.00.039.481 I print_info: n_head           = 16
0.00.039.482 I print_info: n_head_kv        = 16
0.00.039.482 I print_info: n_rot            = 32
0.00.039.482 I print_info: n_swa            = 0
0.00.039.482 I print_info: n_embd_head_k    = 128
0.00.039.482 I print_info: n_embd_head_v    = 128
0.00.039.483 I print_info: n_gqa            = 1
0.00.039.483 I print_info: n_embd_k_gqa     = 2048
0.00.039.484 I print_info: n_embd_v_gqa     = 2048
0.00.039.485 I print_info: f_norm_eps       = 1.0e-05
0.00.039.485 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.485 I print_info: f_logit_scale    = 0.0e+00
0.00.039.486 I print_info: n_ff             = 8192
0.00.039.486 I print_info: n_expert         = 0
0.00.039.486 I print_info: n_expert_used    = 0
0.00.039.486 I print_info: causal attn      = 1
0.00.039.486 I print_info: pooling type     = 0
0.00.039.487 I print_info: rope type        = 2
0.00.039.487 I print_info: rope scaling     = linear
0.00.039.488 I print_info: freq_base_train  = 10000.0
0.00.039.489 I print_info: freq_scale_train = 1
0.00.039.489 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.489 I print_info: rope_finetuned   = unknown
0.00.039.490 I print_info: ssm_d_conv       = 0
0.00.039.490 I print_info: ssm_d_inner      = 0
0.00.039.491 I print_info: ssm_d_state      = 0
0.00.039.491 I print_info: ssm_dt_rank      = 0
0.00.039.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.491 I print_info: model type       = 1.4B
0.00.039.491 I print_info: model params     = 1.41 B
0.00.039.492 I print_info: general.name     = 1.4B
0.00.039.492 I print_info: vocab type       = BPE
0.00.039.492 I print_info: n_vocab          = 50304
0.00.039.492 I print_info: n_merges         = 50009
0.00.039.493 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.494 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.494 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.494 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.494 I print_info: LF token         = 187 'Ċ'
0.00.039.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.496 I print_info: max token length = 1024
0.00.039.496 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.878.352 I load_tensors: offloading 24 repeating layers to GPU
0.00.878.357 I load_tensors: offloading output layer to GPU
0.00.878.358 I load_tensors: offloaded 25/25 layers to GPU
0.00.878.390 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.878.393 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.879.858 I llama_init_from_model: n_seq_max     = 1
0.00.879.859 I llama_init_from_model: n_ctx         = 128
0.00.879.860 I llama_init_from_model: n_ctx_per_seq = 128
0.00.879.860 I llama_init_from_model: n_batch       = 128
0.00.879.860 I llama_init_from_model: n_ubatch      = 128
0.00.879.861 I llama_init_from_model: flash_attn    = 0
0.00.879.861 I llama_init_from_model: freq_base     = 10000.0
0.00.879.862 I llama_init_from_model: freq_scale    = 1
0.00.879.862 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.879.863 I ggml_metal_init: allocating
0.00.879.941 I ggml_metal_init: found device: Apple M4
0.00.879.952 I ggml_metal_init: picking default device: Apple M4
0.00.881.227 I ggml_metal_init: using embedded metal library
0.00.886.433 I ggml_metal_init: GPU name:   Apple M4
0.00.886.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.886.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.886.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.886.438 I ggml_metal_init: simdgroup reduction   = true
0.00.886.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.886.439 I ggml_metal_init: has residency sets    = true
0.00.886.439 I ggml_metal_init: has bfloat            = true
0.00.886.439 I ggml_metal_init: use bfloat            = true
0.00.886.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.886.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.901.959 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.905.328 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.905.331 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.905.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.908.426 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.908.428 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.908.429 I llama_init_from_model: graph nodes  = 967
0.00.908.429 I llama_init_from_model: graph splits = 2
0.00.908.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.908.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.932.869 I 
0.00.932.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.932.961 I perplexity: tokenizing the input ..
0.00.940.202 I perplexity: tokenization took 7.237 ms
0.00.940.228 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.065.609 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.067.023 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.067.042 I llama_perf_context_print:        load time =     923.59 ms
0.01.067.043 I llama_perf_context_print: prompt eval time =     124.48 ms /   128 tokens (    0.97 ms per token,  1028.31 tokens per second)
0.01.067.044 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.067.047 I llama_perf_context_print:       total time =     134.18 ms /   129 tokens
0.01.067.397 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.076s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.640 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.125 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.137 I llama_model_loader: - type  f32:  194 tensors
0.00.026.137 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.138 I print_info: file format = GGUF V3 (latest)
0.00.026.138 I print_info: file type   = Q4_0
0.00.026.140 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.587 I load: special tokens cache size = 25
0.00.040.803 I load: token to piece cache size = 0.2984 MB
0.00.040.809 I print_info: arch             = gptneox
0.00.040.809 I print_info: vocab_only       = 0
0.00.040.810 I print_info: n_ctx_train      = 2048
0.00.040.812 I print_info: n_embd           = 2048
0.00.040.812 I print_info: n_layer          = 24
0.00.040.817 I print_info: n_head           = 16
0.00.040.817 I print_info: n_head_kv        = 16
0.00.040.818 I print_info: n_rot            = 32
0.00.040.818 I print_info: n_swa            = 0
0.00.040.818 I print_info: n_embd_head_k    = 128
0.00.040.818 I print_info: n_embd_head_v    = 128
0.00.040.819 I print_info: n_gqa            = 1
0.00.040.819 I print_info: n_embd_k_gqa     = 2048
0.00.040.820 I print_info: n_embd_v_gqa     = 2048
0.00.040.820 I print_info: f_norm_eps       = 1.0e-05
0.00.040.821 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.821 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.821 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.822 I print_info: f_logit_scale    = 0.0e+00
0.00.040.824 I print_info: n_ff             = 8192
0.00.040.825 I print_info: n_expert         = 0
0.00.040.826 I print_info: n_expert_used    = 0
0.00.040.826 I print_info: causal attn      = 1
0.00.040.826 I print_info: pooling type     = 0
0.00.040.826 I print_info: rope type        = 2
0.00.040.826 I print_info: rope scaling     = linear
0.00.040.826 I print_info: freq_base_train  = 10000.0
0.00.040.827 I print_info: freq_scale_train = 1
0.00.040.827 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.827 I print_info: rope_finetuned   = unknown
0.00.040.827 I print_info: ssm_d_conv       = 0
0.00.040.827 I print_info: ssm_d_inner      = 0
0.00.040.827 I print_info: ssm_d_state      = 0
0.00.040.828 I print_info: ssm_dt_rank      = 0
0.00.040.828 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.828 I print_info: model type       = 1.4B
0.00.040.828 I print_info: model params     = 1.41 B
0.00.040.828 I print_info: general.name     = 1.4B
0.00.040.829 I print_info: vocab type       = BPE
0.00.040.829 I print_info: n_vocab          = 50304
0.00.040.829 I print_info: n_merges         = 50009
0.00.040.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.830 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.834 I print_info: LF token         = 187 'Ċ'
0.00.040.835 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: max token length = 1024
0.00.040.836 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.821 I load_tensors: offloading output layer to GPU
0.00.637.822 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.869 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.637.872 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.638.989 I llama_init_from_model: n_seq_max     = 1
0.00.638.991 I llama_init_from_model: n_ctx         = 128
0.00.638.992 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.992 I llama_init_from_model: n_batch       = 128
0.00.638.993 I llama_init_from_model: n_ubatch      = 128
0.00.638.993 I llama_init_from_model: flash_attn    = 0
0.00.638.995 I llama_init_from_model: freq_base     = 10000.0
0.00.638.995 I llama_init_from_model: freq_scale    = 1
0.00.638.996 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.998 I ggml_metal_init: allocating
0.00.639.087 I ggml_metal_init: found device: Apple M4
0.00.639.138 I ggml_metal_init: picking default device: Apple M4
0.00.641.026 I ggml_metal_init: using embedded metal library
0.00.646.483 I ggml_metal_init: GPU name:   Apple M4
0.00.646.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.493 I ggml_metal_init: simdgroup reduction   = true
0.00.646.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.494 I ggml_metal_init: has residency sets    = true
0.00.646.494 I ggml_metal_init: has bfloat            = true
0.00.646.494 I ggml_metal_init: use bfloat            = true
0.00.646.496 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.706 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.392 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.670.396 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.670.427 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.932 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.673.934 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.673.935 I llama_init_from_model: graph nodes  = 967
0.00.673.935 I llama_init_from_model: graph splits = 2
0.00.673.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.756 I 
0.00.704.837 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.857 I perplexity: tokenizing the input ..
0.00.712.054 I perplexity: tokenization took 7.195 ms
0.00.712.079 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.882 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.850.228 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.850.244 I llama_perf_context_print:        load time =     695.11 ms
0.00.850.245 I llama_perf_context_print: prompt eval time =     135.91 ms /   128 tokens (    1.06 ms per token,   941.77 tokens per second)
0.00.850.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.246 I llama_perf_context_print:       total time =     145.49 ms /   129 tokens
0.00.850.597 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.081s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.917 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.709 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.712 I llama_model_loader: - type  f32:  194 tensors
0.00.024.713 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.713 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.714 I print_info: file format = GGUF V3 (latest)
0.00.024.718 I print_info: file type   = Q4_1
0.00.024.719 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.599 I load: special tokens cache size = 25
0.00.038.675 I load: token to piece cache size = 0.2984 MB
0.00.038.680 I print_info: arch             = gptneox
0.00.038.680 I print_info: vocab_only       = 0
0.00.038.681 I print_info: n_ctx_train      = 2048
0.00.038.681 I print_info: n_embd           = 2048
0.00.038.681 I print_info: n_layer          = 24
0.00.038.685 I print_info: n_head           = 16
0.00.038.686 I print_info: n_head_kv        = 16
0.00.038.686 I print_info: n_rot            = 32
0.00.038.689 I print_info: n_swa            = 0
0.00.038.689 I print_info: n_embd_head_k    = 128
0.00.038.689 I print_info: n_embd_head_v    = 128
0.00.038.690 I print_info: n_gqa            = 1
0.00.038.691 I print_info: n_embd_k_gqa     = 2048
0.00.038.692 I print_info: n_embd_v_gqa     = 2048
0.00.038.692 I print_info: f_norm_eps       = 1.0e-05
0.00.038.709 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.709 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.710 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.710 I print_info: f_logit_scale    = 0.0e+00
0.00.038.720 I print_info: n_ff             = 8192
0.00.038.720 I print_info: n_expert         = 0
0.00.038.720 I print_info: n_expert_used    = 0
0.00.038.720 I print_info: causal attn      = 1
0.00.038.721 I print_info: pooling type     = 0
0.00.038.721 I print_info: rope type        = 2
0.00.038.721 I print_info: rope scaling     = linear
0.00.038.721 I print_info: freq_base_train  = 10000.0
0.00.038.721 I print_info: freq_scale_train = 1
0.00.038.722 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.722 I print_info: rope_finetuned   = unknown
0.00.038.722 I print_info: ssm_d_conv       = 0
0.00.038.722 I print_info: ssm_d_inner      = 0
0.00.038.722 I print_info: ssm_d_state      = 0
0.00.038.722 I print_info: ssm_dt_rank      = 0
0.00.038.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.723 I print_info: model type       = 1.4B
0.00.038.725 I print_info: model params     = 1.41 B
0.00.038.725 I print_info: general.name     = 1.4B
0.00.038.725 I print_info: vocab type       = BPE
0.00.038.726 I print_info: n_vocab          = 50304
0.00.038.726 I print_info: n_merges         = 50009
0.00.038.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.727 I print_info: LF token         = 187 'Ċ'
0.00.038.727 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.727 I print_info: max token length = 1024
0.00.038.728 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.496 I load_tensors: offloading output layer to GPU
0.00.630.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.533 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.630.534 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.631.915 I llama_init_from_model: n_seq_max     = 1
0.00.631.917 I llama_init_from_model: n_ctx         = 128
0.00.631.918 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.918 I llama_init_from_model: n_batch       = 128
0.00.631.919 I llama_init_from_model: n_ubatch      = 128
0.00.631.919 I llama_init_from_model: flash_attn    = 0
0.00.631.921 I llama_init_from_model: freq_base     = 10000.0
0.00.631.921 I llama_init_from_model: freq_scale    = 1
0.00.631.922 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.925 I ggml_metal_init: allocating
0.00.632.068 I ggml_metal_init: found device: Apple M4
0.00.632.083 I ggml_metal_init: picking default device: Apple M4
0.00.634.227 I ggml_metal_init: using embedded metal library
0.00.640.881 I ggml_metal_init: GPU name:   Apple M4
0.00.640.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.891 I ggml_metal_init: simdgroup reduction   = true
0.00.640.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.891 I ggml_metal_init: has residency sets    = true
0.00.640.892 I ggml_metal_init: has bfloat            = true
0.00.640.892 I ggml_metal_init: use bfloat            = true
0.00.640.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.810 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.339 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.348 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.380 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.710 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.712 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.712 I llama_init_from_model: graph nodes  = 967
0.00.665.713 I llama_init_from_model: graph splits = 2
0.00.665.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.345 I 
0.00.693.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.454 I perplexity: tokenizing the input ..
0.00.700.989 I perplexity: tokenization took 7.532 ms
0.00.701.010 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.164 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.838.514 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.838.529 I llama_perf_context_print:        load time =     684.44 ms
0.00.838.530 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.14 tokens per second)
0.00.838.530 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.531 I llama_perf_context_print:       total time =     145.19 ms /   129 tokens
0.00.838.879 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.611 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.617 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.381 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.112 I llama_model_loader: - type  f32:  194 tensors
0.00.025.112 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.113 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.114 I print_info: file format = GGUF V3 (latest)
0.00.025.114 I print_info: file type   = Q5_0
0.00.025.115 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.045 I load: special tokens cache size = 25
0.00.039.106 I load: token to piece cache size = 0.2984 MB
0.00.039.111 I print_info: arch             = gptneox
0.00.039.111 I print_info: vocab_only       = 0
0.00.039.111 I print_info: n_ctx_train      = 2048
0.00.039.112 I print_info: n_embd           = 2048
0.00.039.112 I print_info: n_layer          = 24
0.00.039.116 I print_info: n_head           = 16
0.00.039.117 I print_info: n_head_kv        = 16
0.00.039.117 I print_info: n_rot            = 32
0.00.039.120 I print_info: n_swa            = 0
0.00.039.120 I print_info: n_embd_head_k    = 128
0.00.039.120 I print_info: n_embd_head_v    = 128
0.00.039.121 I print_info: n_gqa            = 1
0.00.039.122 I print_info: n_embd_k_gqa     = 2048
0.00.039.122 I print_info: n_embd_v_gqa     = 2048
0.00.039.123 I print_info: f_norm_eps       = 1.0e-05
0.00.039.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.123 I print_info: f_logit_scale    = 0.0e+00
0.00.039.124 I print_info: n_ff             = 8192
0.00.039.124 I print_info: n_expert         = 0
0.00.039.124 I print_info: n_expert_used    = 0
0.00.039.124 I print_info: causal attn      = 1
0.00.039.125 I print_info: pooling type     = 0
0.00.039.125 I print_info: rope type        = 2
0.00.039.125 I print_info: rope scaling     = linear
0.00.039.125 I print_info: freq_base_train  = 10000.0
0.00.039.126 I print_info: freq_scale_train = 1
0.00.039.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.126 I print_info: rope_finetuned   = unknown
0.00.039.126 I print_info: ssm_d_conv       = 0
0.00.039.126 I print_info: ssm_d_inner      = 0
0.00.039.126 I print_info: ssm_d_state      = 0
0.00.039.126 I print_info: ssm_dt_rank      = 0
0.00.039.126 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.128 I print_info: model type       = 1.4B
0.00.039.128 I print_info: model params     = 1.41 B
0.00.039.128 I print_info: general.name     = 1.4B
0.00.039.128 I print_info: vocab type       = BPE
0.00.039.129 I print_info: n_vocab          = 50304
0.00.039.129 I print_info: n_merges         = 50009
0.00.039.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: LF token         = 187 'Ċ'
0.00.039.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: max token length = 1024
0.00.039.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.719.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.225 I load_tensors: offloading output layer to GPU
0.00.719.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.257 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.719.259 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.720.914 I llama_init_from_model: n_seq_max     = 1
0.00.720.920 I llama_init_from_model: n_ctx         = 128
0.00.720.920 I llama_init_from_model: n_ctx_per_seq = 128
0.00.720.921 I llama_init_from_model: n_batch       = 128
0.00.720.921 I llama_init_from_model: n_ubatch      = 128
0.00.720.921 I llama_init_from_model: flash_attn    = 0
0.00.720.923 I llama_init_from_model: freq_base     = 10000.0
0.00.720.923 I llama_init_from_model: freq_scale    = 1
0.00.720.924 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.720.929 I ggml_metal_init: allocating
0.00.720.988 I ggml_metal_init: found device: Apple M4
0.00.721.001 I ggml_metal_init: picking default device: Apple M4
0.00.722.690 I ggml_metal_init: using embedded metal library
0.00.729.247 I ggml_metal_init: GPU name:   Apple M4
0.00.729.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.254 I ggml_metal_init: simdgroup reduction   = true
0.00.729.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.254 I ggml_metal_init: has residency sets    = true
0.00.729.254 I ggml_metal_init: has bfloat            = true
0.00.729.255 I ggml_metal_init: use bfloat            = true
0.00.729.256 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.746.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.424 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.750.431 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.750.466 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.753.961 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.753.962 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.753.963 I llama_init_from_model: graph nodes  = 967
0.00.753.963 I llama_init_from_model: graph splits = 2
0.00.753.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.753.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.650 I 
0.00.781.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.742 I perplexity: tokenizing the input ..
0.00.788.703 I perplexity: tokenization took 6.959 ms
0.00.788.717 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.923.711 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.925.041 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.925.066 I llama_perf_context_print:        load time =     772.75 ms
0.00.925.067 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.23 tokens per second)
0.00.925.068 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.925.068 I llama_perf_context_print:       total time =     143.42 ms /   129 tokens
0.00.925.442 I ggml_metal_free: deallocating

real	0m0.939s
user	0m0.078s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.175 I llama_model_loader: - type  f32:  194 tensors
0.00.026.176 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.177 I print_info: file format = GGUF V3 (latest)
0.00.026.177 I print_info: file type   = Q5_1
0.00.026.178 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.439 I load: special tokens cache size = 25
0.00.040.263 I load: token to piece cache size = 0.2984 MB
0.00.040.267 I print_info: arch             = gptneox
0.00.040.267 I print_info: vocab_only       = 0
0.00.040.267 I print_info: n_ctx_train      = 2048
0.00.040.267 I print_info: n_embd           = 2048
0.00.040.267 I print_info: n_layer          = 24
0.00.040.272 I print_info: n_head           = 16
0.00.040.272 I print_info: n_head_kv        = 16
0.00.040.272 I print_info: n_rot            = 32
0.00.040.273 I print_info: n_swa            = 0
0.00.040.274 I print_info: n_embd_head_k    = 128
0.00.040.274 I print_info: n_embd_head_v    = 128
0.00.040.275 I print_info: n_gqa            = 1
0.00.040.276 I print_info: n_embd_k_gqa     = 2048
0.00.040.276 I print_info: n_embd_v_gqa     = 2048
0.00.040.277 I print_info: f_norm_eps       = 1.0e-05
0.00.040.278 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.278 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.278 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.278 I print_info: f_logit_scale    = 0.0e+00
0.00.040.281 I print_info: n_ff             = 8192
0.00.040.281 I print_info: n_expert         = 0
0.00.040.281 I print_info: n_expert_used    = 0
0.00.040.281 I print_info: causal attn      = 1
0.00.040.281 I print_info: pooling type     = 0
0.00.040.282 I print_info: rope type        = 2
0.00.040.282 I print_info: rope scaling     = linear
0.00.040.282 I print_info: freq_base_train  = 10000.0
0.00.040.282 I print_info: freq_scale_train = 1
0.00.040.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.283 I print_info: rope_finetuned   = unknown
0.00.040.283 I print_info: ssm_d_conv       = 0
0.00.040.283 I print_info: ssm_d_inner      = 0
0.00.040.283 I print_info: ssm_d_state      = 0
0.00.040.283 I print_info: ssm_dt_rank      = 0
0.00.040.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.284 I print_info: model type       = 1.4B
0.00.040.284 I print_info: model params     = 1.41 B
0.00.040.288 I print_info: general.name     = 1.4B
0.00.040.288 I print_info: vocab type       = BPE
0.00.040.289 I print_info: n_vocab          = 50304
0.00.040.289 I print_info: n_merges         = 50009
0.00.040.289 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.291 I print_info: LF token         = 187 'Ċ'
0.00.040.291 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.291 I print_info: max token length = 1024
0.00.040.292 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.100 I load_tensors: offloading output layer to GPU
0.00.621.101 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.132 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.140 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.622.842 I llama_init_from_model: n_seq_max     = 1
0.00.622.845 I llama_init_from_model: n_ctx         = 128
0.00.622.845 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.846 I llama_init_from_model: n_batch       = 128
0.00.622.846 I llama_init_from_model: n_ubatch      = 128
0.00.622.846 I llama_init_from_model: flash_attn    = 0
0.00.622.849 I llama_init_from_model: freq_base     = 10000.0
0.00.622.849 I llama_init_from_model: freq_scale    = 1
0.00.622.850 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.852 I ggml_metal_init: allocating
0.00.622.928 I ggml_metal_init: found device: Apple M4
0.00.622.941 I ggml_metal_init: picking default device: Apple M4
0.00.624.748 I ggml_metal_init: using embedded metal library
0.00.631.255 I ggml_metal_init: GPU name:   Apple M4
0.00.631.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.261 I ggml_metal_init: simdgroup reduction   = true
0.00.631.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.262 I ggml_metal_init: has residency sets    = true
0.00.631.262 I ggml_metal_init: has bfloat            = true
0.00.631.263 I ggml_metal_init: use bfloat            = true
0.00.631.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.408 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.981 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.984 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.127 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.129 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.129 I llama_init_from_model: graph nodes  = 967
0.00.655.130 I llama_init_from_model: graph splits = 2
0.00.655.133 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.047 I 
0.00.685.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.144 I perplexity: tokenizing the input ..
0.00.692.333 I perplexity: tokenization took 7.185 ms
0.00.692.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.776 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.842.124 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.842.141 I llama_perf_context_print:        load time =     674.80 ms
0.00.842.142 I llama_perf_context_print: prompt eval time =     147.52 ms /   128 tokens (    1.15 ms per token,   867.69 tokens per second)
0.00.842.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.144 I llama_perf_context_print:       total time =     157.10 ms /   129 tokens
0.00.842.550 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.080s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.985 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.996 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.997 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.573 I llama_model_loader: - type  f32:  194 tensors
0.00.024.573 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.573 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.574 I print_info: file format = GGUF V3 (latest)
0.00.024.575 I print_info: file type   = Q2_K - Medium
0.00.024.575 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.634 I load: special tokens cache size = 25
0.00.038.681 I load: token to piece cache size = 0.2984 MB
0.00.038.683 I print_info: arch             = gptneox
0.00.038.684 I print_info: vocab_only       = 0
0.00.038.684 I print_info: n_ctx_train      = 2048
0.00.038.684 I print_info: n_embd           = 2048
0.00.038.684 I print_info: n_layer          = 24
0.00.038.687 I print_info: n_head           = 16
0.00.038.688 I print_info: n_head_kv        = 16
0.00.038.688 I print_info: n_rot            = 32
0.00.038.690 I print_info: n_swa            = 0
0.00.038.690 I print_info: n_embd_head_k    = 128
0.00.038.690 I print_info: n_embd_head_v    = 128
0.00.038.691 I print_info: n_gqa            = 1
0.00.038.692 I print_info: n_embd_k_gqa     = 2048
0.00.038.693 I print_info: n_embd_v_gqa     = 2048
0.00.038.693 I print_info: f_norm_eps       = 1.0e-05
0.00.038.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.701 I print_info: f_logit_scale    = 0.0e+00
0.00.038.704 I print_info: n_ff             = 8192
0.00.038.704 I print_info: n_expert         = 0
0.00.038.705 I print_info: n_expert_used    = 0
0.00.038.709 I print_info: causal attn      = 1
0.00.038.709 I print_info: pooling type     = 0
0.00.038.709 I print_info: rope type        = 2
0.00.038.710 I print_info: rope scaling     = linear
0.00.038.710 I print_info: freq_base_train  = 10000.0
0.00.038.710 I print_info: freq_scale_train = 1
0.00.038.710 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.712 I print_info: rope_finetuned   = unknown
0.00.038.712 I print_info: ssm_d_conv       = 0
0.00.038.712 I print_info: ssm_d_inner      = 0
0.00.038.712 I print_info: ssm_d_state      = 0
0.00.038.712 I print_info: ssm_dt_rank      = 0
0.00.038.713 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.713 I print_info: model type       = 1.4B
0.00.038.713 I print_info: model params     = 1.41 B
0.00.038.713 I print_info: general.name     = 1.4B
0.00.038.714 I print_info: vocab type       = BPE
0.00.038.714 I print_info: n_vocab          = 50304
0.00.038.714 I print_info: n_merges         = 50009
0.00.038.715 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.716 I print_info: LF token         = 187 'Ċ'
0.00.038.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.716 I print_info: max token length = 1024
0.00.038.716 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.341.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.912 I load_tensors: offloading output layer to GPU
0.00.341.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.945 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.946 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.560 I llama_init_from_model: n_seq_max     = 1
0.00.343.566 I llama_init_from_model: n_ctx         = 128
0.00.343.567 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.567 I llama_init_from_model: n_batch       = 128
0.00.343.567 I llama_init_from_model: n_ubatch      = 128
0.00.343.568 I llama_init_from_model: flash_attn    = 0
0.00.343.570 I llama_init_from_model: freq_base     = 10000.0
0.00.343.570 I llama_init_from_model: freq_scale    = 1
0.00.343.571 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.573 I ggml_metal_init: allocating
0.00.343.649 I ggml_metal_init: found device: Apple M4
0.00.343.663 I ggml_metal_init: picking default device: Apple M4
0.00.345.442 I ggml_metal_init: using embedded metal library
0.00.350.834 I ggml_metal_init: GPU name:   Apple M4
0.00.350.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.851 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.852 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.852 I ggml_metal_init: simdgroup reduction   = true
0.00.350.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.853 I ggml_metal_init: has residency sets    = true
0.00.350.853 I ggml_metal_init: has bfloat            = true
0.00.350.854 I ggml_metal_init: use bfloat            = true
0.00.350.858 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.963 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.522 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.529 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.378.890 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.378.892 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.378.893 I llama_init_from_model: graph nodes  = 967
0.00.378.893 I llama_init_from_model: graph splits = 2
0.00.378.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.378.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.497 I 
0.00.410.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.609 I perplexity: tokenizing the input ..
0.00.417.060 I perplexity: tokenization took 6.448 ms
0.00.417.080 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.116 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.460 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.472 I llama_perf_context_print:        load time =     401.54 ms
0.00.560.473 I llama_perf_context_print: prompt eval time =     141.49 ms /   128 tokens (    1.11 ms per token,   904.66 tokens per second)
0.00.560.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.474 I llama_perf_context_print:       total time =     149.98 ms /   129 tokens
0.00.560.853 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.080s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.790 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.603 I llama_model_loader: - type  f32:  194 tensors
0.00.024.604 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.604 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.604 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.604 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.605 I print_info: file format = GGUF V3 (latest)
0.00.024.606 I print_info: file type   = Q3_K - Medium
0.00.024.607 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.397 I load: special tokens cache size = 25
0.00.038.517 I load: token to piece cache size = 0.2984 MB
0.00.038.519 I print_info: arch             = gptneox
0.00.038.519 I print_info: vocab_only       = 0
0.00.038.519 I print_info: n_ctx_train      = 2048
0.00.038.520 I print_info: n_embd           = 2048
0.00.038.520 I print_info: n_layer          = 24
0.00.038.523 I print_info: n_head           = 16
0.00.038.524 I print_info: n_head_kv        = 16
0.00.038.524 I print_info: n_rot            = 32
0.00.038.524 I print_info: n_swa            = 0
0.00.038.524 I print_info: n_embd_head_k    = 128
0.00.038.524 I print_info: n_embd_head_v    = 128
0.00.038.525 I print_info: n_gqa            = 1
0.00.038.528 I print_info: n_embd_k_gqa     = 2048
0.00.038.528 I print_info: n_embd_v_gqa     = 2048
0.00.038.529 I print_info: f_norm_eps       = 1.0e-05
0.00.038.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.530 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.530 I print_info: f_logit_scale    = 0.0e+00
0.00.038.531 I print_info: n_ff             = 8192
0.00.038.531 I print_info: n_expert         = 0
0.00.038.532 I print_info: n_expert_used    = 0
0.00.038.532 I print_info: causal attn      = 1
0.00.038.532 I print_info: pooling type     = 0
0.00.038.532 I print_info: rope type        = 2
0.00.038.532 I print_info: rope scaling     = linear
0.00.038.532 I print_info: freq_base_train  = 10000.0
0.00.038.533 I print_info: freq_scale_train = 1
0.00.038.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.533 I print_info: rope_finetuned   = unknown
0.00.038.533 I print_info: ssm_d_conv       = 0
0.00.038.533 I print_info: ssm_d_inner      = 0
0.00.038.533 I print_info: ssm_d_state      = 0
0.00.038.534 I print_info: ssm_dt_rank      = 0
0.00.038.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.534 I print_info: model type       = 1.4B
0.00.038.535 I print_info: model params     = 1.41 B
0.00.038.535 I print_info: general.name     = 1.4B
0.00.038.535 I print_info: vocab type       = BPE
0.00.038.536 I print_info: n_vocab          = 50304
0.00.038.536 I print_info: n_merges         = 50009
0.00.038.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: LF token         = 187 'Ċ'
0.00.038.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: max token length = 1024
0.00.038.538 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.463.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.463.934 I load_tensors: offloading output layer to GPU
0.00.463.935 I load_tensors: offloaded 25/25 layers to GPU
0.00.463.964 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.463.965 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.465.523 I llama_init_from_model: n_seq_max     = 1
0.00.465.527 I llama_init_from_model: n_ctx         = 128
0.00.465.528 I llama_init_from_model: n_ctx_per_seq = 128
0.00.465.529 I llama_init_from_model: n_batch       = 128
0.00.465.529 I llama_init_from_model: n_ubatch      = 128
0.00.465.529 I llama_init_from_model: flash_attn    = 0
0.00.465.530 I llama_init_from_model: freq_base     = 10000.0
0.00.465.531 I llama_init_from_model: freq_scale    = 1
0.00.465.531 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.465.534 I ggml_metal_init: allocating
0.00.465.602 I ggml_metal_init: found device: Apple M4
0.00.465.615 I ggml_metal_init: picking default device: Apple M4
0.00.467.389 I ggml_metal_init: using embedded metal library
0.00.473.068 I ggml_metal_init: GPU name:   Apple M4
0.00.473.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.473.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.473.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.473.080 I ggml_metal_init: simdgroup reduction   = true
0.00.473.081 I ggml_metal_init: simdgroup matrix mul. = true
0.00.473.081 I ggml_metal_init: has residency sets    = true
0.00.473.081 I ggml_metal_init: has bfloat            = true
0.00.473.082 I ggml_metal_init: use bfloat            = true
0.00.473.086 I ggml_metal_init: hasUnifiedMemory      = true
0.00.473.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.493.287 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.496.914 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.496.921 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.496.960 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.500.280 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.500.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.500.283 I llama_init_from_model: graph nodes  = 967
0.00.500.283 I llama_init_from_model: graph splits = 2
0.00.500.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.500.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.676 I 
0.00.531.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.775 I perplexity: tokenizing the input ..
0.00.538.588 I perplexity: tokenization took 6.812 ms
0.00.538.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.006 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.418 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.432 I llama_perf_context_print:        load time =     522.88 ms
0.00.679.432 I llama_perf_context_print: prompt eval time =     139.16 ms /   128 tokens (    1.09 ms per token,   919.80 tokens per second)
0.00.679.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.433 I llama_perf_context_print:       total time =     147.76 ms /   129 tokens
0.00.679.779 I ggml_metal_free: deallocating

real	0m0.693s
user	0m0.079s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.179 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.123 I llama_model_loader: - type  f32:  194 tensors
0.00.026.124 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.124 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.124 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.125 I print_info: file format = GGUF V3 (latest)
0.00.026.125 I print_info: file type   = Q4_K - Medium
0.00.026.126 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.890 I load: special tokens cache size = 25
0.00.039.853 I load: token to piece cache size = 0.2984 MB
0.00.039.856 I print_info: arch             = gptneox
0.00.039.856 I print_info: vocab_only       = 0
0.00.039.856 I print_info: n_ctx_train      = 2048
0.00.039.856 I print_info: n_embd           = 2048
0.00.039.856 I print_info: n_layer          = 24
0.00.039.860 I print_info: n_head           = 16
0.00.039.861 I print_info: n_head_kv        = 16
0.00.039.861 I print_info: n_rot            = 32
0.00.039.861 I print_info: n_swa            = 0
0.00.039.861 I print_info: n_embd_head_k    = 128
0.00.039.861 I print_info: n_embd_head_v    = 128
0.00.039.862 I print_info: n_gqa            = 1
0.00.039.863 I print_info: n_embd_k_gqa     = 2048
0.00.039.864 I print_info: n_embd_v_gqa     = 2048
0.00.039.866 I print_info: f_norm_eps       = 1.0e-05
0.00.039.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.866 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.867 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.867 I print_info: f_logit_scale    = 0.0e+00
0.00.039.868 I print_info: n_ff             = 8192
0.00.039.868 I print_info: n_expert         = 0
0.00.039.868 I print_info: n_expert_used    = 0
0.00.039.868 I print_info: causal attn      = 1
0.00.039.868 I print_info: pooling type     = 0
0.00.039.869 I print_info: rope type        = 2
0.00.039.869 I print_info: rope scaling     = linear
0.00.039.869 I print_info: freq_base_train  = 10000.0
0.00.039.869 I print_info: freq_scale_train = 1
0.00.039.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.870 I print_info: rope_finetuned   = unknown
0.00.039.870 I print_info: ssm_d_conv       = 0
0.00.039.870 I print_info: ssm_d_inner      = 0
0.00.039.870 I print_info: ssm_d_state      = 0
0.00.039.870 I print_info: ssm_dt_rank      = 0
0.00.039.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.870 I print_info: model type       = 1.4B
0.00.039.871 I print_info: model params     = 1.41 B
0.00.039.871 I print_info: general.name     = 1.4B
0.00.039.871 I print_info: vocab type       = BPE
0.00.039.871 I print_info: n_vocab          = 50304
0.00.039.871 I print_info: n_merges         = 50009
0.00.039.872 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: LF token         = 187 'Ċ'
0.00.039.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.873 I print_info: max token length = 1024
0.00.039.873 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.619 I load_tensors: offloading output layer to GPU
0.00.511.620 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.640 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.641 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.512.552 I llama_init_from_model: n_seq_max     = 1
0.00.512.557 I llama_init_from_model: n_ctx         = 128
0.00.512.557 I llama_init_from_model: n_ctx_per_seq = 128
0.00.512.558 I llama_init_from_model: n_batch       = 128
0.00.512.558 I llama_init_from_model: n_ubatch      = 128
0.00.512.558 I llama_init_from_model: flash_attn    = 0
0.00.512.560 I llama_init_from_model: freq_base     = 10000.0
0.00.512.560 I llama_init_from_model: freq_scale    = 1
0.00.512.561 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.512.562 I ggml_metal_init: allocating
0.00.512.612 I ggml_metal_init: found device: Apple M4
0.00.512.631 I ggml_metal_init: picking default device: Apple M4
0.00.513.681 I ggml_metal_init: using embedded metal library
0.00.518.151 I ggml_metal_init: GPU name:   Apple M4
0.00.518.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.518.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.518.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.518.160 I ggml_metal_init: simdgroup reduction   = true
0.00.518.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.518.160 I ggml_metal_init: has residency sets    = true
0.00.518.161 I ggml_metal_init: has bfloat            = true
0.00.518.161 I ggml_metal_init: use bfloat            = true
0.00.518.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.518.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.533.023 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.534.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.534.717 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.534.730 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.308 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.536.310 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.536.310 I llama_init_from_model: graph nodes  = 967
0.00.536.310 I llama_init_from_model: graph splits = 2
0.00.536.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.536.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.578 I 
0.00.564.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.622 I perplexity: tokenizing the input ..
0.00.568.625 I perplexity: tokenization took 4 ms
0.00.568.635 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.713.136 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.714.291 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.714.308 I llama_perf_context_print:        load time =     554.39 ms
0.00.714.310 I llama_perf_context_print: prompt eval time =     144.27 ms /   128 tokens (    1.13 ms per token,   887.26 tokens per second)
0.00.714.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.714.311 I llama_perf_context_print:       total time =     149.73 ms /   129 tokens
0.00.714.739 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.069s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.481 I llama_model_loader: - type  f32:  194 tensors
0.00.024.482 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.482 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.482 I print_info: file format = GGUF V3 (latest)
0.00.024.483 I print_info: file type   = Q5_K - Medium
0.00.024.484 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.164 I load: special tokens cache size = 25
0.00.038.153 I load: token to piece cache size = 0.2984 MB
0.00.038.156 I print_info: arch             = gptneox
0.00.038.156 I print_info: vocab_only       = 0
0.00.038.156 I print_info: n_ctx_train      = 2048
0.00.038.156 I print_info: n_embd           = 2048
0.00.038.156 I print_info: n_layer          = 24
0.00.038.159 I print_info: n_head           = 16
0.00.038.160 I print_info: n_head_kv        = 16
0.00.038.160 I print_info: n_rot            = 32
0.00.038.160 I print_info: n_swa            = 0
0.00.038.160 I print_info: n_embd_head_k    = 128
0.00.038.160 I print_info: n_embd_head_v    = 128
0.00.038.161 I print_info: n_gqa            = 1
0.00.038.162 I print_info: n_embd_k_gqa     = 2048
0.00.038.165 I print_info: n_embd_v_gqa     = 2048
0.00.038.166 I print_info: f_norm_eps       = 1.0e-05
0.00.038.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.166 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.167 I print_info: f_logit_scale    = 0.0e+00
0.00.038.167 I print_info: n_ff             = 8192
0.00.038.167 I print_info: n_expert         = 0
0.00.038.168 I print_info: n_expert_used    = 0
0.00.038.168 I print_info: causal attn      = 1
0.00.038.168 I print_info: pooling type     = 0
0.00.038.168 I print_info: rope type        = 2
0.00.038.168 I print_info: rope scaling     = linear
0.00.038.169 I print_info: freq_base_train  = 10000.0
0.00.038.169 I print_info: freq_scale_train = 1
0.00.038.169 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.169 I print_info: rope_finetuned   = unknown
0.00.038.169 I print_info: ssm_d_conv       = 0
0.00.038.170 I print_info: ssm_d_inner      = 0
0.00.038.170 I print_info: ssm_d_state      = 0
0.00.038.170 I print_info: ssm_dt_rank      = 0
0.00.038.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.170 I print_info: model type       = 1.4B
0.00.038.171 I print_info: model params     = 1.41 B
0.00.038.171 I print_info: general.name     = 1.4B
0.00.038.171 I print_info: vocab type       = BPE
0.00.038.171 I print_info: n_vocab          = 50304
0.00.038.171 I print_info: n_merges         = 50009
0.00.038.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: LF token         = 187 'Ċ'
0.00.038.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.173 I print_info: max token length = 1024
0.00.038.173 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.315 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.326 I load_tensors: offloading output layer to GPU
0.00.631.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.357 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.631.362 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.632.899 I llama_init_from_model: n_seq_max     = 1
0.00.632.902 I llama_init_from_model: n_ctx         = 128
0.00.632.903 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.903 I llama_init_from_model: n_batch       = 128
0.00.632.904 I llama_init_from_model: n_ubatch      = 128
0.00.632.904 I llama_init_from_model: flash_attn    = 0
0.00.632.907 I llama_init_from_model: freq_base     = 10000.0
0.00.632.907 I llama_init_from_model: freq_scale    = 1
0.00.632.908 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.911 I ggml_metal_init: allocating
0.00.633.004 I ggml_metal_init: found device: Apple M4
0.00.633.017 I ggml_metal_init: picking default device: Apple M4
0.00.634.731 I ggml_metal_init: using embedded metal library
0.00.640.136 I ggml_metal_init: GPU name:   Apple M4
0.00.640.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.152 I ggml_metal_init: simdgroup reduction   = true
0.00.640.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.153 I ggml_metal_init: has residency sets    = true
0.00.640.153 I ggml_metal_init: has bfloat            = true
0.00.640.153 I ggml_metal_init: use bfloat            = true
0.00.640.157 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.412 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.438 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.758 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.759 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.760 I llama_init_from_model: graph nodes  = 967
0.00.666.760 I llama_init_from_model: graph splits = 2
0.00.666.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.081 I 
0.00.700.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.188 I perplexity: tokenizing the input ..
0.00.707.705 I perplexity: tokenization took 7.513 ms
0.00.707.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.623 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.850.978 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.850.998 I llama_perf_context_print:        load time =     691.33 ms
0.00.850.999 I llama_perf_context_print: prompt eval time =     140.94 ms /   128 tokens (    1.10 ms per token,   908.17 tokens per second)
0.00.850.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.000 I llama_perf_context_print:       total time =     150.92 ms /   129 tokens
0.00.851.354 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.079s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.937 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.682 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.683 I llama_model_loader: - type  f32:  194 tensors
0.00.024.683 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.684 I print_info: file format = GGUF V3 (latest)
0.00.024.685 I print_info: file type   = Q6_K
0.00.024.686 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.960 I load: special tokens cache size = 25
0.00.039.057 I load: token to piece cache size = 0.2984 MB
0.00.039.061 I print_info: arch             = gptneox
0.00.039.061 I print_info: vocab_only       = 0
0.00.039.061 I print_info: n_ctx_train      = 2048
0.00.039.061 I print_info: n_embd           = 2048
0.00.039.061 I print_info: n_layer          = 24
0.00.039.066 I print_info: n_head           = 16
0.00.039.066 I print_info: n_head_kv        = 16
0.00.039.071 I print_info: n_rot            = 32
0.00.039.071 I print_info: n_swa            = 0
0.00.039.071 I print_info: n_embd_head_k    = 128
0.00.039.071 I print_info: n_embd_head_v    = 128
0.00.039.072 I print_info: n_gqa            = 1
0.00.039.072 I print_info: n_embd_k_gqa     = 2048
0.00.039.073 I print_info: n_embd_v_gqa     = 2048
0.00.039.073 I print_info: f_norm_eps       = 1.0e-05
0.00.039.074 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.074 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.074 I print_info: f_logit_scale    = 0.0e+00
0.00.039.075 I print_info: n_ff             = 8192
0.00.039.075 I print_info: n_expert         = 0
0.00.039.075 I print_info: n_expert_used    = 0
0.00.039.075 I print_info: causal attn      = 1
0.00.039.075 I print_info: pooling type     = 0
0.00.039.076 I print_info: rope type        = 2
0.00.039.077 I print_info: rope scaling     = linear
0.00.039.077 I print_info: freq_base_train  = 10000.0
0.00.039.077 I print_info: freq_scale_train = 1
0.00.039.078 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.078 I print_info: rope_finetuned   = unknown
0.00.039.079 I print_info: ssm_d_conv       = 0
0.00.039.079 I print_info: ssm_d_inner      = 0
0.00.039.079 I print_info: ssm_d_state      = 0
0.00.039.079 I print_info: ssm_dt_rank      = 0
0.00.039.079 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.080 I print_info: model type       = 1.4B
0.00.039.081 I print_info: model params     = 1.41 B
0.00.039.081 I print_info: general.name     = 1.4B
0.00.039.081 I print_info: vocab type       = BPE
0.00.039.082 I print_info: n_vocab          = 50304
0.00.039.082 I print_info: n_merges         = 50009
0.00.039.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.082 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.082 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.082 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: LF token         = 187 'Ċ'
0.00.039.083 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: max token length = 1024
0.00.039.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.043 I load_tensors: offloading output layer to GPU
0.00.515.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.073 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.515.076 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.516.610 I llama_init_from_model: n_seq_max     = 1
0.00.516.612 I llama_init_from_model: n_ctx         = 128
0.00.516.613 I llama_init_from_model: n_ctx_per_seq = 128
0.00.516.613 I llama_init_from_model: n_batch       = 128
0.00.516.613 I llama_init_from_model: n_ubatch      = 128
0.00.516.614 I llama_init_from_model: flash_attn    = 0
0.00.516.615 I llama_init_from_model: freq_base     = 10000.0
0.00.516.616 I llama_init_from_model: freq_scale    = 1
0.00.516.616 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.516.618 I ggml_metal_init: allocating
0.00.516.689 I ggml_metal_init: found device: Apple M4
0.00.516.702 I ggml_metal_init: picking default device: Apple M4
0.00.518.084 I ggml_metal_init: using embedded metal library
0.00.524.127 I ggml_metal_init: GPU name:   Apple M4
0.00.524.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.133 I ggml_metal_init: simdgroup reduction   = true
0.00.524.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.134 I ggml_metal_init: has residency sets    = true
0.00.524.134 I ggml_metal_init: has bfloat            = true
0.00.524.135 I ggml_metal_init: use bfloat            = true
0.00.524.136 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.453 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.976 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.979 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.544.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.231 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.547.233 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.547.234 I llama_init_from_model: graph nodes  = 967
0.00.547.234 I llama_init_from_model: graph splits = 2
0.00.547.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.547.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.137 I 
0.00.582.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.243 I perplexity: tokenizing the input ..
0.00.587.530 I perplexity: tokenization took 5.285 ms
0.00.587.542 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.722 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.728.062 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.728.082 I llama_perf_context_print:        load time =     573.19 ms
0.00.728.082 I llama_perf_context_print: prompt eval time =     138.94 ms /   128 tokens (    1.09 ms per token,   921.26 tokens per second)
0.00.728.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.084 I llama_perf_context_print:       total time =     145.95 ms /   129 tokens
0.00.728.465 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.076s
sys	0m0.126s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.288 I build: 4683 (19b392d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.657 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.075 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.925 I llama_model_loader: - type  f32:  194 tensors
0.00.055.925 I llama_model_loader: - type  f16:   98 tensors
0.00.055.926 I print_info: file format = GGUF V3 (latest)
0.00.055.927 I print_info: file type   = all F32 (guessed)
0.00.055.929 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.134 I load: special tokens cache size = 25
0.00.076.140 I load: token to piece cache size = 0.2984 MB
0.00.076.143 I print_info: arch             = gptneox
0.00.076.144 I print_info: vocab_only       = 0
0.00.076.144 I print_info: n_ctx_train      = 2048
0.00.076.144 I print_info: n_embd           = 2048
0.00.076.144 I print_info: n_layer          = 24
0.00.076.148 I print_info: n_head           = 16
0.00.076.149 I print_info: n_head_kv        = 16
0.00.076.149 I print_info: n_rot            = 32
0.00.076.149 I print_info: n_swa            = 0
0.00.076.149 I print_info: n_embd_head_k    = 128
0.00.076.150 I print_info: n_embd_head_v    = 128
0.00.076.151 I print_info: n_gqa            = 1
0.00.076.152 I print_info: n_embd_k_gqa     = 2048
0.00.076.153 I print_info: n_embd_v_gqa     = 2048
0.00.076.153 I print_info: f_norm_eps       = 1.0e-05
0.00.076.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.154 I print_info: f_logit_scale    = 0.0e+00
0.00.076.155 I print_info: n_ff             = 8192
0.00.076.155 I print_info: n_expert         = 0
0.00.076.155 I print_info: n_expert_used    = 0
0.00.076.156 I print_info: causal attn      = 1
0.00.076.156 I print_info: pooling type     = 0
0.00.076.157 I print_info: rope type        = 2
0.00.076.157 I print_info: rope scaling     = linear
0.00.076.157 I print_info: freq_base_train  = 10000.0
0.00.076.158 I print_info: freq_scale_train = 1
0.00.076.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.158 I print_info: rope_finetuned   = unknown
0.00.076.159 I print_info: ssm_d_conv       = 0
0.00.076.159 I print_info: ssm_d_inner      = 0
0.00.076.159 I print_info: ssm_d_state      = 0
0.00.076.159 I print_info: ssm_dt_rank      = 0
0.00.076.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.161 I print_info: model type       = 1.4B
0.00.076.161 I print_info: model params     = 1.41 B
0.00.076.161 I print_info: general.name     = 1.4B
0.00.076.162 I print_info: vocab type       = BPE
0.00.076.162 I print_info: n_vocab          = 50304
0.00.076.162 I print_info: n_merges         = 50009
0.00.076.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.163 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.163 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.163 I print_info: LF token         = 187 'Ċ'
0.00.076.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.167 I print_info: max token length = 1024
0.00.076.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.370.975 I load_tensors: offloading 24 repeating layers to GPU
0.01.370.979 I load_tensors: offloading output layer to GPU
0.01.370.979 I load_tensors: offloaded 25/25 layers to GPU
0.01.371.006 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.371.007 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.371.776 I llama_init_from_model: n_seq_max     = 1
0.01.371.777 I llama_init_from_model: n_ctx         = 128
0.01.371.777 I llama_init_from_model: n_ctx_per_seq = 128
0.01.371.778 I llama_init_from_model: n_batch       = 128
0.01.371.778 I llama_init_from_model: n_ubatch      = 128
0.01.371.778 I llama_init_from_model: flash_attn    = 0
0.01.371.779 I llama_init_from_model: freq_base     = 10000.0
0.01.371.779 I llama_init_from_model: freq_scale    = 1
0.01.371.779 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.371.783 I ggml_metal_init: allocating
0.01.371.842 I ggml_metal_init: found device: Apple M4
0.01.371.847 I ggml_metal_init: picking default device: Apple M4
0.01.373.049 I ggml_metal_init: using embedded metal library
0.01.376.904 I ggml_metal_init: GPU name:   Apple M4
0.01.376.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.376.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.376.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.376.908 I ggml_metal_init: simdgroup reduction   = true
0.01.376.908 I ggml_metal_init: simdgroup matrix mul. = true
0.01.376.908 I ggml_metal_init: has residency sets    = true
0.01.376.908 I ggml_metal_init: has bfloat            = true
0.01.376.908 I ggml_metal_init: use bfloat            = true
0.01.376.909 I ggml_metal_init: hasUnifiedMemory      = true
0.01.376.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.387.495 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.389.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.389.230 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.389.248 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.391.001 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.391.002 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.391.002 I llama_init_from_model: graph nodes  = 967
0.01.391.003 I llama_init_from_model: graph splits = 2
0.01.391.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.391.004 I 
0.01.391.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.391.042 I compute_imatrix: tokenizing the input ..
0.01.395.244 I compute_imatrix: tokenization took 4.201 ms
0.01.395.246 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.658.673 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.661.226 I llama_perf_context_print:        load time =    1634.96 ms
0.01.661.227 I llama_perf_context_print: prompt eval time =     261.67 ms /   128 tokens (    2.04 ms per token,   489.16 tokens per second)
0.01.661.228 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.661.228 I llama_perf_context_print:       total time =    1637.51 ms /   129 tokens
0.01.661.713 I ggml_metal_free: deallocating

real	0m1.847s
user	0m0.127s
sys	0m0.263s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4683 (19b392d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128f08e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128f09530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128f09ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128f0a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128f0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128f0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128f0b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128f0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128f0c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128f0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128f0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128f0d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128f0ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128f0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128f0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128f0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128f0fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128f10360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128f10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128f11250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128f11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128f12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128f12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128f13050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128f13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128f13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128f14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128f14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128f14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128f15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128f154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128f162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128f16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129905830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129905ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129906110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129906580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1299069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1299099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129909e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12990a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12990a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12990aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12990ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12990b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12990bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12990c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12990c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12990ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12990d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12990d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12990df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12990e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12990ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12990f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12990f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12990f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129910110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1299105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129910a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129910ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129911390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129911830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129911cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129912170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129912610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129912ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129912f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1299133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129913890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129913d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129914280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1299147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129914d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129915270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1299157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129915d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129916260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1299167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129916d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129917250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1299177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129917cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129918240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129918790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129918ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129919230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129919780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129919cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12991a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12991a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12991acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12991b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12991b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12991bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12990b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12991c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12991c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12991cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12991d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12991d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12991dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12991e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12991e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12991ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12991f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12991f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12991fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1299201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129920710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129920c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129921100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1299215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129921a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129921ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129922380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129922820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129922cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129923160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129923600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129923aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129923f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1299243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129924880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129924d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1299251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129925660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129925b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129925fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129926440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1299268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129926d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129927220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1299276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129927b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129928000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1299284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129928940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129928de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129929280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129929720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129929bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12992a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12992a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12992a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12992ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12992b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12992b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12992bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12992c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12992c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12992ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12992cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12992d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12992d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12992dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12992e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12992e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12992ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12992ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12992f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12992f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12992fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129930180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129930620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129930ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129930f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129931400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1299318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129931d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1299321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129932680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129932b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129932fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129933460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129933900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129933da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129934240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1299346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129934b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129935020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1299354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129935960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129935e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1299362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129936740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129936be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129937080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129937520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1299379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129937e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1299383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129938900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129938e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1299393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129939660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129939c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12993a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12993a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12993b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12993b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12993b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12993bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12993c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12993cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12993d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12993d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12993d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12993e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12993e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12993ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12993f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12993f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12993fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129940160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1299406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129940c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129941150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1299416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129941bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129942140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129942690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129942be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129943130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129943680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129943bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129944120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129944670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129944bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129945110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129945660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129945bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129946100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129946650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129946ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1299470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129947640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129947b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1299480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129948630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129948b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1299490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129949620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129949b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12994a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12994a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12994ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12994b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12994b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12994bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12994c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12994c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12994cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12994d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12994d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12994db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12994e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12994e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12994eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12994f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12994f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12994fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129950060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1299505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129950b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129950fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129951440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1299518e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129951d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129952220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1299526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129952b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129953000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1299534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129953940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129953de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129954280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129954720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129954bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129955060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1299555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129955cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1299563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129956b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129957230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1299574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129957ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129957fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1299585b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.749.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129a05f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129a063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129a06830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129a06ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129a07110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129a07580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129a079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129a07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129a082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129a08740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129a08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129a09270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129a09d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129a0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129a0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129a0b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129a0bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129a0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129a0c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129a0d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129a0d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129a0dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129a0e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129a0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129a0f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129a0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129a0fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129a0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129a103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129a10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129a10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129a11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129a11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129a11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129a11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129a12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129a12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129a12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129a13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129a13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129a13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129a14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129a14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129a14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129a14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129a15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129a15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129a15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129a16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129a165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129a16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129a16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129a17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129a17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129a17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129a183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129a18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129a18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129a19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129a19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129a19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129a1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129a1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129a1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129a1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129a1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129a1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129a1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129a1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129a1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129a1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129a1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129a1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129a1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129a1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129a1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129a1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129a1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129a1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129a1fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129a1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129a20510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129a20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129a20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129a21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129a21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129a21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129a224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129a22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129a22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129a234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129a23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129a23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129a244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129a24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129a24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129a254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129a25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129a25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129a264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129a26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129a26f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129a274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129a279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129a27f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129a28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129a289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129a28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129a29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129a299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129a29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129a2a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129a2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129a2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129a2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129a2b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129a2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129a2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129a2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129a2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129a2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129a2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129a2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129a2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129a2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129a2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129a2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129a2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129a2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129a2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129a2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129a301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129a30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129a30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129a30f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129a318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129a31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129a32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129a326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129a32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129a32fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129a33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129a33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129a33dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129a34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129a34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129a34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129a35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129a354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129a35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129a35e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129a362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129a36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129a36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129a370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129a37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129a379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129a37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129a38320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129a387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129a38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129a39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129a395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129a39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129a39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129a3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129a3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129a3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129a3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129a3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129a3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129a3bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129a3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129a3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129a3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129a3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129a3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129a3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129a3dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129a3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129a3e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129a3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129a3f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129a3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129a40000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129a404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129a40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c304080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c3044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c304960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c304dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c305240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c3056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c305b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c305f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c306400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c306870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c306ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c307150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c3075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c307a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c307ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c308310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c308780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c308bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c309170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c3095e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c309a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c30a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c30a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c30ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c30af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c30b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c30b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c30bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c30c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c30c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c30ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c30cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c30d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c30d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c30dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c30e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c30e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c30e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c30edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c30f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c30f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c30fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c30ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c3103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c310850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c310cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c311130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c3115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c311a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c311e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c3122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c312760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c312bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c313040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c3134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c313920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c313d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c314200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c314670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c314ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c314f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c3153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c315830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c315ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c316110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c316580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c3169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c316e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c3172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c317740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c317bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c318020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c318490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c318900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c318d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c3191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c319650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c319ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c319f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c31a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c31a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c31ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c31b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c31b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c31b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c31be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c31c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c31c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c31cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c31d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c31d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c31d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c31dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c31e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c31ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c31f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c31fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c320190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c320450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c3208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c320ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c3214d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c309d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c321bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c321e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c322130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c3223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c3226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c322970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c322c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c322ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c3231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c323470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c323730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c323d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c3242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c324900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c324e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c325380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c3258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c325e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c3265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c327050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c327590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c327ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c328010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c328550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c328810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c328ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c328d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c329050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c329310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c3295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c329890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c329b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c32a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c32a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c32a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c32a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c32abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c32ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c32b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c32b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c32b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c32b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c32bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c32bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c32c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c32c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c32c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c32ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c32ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c32cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c32d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c32d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c32d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c32da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c32dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c32e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c32e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c32e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c32e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c32eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c32edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c32f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c32f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c32f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c32f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c32fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c32fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c330110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c3303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c330690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c330950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c330c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c330ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c331190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c331450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c331710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c3319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c331c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c331f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c332210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c3324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c332790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c332a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c332d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c332fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c333290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c333550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c333810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c333ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c333d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c3342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c334830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c334d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c3352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c335820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c335d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c3362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c336810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c336d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c3372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c337800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c337d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c3382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c3387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c338d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c339290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c3397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c339d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c33a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c33a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c33abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c33ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c33b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c33b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c33ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c33bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c33c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c33c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c33cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c33d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c33d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c33d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c33ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c33e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c33e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c33eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c33ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c33f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c33f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c33fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c340130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c3405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c340a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c340e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c3412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c341760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c341bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c342040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c3424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c342920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c342d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c343200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c343670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c343ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c343f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c3443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c344830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c344ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c345110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c345580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c3459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c345e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c3462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c346740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c346bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c347020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c347490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c347900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c347d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c3481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c348650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c348ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c348f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c3493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c349810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c349c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c34a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c34a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c34a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c34ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c34b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c34b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c34bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c34c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c34c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c34c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c34cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c34d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c34d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c34daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c34df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c34e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c34e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c34ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c34f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c34f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c34f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c34fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c350290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c350700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c350b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c350fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c351450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c3518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c351d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c3521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c352610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c352a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c352ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c3534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c3539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c353e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c354290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c354700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c354b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c355090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c3555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c356110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c3563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c356990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c356f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c357510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c357ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c358090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c358650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c358c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c3591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c359790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c359d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c35a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c35a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c35ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c35b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c35ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c35bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c35c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c35cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c35d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c35d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c35dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c35e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c35e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c35edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c35f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c35f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c35ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c3604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c360a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c361050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c361610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c361bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c362190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c362750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c362d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c3632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c363890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c363e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c364410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c3649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c364f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c365550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c365b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c3660d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c366690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c366c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c367210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c3677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c367d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c368350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c368910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c368ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c369490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c369a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c36a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c36a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c36aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c36afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c36b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c36b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c36bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c36c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c36c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c36cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c36d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c36d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c36dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c36e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c36e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c36ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c36f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c36fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c370200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c370920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c371040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c371300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c371af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c371db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c3723c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.810s
user	0m0.273s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4683 (19b392d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e80ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e80b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e80ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e80bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e80c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e80cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e80d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e80d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e80dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e80e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e80e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e80eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e80f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e80fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e810640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e810d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e811480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e811ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e8122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e812a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e8131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e8138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e813ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e814890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e814fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e815270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e815880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e8164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e816a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e816cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e817190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e817450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e817ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e818220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e8184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e818980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e818e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e8192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e819760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e819c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e81a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e81a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e81a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e81ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e81b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e81b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e81bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e81c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e81cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e81d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e81d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e81dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e81e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e81eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e81f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e81f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e81fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e81fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e8204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e820cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e820f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e821430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e8218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e821d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e822210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e8226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e822b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e822ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e823490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e823930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e823dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e824270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e824710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e824c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e8251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e825700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e825c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e8261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e8266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e826c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e827190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e8276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e827c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e828180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e8286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e828c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e829170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e8296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e829c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e82a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e82a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e82ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e82b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e82b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e82bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e82c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e82c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e81c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e82cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e82d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e82d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e82dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e82e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e82e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e82ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e82f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e82f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e82fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e830280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e8307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e830d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e831270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e8317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e831c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e832100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e8325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e832a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e832ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e833380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e833820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e833cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e834160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e834600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e834aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e834f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e8353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e835880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e835d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e8361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e836660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e836b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e836fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e837440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e8378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e837d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e838220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e8386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e838b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e8394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e839940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e839de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e83a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e83a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e83abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e83b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e83b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e83b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e83be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e83c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e83c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e83cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e83d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e83d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e83da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e83dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e83e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e83e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e83ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e83f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e83f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e83fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e83ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e8403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e840840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e840ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e841620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e841ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e841f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e842400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e8428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e842d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e8431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e843680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e843fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e844460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e844900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e844da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e845240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e8456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e845b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e846020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e8464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e846960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e846e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e8472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e847740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e847be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e848080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e848520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e8489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e848f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e849460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e8499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e849f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e84a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e84a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e84ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e84b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e84bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e84c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e84c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e84c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e84cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e84d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e84dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e84e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e84ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e84f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e84f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e84fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e850220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e850770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e850cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e851210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e851760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e851cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e852200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e852750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e852ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e8531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e853740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e8541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e854730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e854c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e8551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e855c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e8561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e856710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e856c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e8571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e857700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e857c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e8581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e8586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e858c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e859190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e8596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e859c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e85a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e85a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e85ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e85b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e85b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e85bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e85c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e85c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e85cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e85d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e85d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e85dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e85e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e85e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e85ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e85f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e85f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e85fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e860120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e860670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e860bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e861110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e861660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e861b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e861fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e862440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e8628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e862d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e863220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e8636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e863b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e864000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e8644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e864940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e864de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e865280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e865720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e865bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e866110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e866830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e866f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e867670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e867d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e868050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e868840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e868b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e869110 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.017 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e868dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e84aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e84a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e84b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e81e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e81db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e820190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e84cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e815530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e81c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e81c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e81cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e81b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e81d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e814530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e8207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e82cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e868310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e817710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e8179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e84d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e84b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e815b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e815e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e8160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e869570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e869830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e869af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e869db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e86a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e86a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e86a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e86a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e86ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e86ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e86b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e86b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e86b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e86b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e86bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e86beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e86c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e86c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e86c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e86c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e86cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e86cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e86d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e86d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e86d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e86da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e86dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e86dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e86e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e86e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e86e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e86eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e86ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e86f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e86f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e86f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e86f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e86fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e86fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e8700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e870370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e870630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e8708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e870bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e870e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e871130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e8713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e8716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e871970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e871c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e871ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e8721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e872470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e872730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e8729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e872cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e872f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e873230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e8734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e8737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e873a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e873d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e873ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e8742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e874570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e874830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e874af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e874db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e875070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e875330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e8755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e8758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e875b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e875e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e8760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e8763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e876670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e876930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e876bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e876eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e877170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e877430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e8776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e8779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e877c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e877f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e8781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e8784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e878770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e878a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e878cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e878fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e879270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e879530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e8797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e879ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e879d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e87a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e87a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e87a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e87a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e87ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e87adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e87b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e87b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e87b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e87b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e87bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e87be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e87c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e87c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e87c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e87c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e87cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e87cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e87d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e87d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e87d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e87d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e87dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e87df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e87e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e87e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e87e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e87ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e87ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e87eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e87f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e87f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e87f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e87faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e87fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e880070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e880330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e8805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e8808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e880b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e880e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e8810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e8813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e881670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e881930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e881bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e881eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e882170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e882430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e8826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e8829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e882c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e882f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e8831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e8834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e883770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e883a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e883cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e883fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e884270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e884530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e8847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e884ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e884d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e885030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e8852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e8855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e885870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e885b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e885df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e8860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e886370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e886630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e8868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e886bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e886e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e887130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e8873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e8876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e887970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e887c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e887ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e8881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e8886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e8889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e888e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e8892f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e889790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e889f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e88a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e88a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e88a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e88ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e88b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e88b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e88baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e88bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e88c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e88c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e88ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e88d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e88d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e88da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e88de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e88e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e88e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e88ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e88f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e88f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e88f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e88fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e8901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e890660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e890ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e890f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e8913b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e891820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e891c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e892100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e892570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e8929e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e892e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e8932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e893730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e893ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e894010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e894480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e8948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e894d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e8951d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e895640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e895ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e895f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e896390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e896800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e896c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e8970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e897550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e8979c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e897e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e8982a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e898710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e898b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e898ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e899460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e8998d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e899d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e89a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e89a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e89aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e89af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e89b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e89b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e89bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e89c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e89c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e89c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e89ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e89d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e89d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e89db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e89e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e89ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e89f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e89fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e89fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e8a05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e8a08a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e8a0eb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de04f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de05380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de05c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de06540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de06e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de07290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de07800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de07c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de09dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de0a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de0ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de0e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de0e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de16b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de58f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de59510 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.230s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
