### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.67 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.64 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.30 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.93 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.22 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.94 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.02 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  198.35 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.23 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 244.49 sec*proc (28 tests)

Total Test time (real) = 244.50 sec

real	4m4.632s
user	8m28.168s
sys	0m6.816s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.44 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.44 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.74 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  53.09 sec*proc (28 tests)

Total Test time (real) =  53.10 sec

real	0m53.116s
user	1m16.011s
sys	0m6.186s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.169 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.663 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.674 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.676 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.677 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.677 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.679 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.680 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.681 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.682 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.682 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.686 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.687 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.688 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.688 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.689 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.689 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.690 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.036.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.037.439 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.440 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.037.441 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.037.442 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.037.442 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.037.442 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.037.443 I llama_model_loader: - type  f32:  124 tensors
0.00.037.444 I llama_model_loader: - type  f16:   73 tensors
0.00.037.444 I print_info: file format = GGUF V3 (latest)
0.00.037.445 I print_info: file type   = F16
0.00.037.447 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.041.818 I load: special tokens cache size = 5
0.00.044.043 I load: token to piece cache size = 0.2032 MB
0.00.044.050 I print_info: arch             = bert
0.00.044.051 I print_info: vocab_only       = 0
0.00.044.051 I print_info: n_ctx_train      = 512
0.00.044.051 I print_info: n_embd           = 384
0.00.044.052 I print_info: n_layer          = 12
0.00.044.056 I print_info: n_head           = 12
0.00.044.057 I print_info: n_head_kv        = 12
0.00.044.058 I print_info: n_rot            = 32
0.00.044.058 I print_info: n_swa            = 0
0.00.044.058 I print_info: n_embd_head_k    = 32
0.00.044.058 I print_info: n_embd_head_v    = 32
0.00.044.059 I print_info: n_gqa            = 1
0.00.044.060 I print_info: n_embd_k_gqa     = 384
0.00.044.061 I print_info: n_embd_v_gqa     = 384
0.00.044.062 I print_info: f_norm_eps       = 1.0e-12
0.00.044.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.063 I print_info: f_logit_scale    = 0.0e+00
0.00.044.065 I print_info: n_ff             = 1536
0.00.044.065 I print_info: n_expert         = 0
0.00.044.065 I print_info: n_expert_used    = 0
0.00.044.065 I print_info: causal attn      = 0
0.00.044.066 I print_info: pooling type     = 2
0.00.044.066 I print_info: rope type        = 2
0.00.044.067 I print_info: rope scaling     = linear
0.00.044.067 I print_info: freq_base_train  = 10000.0
0.00.044.067 I print_info: freq_scale_train = 1
0.00.044.068 I print_info: n_ctx_orig_yarn  = 512
0.00.044.068 I print_info: rope_finetuned   = unknown
0.00.044.068 I print_info: ssm_d_conv       = 0
0.00.044.069 I print_info: ssm_d_inner      = 0
0.00.044.069 I print_info: ssm_d_state      = 0
0.00.044.069 I print_info: ssm_dt_rank      = 0
0.00.044.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.070 I print_info: model type       = 33M
0.00.044.070 I print_info: model params     = 33.21 M
0.00.044.070 I print_info: general.name     = Bge Small
0.00.044.071 I print_info: vocab type       = WPM
0.00.044.072 I print_info: n_vocab          = 30522
0.00.044.072 I print_info: n_merges         = 0
0.00.044.072 I print_info: BOS token        = 101 '[CLS]'
0.00.044.072 I print_info: UNK token        = 100 '[UNK]'
0.00.044.073 I print_info: SEP token        = 102 '[SEP]'
0.00.044.076 I print_info: PAD token        = 0 '[PAD]'
0.00.044.076 I print_info: MASK token       = 103 '[MASK]'
0.00.044.076 I print_info: LF token         = 0 '[PAD]'
0.00.044.077 I print_info: max token length = 21
0.00.046.513 I load_tensors: offloading 12 repeating layers to GPU
0.00.046.516 I load_tensors: offloading output layer to GPU
0.00.046.516 I load_tensors: offloaded 13/13 layers to GPU
0.00.046.546 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.046.548 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.046.837 I llama_init_from_model: n_seq_max     = 1
0.00.046.838 I llama_init_from_model: n_ctx         = 512
0.00.046.839 I llama_init_from_model: n_ctx_per_seq = 512
0.00.046.839 I llama_init_from_model: n_batch       = 2048
0.00.046.839 I llama_init_from_model: n_ubatch      = 2048
0.00.046.840 I llama_init_from_model: flash_attn    = 0
0.00.046.840 I llama_init_from_model: freq_base     = 10000.0
0.00.046.840 I llama_init_from_model: freq_scale    = 1
0.00.046.841 I ggml_metal_init: allocating
0.00.046.846 I ggml_metal_init: found device: Apple M4
0.00.046.849 I ggml_metal_init: picking default device: Apple M4
0.00.047.963 I ggml_metal_init: using embedded metal library
0.00.052.535 I ggml_metal_init: GPU name:   Apple M4
0.00.052.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.539 I ggml_metal_init: simdgroup reduction   = true
0.00.052.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.540 I ggml_metal_init: has bfloat            = true
0.00.052.540 I ggml_metal_init: use bfloat            = true
0.00.052.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.227 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.065.827 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.065.830 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.065.831 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.066.563 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.066.565 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.066.565 I llama_init_from_model: graph nodes  = 429
0.00.066.566 I llama_init_from_model: graph splits = 2
0.00.066.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.066.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.072.355 I 
0.00.072.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.073.004 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.505 I llama_perf_context_print:        load time =      47.37 ms
0.00.076.507 I llama_perf_context_print: prompt eval time =       3.36 ms /     9 tokens (    0.37 ms per token,  2677.77 tokens per second)
0.00.076.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.512 I llama_perf_context_print:       total time =       4.15 ms /    10 tokens
0.00.076.648 I ggml_metal_free: deallocating

real	0m0.539s
user	0m0.053s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.114 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.746 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.751 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.752 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.752 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.752 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.753 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.753 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.754 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.754 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.755 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.757 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.758 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.758 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.758 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.758 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.759 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.362 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.090 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.091 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.092 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.092 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.092 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.092 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.093 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.093 I llama_model_loader: - type  f32:  124 tensors
0.00.015.093 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.094 I print_info: file format = GGUF V3 (latest)
0.00.015.094 I print_info: file type   = Q8_0
0.00.015.095 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.526 I load: special tokens cache size = 5
0.00.018.818 I load: token to piece cache size = 0.2032 MB
0.00.018.821 I print_info: arch             = bert
0.00.018.821 I print_info: vocab_only       = 0
0.00.018.822 I print_info: n_ctx_train      = 512
0.00.018.822 I print_info: n_embd           = 384
0.00.018.822 I print_info: n_layer          = 12
0.00.018.825 I print_info: n_head           = 12
0.00.018.826 I print_info: n_head_kv        = 12
0.00.018.826 I print_info: n_rot            = 32
0.00.018.826 I print_info: n_swa            = 0
0.00.018.826 I print_info: n_embd_head_k    = 32
0.00.018.827 I print_info: n_embd_head_v    = 32
0.00.018.827 I print_info: n_gqa            = 1
0.00.018.828 I print_info: n_embd_k_gqa     = 384
0.00.018.828 I print_info: n_embd_v_gqa     = 384
0.00.018.829 I print_info: f_norm_eps       = 1.0e-12
0.00.018.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.830 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.830 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.830 I print_info: f_logit_scale    = 0.0e+00
0.00.018.831 I print_info: n_ff             = 1536
0.00.018.831 I print_info: n_expert         = 0
0.00.018.831 I print_info: n_expert_used    = 0
0.00.018.831 I print_info: causal attn      = 0
0.00.018.832 I print_info: pooling type     = 2
0.00.018.832 I print_info: rope type        = 2
0.00.018.832 I print_info: rope scaling     = linear
0.00.018.832 I print_info: freq_base_train  = 10000.0
0.00.018.833 I print_info: freq_scale_train = 1
0.00.018.833 I print_info: n_ctx_orig_yarn  = 512
0.00.018.833 I print_info: rope_finetuned   = unknown
0.00.018.833 I print_info: ssm_d_conv       = 0
0.00.018.833 I print_info: ssm_d_inner      = 0
0.00.018.833 I print_info: ssm_d_state      = 0
0.00.018.833 I print_info: ssm_dt_rank      = 0
0.00.018.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.835 I print_info: model type       = 33M
0.00.018.836 I print_info: model params     = 33.21 M
0.00.018.836 I print_info: general.name     = Bge Small
0.00.018.836 I print_info: vocab type       = WPM
0.00.018.836 I print_info: n_vocab          = 30522
0.00.018.837 I print_info: n_merges         = 0
0.00.018.837 I print_info: BOS token        = 101 '[CLS]'
0.00.018.837 I print_info: UNK token        = 100 '[UNK]'
0.00.018.837 I print_info: SEP token        = 102 '[SEP]'
0.00.018.837 I print_info: PAD token        = 0 '[PAD]'
0.00.018.837 I print_info: MASK token       = 103 '[MASK]'
0.00.018.838 I print_info: LF token         = 0 '[PAD]'
0.00.018.838 I print_info: max token length = 21
0.00.020.140 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.140 I load_tensors: offloading output layer to GPU
0.00.020.141 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.148 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.150 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.302 I llama_init_from_model: n_seq_max     = 1
0.00.020.303 I llama_init_from_model: n_ctx         = 512
0.00.020.303 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.303 I llama_init_from_model: n_batch       = 2048
0.00.020.303 I llama_init_from_model: n_ubatch      = 2048
0.00.020.303 I llama_init_from_model: flash_attn    = 0
0.00.020.304 I llama_init_from_model: freq_base     = 10000.0
0.00.020.304 I llama_init_from_model: freq_scale    = 1
0.00.020.304 I ggml_metal_init: allocating
0.00.020.308 I ggml_metal_init: found device: Apple M4
0.00.020.311 I ggml_metal_init: picking default device: Apple M4
0.00.020.944 I ggml_metal_init: using embedded metal library
0.00.023.476 I ggml_metal_init: GPU name:   Apple M4
0.00.023.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.478 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.479 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.479 I ggml_metal_init: simdgroup reduction   = true
0.00.023.479 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.479 I ggml_metal_init: has bfloat            = true
0.00.023.480 I ggml_metal_init: use bfloat            = true
0.00.023.480 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.811 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.299 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.302 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.303 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.886 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.887 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.888 I llama_init_from_model: graph nodes  = 429
0.00.034.888 I llama_init_from_model: graph splits = 2
0.00.034.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.679 I 
0.00.038.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.212 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.414 I llama_perf_context_print:        load time =      29.56 ms
0.00.042.415 I llama_perf_context_print: prompt eval time =       3.07 ms /     9 tokens (    0.34 ms per token,  2931.60 tokens per second)
0.00.042.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.417 I llama_perf_context_print:       total time =       3.73 ms /    10 tokens
0.00.042.610 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.210 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.595 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.601 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.602 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.608 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.608 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.610 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.610 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.610 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.611 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.611 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.613 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.614 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.614 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.497 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.498 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.499 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.499 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.499 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.500 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.500 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.500 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.501 I llama_model_loader: - type  f32:   40 tensors
0.00.051.501 I llama_model_loader: - type  f16:   30 tensors
0.00.051.502 I print_info: file format = GGUF V3 (latest)
0.00.051.503 I print_info: file type   = F16
0.00.051.504 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.068.536 W load: empty token at index 5
0.00.073.358 W load: model vocab missing newline token, using special_pad_id instead
0.00.074.747 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.780 I load: special tokens cache size = 5
0.00.343.209 I load: token to piece cache size = 1.5060 MB
0.00.343.222 I print_info: arch             = jina-bert-v2
0.00.343.222 I print_info: vocab_only       = 0
0.00.343.222 I print_info: n_ctx_train      = 8192
0.00.343.222 I print_info: n_embd           = 384
0.00.343.223 I print_info: n_layer          = 4
0.00.343.227 I print_info: n_head           = 12
0.00.343.227 I print_info: n_head_kv        = 12
0.00.343.227 I print_info: n_rot            = 32
0.00.343.227 I print_info: n_swa            = 0
0.00.343.227 I print_info: n_embd_head_k    = 32
0.00.343.228 I print_info: n_embd_head_v    = 32
0.00.343.229 I print_info: n_gqa            = 1
0.00.343.230 I print_info: n_embd_k_gqa     = 384
0.00.343.232 I print_info: n_embd_v_gqa     = 384
0.00.343.233 I print_info: f_norm_eps       = 1.0e-12
0.00.343.233 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.343.233 I print_info: f_clamp_kqv      = 0.0e+00
0.00.343.234 I print_info: f_max_alibi_bias = 8.0e+00
0.00.343.234 I print_info: f_logit_scale    = 0.0e+00
0.00.343.234 I print_info: n_ff             = 1536
0.00.343.234 I print_info: n_expert         = 0
0.00.343.234 I print_info: n_expert_used    = 0
0.00.343.235 I print_info: causal attn      = 0
0.00.343.235 I print_info: pooling type     = -1
0.00.343.235 I print_info: rope type        = -1
0.00.343.235 I print_info: rope scaling     = linear
0.00.343.236 I print_info: freq_base_train  = 10000.0
0.00.343.236 I print_info: freq_scale_train = 1
0.00.343.236 I print_info: n_ctx_orig_yarn  = 8192
0.00.343.236 I print_info: rope_finetuned   = unknown
0.00.343.236 I print_info: ssm_d_conv       = 0
0.00.343.236 I print_info: ssm_d_inner      = 0
0.00.343.236 I print_info: ssm_d_state      = 0
0.00.343.237 I print_info: ssm_dt_rank      = 0
0.00.343.237 I print_info: ssm_dt_b_c_rms   = 0
0.00.343.240 I print_info: model type       = 33M
0.00.343.242 I print_info: model params     = 32.90 M
0.00.343.242 I print_info: general.name     = Jina Bert Implementation
0.00.343.243 I print_info: vocab type       = BPE
0.00.343.243 I print_info: n_vocab          = 61056
0.00.343.243 I print_info: n_merges         = 39382
0.00.343.244 I print_info: BOS token        = 0 '<s>'
0.00.343.245 I print_info: EOS token        = 2 '</s>'
0.00.343.245 I print_info: UNK token        = 3 '<unk>'
0.00.343.246 I print_info: SEP token        = 2 '</s>'
0.00.343.247 I print_info: PAD token        = 1 '<pad>'
0.00.343.247 I print_info: MASK token       = 4 '<mask>'
0.00.343.247 I print_info: EOG token        = 2 '</s>'
0.00.343.247 I print_info: max token length = 45
0.00.344.110 I load_tensors: offloading 4 repeating layers to GPU
0.00.344.110 I load_tensors: offloading output layer to GPU
0.00.344.110 I load_tensors: offloaded 5/5 layers to GPU
0.00.344.130 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.344.131 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.344.337 I llama_init_from_model: n_seq_max     = 1
0.00.344.338 I llama_init_from_model: n_ctx         = 8192
0.00.344.338 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.344.339 I llama_init_from_model: n_batch       = 2048
0.00.344.339 I llama_init_from_model: n_ubatch      = 2048
0.00.344.339 I llama_init_from_model: flash_attn    = 0
0.00.344.339 I llama_init_from_model: freq_base     = 10000.0
0.00.344.340 I llama_init_from_model: freq_scale    = 1
0.00.344.340 I ggml_metal_init: allocating
0.00.344.343 I ggml_metal_init: found device: Apple M4
0.00.344.346 I ggml_metal_init: picking default device: Apple M4
0.00.345.002 I ggml_metal_init: using embedded metal library
0.00.347.671 I ggml_metal_init: GPU name:   Apple M4
0.00.347.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.673 I ggml_metal_init: simdgroup reduction   = true
0.00.347.674 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.674 I ggml_metal_init: has bfloat            = true
0.00.347.674 I ggml_metal_init: use bfloat            = true
0.00.347.675 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.358.044 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.360.552 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.360.555 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.360.559 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.361.087 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.361.088 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.361.089 I llama_init_from_model: graph nodes  = 154
0.00.361.089 I llama_init_from_model: graph splits = 2
0.00.361.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.361.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.315 I 
0.00.373.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.636 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.373.637 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.373.640 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.373.641 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.373.645 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.373.645 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.374.211 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.377.758 I llama_perf_context_print:        load time =     349.30 ms
0.00.377.759 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17603.63 tokens per second)
0.00.377.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.377.760 I llama_perf_context_print:       total time =       4.44 ms /    63 tokens
0.00.377.996 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.351s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.180 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.393 I main: llama backend init
0.00.000.407 I main: load the model and apply lora adapter, if any
0.00.036.312 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.050.668 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.691 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.693 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.694 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.695 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.698 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.061.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.068.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.068.415 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.068.416 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.068.416 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.068.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.068.417 I llama_model_loader: - type  f32:  194 tensors
0.00.068.418 I llama_model_loader: - type  f16:   98 tensors
0.00.068.418 I print_info: file format = GGUF V3 (latest)
0.00.068.420 I print_info: file type   = all F32 (guessed)
0.00.068.421 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.095.520 I load: special tokens cache size = 25
0.00.102.337 I load: token to piece cache size = 0.2984 MB
0.00.102.341 I print_info: arch             = gptneox
0.00.102.341 I print_info: vocab_only       = 0
0.00.102.341 I print_info: n_ctx_train      = 2048
0.00.102.341 I print_info: n_embd           = 2048
0.00.102.341 I print_info: n_layer          = 24
0.00.102.345 I print_info: n_head           = 16
0.00.102.345 I print_info: n_head_kv        = 16
0.00.102.346 I print_info: n_rot            = 32
0.00.102.346 I print_info: n_swa            = 0
0.00.102.346 I print_info: n_embd_head_k    = 128
0.00.102.346 I print_info: n_embd_head_v    = 128
0.00.102.347 I print_info: n_gqa            = 1
0.00.102.347 I print_info: n_embd_k_gqa     = 2048
0.00.102.348 I print_info: n_embd_v_gqa     = 2048
0.00.102.349 I print_info: f_norm_eps       = 1.0e-05
0.00.102.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.349 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.349 I print_info: f_logit_scale    = 0.0e+00
0.00.102.350 I print_info: n_ff             = 8192
0.00.102.350 I print_info: n_expert         = 0
0.00.102.350 I print_info: n_expert_used    = 0
0.00.102.350 I print_info: causal attn      = 1
0.00.102.350 I print_info: pooling type     = 0
0.00.102.350 I print_info: rope type        = 2
0.00.102.351 I print_info: rope scaling     = linear
0.00.102.351 I print_info: freq_base_train  = 10000.0
0.00.102.351 I print_info: freq_scale_train = 1
0.00.102.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.352 I print_info: rope_finetuned   = unknown
0.00.102.352 I print_info: ssm_d_conv       = 0
0.00.102.352 I print_info: ssm_d_inner      = 0
0.00.102.352 I print_info: ssm_d_state      = 0
0.00.102.352 I print_info: ssm_dt_rank      = 0
0.00.102.352 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.353 I print_info: model type       = 1.4B
0.00.102.353 I print_info: model params     = 1.41 B
0.00.102.353 I print_info: general.name     = 1.4B
0.00.102.354 I print_info: vocab type       = BPE
0.00.102.354 I print_info: n_vocab          = 50304
0.00.102.354 I print_info: n_merges         = 50009
0.00.102.354 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.354 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.355 I print_info: LF token         = 128 'Ä'
0.00.102.355 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.355 I print_info: max token length = 1024
0.00.104.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.104.978 I load_tensors: offloading output layer to GPU
0.00.104.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.104.997 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.998 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.105.260 I llama_init_from_model: n_seq_max     = 1
0.00.105.261 I llama_init_from_model: n_ctx         = 2048
0.00.105.261 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.105.261 I llama_init_from_model: n_batch       = 2048
0.00.105.262 I llama_init_from_model: n_ubatch      = 512
0.00.105.262 I llama_init_from_model: flash_attn    = 0
0.00.105.262 I llama_init_from_model: freq_base     = 10000.0
0.00.105.262 I llama_init_from_model: freq_scale    = 1
0.00.105.263 I ggml_metal_init: allocating
0.00.105.266 I ggml_metal_init: found device: Apple M4
0.00.105.268 I ggml_metal_init: picking default device: Apple M4
0.00.105.906 I ggml_metal_init: using embedded metal library
0.00.117.650 I ggml_metal_init: GPU name:   Apple M4
0.00.117.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.652 I ggml_metal_init: simdgroup reduction   = true
0.00.117.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.652 I ggml_metal_init: has bfloat            = true
0.00.117.653 I ggml_metal_init: use bfloat            = true
0.00.117.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.534 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.161.251 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.161.258 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.161.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.162.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.162.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.162.243 I llama_init_from_model: graph nodes  = 967
0.00.162.244 I llama_init_from_model: graph splits = 2
0.00.162.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.162.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.162.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.240.313 I main: llama threadpool init, n_threads = 4
0.00.240.350 I 
0.00.240.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.240.383 I 
0.00.240.447 I sampler seed: 1234
0.00.240.451 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.240.475 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.240.477 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.240.477 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.072.319 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.02.072.320 I llama_perf_context_print:        load time =     202.98 ms
0.02.072.320 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.30 tokens per second)
0.02.072.322 I llama_perf_context_print:        eval time =    1785.30 ms /    63 runs   (   28.34 ms per token,    35.29 tokens per second)
0.02.072.322 I llama_perf_context_print:       total time =    1833.02 ms /    70 tokens
0.02.072.533 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.143s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.510 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.950 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.961 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.723 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.023 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.027 I llama_model_loader: - type  f32:  194 tensors
0.00.056.028 I llama_model_loader: - type  f16:   98 tensors
0.00.056.029 I print_info: file format = GGUF V3 (latest)
0.00.056.029 I print_info: file type   = all F32 (guessed)
0.00.056.031 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.127 I load: special tokens cache size = 25
0.00.089.663 I load: token to piece cache size = 0.2984 MB
0.00.089.666 I print_info: arch             = gptneox
0.00.089.666 I print_info: vocab_only       = 0
0.00.089.667 I print_info: n_ctx_train      = 2048
0.00.089.667 I print_info: n_embd           = 2048
0.00.089.667 I print_info: n_layer          = 24
0.00.089.670 I print_info: n_head           = 16
0.00.089.671 I print_info: n_head_kv        = 16
0.00.089.671 I print_info: n_rot            = 32
0.00.089.671 I print_info: n_swa            = 0
0.00.089.671 I print_info: n_embd_head_k    = 128
0.00.089.671 I print_info: n_embd_head_v    = 128
0.00.089.672 I print_info: n_gqa            = 1
0.00.089.673 I print_info: n_embd_k_gqa     = 2048
0.00.089.673 I print_info: n_embd_v_gqa     = 2048
0.00.089.674 I print_info: f_norm_eps       = 1.0e-05
0.00.089.674 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.674 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.674 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.675 I print_info: f_logit_scale    = 0.0e+00
0.00.089.675 I print_info: n_ff             = 8192
0.00.089.675 I print_info: n_expert         = 0
0.00.089.676 I print_info: n_expert_used    = 0
0.00.089.676 I print_info: causal attn      = 1
0.00.089.676 I print_info: pooling type     = 0
0.00.089.676 I print_info: rope type        = 2
0.00.089.676 I print_info: rope scaling     = linear
0.00.089.677 I print_info: freq_base_train  = 10000.0
0.00.089.677 I print_info: freq_scale_train = 1
0.00.089.677 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.677 I print_info: rope_finetuned   = unknown
0.00.089.677 I print_info: ssm_d_conv       = 0
0.00.089.677 I print_info: ssm_d_inner      = 0
0.00.089.678 I print_info: ssm_d_state      = 0
0.00.089.680 I print_info: ssm_dt_rank      = 0
0.00.089.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.680 I print_info: model type       = 1.4B
0.00.089.680 I print_info: model params     = 1.41 B
0.00.089.680 I print_info: general.name     = 1.4B
0.00.089.681 I print_info: vocab type       = BPE
0.00.089.681 I print_info: n_vocab          = 50304
0.00.089.681 I print_info: n_merges         = 50009
0.00.089.681 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.682 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.682 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.682 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.686 I print_info: LF token         = 128 'Ä'
0.00.089.686 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.686 I print_info: max token length = 1024
0.00.092.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.252 I load_tensors: offloading output layer to GPU
0.00.092.252 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.262 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.264 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.575 I llama_init_from_model: n_seq_max     = 1
0.00.092.576 I llama_init_from_model: n_ctx         = 128
0.00.092.576 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.576 I llama_init_from_model: n_batch       = 128
0.00.092.577 I llama_init_from_model: n_ubatch      = 128
0.00.092.577 I llama_init_from_model: flash_attn    = 0
0.00.092.577 I llama_init_from_model: freq_base     = 10000.0
0.00.092.577 I llama_init_from_model: freq_scale    = 1
0.00.092.578 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.578 I ggml_metal_init: allocating
0.00.092.581 I ggml_metal_init: found device: Apple M4
0.00.092.583 I ggml_metal_init: picking default device: Apple M4
0.00.093.194 I ggml_metal_init: using embedded metal library
0.00.095.771 I ggml_metal_init: GPU name:   Apple M4
0.00.095.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.774 I ggml_metal_init: simdgroup reduction   = true
0.00.095.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.774 I ggml_metal_init: has bfloat            = true
0.00.095.774 I ggml_metal_init: use bfloat            = true
0.00.095.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.162 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.418 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.420 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.435 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.290 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.291 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.291 I llama_init_from_model: graph nodes  = 967
0.00.107.292 I llama_init_from_model: graph splits = 2
0.00.107.293 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.317.688 I 
0.01.317.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.317.792 I perplexity: tokenizing the input ..
0.01.331.029 I perplexity: tokenization took 13.234 ms
0.01.331.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.452.598 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.454.252 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.454.277 I llama_perf_context_print:        load time =    1293.88 ms
0.01.454.279 I llama_perf_context_print: prompt eval time =     120.60 ms /   128 tokens (    0.94 ms per token,  1061.39 tokens per second)
0.01.454.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.454.281 I llama_perf_context_print:       total time =     136.59 ms /   129 tokens
0.01.454.951 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.124s
sys	0m0.219s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.341 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.957 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.959 I llama_model_loader: - type  f32:  194 tensors
0.00.034.959 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.960 I print_info: file format = GGUF V3 (latest)
0.00.034.963 I print_info: file type   = Q8_0
0.00.034.965 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.542 I load: special tokens cache size = 25
0.00.061.490 I load: token to piece cache size = 0.2984 MB
0.00.061.495 I print_info: arch             = gptneox
0.00.061.495 I print_info: vocab_only       = 0
0.00.061.495 I print_info: n_ctx_train      = 2048
0.00.061.497 I print_info: n_embd           = 2048
0.00.061.498 I print_info: n_layer          = 24
0.00.061.504 I print_info: n_head           = 16
0.00.061.504 I print_info: n_head_kv        = 16
0.00.061.505 I print_info: n_rot            = 32
0.00.061.505 I print_info: n_swa            = 0
0.00.061.505 I print_info: n_embd_head_k    = 128
0.00.061.505 I print_info: n_embd_head_v    = 128
0.00.061.506 I print_info: n_gqa            = 1
0.00.061.507 I print_info: n_embd_k_gqa     = 2048
0.00.061.507 I print_info: n_embd_v_gqa     = 2048
0.00.061.508 I print_info: f_norm_eps       = 1.0e-05
0.00.061.509 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.509 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.509 I print_info: f_logit_scale    = 0.0e+00
0.00.061.510 I print_info: n_ff             = 8192
0.00.061.510 I print_info: n_expert         = 0
0.00.061.511 I print_info: n_expert_used    = 0
0.00.061.511 I print_info: causal attn      = 1
0.00.061.511 I print_info: pooling type     = 0
0.00.061.511 I print_info: rope type        = 2
0.00.061.511 I print_info: rope scaling     = linear
0.00.061.512 I print_info: freq_base_train  = 10000.0
0.00.061.512 I print_info: freq_scale_train = 1
0.00.061.513 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.513 I print_info: rope_finetuned   = unknown
0.00.061.513 I print_info: ssm_d_conv       = 0
0.00.061.513 I print_info: ssm_d_inner      = 0
0.00.061.513 I print_info: ssm_d_state      = 0
0.00.061.515 I print_info: ssm_dt_rank      = 0
0.00.061.515 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.516 I print_info: model type       = 1.4B
0.00.061.516 I print_info: model params     = 1.41 B
0.00.061.516 I print_info: general.name     = 1.4B
0.00.061.517 I print_info: vocab type       = BPE
0.00.061.517 I print_info: n_vocab          = 50304
0.00.061.517 I print_info: n_merges         = 50009
0.00.061.517 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.518 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.518 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.518 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.518 I print_info: LF token         = 128 'Ä'
0.00.061.519 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.519 I print_info: max token length = 1024
0.00.063.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.954 I load_tensors: offloading output layer to GPU
0.00.063.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.966 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.967 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.303 I llama_init_from_model: n_seq_max     = 1
0.00.064.303 I llama_init_from_model: n_ctx         = 2048
0.00.064.303 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.303 I llama_init_from_model: n_batch       = 2048
0.00.064.304 I llama_init_from_model: n_ubatch      = 512
0.00.064.304 I llama_init_from_model: flash_attn    = 0
0.00.064.304 I llama_init_from_model: freq_base     = 10000.0
0.00.064.304 I llama_init_from_model: freq_scale    = 1
0.00.064.305 I ggml_metal_init: allocating
0.00.064.308 I ggml_metal_init: found device: Apple M4
0.00.064.310 I ggml_metal_init: picking default device: Apple M4
0.00.065.064 I ggml_metal_init: using embedded metal library
0.00.067.674 I ggml_metal_init: GPU name:   Apple M4
0.00.067.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.677 I ggml_metal_init: simdgroup reduction   = true
0.00.067.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.677 I ggml_metal_init: has bfloat            = true
0.00.067.677 I ggml_metal_init: use bfloat            = true
0.00.067.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.388 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.398 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.723 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.103.725 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.103.726 I llama_init_from_model: graph nodes  = 967
0.00.103.726 I llama_init_from_model: graph splits = 2
0.00.103.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.351.590 I main: llama threadpool init, n_threads = 4
0.01.351.632 I 
0.01.351.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.351.651 I 
0.01.351.864 I sampler seed: 1234
0.01.351.868 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.351.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.351.916 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.351.916 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.436.726 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.02.436.727 I llama_perf_context_print:        load time =    1340.87 ms
0.02.436.728 I llama_perf_context_print: prompt eval time =      42.44 ms /     7 tokens (    6.06 ms per token,   164.93 tokens per second)
0.02.436.729 I llama_perf_context_print:        eval time =    1039.49 ms /    63 runs   (   16.50 ms per token,    60.61 tokens per second)
0.02.436.730 I llama_perf_context_print:       total time =    1086.00 ms /    70 tokens
0.02.436.974 I ggml_metal_free: deallocating

real	0m2.458s
user	0m0.114s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.670 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.578 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.491 I llama_model_loader: - type  f32:  194 tensors
0.00.033.492 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.493 I print_info: file format = GGUF V3 (latest)
0.00.033.495 I print_info: file type   = Q8_0
0.00.033.496 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.843 I load: special tokens cache size = 25
0.00.062.997 I load: token to piece cache size = 0.2984 MB
0.00.063.000 I print_info: arch             = gptneox
0.00.063.000 I print_info: vocab_only       = 0
0.00.063.000 I print_info: n_ctx_train      = 2048
0.00.063.000 I print_info: n_embd           = 2048
0.00.063.000 I print_info: n_layer          = 24
0.00.063.006 I print_info: n_head           = 16
0.00.063.007 I print_info: n_head_kv        = 16
0.00.063.007 I print_info: n_rot            = 32
0.00.063.007 I print_info: n_swa            = 0
0.00.063.007 I print_info: n_embd_head_k    = 128
0.00.063.007 I print_info: n_embd_head_v    = 128
0.00.063.008 I print_info: n_gqa            = 1
0.00.063.009 I print_info: n_embd_k_gqa     = 2048
0.00.063.009 I print_info: n_embd_v_gqa     = 2048
0.00.063.010 I print_info: f_norm_eps       = 1.0e-05
0.00.063.011 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.011 I print_info: f_logit_scale    = 0.0e+00
0.00.063.012 I print_info: n_ff             = 8192
0.00.063.012 I print_info: n_expert         = 0
0.00.063.012 I print_info: n_expert_used    = 0
0.00.063.012 I print_info: causal attn      = 1
0.00.063.012 I print_info: pooling type     = 0
0.00.063.012 I print_info: rope type        = 2
0.00.063.019 I print_info: rope scaling     = linear
0.00.063.022 I print_info: freq_base_train  = 10000.0
0.00.063.022 I print_info: freq_scale_train = 1
0.00.063.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.023 I print_info: rope_finetuned   = unknown
0.00.063.023 I print_info: ssm_d_conv       = 0
0.00.063.023 I print_info: ssm_d_inner      = 0
0.00.063.023 I print_info: ssm_d_state      = 0
0.00.063.023 I print_info: ssm_dt_rank      = 0
0.00.063.023 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.024 I print_info: model type       = 1.4B
0.00.063.026 I print_info: model params     = 1.41 B
0.00.063.026 I print_info: general.name     = 1.4B
0.00.063.026 I print_info: vocab type       = BPE
0.00.063.027 I print_info: n_vocab          = 50304
0.00.063.027 I print_info: n_merges         = 50009
0.00.063.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.027 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.027 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.028 I print_info: LF token         = 128 'Ä'
0.00.063.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.028 I print_info: max token length = 1024
0.00.065.021 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.022 I load_tensors: offloading output layer to GPU
0.00.065.022 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.028 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.029 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.322 I llama_init_from_model: n_seq_max     = 1
0.00.065.323 I llama_init_from_model: n_ctx         = 128
0.00.065.323 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.323 I llama_init_from_model: n_batch       = 128
0.00.065.323 I llama_init_from_model: n_ubatch      = 128
0.00.065.323 I llama_init_from_model: flash_attn    = 0
0.00.065.323 I llama_init_from_model: freq_base     = 10000.0
0.00.065.324 I llama_init_from_model: freq_scale    = 1
0.00.065.324 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.325 I ggml_metal_init: allocating
0.00.065.327 I ggml_metal_init: found device: Apple M4
0.00.065.329 I ggml_metal_init: picking default device: Apple M4
0.00.065.958 I ggml_metal_init: using embedded metal library
0.00.068.609 I ggml_metal_init: GPU name:   Apple M4
0.00.068.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.612 I ggml_metal_init: simdgroup reduction   = true
0.00.068.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.612 I ggml_metal_init: has bfloat            = true
0.00.068.613 I ggml_metal_init: use bfloat            = true
0.00.068.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.780 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.185 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.190 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.199 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.200 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.200 I llama_init_from_model: graph nodes  = 967
0.00.081.200 I llama_init_from_model: graph splits = 2
0.00.081.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.850.716 I 
0.00.850.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.850.781 I perplexity: tokenizing the input ..
0.00.858.883 I perplexity: tokenization took 8.1 ms
0.00.858.893 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.983.654 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.984.830 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.984.847 I llama_perf_context_print:        load time =     839.04 ms
0.00.984.850 I llama_perf_context_print: prompt eval time =     124.50 ms /   128 tokens (    0.97 ms per token,  1028.10 tokens per second)
0.00.984.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.984.855 I llama_perf_context_print:       total time =     134.13 ms /   129 tokens
0.00.985.386 I ggml_metal_free: deallocating

real	0m1.004s
user	0m0.092s
sys	0m0.146s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.324 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.027 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.038 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.039 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.039 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.042 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.042 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.043 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.043 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.046 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.046 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.018 I llama_model_loader: - type  f32:  194 tensors
0.00.028.018 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.019 I print_info: file format = GGUF V3 (latest)
0.00.028.024 I print_info: file type   = Q4_0
0.00.028.025 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.006 I load: special tokens cache size = 25
0.00.053.123 I load: token to piece cache size = 0.2984 MB
0.00.053.126 I print_info: arch             = gptneox
0.00.053.126 I print_info: vocab_only       = 0
0.00.053.126 I print_info: n_ctx_train      = 2048
0.00.053.126 I print_info: n_embd           = 2048
0.00.053.127 I print_info: n_layer          = 24
0.00.053.131 I print_info: n_head           = 16
0.00.053.132 I print_info: n_head_kv        = 16
0.00.053.132 I print_info: n_rot            = 32
0.00.053.132 I print_info: n_swa            = 0
0.00.053.132 I print_info: n_embd_head_k    = 128
0.00.053.132 I print_info: n_embd_head_v    = 128
0.00.053.133 I print_info: n_gqa            = 1
0.00.053.134 I print_info: n_embd_k_gqa     = 2048
0.00.053.134 I print_info: n_embd_v_gqa     = 2048
0.00.053.135 I print_info: f_norm_eps       = 1.0e-05
0.00.053.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.136 I print_info: f_logit_scale    = 0.0e+00
0.00.053.137 I print_info: n_ff             = 8192
0.00.053.139 I print_info: n_expert         = 0
0.00.053.139 I print_info: n_expert_used    = 0
0.00.053.139 I print_info: causal attn      = 1
0.00.053.140 I print_info: pooling type     = 0
0.00.053.140 I print_info: rope type        = 2
0.00.053.140 I print_info: rope scaling     = linear
0.00.053.142 I print_info: freq_base_train  = 10000.0
0.00.053.142 I print_info: freq_scale_train = 1
0.00.053.142 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.142 I print_info: rope_finetuned   = unknown
0.00.053.143 I print_info: ssm_d_conv       = 0
0.00.053.143 I print_info: ssm_d_inner      = 0
0.00.053.143 I print_info: ssm_d_state      = 0
0.00.053.143 I print_info: ssm_dt_rank      = 0
0.00.053.143 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.143 I print_info: model type       = 1.4B
0.00.053.144 I print_info: model params     = 1.41 B
0.00.053.144 I print_info: general.name     = 1.4B
0.00.053.145 I print_info: vocab type       = BPE
0.00.053.145 I print_info: n_vocab          = 50304
0.00.053.145 I print_info: n_merges         = 50009
0.00.053.145 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.145 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.146 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.146 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.146 I print_info: LF token         = 128 'Ä'
0.00.053.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.147 I print_info: max token length = 1024
0.00.055.471 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.471 I load_tensors: offloading output layer to GPU
0.00.055.471 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.483 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.484 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.828 I llama_init_from_model: n_seq_max     = 1
0.00.055.828 I llama_init_from_model: n_ctx         = 2048
0.00.055.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.829 I llama_init_from_model: n_batch       = 2048
0.00.055.829 I llama_init_from_model: n_ubatch      = 512
0.00.055.829 I llama_init_from_model: flash_attn    = 0
0.00.055.829 I llama_init_from_model: freq_base     = 10000.0
0.00.055.830 I llama_init_from_model: freq_scale    = 1
0.00.055.830 I ggml_metal_init: allocating
0.00.055.833 I ggml_metal_init: found device: Apple M4
0.00.055.835 I ggml_metal_init: picking default device: Apple M4
0.00.056.591 I ggml_metal_init: using embedded metal library
0.00.059.176 I ggml_metal_init: GPU name:   Apple M4
0.00.059.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.178 I ggml_metal_init: simdgroup reduction   = true
0.00.059.179 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.179 I ggml_metal_init: has bfloat            = true
0.00.059.179 I ggml_metal_init: use bfloat            = true
0.00.059.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.794 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.137 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.152 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.094.351 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.094.354 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.094.354 I llama_init_from_model: graph nodes  = 967
0.00.094.354 I llama_init_from_model: graph splits = 2
0.00.094.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.382 I main: llama threadpool init, n_threads = 4
0.00.677.419 I 
0.00.677.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.442 I 
0.00.677.661 I sampler seed: 1234
0.00.677.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.707 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.354.156 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.354.157 I llama_perf_context_print:        load time =     665.20 ms
0.01.354.158 I llama_perf_context_print: prompt eval time =      39.79 ms /     7 tokens (    5.68 ms per token,   175.95 tokens per second)
0.01.354.158 I llama_perf_context_print:        eval time =     633.69 ms /    63 runs   (   10.06 ms per token,    99.42 tokens per second)
0.01.354.159 I llama_perf_context_print:       total time =     677.62 ms /    70 tokens
0.01.354.361 I ggml_metal_free: deallocating

real	0m1.373s
user	0m0.110s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.634 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.724 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.724 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.720 I llama_model_loader: - type  f32:  194 tensors
0.00.025.720 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.721 I print_info: file format = GGUF V3 (latest)
0.00.025.721 I print_info: file type   = Q4_0
0.00.025.722 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.323 I load: special tokens cache size = 25
0.00.050.199 I load: token to piece cache size = 0.2984 MB
0.00.050.201 I print_info: arch             = gptneox
0.00.050.202 I print_info: vocab_only       = 0
0.00.050.202 I print_info: n_ctx_train      = 2048
0.00.050.202 I print_info: n_embd           = 2048
0.00.050.202 I print_info: n_layer          = 24
0.00.050.205 I print_info: n_head           = 16
0.00.050.206 I print_info: n_head_kv        = 16
0.00.050.208 I print_info: n_rot            = 32
0.00.050.208 I print_info: n_swa            = 0
0.00.050.209 I print_info: n_embd_head_k    = 128
0.00.050.209 I print_info: n_embd_head_v    = 128
0.00.050.209 I print_info: n_gqa            = 1
0.00.050.210 I print_info: n_embd_k_gqa     = 2048
0.00.050.211 I print_info: n_embd_v_gqa     = 2048
0.00.050.211 I print_info: f_norm_eps       = 1.0e-05
0.00.050.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.212 I print_info: f_logit_scale    = 0.0e+00
0.00.050.213 I print_info: n_ff             = 8192
0.00.050.213 I print_info: n_expert         = 0
0.00.050.213 I print_info: n_expert_used    = 0
0.00.050.213 I print_info: causal attn      = 1
0.00.050.213 I print_info: pooling type     = 0
0.00.050.214 I print_info: rope type        = 2
0.00.050.214 I print_info: rope scaling     = linear
0.00.050.214 I print_info: freq_base_train  = 10000.0
0.00.050.215 I print_info: freq_scale_train = 1
0.00.050.215 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.215 I print_info: rope_finetuned   = unknown
0.00.050.215 I print_info: ssm_d_conv       = 0
0.00.050.215 I print_info: ssm_d_inner      = 0
0.00.050.215 I print_info: ssm_d_state      = 0
0.00.050.216 I print_info: ssm_dt_rank      = 0
0.00.050.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.216 I print_info: model type       = 1.4B
0.00.050.216 I print_info: model params     = 1.41 B
0.00.050.216 I print_info: general.name     = 1.4B
0.00.050.217 I print_info: vocab type       = BPE
0.00.050.217 I print_info: n_vocab          = 50304
0.00.050.217 I print_info: n_merges         = 50009
0.00.050.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: LF token         = 128 'Ä'
0.00.050.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.219 I print_info: max token length = 1024
0.00.052.144 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.144 I load_tensors: offloading output layer to GPU
0.00.052.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.155 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.157 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.441 I llama_init_from_model: n_seq_max     = 1
0.00.052.442 I llama_init_from_model: n_ctx         = 128
0.00.052.442 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.442 I llama_init_from_model: n_batch       = 128
0.00.052.442 I llama_init_from_model: n_ubatch      = 128
0.00.052.442 I llama_init_from_model: flash_attn    = 0
0.00.052.443 I llama_init_from_model: freq_base     = 10000.0
0.00.052.443 I llama_init_from_model: freq_scale    = 1
0.00.052.443 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.444 I ggml_metal_init: allocating
0.00.052.447 I ggml_metal_init: found device: Apple M4
0.00.052.448 I ggml_metal_init: picking default device: Apple M4
0.00.053.014 I ggml_metal_init: using embedded metal library
0.00.055.325 I ggml_metal_init: GPU name:   Apple M4
0.00.055.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.327 I ggml_metal_init: simdgroup reduction   = true
0.00.055.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.327 I ggml_metal_init: has bfloat            = true
0.00.055.327 I ggml_metal_init: use bfloat            = true
0.00.055.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.236 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.550 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.555 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.414 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.415 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.416 I llama_init_from_model: graph nodes  = 967
0.00.067.416 I llama_init_from_model: graph splits = 2
0.00.067.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.520 I 
0.00.610.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.573 I perplexity: tokenizing the input ..
0.00.618.790 I perplexity: tokenization took 8.214 ms
0.00.618.801 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.891 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.141 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.159 I llama_perf_context_print:        load time =     600.88 ms
0.00.743.160 I llama_perf_context_print: prompt eval time =     122.86 ms /   128 tokens (    0.96 ms per token,  1041.82 tokens per second)
0.00.743.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.161 I llama_perf_context_print:       total time =     132.64 ms /   129 tokens
0.00.743.659 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.077s
sys	0m0.095s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.406 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.409 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.409 I llama_model_loader: - type  f32:  194 tensors
0.00.027.410 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.410 I print_info: file format = GGUF V3 (latest)
0.00.027.411 I print_info: file type   = Q4_1
0.00.027.415 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.305 I load: special tokens cache size = 25
0.00.052.594 I load: token to piece cache size = 0.2984 MB
0.00.052.597 I print_info: arch             = gptneox
0.00.052.597 I print_info: vocab_only       = 0
0.00.052.597 I print_info: n_ctx_train      = 2048
0.00.052.598 I print_info: n_embd           = 2048
0.00.052.598 I print_info: n_layer          = 24
0.00.052.600 I print_info: n_head           = 16
0.00.052.601 I print_info: n_head_kv        = 16
0.00.052.601 I print_info: n_rot            = 32
0.00.052.601 I print_info: n_swa            = 0
0.00.052.602 I print_info: n_embd_head_k    = 128
0.00.052.602 I print_info: n_embd_head_v    = 128
0.00.052.603 I print_info: n_gqa            = 1
0.00.052.603 I print_info: n_embd_k_gqa     = 2048
0.00.052.604 I print_info: n_embd_v_gqa     = 2048
0.00.052.606 I print_info: f_norm_eps       = 1.0e-05
0.00.052.607 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.607 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.607 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.607 I print_info: f_logit_scale    = 0.0e+00
0.00.052.608 I print_info: n_ff             = 8192
0.00.052.608 I print_info: n_expert         = 0
0.00.052.608 I print_info: n_expert_used    = 0
0.00.052.608 I print_info: causal attn      = 1
0.00.052.608 I print_info: pooling type     = 0
0.00.052.609 I print_info: rope type        = 2
0.00.052.609 I print_info: rope scaling     = linear
0.00.052.610 I print_info: freq_base_train  = 10000.0
0.00.052.611 I print_info: freq_scale_train = 1
0.00.052.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.611 I print_info: rope_finetuned   = unknown
0.00.052.611 I print_info: ssm_d_conv       = 0
0.00.052.613 I print_info: ssm_d_inner      = 0
0.00.052.613 I print_info: ssm_d_state      = 0
0.00.052.613 I print_info: ssm_dt_rank      = 0
0.00.052.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.613 I print_info: model type       = 1.4B
0.00.052.614 I print_info: model params     = 1.41 B
0.00.052.614 I print_info: general.name     = 1.4B
0.00.052.614 I print_info: vocab type       = BPE
0.00.052.615 I print_info: n_vocab          = 50304
0.00.052.615 I print_info: n_merges         = 50009
0.00.052.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.616 I print_info: LF token         = 128 'Ä'
0.00.052.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.616 I print_info: max token length = 1024
0.00.054.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.581 I load_tensors: offloading output layer to GPU
0.00.054.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.592 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.594 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.865 I llama_init_from_model: n_seq_max     = 1
0.00.054.866 I llama_init_from_model: n_ctx         = 2048
0.00.054.866 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.866 I llama_init_from_model: n_batch       = 2048
0.00.054.867 I llama_init_from_model: n_ubatch      = 512
0.00.054.867 I llama_init_from_model: flash_attn    = 0
0.00.054.867 I llama_init_from_model: freq_base     = 10000.0
0.00.054.868 I llama_init_from_model: freq_scale    = 1
0.00.054.868 I ggml_metal_init: allocating
0.00.054.871 I ggml_metal_init: found device: Apple M4
0.00.054.873 I ggml_metal_init: picking default device: Apple M4
0.00.055.466 I ggml_metal_init: using embedded metal library
0.00.057.883 I ggml_metal_init: GPU name:   Apple M4
0.00.057.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.885 I ggml_metal_init: simdgroup reduction   = true
0.00.057.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.886 I ggml_metal_init: has bfloat            = true
0.00.057.886 I ggml_metal_init: use bfloat            = true
0.00.057.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.548 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.993 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.013 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.965 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.966 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.967 I llama_init_from_model: graph nodes  = 967
0.00.087.967 I llama_init_from_model: graph splits = 2
0.00.087.970 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.284 I main: llama threadpool init, n_threads = 4
0.00.703.328 I 
0.00.703.349 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.349 I 
0.00.703.560 I sampler seed: 1234
0.00.703.567 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.612 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.615 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.615 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.433.454 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.433.455 I llama_perf_context_print:        load time =     693.55 ms
0.01.433.456 I llama_perf_context_print: prompt eval time =      43.48 ms /     7 tokens (    6.21 ms per token,   160.98 tokens per second)
0.01.433.457 I llama_perf_context_print:        eval time =     683.62 ms /    63 runs   (   10.85 ms per token,    92.16 tokens per second)
0.01.433.458 I llama_perf_context_print:       total time =     731.04 ms /    70 tokens
0.01.433.723 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.127 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.143 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.143 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.143 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.144 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.098 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.958 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.959 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.959 I print_info: file format = GGUF V3 (latest)
0.00.024.960 I print_info: file type   = Q4_1
0.00.024.964 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.765 I load: special tokens cache size = 25
0.00.049.778 I load: token to piece cache size = 0.2984 MB
0.00.049.781 I print_info: arch             = gptneox
0.00.049.781 I print_info: vocab_only       = 0
0.00.049.781 I print_info: n_ctx_train      = 2048
0.00.049.782 I print_info: n_embd           = 2048
0.00.049.782 I print_info: n_layer          = 24
0.00.049.785 I print_info: n_head           = 16
0.00.049.785 I print_info: n_head_kv        = 16
0.00.049.785 I print_info: n_rot            = 32
0.00.049.786 I print_info: n_swa            = 0
0.00.049.786 I print_info: n_embd_head_k    = 128
0.00.049.788 I print_info: n_embd_head_v    = 128
0.00.049.789 I print_info: n_gqa            = 1
0.00.049.789 I print_info: n_embd_k_gqa     = 2048
0.00.049.790 I print_info: n_embd_v_gqa     = 2048
0.00.049.791 I print_info: f_norm_eps       = 1.0e-05
0.00.049.791 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.791 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.791 I print_info: f_logit_scale    = 0.0e+00
0.00.049.792 I print_info: n_ff             = 8192
0.00.049.792 I print_info: n_expert         = 0
0.00.049.792 I print_info: n_expert_used    = 0
0.00.049.793 I print_info: causal attn      = 1
0.00.049.793 I print_info: pooling type     = 0
0.00.049.793 I print_info: rope type        = 2
0.00.049.793 I print_info: rope scaling     = linear
0.00.049.800 I print_info: freq_base_train  = 10000.0
0.00.049.801 I print_info: freq_scale_train = 1
0.00.049.802 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.802 I print_info: rope_finetuned   = unknown
0.00.049.802 I print_info: ssm_d_conv       = 0
0.00.049.802 I print_info: ssm_d_inner      = 0
0.00.049.802 I print_info: ssm_d_state      = 0
0.00.049.804 I print_info: ssm_dt_rank      = 0
0.00.049.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.804 I print_info: model type       = 1.4B
0.00.049.805 I print_info: model params     = 1.41 B
0.00.049.805 I print_info: general.name     = 1.4B
0.00.049.806 I print_info: vocab type       = BPE
0.00.049.806 I print_info: n_vocab          = 50304
0.00.049.807 I print_info: n_merges         = 50009
0.00.049.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.807 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.808 I print_info: LF token         = 128 'Ä'
0.00.049.808 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.808 I print_info: max token length = 1024
0.00.051.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.768 I load_tensors: offloading output layer to GPU
0.00.051.768 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.779 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.780 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.052 I llama_init_from_model: n_seq_max     = 1
0.00.052.052 I llama_init_from_model: n_ctx         = 128
0.00.052.053 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.053 I llama_init_from_model: n_batch       = 128
0.00.052.053 I llama_init_from_model: n_ubatch      = 128
0.00.052.053 I llama_init_from_model: flash_attn    = 0
0.00.052.054 I llama_init_from_model: freq_base     = 10000.0
0.00.052.054 I llama_init_from_model: freq_scale    = 1
0.00.052.054 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.055 I ggml_metal_init: allocating
0.00.052.058 I ggml_metal_init: found device: Apple M4
0.00.052.060 I ggml_metal_init: picking default device: Apple M4
0.00.052.675 I ggml_metal_init: using embedded metal library
0.00.055.070 I ggml_metal_init: GPU name:   Apple M4
0.00.055.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.073 I ggml_metal_init: simdgroup reduction   = true
0.00.055.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.073 I ggml_metal_init: has bfloat            = true
0.00.055.073 I ggml_metal_init: use bfloat            = true
0.00.055.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.705 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.020 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.023 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.931 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.932 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.933 I llama_init_from_model: graph nodes  = 967
0.00.066.933 I llama_init_from_model: graph splits = 2
0.00.066.934 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.934 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.404 I 
0.00.655.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.459 I perplexity: tokenizing the input ..
0.00.663.627 I perplexity: tokenization took 8.166 ms
0.00.663.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.600 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.787.792 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.787.813 I llama_perf_context_print:        load time =     646.60 ms
0.00.787.814 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.91 tokens per second)
0.00.787.815 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.815 I llama_perf_context_print:       total time =     132.41 ms /   129 tokens
0.00.788.355 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.078s
sys	0m0.096s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.820 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.621 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.623 I llama_model_loader: - type  f32:  194 tensors
0.00.028.623 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.623 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.624 I print_info: file format = GGUF V3 (latest)
0.00.028.624 I print_info: file type   = Q5_0
0.00.028.627 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.094 I load: special tokens cache size = 25
0.00.054.335 I load: token to piece cache size = 0.2984 MB
0.00.054.338 I print_info: arch             = gptneox
0.00.054.338 I print_info: vocab_only       = 0
0.00.054.339 I print_info: n_ctx_train      = 2048
0.00.054.339 I print_info: n_embd           = 2048
0.00.054.339 I print_info: n_layer          = 24
0.00.054.342 I print_info: n_head           = 16
0.00.054.343 I print_info: n_head_kv        = 16
0.00.054.343 I print_info: n_rot            = 32
0.00.054.343 I print_info: n_swa            = 0
0.00.054.345 I print_info: n_embd_head_k    = 128
0.00.054.345 I print_info: n_embd_head_v    = 128
0.00.054.346 I print_info: n_gqa            = 1
0.00.054.347 I print_info: n_embd_k_gqa     = 2048
0.00.054.347 I print_info: n_embd_v_gqa     = 2048
0.00.054.348 I print_info: f_norm_eps       = 1.0e-05
0.00.054.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.349 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.349 I print_info: f_logit_scale    = 0.0e+00
0.00.054.349 I print_info: n_ff             = 8192
0.00.054.350 I print_info: n_expert         = 0
0.00.054.350 I print_info: n_expert_used    = 0
0.00.054.350 I print_info: causal attn      = 1
0.00.054.350 I print_info: pooling type     = 0
0.00.054.351 I print_info: rope type        = 2
0.00.054.353 I print_info: rope scaling     = linear
0.00.054.353 I print_info: freq_base_train  = 10000.0
0.00.054.354 I print_info: freq_scale_train = 1
0.00.054.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.354 I print_info: rope_finetuned   = unknown
0.00.054.354 I print_info: ssm_d_conv       = 0
0.00.054.354 I print_info: ssm_d_inner      = 0
0.00.054.354 I print_info: ssm_d_state      = 0
0.00.054.355 I print_info: ssm_dt_rank      = 0
0.00.054.355 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.355 I print_info: model type       = 1.4B
0.00.054.355 I print_info: model params     = 1.41 B
0.00.054.356 I print_info: general.name     = 1.4B
0.00.054.356 I print_info: vocab type       = BPE
0.00.054.356 I print_info: n_vocab          = 50304
0.00.054.357 I print_info: n_merges         = 50009
0.00.054.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.357 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.357 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.358 I print_info: LF token         = 128 'Ä'
0.00.054.358 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.358 I print_info: max token length = 1024
0.00.056.328 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.328 I load_tensors: offloading output layer to GPU
0.00.056.328 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.339 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.340 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.608 I llama_init_from_model: n_seq_max     = 1
0.00.056.609 I llama_init_from_model: n_ctx         = 2048
0.00.056.609 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.609 I llama_init_from_model: n_batch       = 2048
0.00.056.609 I llama_init_from_model: n_ubatch      = 512
0.00.056.610 I llama_init_from_model: flash_attn    = 0
0.00.056.610 I llama_init_from_model: freq_base     = 10000.0
0.00.056.610 I llama_init_from_model: freq_scale    = 1
0.00.056.611 I ggml_metal_init: allocating
0.00.056.614 I ggml_metal_init: found device: Apple M4
0.00.056.615 I ggml_metal_init: picking default device: Apple M4
0.00.057.218 I ggml_metal_init: using embedded metal library
0.00.059.547 I ggml_metal_init: GPU name:   Apple M4
0.00.059.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.549 I ggml_metal_init: simdgroup reduction   = true
0.00.059.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.550 I ggml_metal_init: has bfloat            = true
0.00.059.550 I ggml_metal_init: use bfloat            = true
0.00.059.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.045 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.926 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.938 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.966 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.967 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.967 I llama_init_from_model: graph nodes  = 967
0.00.089.967 I llama_init_from_model: graph splits = 2
0.00.089.970 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.264 I main: llama threadpool init, n_threads = 4
0.00.732.302 I 
0.00.732.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.323 I 
0.00.732.548 I sampler seed: 1234
0.00.732.552 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.591 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.526.986 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.01.526.987 I llama_perf_context_print:        load time =     719.59 ms
0.01.526.988 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.69 tokens per second)
0.01.526.988 I llama_perf_context_print:        eval time =     744.41 ms /    63 runs   (   11.82 ms per token,    84.63 tokens per second)
0.01.526.989 I llama_perf_context_print:       total time =     795.58 ms /    70 tokens
0.01.527.269 I ggml_metal_free: deallocating

real	0m1.546s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.394 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.394 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.399 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.164 I llama_model_loader: - type  f32:  194 tensors
0.00.026.164 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.164 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.165 I print_info: file format = GGUF V3 (latest)
0.00.026.165 I print_info: file type   = Q5_0
0.00.026.166 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.702 I load: special tokens cache size = 25
0.00.051.969 I load: token to piece cache size = 0.2984 MB
0.00.051.972 I print_info: arch             = gptneox
0.00.051.972 I print_info: vocab_only       = 0
0.00.051.973 I print_info: n_ctx_train      = 2048
0.00.051.973 I print_info: n_embd           = 2048
0.00.051.973 I print_info: n_layer          = 24
0.00.051.976 I print_info: n_head           = 16
0.00.051.977 I print_info: n_head_kv        = 16
0.00.051.977 I print_info: n_rot            = 32
0.00.051.977 I print_info: n_swa            = 0
0.00.051.977 I print_info: n_embd_head_k    = 128
0.00.051.978 I print_info: n_embd_head_v    = 128
0.00.051.978 I print_info: n_gqa            = 1
0.00.051.979 I print_info: n_embd_k_gqa     = 2048
0.00.051.980 I print_info: n_embd_v_gqa     = 2048
0.00.051.980 I print_info: f_norm_eps       = 1.0e-05
0.00.051.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.982 I print_info: f_logit_scale    = 0.0e+00
0.00.051.983 I print_info: n_ff             = 8192
0.00.051.983 I print_info: n_expert         = 0
0.00.051.984 I print_info: n_expert_used    = 0
0.00.051.984 I print_info: causal attn      = 1
0.00.051.984 I print_info: pooling type     = 0
0.00.051.984 I print_info: rope type        = 2
0.00.051.984 I print_info: rope scaling     = linear
0.00.051.985 I print_info: freq_base_train  = 10000.0
0.00.051.985 I print_info: freq_scale_train = 1
0.00.051.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.985 I print_info: rope_finetuned   = unknown
0.00.051.985 I print_info: ssm_d_conv       = 0
0.00.051.986 I print_info: ssm_d_inner      = 0
0.00.051.986 I print_info: ssm_d_state      = 0
0.00.051.986 I print_info: ssm_dt_rank      = 0
0.00.051.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.986 I print_info: model type       = 1.4B
0.00.051.987 I print_info: model params     = 1.41 B
0.00.051.987 I print_info: general.name     = 1.4B
0.00.051.988 I print_info: vocab type       = BPE
0.00.051.988 I print_info: n_vocab          = 50304
0.00.051.988 I print_info: n_merges         = 50009
0.00.051.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.991 I print_info: LF token         = 128 'Ä'
0.00.051.991 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.991 I print_info: max token length = 1024
0.00.054.042 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.043 I load_tensors: offloading output layer to GPU
0.00.054.043 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.053 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.055 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.332 I llama_init_from_model: n_seq_max     = 1
0.00.054.333 I llama_init_from_model: n_ctx         = 128
0.00.054.333 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.333 I llama_init_from_model: n_batch       = 128
0.00.054.334 I llama_init_from_model: n_ubatch      = 128
0.00.054.334 I llama_init_from_model: flash_attn    = 0
0.00.054.334 I llama_init_from_model: freq_base     = 10000.0
0.00.054.334 I llama_init_from_model: freq_scale    = 1
0.00.054.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.335 I ggml_metal_init: allocating
0.00.054.339 I ggml_metal_init: found device: Apple M4
0.00.054.341 I ggml_metal_init: picking default device: Apple M4
0.00.054.929 I ggml_metal_init: using embedded metal library
0.00.057.308 I ggml_metal_init: GPU name:   Apple M4
0.00.057.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.310 I ggml_metal_init: simdgroup reduction   = true
0.00.057.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.311 I ggml_metal_init: has bfloat            = true
0.00.057.311 I ggml_metal_init: use bfloat            = true
0.00.057.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.169 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.468 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.471 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.426 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.428 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.428 I llama_init_from_model: graph nodes  = 967
0.00.069.428 I llama_init_from_model: graph splits = 2
0.00.069.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.030 I 
0.00.672.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.088 I perplexity: tokenizing the input ..
0.00.680.110 I perplexity: tokenization took 8.021 ms
0.00.680.121 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.699 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.816.984 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.816.998 I llama_perf_context_print:        load time =     661.84 ms
0.00.816.998 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.69 tokens per second)
0.00.816.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.999 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.817.359 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.078s
sys	0m0.102s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.835 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.245 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.874 I llama_model_loader: - type  f32:  194 tensors
0.00.025.874 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.875 I print_info: file format = GGUF V3 (latest)
0.00.025.875 I print_info: file type   = Q5_1
0.00.025.876 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.420 I load: special tokens cache size = 25
0.00.051.514 I load: token to piece cache size = 0.2984 MB
0.00.051.517 I print_info: arch             = gptneox
0.00.051.517 I print_info: vocab_only       = 0
0.00.051.517 I print_info: n_ctx_train      = 2048
0.00.051.517 I print_info: n_embd           = 2048
0.00.051.518 I print_info: n_layer          = 24
0.00.051.521 I print_info: n_head           = 16
0.00.051.522 I print_info: n_head_kv        = 16
0.00.051.522 I print_info: n_rot            = 32
0.00.051.522 I print_info: n_swa            = 0
0.00.051.522 I print_info: n_embd_head_k    = 128
0.00.051.522 I print_info: n_embd_head_v    = 128
0.00.051.523 I print_info: n_gqa            = 1
0.00.051.524 I print_info: n_embd_k_gqa     = 2048
0.00.051.524 I print_info: n_embd_v_gqa     = 2048
0.00.051.525 I print_info: f_norm_eps       = 1.0e-05
0.00.051.525 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.525 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.525 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.526 I print_info: f_logit_scale    = 0.0e+00
0.00.051.526 I print_info: n_ff             = 8192
0.00.051.526 I print_info: n_expert         = 0
0.00.051.527 I print_info: n_expert_used    = 0
0.00.051.527 I print_info: causal attn      = 1
0.00.051.527 I print_info: pooling type     = 0
0.00.051.527 I print_info: rope type        = 2
0.00.051.527 I print_info: rope scaling     = linear
0.00.051.528 I print_info: freq_base_train  = 10000.0
0.00.051.528 I print_info: freq_scale_train = 1
0.00.051.528 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.529 I print_info: rope_finetuned   = unknown
0.00.051.529 I print_info: ssm_d_conv       = 0
0.00.051.529 I print_info: ssm_d_inner      = 0
0.00.051.529 I print_info: ssm_d_state      = 0
0.00.051.529 I print_info: ssm_dt_rank      = 0
0.00.051.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.530 I print_info: model type       = 1.4B
0.00.051.530 I print_info: model params     = 1.41 B
0.00.051.530 I print_info: general.name     = 1.4B
0.00.051.531 I print_info: vocab type       = BPE
0.00.051.531 I print_info: n_vocab          = 50304
0.00.051.531 I print_info: n_merges         = 50009
0.00.051.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.532 I print_info: LF token         = 128 'Ä'
0.00.051.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.535 I print_info: max token length = 1024
0.00.053.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.521 I load_tensors: offloading output layer to GPU
0.00.053.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.532 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.533 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.809 I llama_init_from_model: n_seq_max     = 1
0.00.053.810 I llama_init_from_model: n_ctx         = 2048
0.00.053.810 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.810 I llama_init_from_model: n_batch       = 2048
0.00.053.810 I llama_init_from_model: n_ubatch      = 512
0.00.053.810 I llama_init_from_model: flash_attn    = 0
0.00.053.811 I llama_init_from_model: freq_base     = 10000.0
0.00.053.811 I llama_init_from_model: freq_scale    = 1
0.00.053.811 I ggml_metal_init: allocating
0.00.053.814 I ggml_metal_init: found device: Apple M4
0.00.053.816 I ggml_metal_init: picking default device: Apple M4
0.00.054.436 I ggml_metal_init: using embedded metal library
0.00.056.733 I ggml_metal_init: GPU name:   Apple M4
0.00.056.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.735 I ggml_metal_init: simdgroup reduction   = true
0.00.056.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.735 I ggml_metal_init: has bfloat            = true
0.00.056.736 I ggml_metal_init: use bfloat            = true
0.00.056.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.413 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.780 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.785 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.804 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.879 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.881 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.881 I llama_init_from_model: graph nodes  = 967
0.00.086.881 I llama_init_from_model: graph splits = 2
0.00.086.884 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.322 I main: llama threadpool init, n_threads = 4
0.00.714.356 I 
0.00.714.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.404 I 
0.00.714.540 I sampler seed: 1234
0.00.714.545 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.554 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.555 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.557.142 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.557.143 I llama_perf_context_print:        load time =     704.63 ms
0.01.557.144 I llama_perf_context_print: prompt eval time =      45.15 ms /     7 tokens (    6.45 ms per token,   155.04 tokens per second)
0.01.557.145 I llama_perf_context_print:        eval time =     794.44 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.557.146 I llama_perf_context_print:       total time =     843.68 ms /    70 tokens
0.01.557.334 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.110s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.342 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.830 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.831 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.834 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.929 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.930 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.930 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.931 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.931 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.931 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.932 I llama_model_loader: - type  f32:  194 tensors
0.00.027.932 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.932 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.933 I print_info: file format = GGUF V3 (latest)
0.00.027.933 I print_info: file type   = Q5_1
0.00.027.934 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.686 I load: special tokens cache size = 25
0.00.052.640 I load: token to piece cache size = 0.2984 MB
0.00.052.644 I print_info: arch             = gptneox
0.00.052.645 I print_info: vocab_only       = 0
0.00.052.645 I print_info: n_ctx_train      = 2048
0.00.052.645 I print_info: n_embd           = 2048
0.00.052.647 I print_info: n_layer          = 24
0.00.052.650 I print_info: n_head           = 16
0.00.052.651 I print_info: n_head_kv        = 16
0.00.052.651 I print_info: n_rot            = 32
0.00.052.652 I print_info: n_swa            = 0
0.00.052.652 I print_info: n_embd_head_k    = 128
0.00.052.653 I print_info: n_embd_head_v    = 128
0.00.052.654 I print_info: n_gqa            = 1
0.00.052.654 I print_info: n_embd_k_gqa     = 2048
0.00.052.655 I print_info: n_embd_v_gqa     = 2048
0.00.052.656 I print_info: f_norm_eps       = 1.0e-05
0.00.052.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.657 I print_info: f_logit_scale    = 0.0e+00
0.00.052.657 I print_info: n_ff             = 8192
0.00.052.657 I print_info: n_expert         = 0
0.00.052.658 I print_info: n_expert_used    = 0
0.00.052.658 I print_info: causal attn      = 1
0.00.052.658 I print_info: pooling type     = 0
0.00.052.658 I print_info: rope type        = 2
0.00.052.658 I print_info: rope scaling     = linear
0.00.052.659 I print_info: freq_base_train  = 10000.0
0.00.052.659 I print_info: freq_scale_train = 1
0.00.052.659 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.659 I print_info: rope_finetuned   = unknown
0.00.052.660 I print_info: ssm_d_conv       = 0
0.00.052.660 I print_info: ssm_d_inner      = 0
0.00.052.660 I print_info: ssm_d_state      = 0
0.00.052.660 I print_info: ssm_dt_rank      = 0
0.00.052.660 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.660 I print_info: model type       = 1.4B
0.00.052.663 I print_info: model params     = 1.41 B
0.00.052.663 I print_info: general.name     = 1.4B
0.00.052.664 I print_info: vocab type       = BPE
0.00.052.664 I print_info: n_vocab          = 50304
0.00.052.664 I print_info: n_merges         = 50009
0.00.052.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.665 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.665 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.665 I print_info: LF token         = 128 'Ä'
0.00.052.665 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.665 I print_info: max token length = 1024
0.00.054.685 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.686 I load_tensors: offloading output layer to GPU
0.00.054.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.696 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.697 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.055.051 I llama_init_from_model: n_seq_max     = 1
0.00.055.051 I llama_init_from_model: n_ctx         = 128
0.00.055.051 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.052 I llama_init_from_model: n_batch       = 128
0.00.055.052 I llama_init_from_model: n_ubatch      = 128
0.00.055.052 I llama_init_from_model: flash_attn    = 0
0.00.055.052 I llama_init_from_model: freq_base     = 10000.0
0.00.055.052 I llama_init_from_model: freq_scale    = 1
0.00.055.053 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.053 I ggml_metal_init: allocating
0.00.055.056 I ggml_metal_init: found device: Apple M4
0.00.055.058 I ggml_metal_init: picking default device: Apple M4
0.00.055.670 I ggml_metal_init: using embedded metal library
0.00.058.055 I ggml_metal_init: GPU name:   Apple M4
0.00.058.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.057 I ggml_metal_init: simdgroup reduction   = true
0.00.058.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.057 I ggml_metal_init: has bfloat            = true
0.00.058.057 I ggml_metal_init: use bfloat            = true
0.00.058.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.058 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.935 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.938 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.910 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.911 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.912 I llama_init_from_model: graph nodes  = 967
0.00.069.912 I llama_init_from_model: graph splits = 2
0.00.069.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.266 I 
0.00.662.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.347 I perplexity: tokenizing the input ..
0.00.670.588 I perplexity: tokenization took 8.239 ms
0.00.670.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.852 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.806.415 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.806.437 I llama_perf_context_print:        load time =     652.91 ms
0.00.806.437 I llama_perf_context_print: prompt eval time =     134.01 ms /   128 tokens (    1.05 ms per token,   955.17 tokens per second)
0.00.806.438 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.438 I llama_perf_context_print:       total time =     144.17 ms /   129 tokens
0.00.806.860 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.079s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.527 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.408 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.408 I llama_model_loader: - type  f32:  194 tensors
0.00.025.408 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.409 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.409 I print_info: file format = GGUF V3 (latest)
0.00.025.410 I print_info: file type   = Q2_K - Medium
0.00.025.411 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.231 I load: special tokens cache size = 25
0.00.050.288 I load: token to piece cache size = 0.2984 MB
0.00.050.290 I print_info: arch             = gptneox
0.00.050.291 I print_info: vocab_only       = 0
0.00.050.291 I print_info: n_ctx_train      = 2048
0.00.050.291 I print_info: n_embd           = 2048
0.00.050.291 I print_info: n_layer          = 24
0.00.050.294 I print_info: n_head           = 16
0.00.050.295 I print_info: n_head_kv        = 16
0.00.050.295 I print_info: n_rot            = 32
0.00.050.295 I print_info: n_swa            = 0
0.00.050.298 I print_info: n_embd_head_k    = 128
0.00.050.298 I print_info: n_embd_head_v    = 128
0.00.050.299 I print_info: n_gqa            = 1
0.00.050.300 I print_info: n_embd_k_gqa     = 2048
0.00.050.301 I print_info: n_embd_v_gqa     = 2048
0.00.050.306 I print_info: f_norm_eps       = 1.0e-05
0.00.050.306 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.307 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.307 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.307 I print_info: f_logit_scale    = 0.0e+00
0.00.050.308 I print_info: n_ff             = 8192
0.00.050.308 I print_info: n_expert         = 0
0.00.050.308 I print_info: n_expert_used    = 0
0.00.050.308 I print_info: causal attn      = 1
0.00.050.309 I print_info: pooling type     = 0
0.00.050.309 I print_info: rope type        = 2
0.00.050.309 I print_info: rope scaling     = linear
0.00.050.309 I print_info: freq_base_train  = 10000.0
0.00.050.310 I print_info: freq_scale_train = 1
0.00.050.310 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.310 I print_info: rope_finetuned   = unknown
0.00.050.310 I print_info: ssm_d_conv       = 0
0.00.050.310 I print_info: ssm_d_inner      = 0
0.00.050.310 I print_info: ssm_d_state      = 0
0.00.050.311 I print_info: ssm_dt_rank      = 0
0.00.050.311 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.313 I print_info: model type       = 1.4B
0.00.050.313 I print_info: model params     = 1.41 B
0.00.050.313 I print_info: general.name     = 1.4B
0.00.050.314 I print_info: vocab type       = BPE
0.00.050.314 I print_info: n_vocab          = 50304
0.00.050.314 I print_info: n_merges         = 50009
0.00.050.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.315 I print_info: LF token         = 128 'Ä'
0.00.050.316 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.316 I print_info: max token length = 1024
0.00.052.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.179 I load_tensors: offloading output layer to GPU
0.00.052.179 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.190 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.191 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.462 I llama_init_from_model: n_seq_max     = 1
0.00.052.463 I llama_init_from_model: n_ctx         = 2048
0.00.052.463 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.463 I llama_init_from_model: n_batch       = 2048
0.00.052.463 I llama_init_from_model: n_ubatch      = 512
0.00.052.464 I llama_init_from_model: flash_attn    = 0
0.00.052.464 I llama_init_from_model: freq_base     = 10000.0
0.00.052.464 I llama_init_from_model: freq_scale    = 1
0.00.052.465 I ggml_metal_init: allocating
0.00.052.468 I ggml_metal_init: found device: Apple M4
0.00.052.470 I ggml_metal_init: picking default device: Apple M4
0.00.053.086 I ggml_metal_init: using embedded metal library
0.00.055.433 I ggml_metal_init: GPU name:   Apple M4
0.00.055.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.436 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.436 I ggml_metal_init: simdgroup reduction   = true
0.00.055.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.436 I ggml_metal_init: has bfloat            = true
0.00.055.436 I ggml_metal_init: use bfloat            = true
0.00.055.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.309 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.320 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.429 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.431 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.432 I llama_init_from_model: graph nodes  = 967
0.00.085.432 I llama_init_from_model: graph splits = 2
0.00.085.435 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.450.876 I main: llama threadpool init, n_threads = 4
0.00.450.912 I 
0.00.450.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.450.936 I 
0.00.451.162 I sampler seed: 1234
0.00.451.166 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.451.177 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.451.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.451.177 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.126.985 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.126.986 I llama_perf_context_print:        load time =     440.08 ms
0.01.126.986 I llama_perf_context_print: prompt eval time =      35.86 ms /     7 tokens (    5.12 ms per token,   195.21 tokens per second)
0.01.126.987 I llama_perf_context_print:        eval time =     636.83 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.126.987 I llama_perf_context_print:       total time =     676.97 ms /    70 tokens
0.01.127.270 I ggml_metal_free: deallocating

real	0m1.145s
user	0m0.109s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.868 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.278 I llama_model_loader: - type  f32:  194 tensors
0.00.026.278 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.278 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.278 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.279 I print_info: file format = GGUF V3 (latest)
0.00.026.280 I print_info: file type   = Q2_K - Medium
0.00.026.283 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.560 I load: special tokens cache size = 25
0.00.052.711 I load: token to piece cache size = 0.2984 MB
0.00.052.717 I print_info: arch             = gptneox
0.00.052.718 I print_info: vocab_only       = 0
0.00.052.718 I print_info: n_ctx_train      = 2048
0.00.052.718 I print_info: n_embd           = 2048
0.00.052.718 I print_info: n_layer          = 24
0.00.052.723 I print_info: n_head           = 16
0.00.052.723 I print_info: n_head_kv        = 16
0.00.052.723 I print_info: n_rot            = 32
0.00.052.724 I print_info: n_swa            = 0
0.00.052.724 I print_info: n_embd_head_k    = 128
0.00.052.724 I print_info: n_embd_head_v    = 128
0.00.052.724 I print_info: n_gqa            = 1
0.00.052.725 I print_info: n_embd_k_gqa     = 2048
0.00.052.726 I print_info: n_embd_v_gqa     = 2048
0.00.052.726 I print_info: f_norm_eps       = 1.0e-05
0.00.052.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.727 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.727 I print_info: f_logit_scale    = 0.0e+00
0.00.052.729 I print_info: n_ff             = 8192
0.00.052.729 I print_info: n_expert         = 0
0.00.052.729 I print_info: n_expert_used    = 0
0.00.052.729 I print_info: causal attn      = 1
0.00.052.729 I print_info: pooling type     = 0
0.00.052.730 I print_info: rope type        = 2
0.00.052.734 I print_info: rope scaling     = linear
0.00.052.734 I print_info: freq_base_train  = 10000.0
0.00.052.735 I print_info: freq_scale_train = 1
0.00.052.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.736 I print_info: rope_finetuned   = unknown
0.00.052.736 I print_info: ssm_d_conv       = 0
0.00.052.736 I print_info: ssm_d_inner      = 0
0.00.052.736 I print_info: ssm_d_state      = 0
0.00.052.736 I print_info: ssm_dt_rank      = 0
0.00.052.737 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.737 I print_info: model type       = 1.4B
0.00.052.737 I print_info: model params     = 1.41 B
0.00.052.737 I print_info: general.name     = 1.4B
0.00.052.738 I print_info: vocab type       = BPE
0.00.052.738 I print_info: n_vocab          = 50304
0.00.052.738 I print_info: n_merges         = 50009
0.00.052.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.740 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.740 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.741 I print_info: LF token         = 128 'Ä'
0.00.052.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.741 I print_info: max token length = 1024
0.00.054.784 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.784 I load_tensors: offloading output layer to GPU
0.00.054.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.795 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.796 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.055.135 I llama_init_from_model: n_seq_max     = 1
0.00.055.136 I llama_init_from_model: n_ctx         = 128
0.00.055.136 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.136 I llama_init_from_model: n_batch       = 128
0.00.055.136 I llama_init_from_model: n_ubatch      = 128
0.00.055.137 I llama_init_from_model: flash_attn    = 0
0.00.055.137 I llama_init_from_model: freq_base     = 10000.0
0.00.055.137 I llama_init_from_model: freq_scale    = 1
0.00.055.138 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.138 I ggml_metal_init: allocating
0.00.055.142 I ggml_metal_init: found device: Apple M4
0.00.055.144 I ggml_metal_init: picking default device: Apple M4
0.00.055.791 I ggml_metal_init: using embedded metal library
0.00.058.472 I ggml_metal_init: GPU name:   Apple M4
0.00.058.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.474 I ggml_metal_init: simdgroup reduction   = true
0.00.058.475 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.475 I ggml_metal_init: has bfloat            = true
0.00.058.475 I ggml_metal_init: use bfloat            = true
0.00.058.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.637 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.127 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.133 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.147 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.011 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.012 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.012 I llama_init_from_model: graph nodes  = 967
0.00.070.012 I llama_init_from_model: graph splits = 2
0.00.070.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.971 I 
0.00.419.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.419.008 I perplexity: tokenizing the input ..
0.00.426.136 I perplexity: tokenization took 7.126 ms
0.00.426.145 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.007 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.559.406 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.559.421 I llama_perf_context_print:        load time =     409.20 ms
0.00.559.422 I llama_perf_context_print: prompt eval time =     131.63 ms /   128 tokens (    1.03 ms per token,   972.40 tokens per second)
0.00.559.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.559.423 I llama_perf_context_print:       total time =     140.45 ms /   129 tokens
0.00.559.794 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.080s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.121 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.529 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.531 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.532 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.532 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.533 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.535 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.368 I llama_model_loader: - type  f32:  194 tensors
0.00.025.368 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.369 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.369 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.369 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.370 I print_info: file format = GGUF V3 (latest)
0.00.025.370 I print_info: file type   = Q3_K - Medium
0.00.025.371 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.190 I load: special tokens cache size = 25
0.00.050.216 I load: token to piece cache size = 0.2984 MB
0.00.050.219 I print_info: arch             = gptneox
0.00.050.220 I print_info: vocab_only       = 0
0.00.050.220 I print_info: n_ctx_train      = 2048
0.00.050.220 I print_info: n_embd           = 2048
0.00.050.220 I print_info: n_layer          = 24
0.00.050.224 I print_info: n_head           = 16
0.00.050.225 I print_info: n_head_kv        = 16
0.00.050.225 I print_info: n_rot            = 32
0.00.050.225 I print_info: n_swa            = 0
0.00.050.225 I print_info: n_embd_head_k    = 128
0.00.050.225 I print_info: n_embd_head_v    = 128
0.00.050.226 I print_info: n_gqa            = 1
0.00.050.227 I print_info: n_embd_k_gqa     = 2048
0.00.050.227 I print_info: n_embd_v_gqa     = 2048
0.00.050.228 I print_info: f_norm_eps       = 1.0e-05
0.00.050.230 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.230 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.231 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.231 I print_info: f_logit_scale    = 0.0e+00
0.00.050.232 I print_info: n_ff             = 8192
0.00.050.232 I print_info: n_expert         = 0
0.00.050.232 I print_info: n_expert_used    = 0
0.00.050.233 I print_info: causal attn      = 1
0.00.050.235 I print_info: pooling type     = 0
0.00.050.235 I print_info: rope type        = 2
0.00.050.235 I print_info: rope scaling     = linear
0.00.050.236 I print_info: freq_base_train  = 10000.0
0.00.050.236 I print_info: freq_scale_train = 1
0.00.050.236 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.236 I print_info: rope_finetuned   = unknown
0.00.050.236 I print_info: ssm_d_conv       = 0
0.00.050.237 I print_info: ssm_d_inner      = 0
0.00.050.237 I print_info: ssm_d_state      = 0
0.00.050.237 I print_info: ssm_dt_rank      = 0
0.00.050.237 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.237 I print_info: model type       = 1.4B
0.00.050.238 I print_info: model params     = 1.41 B
0.00.050.238 I print_info: general.name     = 1.4B
0.00.050.238 I print_info: vocab type       = BPE
0.00.050.239 I print_info: n_vocab          = 50304
0.00.050.239 I print_info: n_merges         = 50009
0.00.050.240 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.241 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.241 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.241 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.241 I print_info: LF token         = 128 'Ä'
0.00.050.241 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.241 I print_info: max token length = 1024
0.00.052.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.117 I load_tensors: offloading output layer to GPU
0.00.052.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.127 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.129 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.407 I llama_init_from_model: n_seq_max     = 1
0.00.052.408 I llama_init_from_model: n_ctx         = 2048
0.00.052.408 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.408 I llama_init_from_model: n_batch       = 2048
0.00.052.408 I llama_init_from_model: n_ubatch      = 512
0.00.052.409 I llama_init_from_model: flash_attn    = 0
0.00.052.409 I llama_init_from_model: freq_base     = 10000.0
0.00.052.409 I llama_init_from_model: freq_scale    = 1
0.00.052.410 I ggml_metal_init: allocating
0.00.052.413 I ggml_metal_init: found device: Apple M4
0.00.052.415 I ggml_metal_init: picking default device: Apple M4
0.00.053.025 I ggml_metal_init: using embedded metal library
0.00.055.342 I ggml_metal_init: GPU name:   Apple M4
0.00.055.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.345 I ggml_metal_init: simdgroup reduction   = true
0.00.055.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.345 I ggml_metal_init: has bfloat            = true
0.00.055.346 I ggml_metal_init: use bfloat            = true
0.00.055.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.054 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.850 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.824 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.825 I llama_init_from_model: graph nodes  = 967
0.00.085.826 I llama_init_from_model: graph splits = 2
0.00.085.829 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.641 I main: llama threadpool init, n_threads = 4
0.00.522.677 I 
0.00.522.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.700 I 
0.00.522.924 I sampler seed: 1234
0.00.522.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.971 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.972 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.972 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.262.900 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62500.00 tokens per second)
0.01.262.902 I llama_perf_context_print:        load time =     512.64 ms
0.01.262.902 I llama_perf_context_print: prompt eval time =      40.54 ms /     7 tokens (    5.79 ms per token,   172.65 tokens per second)
0.01.262.903 I llama_perf_context_print:        eval time =     696.51 ms /    63 runs   (   11.06 ms per token,    90.45 tokens per second)
0.01.262.903 I llama_perf_context_print:       total time =     741.13 ms /    70 tokens
0.01.263.140 I ggml_metal_free: deallocating

real	0m1.279s
user	0m0.110s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.132 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.372 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.455 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.668 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.669 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.671 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.671 I llama_model_loader: - type  f32:  194 tensors
0.00.025.671 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.672 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.672 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.672 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.673 I print_info: file format = GGUF V3 (latest)
0.00.025.673 I print_info: file type   = Q3_K - Medium
0.00.025.674 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.760 I load: special tokens cache size = 25
0.00.051.950 I load: token to piece cache size = 0.2984 MB
0.00.051.956 I print_info: arch             = gptneox
0.00.051.956 I print_info: vocab_only       = 0
0.00.051.956 I print_info: n_ctx_train      = 2048
0.00.051.958 I print_info: n_embd           = 2048
0.00.051.958 I print_info: n_layer          = 24
0.00.051.962 I print_info: n_head           = 16
0.00.051.963 I print_info: n_head_kv        = 16
0.00.051.963 I print_info: n_rot            = 32
0.00.051.964 I print_info: n_swa            = 0
0.00.051.964 I print_info: n_embd_head_k    = 128
0.00.051.964 I print_info: n_embd_head_v    = 128
0.00.051.965 I print_info: n_gqa            = 1
0.00.051.965 I print_info: n_embd_k_gqa     = 2048
0.00.051.966 I print_info: n_embd_v_gqa     = 2048
0.00.051.966 I print_info: f_norm_eps       = 1.0e-05
0.00.051.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.967 I print_info: f_logit_scale    = 0.0e+00
0.00.051.967 I print_info: n_ff             = 8192
0.00.051.968 I print_info: n_expert         = 0
0.00.051.968 I print_info: n_expert_used    = 0
0.00.051.968 I print_info: causal attn      = 1
0.00.051.968 I print_info: pooling type     = 0
0.00.051.968 I print_info: rope type        = 2
0.00.051.969 I print_info: rope scaling     = linear
0.00.051.969 I print_info: freq_base_train  = 10000.0
0.00.051.969 I print_info: freq_scale_train = 1
0.00.051.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.969 I print_info: rope_finetuned   = unknown
0.00.051.969 I print_info: ssm_d_conv       = 0
0.00.051.970 I print_info: ssm_d_inner      = 0
0.00.051.970 I print_info: ssm_d_state      = 0
0.00.051.970 I print_info: ssm_dt_rank      = 0
0.00.051.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.970 I print_info: model type       = 1.4B
0.00.051.970 I print_info: model params     = 1.41 B
0.00.051.971 I print_info: general.name     = 1.4B
0.00.051.971 I print_info: vocab type       = BPE
0.00.051.971 I print_info: n_vocab          = 50304
0.00.051.971 I print_info: n_merges         = 50009
0.00.051.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: LF token         = 128 'Ä'
0.00.051.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: max token length = 1024
0.00.053.800 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.801 I load_tensors: offloading output layer to GPU
0.00.053.801 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.812 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.813 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.098 I llama_init_from_model: n_seq_max     = 1
0.00.054.099 I llama_init_from_model: n_ctx         = 128
0.00.054.100 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.100 I llama_init_from_model: n_batch       = 128
0.00.054.100 I llama_init_from_model: n_ubatch      = 128
0.00.054.100 I llama_init_from_model: flash_attn    = 0
0.00.054.100 I llama_init_from_model: freq_base     = 10000.0
0.00.054.101 I llama_init_from_model: freq_scale    = 1
0.00.054.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.101 I ggml_metal_init: allocating
0.00.054.104 I ggml_metal_init: found device: Apple M4
0.00.054.106 I ggml_metal_init: picking default device: Apple M4
0.00.054.724 I ggml_metal_init: using embedded metal library
0.00.057.182 I ggml_metal_init: GPU name:   Apple M4
0.00.057.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.184 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.184 I ggml_metal_init: simdgroup reduction   = true
0.00.057.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.184 I ggml_metal_init: has bfloat            = true
0.00.057.185 I ggml_metal_init: use bfloat            = true
0.00.057.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.257 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.564 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.566 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.581 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.465 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.466 I llama_init_from_model: graph nodes  = 967
0.00.069.466 I llama_init_from_model: graph splits = 2
0.00.069.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.837 I 
0.00.477.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.881 I perplexity: tokenizing the input ..
0.00.486.243 I perplexity: tokenization took 8.358 ms
0.00.486.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.488 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.860 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.877 I llama_perf_context_print:        load time =     468.70 ms
0.00.618.878 I llama_perf_context_print: prompt eval time =     130.98 ms /   128 tokens (    1.02 ms per token,   977.23 tokens per second)
0.00.618.879 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.879 I llama_perf_context_print:       total time =     141.04 ms /   129 tokens
0.00.619.263 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.080s
sys	0m0.080s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.660 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.665 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.678 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.861 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.855 I llama_model_loader: - type  f32:  194 tensors
0.00.025.856 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.856 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.856 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.857 I print_info: file format = GGUF V3 (latest)
0.00.025.857 I print_info: file type   = Q4_K - Medium
0.00.025.858 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.242 I load: special tokens cache size = 25
0.00.051.121 I load: token to piece cache size = 0.2984 MB
0.00.051.123 I print_info: arch             = gptneox
0.00.051.124 I print_info: vocab_only       = 0
0.00.051.124 I print_info: n_ctx_train      = 2048
0.00.051.124 I print_info: n_embd           = 2048
0.00.051.124 I print_info: n_layer          = 24
0.00.051.127 I print_info: n_head           = 16
0.00.051.128 I print_info: n_head_kv        = 16
0.00.051.128 I print_info: n_rot            = 32
0.00.051.131 I print_info: n_swa            = 0
0.00.051.131 I print_info: n_embd_head_k    = 128
0.00.051.131 I print_info: n_embd_head_v    = 128
0.00.051.132 I print_info: n_gqa            = 1
0.00.051.132 I print_info: n_embd_k_gqa     = 2048
0.00.051.133 I print_info: n_embd_v_gqa     = 2048
0.00.051.133 I print_info: f_norm_eps       = 1.0e-05
0.00.051.134 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.134 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.134 I print_info: f_logit_scale    = 0.0e+00
0.00.051.135 I print_info: n_ff             = 8192
0.00.051.135 I print_info: n_expert         = 0
0.00.051.135 I print_info: n_expert_used    = 0
0.00.051.140 I print_info: causal attn      = 1
0.00.051.141 I print_info: pooling type     = 0
0.00.051.142 I print_info: rope type        = 2
0.00.051.142 I print_info: rope scaling     = linear
0.00.051.143 I print_info: freq_base_train  = 10000.0
0.00.051.143 I print_info: freq_scale_train = 1
0.00.051.143 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.144 I print_info: rope_finetuned   = unknown
0.00.051.144 I print_info: ssm_d_conv       = 0
0.00.051.144 I print_info: ssm_d_inner      = 0
0.00.051.144 I print_info: ssm_d_state      = 0
0.00.051.144 I print_info: ssm_dt_rank      = 0
0.00.051.144 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.147 I print_info: model type       = 1.4B
0.00.051.149 I print_info: model params     = 1.41 B
0.00.051.149 I print_info: general.name     = 1.4B
0.00.051.150 I print_info: vocab type       = BPE
0.00.051.150 I print_info: n_vocab          = 50304
0.00.051.150 I print_info: n_merges         = 50009
0.00.051.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.151 I print_info: LF token         = 128 'Ä'
0.00.051.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.151 I print_info: max token length = 1024
0.00.053.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.127 I load_tensors: offloading output layer to GPU
0.00.053.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.138 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.139 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.424 I llama_init_from_model: n_seq_max     = 1
0.00.053.425 I llama_init_from_model: n_ctx         = 2048
0.00.053.425 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.425 I llama_init_from_model: n_batch       = 2048
0.00.053.425 I llama_init_from_model: n_ubatch      = 512
0.00.053.425 I llama_init_from_model: flash_attn    = 0
0.00.053.426 I llama_init_from_model: freq_base     = 10000.0
0.00.053.426 I llama_init_from_model: freq_scale    = 1
0.00.053.426 I ggml_metal_init: allocating
0.00.053.429 I ggml_metal_init: found device: Apple M4
0.00.053.431 I ggml_metal_init: picking default device: Apple M4
0.00.054.042 I ggml_metal_init: using embedded metal library
0.00.056.416 I ggml_metal_init: GPU name:   Apple M4
0.00.056.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.419 I ggml_metal_init: simdgroup reduction   = true
0.00.056.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.419 I ggml_metal_init: has bfloat            = true
0.00.056.419 I ggml_metal_init: use bfloat            = true
0.00.056.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.346 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.439 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.465 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.469 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.470 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.470 I llama_init_from_model: graph nodes  = 967
0.00.086.470 I llama_init_from_model: graph splits = 2
0.00.086.474 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.754 I main: llama threadpool init, n_threads = 4
0.00.623.788 I 
0.00.623.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.835 I 
0.00.624.061 I sampler seed: 1234
0.00.624.065 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.624.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.624.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.624.075 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.690 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.376.691 I llama_perf_context_print:        load time =     613.95 ms
0.01.376.692 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.60 tokens per second)
0.01.376.693 I llama_perf_context_print:        eval time =     702.50 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.376.693 I llama_perf_context_print:       total time =     753.81 ms /    70 tokens
0.01.376.892 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.111s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.744 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.476 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.336 I llama_model_loader: - type  f32:  194 tensors
0.00.025.337 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.337 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.337 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.338 I print_info: file format = GGUF V3 (latest)
0.00.025.338 I print_info: file type   = Q4_K - Medium
0.00.025.339 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.518 I load: special tokens cache size = 25
0.00.050.677 I load: token to piece cache size = 0.2984 MB
0.00.050.682 I print_info: arch             = gptneox
0.00.050.682 I print_info: vocab_only       = 0
0.00.050.682 I print_info: n_ctx_train      = 2048
0.00.050.682 I print_info: n_embd           = 2048
0.00.050.683 I print_info: n_layer          = 24
0.00.050.687 I print_info: n_head           = 16
0.00.050.688 I print_info: n_head_kv        = 16
0.00.050.688 I print_info: n_rot            = 32
0.00.050.688 I print_info: n_swa            = 0
0.00.050.688 I print_info: n_embd_head_k    = 128
0.00.050.688 I print_info: n_embd_head_v    = 128
0.00.050.689 I print_info: n_gqa            = 1
0.00.050.690 I print_info: n_embd_k_gqa     = 2048
0.00.050.691 I print_info: n_embd_v_gqa     = 2048
0.00.050.691 I print_info: f_norm_eps       = 1.0e-05
0.00.050.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.691 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.694 I print_info: f_logit_scale    = 0.0e+00
0.00.050.694 I print_info: n_ff             = 8192
0.00.050.695 I print_info: n_expert         = 0
0.00.050.695 I print_info: n_expert_used    = 0
0.00.050.695 I print_info: causal attn      = 1
0.00.050.695 I print_info: pooling type     = 0
0.00.050.695 I print_info: rope type        = 2
0.00.050.695 I print_info: rope scaling     = linear
0.00.050.696 I print_info: freq_base_train  = 10000.0
0.00.050.696 I print_info: freq_scale_train = 1
0.00.050.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.696 I print_info: rope_finetuned   = unknown
0.00.050.697 I print_info: ssm_d_conv       = 0
0.00.050.697 I print_info: ssm_d_inner      = 0
0.00.050.697 I print_info: ssm_d_state      = 0
0.00.050.697 I print_info: ssm_dt_rank      = 0
0.00.050.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.697 I print_info: model type       = 1.4B
0.00.050.698 I print_info: model params     = 1.41 B
0.00.050.698 I print_info: general.name     = 1.4B
0.00.050.698 I print_info: vocab type       = BPE
0.00.050.698 I print_info: n_vocab          = 50304
0.00.050.698 I print_info: n_merges         = 50009
0.00.050.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: LF token         = 128 'Ä'
0.00.050.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.700 I print_info: max token length = 1024
0.00.052.641 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.641 I load_tensors: offloading output layer to GPU
0.00.052.641 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.652 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.653 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.936 I llama_init_from_model: n_seq_max     = 1
0.00.052.936 I llama_init_from_model: n_ctx         = 128
0.00.052.936 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.936 I llama_init_from_model: n_batch       = 128
0.00.052.937 I llama_init_from_model: n_ubatch      = 128
0.00.052.937 I llama_init_from_model: flash_attn    = 0
0.00.052.937 I llama_init_from_model: freq_base     = 10000.0
0.00.052.937 I llama_init_from_model: freq_scale    = 1
0.00.052.938 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.938 I ggml_metal_init: allocating
0.00.052.942 I ggml_metal_init: found device: Apple M4
0.00.052.944 I ggml_metal_init: picking default device: Apple M4
0.00.053.538 I ggml_metal_init: using embedded metal library
0.00.055.925 I ggml_metal_init: GPU name:   Apple M4
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.928 I ggml_metal_init: simdgroup reduction   = true
0.00.055.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.928 I ggml_metal_init: has bfloat            = true
0.00.055.928 I ggml_metal_init: use bfloat            = true
0.00.055.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.799 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.801 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.694 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.695 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.695 I llama_init_from_model: graph nodes  = 967
0.00.067.696 I llama_init_from_model: graph splits = 2
0.00.067.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.560.288 I 
0.00.560.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.426 I perplexity: tokenizing the input ..
0.00.568.146 I perplexity: tokenization took 7.717 ms
0.00.568.159 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.479 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.703.728 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.703.746 I llama_perf_context_print:        load time =     550.84 ms
0.00.703.747 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.71 tokens per second)
0.00.703.748 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.750 I llama_perf_context_print:       total time =     143.46 ms /   129 tokens
0.00.704.205 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.078s
sys	0m0.101s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.553 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.022 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.022 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.024 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.025 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.012 I llama_model_loader: - type  f32:  194 tensors
0.00.028.013 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.013 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.013 I print_info: file format = GGUF V3 (latest)
0.00.028.014 I print_info: file type   = Q5_K - Medium
0.00.028.015 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.435 I load: special tokens cache size = 25
0.00.053.502 I load: token to piece cache size = 0.2984 MB
0.00.053.506 I print_info: arch             = gptneox
0.00.053.506 I print_info: vocab_only       = 0
0.00.053.506 I print_info: n_ctx_train      = 2048
0.00.053.506 I print_info: n_embd           = 2048
0.00.053.506 I print_info: n_layer          = 24
0.00.053.510 I print_info: n_head           = 16
0.00.053.510 I print_info: n_head_kv        = 16
0.00.053.511 I print_info: n_rot            = 32
0.00.053.511 I print_info: n_swa            = 0
0.00.053.511 I print_info: n_embd_head_k    = 128
0.00.053.511 I print_info: n_embd_head_v    = 128
0.00.053.512 I print_info: n_gqa            = 1
0.00.053.513 I print_info: n_embd_k_gqa     = 2048
0.00.053.513 I print_info: n_embd_v_gqa     = 2048
0.00.053.514 I print_info: f_norm_eps       = 1.0e-05
0.00.053.514 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.515 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.515 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.515 I print_info: f_logit_scale    = 0.0e+00
0.00.053.517 I print_info: n_ff             = 8192
0.00.053.517 I print_info: n_expert         = 0
0.00.053.517 I print_info: n_expert_used    = 0
0.00.053.517 I print_info: causal attn      = 1
0.00.053.517 I print_info: pooling type     = 0
0.00.053.519 I print_info: rope type        = 2
0.00.053.521 I print_info: rope scaling     = linear
0.00.053.521 I print_info: freq_base_train  = 10000.0
0.00.053.521 I print_info: freq_scale_train = 1
0.00.053.522 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.522 I print_info: rope_finetuned   = unknown
0.00.053.522 I print_info: ssm_d_conv       = 0
0.00.053.522 I print_info: ssm_d_inner      = 0
0.00.053.522 I print_info: ssm_d_state      = 0
0.00.053.522 I print_info: ssm_dt_rank      = 0
0.00.053.523 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.523 I print_info: model type       = 1.4B
0.00.053.527 I print_info: model params     = 1.41 B
0.00.053.527 I print_info: general.name     = 1.4B
0.00.053.528 I print_info: vocab type       = BPE
0.00.053.528 I print_info: n_vocab          = 50304
0.00.053.528 I print_info: n_merges         = 50009
0.00.053.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.529 I print_info: LF token         = 128 'Ä'
0.00.053.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.530 I print_info: max token length = 1024
0.00.055.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.545 I load_tensors: offloading output layer to GPU
0.00.055.545 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.556 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.557 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.840 I llama_init_from_model: n_seq_max     = 1
0.00.055.841 I llama_init_from_model: n_ctx         = 2048
0.00.055.841 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.841 I llama_init_from_model: n_batch       = 2048
0.00.055.841 I llama_init_from_model: n_ubatch      = 512
0.00.055.841 I llama_init_from_model: flash_attn    = 0
0.00.055.842 I llama_init_from_model: freq_base     = 10000.0
0.00.055.842 I llama_init_from_model: freq_scale    = 1
0.00.055.842 I ggml_metal_init: allocating
0.00.055.845 I ggml_metal_init: found device: Apple M4
0.00.055.847 I ggml_metal_init: picking default device: Apple M4
0.00.056.464 I ggml_metal_init: using embedded metal library
0.00.058.851 I ggml_metal_init: GPU name:   Apple M4
0.00.058.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.854 I ggml_metal_init: simdgroup reduction   = true
0.00.058.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.854 I ggml_metal_init: has bfloat            = true
0.00.058.854 I ggml_metal_init: use bfloat            = true
0.00.058.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.770 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.813 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.842 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.891 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.892 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.893 I llama_init_from_model: graph nodes  = 967
0.00.088.893 I llama_init_from_model: graph splits = 2
0.00.088.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.467 I main: llama threadpool init, n_threads = 4
0.00.712.501 I 
0.00.712.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.540 I 
0.00.712.770 I sampler seed: 1234
0.00.712.774 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.800 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.801 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.801 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.554.549 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.554.550 I llama_perf_context_print:        load time =     700.04 ms
0.01.554.551 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.554.551 I llama_perf_context_print:        eval time =     787.28 ms /    63 runs   (   12.50 ms per token,    80.02 tokens per second)
0.01.554.552 I llama_perf_context_print:       total time =     842.95 ms /    70 tokens
0.01.554.744 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.030.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.393 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.393 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.041.383 I llama_model_loader: - type  f32:  194 tensors
0.00.041.383 I llama_model_loader: - type q5_K:   61 tensors
0.00.041.384 I llama_model_loader: - type q6_K:   37 tensors
0.00.041.384 I print_info: file format = GGUF V3 (latest)
0.00.041.385 I print_info: file type   = Q5_K - Medium
0.00.041.386 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.069.691 I load: special tokens cache size = 25
0.00.081.585 I load: token to piece cache size = 0.2984 MB
0.00.081.590 I print_info: arch             = gptneox
0.00.081.590 I print_info: vocab_only       = 0
0.00.081.590 I print_info: n_ctx_train      = 2048
0.00.081.591 I print_info: n_embd           = 2048
0.00.081.591 I print_info: n_layer          = 24
0.00.081.594 I print_info: n_head           = 16
0.00.081.596 I print_info: n_head_kv        = 16
0.00.081.596 I print_info: n_rot            = 32
0.00.081.596 I print_info: n_swa            = 0
0.00.081.596 I print_info: n_embd_head_k    = 128
0.00.081.597 I print_info: n_embd_head_v    = 128
0.00.081.598 I print_info: n_gqa            = 1
0.00.081.599 I print_info: n_embd_k_gqa     = 2048
0.00.081.600 I print_info: n_embd_v_gqa     = 2048
0.00.081.600 I print_info: f_norm_eps       = 1.0e-05
0.00.081.601 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.602 I print_info: f_logit_scale    = 0.0e+00
0.00.081.603 I print_info: n_ff             = 8192
0.00.081.603 I print_info: n_expert         = 0
0.00.081.603 I print_info: n_expert_used    = 0
0.00.081.603 I print_info: causal attn      = 1
0.00.081.603 I print_info: pooling type     = 0
0.00.081.604 I print_info: rope type        = 2
0.00.081.604 I print_info: rope scaling     = linear
0.00.081.607 I print_info: freq_base_train  = 10000.0
0.00.081.608 I print_info: freq_scale_train = 1
0.00.081.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.608 I print_info: rope_finetuned   = unknown
0.00.081.608 I print_info: ssm_d_conv       = 0
0.00.081.609 I print_info: ssm_d_inner      = 0
0.00.081.609 I print_info: ssm_d_state      = 0
0.00.081.609 I print_info: ssm_dt_rank      = 0
0.00.081.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.610 I print_info: model type       = 1.4B
0.00.081.610 I print_info: model params     = 1.41 B
0.00.081.610 I print_info: general.name     = 1.4B
0.00.081.611 I print_info: vocab type       = BPE
0.00.081.611 I print_info: n_vocab          = 50304
0.00.081.611 I print_info: n_merges         = 50009
0.00.081.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.619 I print_info: LF token         = 128 'Ä'
0.00.081.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.619 I print_info: max token length = 1024
0.00.084.555 I load_tensors: offloading 24 repeating layers to GPU
0.00.084.556 I load_tensors: offloading output layer to GPU
0.00.084.556 I load_tensors: offloaded 25/25 layers to GPU
0.00.084.568 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.084.570 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.085.011 I llama_init_from_model: n_seq_max     = 1
0.00.085.013 I llama_init_from_model: n_ctx         = 128
0.00.085.013 I llama_init_from_model: n_ctx_per_seq = 128
0.00.085.013 I llama_init_from_model: n_batch       = 128
0.00.085.013 I llama_init_from_model: n_ubatch      = 128
0.00.085.014 I llama_init_from_model: flash_attn    = 0
0.00.085.014 I llama_init_from_model: freq_base     = 10000.0
0.00.085.015 I llama_init_from_model: freq_scale    = 1
0.00.085.015 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.016 I ggml_metal_init: allocating
0.00.085.020 I ggml_metal_init: found device: Apple M4
0.00.085.023 I ggml_metal_init: picking default device: Apple M4
0.00.085.922 I ggml_metal_init: using embedded metal library
0.00.090.061 I ggml_metal_init: GPU name:   Apple M4
0.00.090.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.065 I ggml_metal_init: simdgroup reduction   = true
0.00.090.066 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.066 I ggml_metal_init: has bfloat            = true
0.00.090.066 I ggml_metal_init: use bfloat            = true
0.00.090.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.564 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.210 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.216 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.412 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.413 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.414 I llama_init_from_model: graph nodes  = 967
0.00.105.414 I llama_init_from_model: graph splits = 2
0.00.105.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.860 I 
0.00.738.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.937 I perplexity: tokenizing the input ..
0.00.752.251 I perplexity: tokenization took 13.311 ms
0.00.752.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.107 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.905.436 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.905.449 I llama_perf_context_print:        load time =     722.99 ms
0.00.905.450 I llama_perf_context_print: prompt eval time =     150.83 ms /   128 tokens (    1.18 ms per token,   848.62 tokens per second)
0.00.905.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.905.451 I llama_perf_context_print:       total time =     166.59 ms /   129 tokens
0.00.905.851 I ggml_metal_free: deallocating

real	0m0.933s
user	0m0.109s
sys	0m0.114s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.610 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.185 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.192 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.192 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.201 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.204 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.191 I llama_model_loader: - type  f32:  194 tensors
0.00.025.191 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.192 I print_info: file format = GGUF V3 (latest)
0.00.025.192 I print_info: file type   = Q6_K
0.00.025.193 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.250 I load: special tokens cache size = 25
0.00.050.191 I load: token to piece cache size = 0.2984 MB
0.00.050.194 I print_info: arch             = gptneox
0.00.050.195 I print_info: vocab_only       = 0
0.00.050.195 I print_info: n_ctx_train      = 2048
0.00.050.195 I print_info: n_embd           = 2048
0.00.050.195 I print_info: n_layer          = 24
0.00.050.198 I print_info: n_head           = 16
0.00.050.199 I print_info: n_head_kv        = 16
0.00.050.199 I print_info: n_rot            = 32
0.00.050.199 I print_info: n_swa            = 0
0.00.050.199 I print_info: n_embd_head_k    = 128
0.00.050.199 I print_info: n_embd_head_v    = 128
0.00.050.200 I print_info: n_gqa            = 1
0.00.050.201 I print_info: n_embd_k_gqa     = 2048
0.00.050.201 I print_info: n_embd_v_gqa     = 2048
0.00.050.203 I print_info: f_norm_eps       = 1.0e-05
0.00.050.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.204 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.204 I print_info: f_logit_scale    = 0.0e+00
0.00.050.205 I print_info: n_ff             = 8192
0.00.050.205 I print_info: n_expert         = 0
0.00.050.205 I print_info: n_expert_used    = 0
0.00.050.205 I print_info: causal attn      = 1
0.00.050.205 I print_info: pooling type     = 0
0.00.050.206 I print_info: rope type        = 2
0.00.050.206 I print_info: rope scaling     = linear
0.00.050.208 I print_info: freq_base_train  = 10000.0
0.00.050.209 I print_info: freq_scale_train = 1
0.00.050.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.209 I print_info: rope_finetuned   = unknown
0.00.050.209 I print_info: ssm_d_conv       = 0
0.00.050.209 I print_info: ssm_d_inner      = 0
0.00.050.209 I print_info: ssm_d_state      = 0
0.00.050.209 I print_info: ssm_dt_rank      = 0
0.00.050.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.210 I print_info: model type       = 1.4B
0.00.050.210 I print_info: model params     = 1.41 B
0.00.050.210 I print_info: general.name     = 1.4B
0.00.050.211 I print_info: vocab type       = BPE
0.00.050.211 I print_info: n_vocab          = 50304
0.00.050.211 I print_info: n_merges         = 50009
0.00.050.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: LF token         = 128 'Ä'
0.00.050.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.213 I print_info: max token length = 1024
0.00.052.219 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.219 I load_tensors: offloading output layer to GPU
0.00.052.219 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.230 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.231 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.510 I llama_init_from_model: n_seq_max     = 1
0.00.052.511 I llama_init_from_model: n_ctx         = 2048
0.00.052.511 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.511 I llama_init_from_model: n_batch       = 2048
0.00.052.511 I llama_init_from_model: n_ubatch      = 512
0.00.052.511 I llama_init_from_model: flash_attn    = 0
0.00.052.512 I llama_init_from_model: freq_base     = 10000.0
0.00.052.512 I llama_init_from_model: freq_scale    = 1
0.00.052.512 I ggml_metal_init: allocating
0.00.052.515 I ggml_metal_init: found device: Apple M4
0.00.052.517 I ggml_metal_init: picking default device: Apple M4
0.00.053.125 I ggml_metal_init: using embedded metal library
0.00.055.450 I ggml_metal_init: GPU name:   Apple M4
0.00.055.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.452 I ggml_metal_init: simdgroup reduction   = true
0.00.055.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.452 I ggml_metal_init: has bfloat            = true
0.00.055.452 I ggml_metal_init: use bfloat            = true
0.00.055.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.132 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.596 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.527 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.528 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.528 I llama_init_from_model: graph nodes  = 967
0.00.085.528 I llama_init_from_model: graph splits = 2
0.00.085.531 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.662 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.959 I main: llama threadpool init, n_threads = 4
0.00.746.992 I 
0.00.747.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.040 I 
0.00.747.254 I sampler seed: 1234
0.00.747.260 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.293 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.296 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.296 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.623.594 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.623.595 I llama_perf_context_print:        load time =     737.48 ms
0.01.623.597 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.623.597 I llama_perf_context_print:        eval time =     819.02 ms /    63 runs   (   13.00 ms per token,    76.92 tokens per second)
0.01.623.598 I llama_perf_context_print:       total time =     877.50 ms /    70 tokens
0.01.623.839 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4533 (1971adf5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.698 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.418 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.423 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.423 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.424 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.546 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.598 I llama_model_loader: - type  f32:  194 tensors
0.00.028.599 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.599 I print_info: file format = GGUF V3 (latest)
0.00.028.600 I print_info: file type   = Q6_K
0.00.028.601 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.048.527 I load: special tokens cache size = 25
0.00.054.719 I load: token to piece cache size = 0.2984 MB
0.00.054.723 I print_info: arch             = gptneox
0.00.054.724 I print_info: vocab_only       = 0
0.00.054.724 I print_info: n_ctx_train      = 2048
0.00.054.724 I print_info: n_embd           = 2048
0.00.054.724 I print_info: n_layer          = 24
0.00.054.728 I print_info: n_head           = 16
0.00.054.728 I print_info: n_head_kv        = 16
0.00.054.729 I print_info: n_rot            = 32
0.00.054.729 I print_info: n_swa            = 0
0.00.054.729 I print_info: n_embd_head_k    = 128
0.00.054.729 I print_info: n_embd_head_v    = 128
0.00.054.730 I print_info: n_gqa            = 1
0.00.054.731 I print_info: n_embd_k_gqa     = 2048
0.00.054.731 I print_info: n_embd_v_gqa     = 2048
0.00.054.732 I print_info: f_norm_eps       = 1.0e-05
0.00.054.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.733 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.733 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.733 I print_info: f_logit_scale    = 0.0e+00
0.00.054.735 I print_info: n_ff             = 8192
0.00.054.735 I print_info: n_expert         = 0
0.00.054.735 I print_info: n_expert_used    = 0
0.00.054.735 I print_info: causal attn      = 1
0.00.054.736 I print_info: pooling type     = 0
0.00.054.736 I print_info: rope type        = 2
0.00.054.736 I print_info: rope scaling     = linear
0.00.054.738 I print_info: freq_base_train  = 10000.0
0.00.054.738 I print_info: freq_scale_train = 1
0.00.054.738 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.739 I print_info: rope_finetuned   = unknown
0.00.054.739 I print_info: ssm_d_conv       = 0
0.00.054.739 I print_info: ssm_d_inner      = 0
0.00.054.739 I print_info: ssm_d_state      = 0
0.00.054.739 I print_info: ssm_dt_rank      = 0
0.00.054.740 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.741 I print_info: model type       = 1.4B
0.00.054.741 I print_info: model params     = 1.41 B
0.00.054.741 I print_info: general.name     = 1.4B
0.00.054.742 I print_info: vocab type       = BPE
0.00.054.743 I print_info: n_vocab          = 50304
0.00.054.743 I print_info: n_merges         = 50009
0.00.054.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.743 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.743 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.745 I print_info: LF token         = 128 'Ä'
0.00.054.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.745 I print_info: max token length = 1024
0.00.056.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.805 I load_tensors: offloading output layer to GPU
0.00.056.805 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.816 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.817 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.057.102 I llama_init_from_model: n_seq_max     = 1
0.00.057.103 I llama_init_from_model: n_ctx         = 128
0.00.057.103 I llama_init_from_model: n_ctx_per_seq = 128
0.00.057.103 I llama_init_from_model: n_batch       = 128
0.00.057.104 I llama_init_from_model: n_ubatch      = 128
0.00.057.104 I llama_init_from_model: flash_attn    = 0
0.00.057.104 I llama_init_from_model: freq_base     = 10000.0
0.00.057.104 I llama_init_from_model: freq_scale    = 1
0.00.057.105 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.105 I ggml_metal_init: allocating
0.00.057.109 I ggml_metal_init: found device: Apple M4
0.00.057.111 I ggml_metal_init: picking default device: Apple M4
0.00.057.694 I ggml_metal_init: using embedded metal library
0.00.060.065 I ggml_metal_init: GPU name:   Apple M4
0.00.060.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.068 I ggml_metal_init: simdgroup reduction   = true
0.00.060.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.068 I ggml_metal_init: has bfloat            = true
0.00.060.069 I ggml_metal_init: use bfloat            = true
0.00.060.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.349 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.716 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.718 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.734 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.581 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.583 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.583 I llama_init_from_model: graph nodes  = 967
0.00.072.583 I llama_init_from_model: graph splits = 2
0.00.072.584 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.258 I 
0.00.686.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.304 I perplexity: tokenizing the input ..
0.00.694.473 I perplexity: tokenization took 8.166 ms
0.00.694.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.675 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.835.865 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.835.885 I llama_perf_context_print:        load time =     677.55 ms
0.00.835.886 I llama_perf_context_print: prompt eval time =     139.96 ms /   128 tokens (    1.09 ms per token,   914.52 tokens per second)
0.00.835.887 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.887 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.836.426 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.081s
sys	0m0.118s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4533 (1971adf5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12440a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12440aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12440b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12440b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12440bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12440c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12440c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12440cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12440d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12440d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12440dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12440e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12440ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12440f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12440fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124410350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124410a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124411190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1244118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124412080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1244127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124412ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1244135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124413e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1244145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124414860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124414e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124415ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124416020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1244162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124416780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124416a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1244172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124417810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124417ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124417f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124418410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1244188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124418d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1244191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124419690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124419b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124419fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12441a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12441a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12441ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12441b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12441bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12441c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12441c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12441cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12441d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12441dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12441e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12441e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12441ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12441f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12441f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12441fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1244202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124420580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124420a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124420ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124421360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124421800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124421ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124422140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1244225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124422a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124422f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1244233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124423860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124423d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124424250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1244247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124424cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124425240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124425790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124425ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124426230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124426780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124426cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124427220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124427770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124427cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124428210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124428760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124428cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124429200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124429750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124429ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12442a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12442a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12442ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12442b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12442b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12442bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12441b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12442c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12442c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12442cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12442d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12442d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12442dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12442e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12442e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12442edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12442f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12442f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12442fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124430310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124430860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124430db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124431250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1244316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124431b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124432030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1244324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124432970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124432e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1244332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124433750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124433bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124434090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124434530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1244349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124434e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124435310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1244357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124435c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1244360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124436590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124436a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124436ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124437370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124437810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124437cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124438150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1244385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124438a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124438f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1244393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124439870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124439d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12443a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12443a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12443aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12443af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12443b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12443b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12443bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12443c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12443c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12443cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12443cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12443d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12443d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12443ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12443e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12443e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12443ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12443f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12443f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12443f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12443fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1244402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124440770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124440c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1244410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124441550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1244419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124441e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124442330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1244427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124442c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124443110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1244435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124443a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124443ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124444390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124444830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124444cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124445170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124445610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124445ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124445f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1244463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124446890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124446d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1244471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124447670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124447b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124447fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124448500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124448a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124448fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1244494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1244497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124449dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12444a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12444a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12444b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12444b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12444b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12444bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12444c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12444cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12444d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12444d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12444db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12444e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12444e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12444ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12444f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12444f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12444fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1244502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124450800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124450d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1244512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1244517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124451d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124452290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1244527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124452d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124453280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1244537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124453d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124454270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1244547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124454d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124455260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1244557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124455d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124456250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1244567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124456cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124457240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124457790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124457ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124458230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124458780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124458cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124459220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124459770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124459cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12445a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12445a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12445acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12445b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12445b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12445bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12445c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12445c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12445cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12445d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12445d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12445dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12445e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12445e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12445ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12445f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12445f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12445fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1244601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124460700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124460c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1244610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124461590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124461a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124461ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124462370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124462810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124462cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124463150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1244635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124463a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124463f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1244643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124464870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124464d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1244651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124465700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124465e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124466540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124466c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124467380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124467640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124467e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1244680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124468700 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.138.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d204dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d205240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d2056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d205b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d205f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d206400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d206870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d206ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d207150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d2075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d207a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d208120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d208c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d2093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d209c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d20a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d20aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d20b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d20b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d20bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d20c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d20cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d20d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d20dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d20e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d20e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d20e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d20ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d20f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d20f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d20fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d20ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d210430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d2106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d210b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d210fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d211440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d2118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d211d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d212190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d212600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d212a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d212ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d213350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d2137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d213c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d2140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d214510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d214980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d214df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d215260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d2156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d215b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d215fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d216420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d216890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d216e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d217300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d217770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d217be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d218050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d2184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d218930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d218da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d219210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d219680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d219af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d219f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d21a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d21a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d21acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d21b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d21b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d21ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d21be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d21c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d21c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d21cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d21d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d21d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d21d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d21dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d21e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d21e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d21ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d21ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d21f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d21f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d21fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d220100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d220570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d2209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d220e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d2212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d221730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d221ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d222010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d222480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d2228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d222d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d2231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d223640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d223ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d223f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d224390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d224800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d224c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d2250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d225550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d2259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d225e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d2262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d226710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d226b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d226ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d227460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d2278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d227d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d2281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d228620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d228a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d228f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d229370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d2297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d229c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d22a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d22a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d22a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d22ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d22b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d22b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d22bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d22bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d22c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d22c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d22cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d22d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d22d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d22da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d22dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d22e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d22e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d22ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d22f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d22f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d22f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d22fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d230260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d2306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d230b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d230fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d231420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d231890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d231d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d232170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d2325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d232a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d232ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d233330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d2337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d233c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d234080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d2344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d234960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d234dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d235240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d235e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d236130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d2363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d236860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d236cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d237140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d2375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d237a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d237e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d238300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d238770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d238be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d239050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d2394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d239930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d239da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d23a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d23a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d23aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d23af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d23b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d23b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d23bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d23c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d23c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d23ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d23ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d23d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d23d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d23dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d23e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d23e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d23e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d23ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d23f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d23f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d23fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d2400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d240540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d2409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d240e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d241290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d2417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d241cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d242830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d242af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d2430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d243670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d243c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d2441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d2447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d244d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d245330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d2458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d245eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d246470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d246a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d246ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d2475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d247b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d248130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d2486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d248cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d249270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d249830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d249df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d24a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d24a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d24af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d24b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d24bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d24c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d24c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d24cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d24d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d24d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d24dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d24e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d24e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d24ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d24f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d24f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d24ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d250570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d250b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d2510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d2516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d251c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d252230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d2527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d252db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d253370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d253930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d253ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d2544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d254a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d255030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d2555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d255bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d256170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d256730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d256cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d2571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d2576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d257bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d2580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d2585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d258af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d258ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d2594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d2599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d259ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d25a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d25a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d25adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d25b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d25b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d25c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d25c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d25d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d25d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d25da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d25e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d25e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d25eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1244683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12444a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124449a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12444a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12441d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12441d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12441f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12444c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124414b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12441b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12441bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12441c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12441a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12441cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124413b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12441fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12442c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124467900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124416d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124416fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12444c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12444aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124415130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1244153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1244156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124468b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124468e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1244690e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1244693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124469660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124469920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124469be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124469ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12446a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12446a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12446a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12446a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12446ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12446af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12446b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12446b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12446b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12446ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12446bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12446bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12446c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12446c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12446c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12446caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12446cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12446d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12446d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12446d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12446d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12446db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12446dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12446e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12446e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12446e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12446e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12446eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12446ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12446f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12446f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12446f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12446f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12446fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12446fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1244701a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124470460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124470720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1244709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124470ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124470f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124471220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1244714e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1244717a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124471a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124471d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124471fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1244722a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124472560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124472820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124472ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124472da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124473060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124473320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1244735e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1244738a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124473b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124473e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1244740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1244743a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124474660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124474920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124474be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124474ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124475160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124475420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1244756e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1244759a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124475c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124475f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1244761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1244764a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124476760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124476a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124476ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124476fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124477260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124477520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1244777e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124477aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124477d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124478020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1244782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1244785a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124478860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124478b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124478de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1244790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124479360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124479620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1244798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124479ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124479e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12447a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12447a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12447a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12447a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12447ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12447aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12447b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12447b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12447b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12447b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12447bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12447bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12447c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12447c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12447c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12447ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12447cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12447cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12447d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12447d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12447d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12447dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12447dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12447e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12447e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12447e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12447e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12447eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12447ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12447f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12447f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12447f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12447f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12447fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12447fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124480160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124480420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1244806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1244809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124480c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124480f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1244811e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1244814a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124481760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124481a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124481ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124481fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124482260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124482520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1244827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124482aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124482d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124483020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1244832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1244835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124483860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124483b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124483de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1244840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124484360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124484620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1244848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124484ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124484e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124485120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1244853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1244856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124485960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124485c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124485ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1244861a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124486460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124486720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1244869e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124486ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d25bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d24c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d24b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d2483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d245bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d2552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d252ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d250830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d24e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d246730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d243ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d248f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d24a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d24f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d24c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d2541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d246cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d247e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d24f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d2513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d249af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d24ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d250270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d242db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d24ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d24d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d247870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d2489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d2558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d253070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d244a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d24dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d243370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d243930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d2455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d255e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d24b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d253630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d249530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d24bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d24fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d2472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d251970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d246170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d254770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d251f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d24da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d2569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d245030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d256430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d2444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d254d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d24eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d250df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d253bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d2524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d24a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d2083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d235500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d241f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d204880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d25dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d20bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d25ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d25f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d25f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d25f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d25fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d25fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d25ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d260280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d260540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d260800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d260ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d260d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d261040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d261300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d2615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d261880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d261b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d261e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d2620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d262380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d262640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d262900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d262e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d263380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d263640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d263900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d263bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.827s
user	0m0.292s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4533 (1971adf5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13960bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13960c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13960c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13960cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13960d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13960da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13960e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13960e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13960eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13960f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13960f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13960fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1396105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139610d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139611570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139611c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1396123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139612ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1396131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1396139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1396140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139614800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139614f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1396157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139615ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1396161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1396167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139617960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139617c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1396180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139618380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139619150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139619410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1396198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139619d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13961a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13961a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13961ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13961afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13961b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13961b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13961bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13961c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13961c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13961cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13961d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13961dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13961e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13961e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13961edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13961f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13961fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1396206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139620e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139621410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139621c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139622360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139623140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1396235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139623a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1396243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139624860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139624d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1396251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139625640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139625b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1396260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139626630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1396270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139627620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139627b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1396280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139628b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1396290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139629600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139629b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13962a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13962a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13962ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13962b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13962b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13962bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13962c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13962c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13962cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13962d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13962d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13961d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13962da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13962e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13962e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13962ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13962f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13962f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13962fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1396301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139630c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1396311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139631700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139631c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1396321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1396326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139632b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139633030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1396334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139633970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139633e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1396342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139634750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139634bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139635090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139635530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1396359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139636310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1396367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139636c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1396370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139637590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139637a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139637ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139638370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139638810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139639150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1396395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139639f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13963a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13963a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13963ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13963b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13963b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13963baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13963bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13963c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13963c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13963cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13963d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13963d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13963db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13963dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13963e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13963e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13963edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13963f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13963f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13963fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139640050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1396404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139640990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139640e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1396412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139641770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139641c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1396420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139642550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1396429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139642e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1396437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139643c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139644110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1396445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139644a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139644ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139645390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139645830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139645cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139646610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139646ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139646f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1396473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139647890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1396481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139648670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139648b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139648fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139649450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1396498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139649e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13964a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13964a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13964ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13964b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13964b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13964bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13964c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13964cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13964cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13964d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13964d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13964de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13964e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13964eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13964efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13964f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13964fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139650160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1396506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139650c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139651150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1396516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139651bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139652140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139652690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139652be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139653130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139653680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139653bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139654670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139654bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139655110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139655660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139656100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139656650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139656ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1396570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139657640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139657b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1396580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139658630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139658b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1396590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139659620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139659b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13965a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13965a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13965ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13965b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13965b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13965bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13965c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13965c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13965cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13965d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13965d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13965db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13965e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13965e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13965eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13965f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13965f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13965fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139660060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1396605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139660b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139661050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1396615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139661af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139662040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139662590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139662a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139662ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139663370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139663810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139663cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139664150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1396645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139664a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139664f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1396653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139665870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139665d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1396661b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139666650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139666af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139667040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139667760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139667e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1396685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139668cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139668f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139669770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139669a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13966a040 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139669cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13964b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13964b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13964bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13961f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13961eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1396210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13964db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139616460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13961cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13961d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13961de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13961c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13961e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139615460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1396216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13962dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139669240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139618640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13964e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13964c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139616a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139616d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139616ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13966a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13966a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13966aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13966ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13966afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13966b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13966b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13966b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13966baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13966bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13966c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13966c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13966c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13966c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13966cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13966cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13966d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13966d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13966d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13966d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13966dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13966de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13966e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13966e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13966e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13966e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13966ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13966eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13966f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13966f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13966f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13966f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13966fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13966ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139670220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1396704e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1396707a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139670a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139670d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139670fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1396712a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139671560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139671820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139671ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139671da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139672060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139672320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1396725e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1396728a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139672b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139672e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1396730e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1396733a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139673660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139673920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139673be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139673ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139674160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139674420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1396746e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1396749a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139674c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139674f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1396751e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1396754a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139675760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139675a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139675ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139675fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139676260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139676520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1396767e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139676aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139676d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139677020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1396772e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1396775a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139677860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139677b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139677de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1396780a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139678360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139678620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1396788e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139678ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139678e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139679120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1396793e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1396796a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139679960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139679c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139679ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13967a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13967a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13967a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13967a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13967aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13967af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13967b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13967b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13967b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13967ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13967bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13967bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13967c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13967c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13967c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13967cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13967cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13967d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13967d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13967d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13967d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13967db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13967de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13967e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13967e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13967e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13967e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13967ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13967eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13967f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13967f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13967f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13967f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13967fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13967ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1396801e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1396804a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139680760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139680a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139680ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139680fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139681260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139681520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1396817e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139681aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139681d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139682020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1396822e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1396825a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139682860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139682b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139682de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1396830a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139683360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139683620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1396838e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139683ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139683e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139684120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1396843e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1396846a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139684960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139684c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139684ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1396851a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139685460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139685720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1396859e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139685ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139685f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139686220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1396864e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1396867a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139686a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139686d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139686fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1396872a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139687560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139687820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139687ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139687da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139688060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139688320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1396885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1396888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139688b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139688e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1396890e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1396893a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139689660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139689920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139689be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139689ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13968a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13968a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13968a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13968ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13968b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13968b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13968bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13968c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13968c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13968c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13968cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13968d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13968d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13968dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13968df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13968e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13968e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13968ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13968f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13968f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13968f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13968fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1396902b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139690720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139690b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139691000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139691470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1396918e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139691d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1396921c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139692630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139692aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139692f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139693380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1396937f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139693c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1396940d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139694540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1396949b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139694e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139695290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139695700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139695b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139695fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139696450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1396968c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139696d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1396971a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139697610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139697a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139697ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139698360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1396987d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139698c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1396990b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139699520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139699990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139699e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13969a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13969a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13969ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13969afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13969b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13969b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13969bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13969c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13969c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13969ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13969ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13969d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13969d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13969dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13969e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13969eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13969f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13969f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1396a0060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1396a0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1396a0b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1396a0dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1396a13e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139708650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139708ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139708f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1397093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139709810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139709c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13970a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13970a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13970a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13970af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13970b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13970ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13970c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13970cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13970d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13970dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13970e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13970ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13970f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13970f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139710080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1397107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139710ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1397115e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139711d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139711fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139712280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1397126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139712b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139712fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139713440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139713970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1397140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1397156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139716d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1397175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139717a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139717ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139718330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1397187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139718c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139719080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1397194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139719960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13971a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13971a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13971acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13971b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13971b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13971ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13971be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13971c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13971c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13971cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13971d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13971d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13971d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13971dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13971e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13971e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13971ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13971ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13971f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13971f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13971fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1397209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139720e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1397212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1397228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1397231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139723f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1397250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1397259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1397262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139728d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139729170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1397295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139729ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13972a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13972a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13972ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13972b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13972b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13972b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13972bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13972c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13972c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13972cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13972cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13972d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13972d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13972dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13972e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13972e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13972ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13972eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13972f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13972f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13972fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1397304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139730940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139731b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139731f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1397323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1397335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139733a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1397342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139734bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1397354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139735d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139736200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139736670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139736f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1397373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139737830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139737ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1397389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139738e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1397392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13973a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13973a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13973a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13973ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13973b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13973b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13973bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13973bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13973c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13973c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13973cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13973d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13973d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13973d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13973de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13973e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13973e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13973eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13973f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13973f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13973f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13973fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1397401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139740630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139740aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139740f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1397417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1397420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1397429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139743700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139744450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1397448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139744d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1397451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139745d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139745fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1397462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139746710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139746b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139746ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139747460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1397478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139747d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1397481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139748620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139748a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139748f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139749370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1397497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13974a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13974a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13974a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13974ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13974b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13974b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13974bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13974bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13974c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13974c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13974cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13974d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13974d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13974da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13974dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13974e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13974e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13974ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13974f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13974f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13974f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13974fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139750260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1397506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139750b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139750fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139751420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139751890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139751d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139752170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1397525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139752a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139752ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139753330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1397537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139753c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139754080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1397544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139754960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139754dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139755240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1397556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139755b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139755f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139756400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139756ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139757150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1397575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139757a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139757ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139758310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139758780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139758bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139759060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1397594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139759940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13975a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13975aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13975b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13975b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13975bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13975c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13975c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13975cc50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.244s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.09 sec*proc (2 tests)

Total Test time (real) =   1.10 sec
        1.12 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
