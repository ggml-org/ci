Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.8.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.8.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.8.0-py3-none-any.whl size=3229 sha256=e3aafedf1f270ac30d9f1b3b317301ce657a1cae1c7f5a947ba0148604ce0003
  Stored in directory: /tmp/pip-ephem-wheel-cache-6duesazu/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.8.0
    Uninstalling gguf-0.8.0:
      Successfully uninstalled gguf-0.8.0
Successfully installed gguf-0.8.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.456s
user	0m0.312s
sys	0m0.122s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  0%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  3%] Built target ggml
[  4%] Linking C static library libggml_static.a
[  5%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Linking CXX executable ../../bin/gguf
[  7%] Built target json-schema-to-grammar
[  7%] Built target ggml_static
[  7%] Built target build_info
[  7%] Linking CXX static library libllama.a
[  7%] Built target gguf
[  7%] Built target llama
[  7%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[  8%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[  8%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[  9%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[  9%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 10%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 11%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 14%] Linking CXX executable ../bin/test-c
[ 15%] Linking CXX executable ../../bin/benchmark
[ 16%] Linking CXX executable ../../bin/quantize
[ 16%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 16%] Linking CXX executable ../../bin/quantize-stats
[ 16%] Built target llava
[ 17%] Linking CXX static library libcommon.a
[ 18%] Linking CXX static library libllava_static.a
[ 18%] Built target test-c
[ 18%] Built target benchmark
[ 18%] Built target llava_static
[ 18%] Built target common
[ 18%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 29%] Linking CXX executable ../bin/test-quantize-perf
[ 30%] Linking CXX executable ../bin/test-chat-template
[ 31%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Linking CXX executable ../bin/test-quantize-fns
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-sampling
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Linking CXX executable ../bin/test-grad0
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-llama-grammar
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 64%] Linking CXX executable ../../bin/baby-llama
[ 64%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 66%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 66%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 66%] Linking CXX executable ../../bin/batched-bench
[ 66%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 69%] Linking CXX executable ../../bin/beam-search
[ 70%] Linking CXX executable ../../bin/batched
[ 71%] Linking CXX executable ../../bin/finetune
[ 71%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 72%] Linking CXX executable ../../bin/gritlm
[ 72%] Built target quantize-stats
[ 72%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Linking CXX executable ../../bin/embedding
[ 73%] Linking CXX executable ../../bin/gguf-split
[ 73%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/infill
[ 74%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/main
[ 78%] Linking CXX executable ../../bin/llava-cli
[ 79%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/tokenize
[ 80%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 81%] Linking CXX executable ../../bin/parallel
[ 82%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 82%] Built target quantize
[ 83%] Linking CXX executable ../../bin/simple
[ 84%] Linking CXX executable ../../bin/perplexity
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 84%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 84%] Linking CXX executable ../../bin/save-load-state
[ 84%] Linking CXX executable ../../bin/passkey
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 87%] Linking CXX executable ../../bin/speculative
[ 88%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 89%] Linking CXX executable ../../bin/lookup
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 90%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 91%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 92%] Linking CXX executable ../../bin/lookahead
[ 92%] Linking CXX executable ../../bin/lookup-create
[ 92%] Linking CXX executable ../../bin/lookup-merge
[ 92%] Linking CXX executable ../../bin/train-text-from-scratch
[ 93%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/lookup-stats
[ 96%] Linking CXX executable ../../bin/imatrix
[ 96%] Built target test-quantize-fns
[ 97%] Linking CXX executable ../../bin/q8dot
[ 97%] Built target convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/vdot
[100%] Linking CXX executable ../../bin/server
[100%] Built target test-quantize-perf
[100%] Built target test-grad0
[100%] Built target lookup-merge
[100%] Built target q8dot
[100%] Built target export-lora
[100%] Built target vdot
[100%] Built target test-grammar-parser
[100%] Built target test-backend-ops
[100%] Built target test-rope
[100%] Built target test-chat-template
[100%] Built target test-sampling
[100%] Built target gguf-split
[100%] Built target test-tokenizer-1-baichuan
[100%] Built target test-json-schema-to-grammar
[100%] Built target test-autorelease
[100%] Built target test-tokenizer-0-falcon
[100%] Built target beam-search
[100%] Built target test-model-load-cancel
[100%] Built target batched-bench
[100%] Built target test-tokenizer-0-llama
[100%] Built target baby-llama
[100%] Built target gritlm
[100%] Built target test-tokenizer-1-aquila
[100%] Built target batched
[100%] Built target test-tokenizer-1-stablelm-3b-4e1t
[100%] Built target test-tokenizer-1-falcon
[100%] Built target finetune
[100%] Built target embedding
[100%] Built target simple
[100%] Built target llava-cli
[100%] Built target tokenize
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target lookahead
[100%] Built target passkey
[100%] Built target perplexity
[100%] Built target parallel
[100%] Built target test-tokenizer-1-llama
[100%] Built target save-load-state
[100%] Built target test-tokenizer-1-refact
[100%] Built target main
[100%] Built target test-llama-grammar
[100%] Built target lookup-create
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target test-tokenizer-1-mpt
[100%] Built target infill
[100%] Built target imatrix
[100%] Built target test-tokenizer-1-gpt-neox
[100%] Built target lookup
[100%] Built target train-text-from-scratch
[100%] Built target lookup-stats
[100%] Built target llama-bench
[100%] Built target speculative
[100%] Built target server

real	0m4.979s
user	0m30.498s
sys	0m4.635s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/22 Test  #1: test-quantize-fns ...................   Passed   25.54 sec
      Start  2: test-quantize-perf
 2/22 Test  #2: test-quantize-perf ..................   Passed    9.19 sec
      Start  3: test-sampling
 3/22 Test  #3: test-sampling .......................   Passed    0.03 sec
      Start  4: test-chat-template
 4/22 Test  #4: test-chat-template ..................   Passed    0.00 sec
      Start  5: test-tokenizer-0-llama
 5/22 Test  #5: test-tokenizer-0-llama ..............   Passed    0.14 sec
      Start  6: test-tokenizer-0-falcon
 6/22 Test  #6: test-tokenizer-0-falcon .............   Passed    0.88 sec
      Start  7: test-tokenizer-1-llama
 7/22 Test  #7: test-tokenizer-1-llama ..............   Passed    3.40 sec
      Start  8: test-tokenizer-1-baichuan
 8/22 Test  #8: test-tokenizer-1-baichuan ...........   Passed    3.64 sec
      Start  9: test-tokenizer-1-falcon
 9/22 Test  #9: test-tokenizer-1-falcon .............   Passed    6.07 sec
      Start 10: test-tokenizer-1-aquila
10/22 Test #10: test-tokenizer-1-aquila .............   Passed    8.62 sec
      Start 11: test-tokenizer-1-mpt
11/22 Test #11: test-tokenizer-1-mpt ................   Passed    5.04 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/22 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    5.06 sec
      Start 13: test-tokenizer-1-gpt-neox
13/22 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    5.05 sec
      Start 14: test-tokenizer-1-refact
14/22 Test #14: test-tokenizer-1-refact .............   Passed    4.84 sec
      Start 15: test-tokenizer-1-starcoder
15/22 Test #15: test-tokenizer-1-starcoder ..........   Passed    4.83 sec
      Start 16: test-tokenizer-1-gpt2
16/22 Test #16: test-tokenizer-1-gpt2 ...............   Passed    5.08 sec
      Start 17: test-grammar-parser
17/22 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/22 Test #18: test-llama-grammar ..................   Passed    0.00 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 ..........................   Passed    2.48 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops ....................   Passed    0.00 sec
      Start 21: test-rope
21/22 Test #21: test-rope ...........................   Passed    0.06 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar .........   Passed    1.00 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    =  90.93 sec*proc (22 tests)

Total Test time (real) =  90.94 sec

real	1m30.961s
user	3m28.529s
sys	0m3.361s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_release.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.453s
user	0m0.338s
sys	0m0.099s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_release-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Built target ggml
[  4%] Built target json-schema-to-grammar
[  4%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  5%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  6%] Linking C static library libggml_static.a
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Linking CXX executable ../../bin/gguf
[  7%] Built target build_info
[  7%] Linking CXX static library libllama.a
[  7%] Built target ggml_static
[  7%] Built target llama
[  7%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[  7%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[  8%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[  8%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[  9%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[  9%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 10%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 13%] Linking CXX executable ../bin/test-c
[ 13%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 15%] Linking CXX executable ../../bin/quantize
[ 16%] Linking CXX executable ../../bin/benchmark
[ 16%] Linking CXX executable ../../bin/quantize-stats
[ 16%] Built target llava
[ 17%] Linking CXX static library libcommon.a
[ 18%] Linking CXX static library libllava_static.a
[ 18%] Built target gguf
[ 18%] Built target llava_static
[ 18%] Built target common
[ 18%] Built target test-c
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 24%] Linking CXX executable ../bin/test-sampling
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 27%] Linking CXX executable ../bin/test-chat-template
[ 28%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 29%] Linking CXX executable ../bin/test-quantize-fns
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-quantize-perf
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 30%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 35%] Built target quantize-stats
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 36%] Built target benchmark
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 41%] Built target quantize
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Built target test-quantize-perf
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 49%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-sampling
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Built target test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-grammar-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-grad0
[ 55%] Built target test-tokenizer-1-falcon
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Built target test-tokenizer-0-falcon
[ 56%] Linking CXX executable ../bin/test-rope
[ 57%] Linking CXX executable ../bin/test-llama-grammar
[ 57%] Built target test-tokenizer-1-aquila
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 61%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../../bin/baby-llama
[ 62%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 62%] Built target test-tokenizer-0-llama
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Linking CXX executable ../bin/test-autorelease
[ 63%] Built target test-tokenizer-1-llama
[ 64%] Linking CXX executable ../../bin/batched
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-tokenizer-1-baichuan
[ 64%] Built target test-grammar-parser
[ 64%] Built target test-grad0
[ 64%] Built target test-tokenizer-1-mpt
[ 64%] Built target test-backend-ops
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 65%] Built target test-tokenizer-1-gpt-neox
[ 65%] Built target test-tokenizer-1-gpt2
[ 65%] Built target test-tokenizer-1-refact
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 67%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 67%] Built target test-tokenizer-1-starcoder
[ 67%] Built target test-llama-grammar
[ 68%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 68%] Linking CXX executable ../../bin/beam-search
[ 69%] Linking CXX executable ../../bin/gritlm
[ 69%] Built target test-model-load-cancel
[ 70%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 71%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 71%] Linking CXX executable ../../bin/embedding
[ 72%] Linking CXX executable ../../bin/finetune
[ 72%] Built target test-json-schema-to-grammar
[ 73%] Linking CXX executable ../../bin/infill
[ 73%] Built target batched
[ 74%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 74%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 74%] Built target test-autorelease
[ 75%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-bench
[ 77%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 77%] Built target baby-llama
[ 78%] Linking CXX executable ../../bin/llava-cli
[ 79%] Linking CXX executable ../../bin/perplexity
[ 79%] Linking CXX executable ../../bin/parallel
[ 80%] Linking CXX executable ../../bin/tokenize
[ 80%] Linking CXX executable ../../bin/gguf-split
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 82%] Built target convert-llama2c-to-ggml
[ 82%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 83%] Linking CXX executable ../../bin/main
[ 83%] Linking CXX executable ../../bin/passkey
[ 83%] Built target batched-bench
[ 83%] Linking CXX executable ../../bin/save-load-state
[ 84%] Linking CXX executable ../../bin/simple
[ 84%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 84%] Built target gritlm
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 85%] Linking CXX executable ../../bin/lookahead
[ 85%] Built target infill
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 86%] Built target embedding
[ 86%] Built target beam-search
[ 87%] Linking CXX executable ../../bin/speculative
[ 88%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 89%] Linking CXX executable ../../bin/lookup
[ 89%] Built target llava-cli
[ 89%] Built target perplexity
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 90%] Built target tokenize
[ 91%] Linking CXX executable ../../bin/lookup-stats
[ 91%] Built target finetune
[ 91%] Linking CXX executable ../../bin/lookup-create
[ 91%] Linking CXX executable ../../bin/lookup-merge
[ 91%] Built target passkey
[ 91%] Built target parallel
[ 92%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 92%] Built target llama-bench
[ 92%] Linking CXX executable ../../bin/train-text-from-scratch
[ 92%] Built target save-load-state
[ 93%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 93%] Built target main
[ 93%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 94%] Linking CXX executable ../../bin/server
[ 94%] Built target gguf-split
[ 94%] Built target lookahead
[ 94%] Built target speculative
[ 95%] Linking CXX executable ../../bin/imatrix
[ 95%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 95%] Built target simple
[ 96%] Linking CXX executable ../../bin/q8dot
[ 96%] Built target lookup
[ 97%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 98%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 98%] Built target lookup-stats
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target lookup-merge
[100%] Linking CXX executable ../../bin/export-lora
[100%] Built target lookup-create
[100%] Built target train-text-from-scratch
[100%] Built target q8dot
[100%] Built target export-lora
[100%] Built target vdot
[100%] Built target server
[100%] Built target imatrix

real	0m1.122s
user	0m6.659s
sys	0m1.701s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/22 Test  #1: test-quantize-fns ...................   Passed   13.29 sec
      Start  2: test-quantize-perf
 2/22 Test  #2: test-quantize-perf ..................   Passed    4.44 sec
      Start  3: test-sampling
 3/22 Test  #3: test-sampling .......................   Passed    0.01 sec
      Start  4: test-chat-template
 4/22 Test  #4: test-chat-template ..................   Passed    0.00 sec
      Start  5: test-tokenizer-0-llama
 5/22 Test  #5: test-tokenizer-0-llama ..............   Passed    0.03 sec
      Start  6: test-tokenizer-0-falcon
 6/22 Test  #6: test-tokenizer-0-falcon .............   Passed    0.18 sec
      Start  7: test-tokenizer-1-llama
 7/22 Test  #7: test-tokenizer-1-llama ..............   Passed    0.45 sec
      Start  8: test-tokenizer-1-baichuan
 8/22 Test  #8: test-tokenizer-1-baichuan ...........   Passed    0.48 sec
      Start  9: test-tokenizer-1-falcon
 9/22 Test  #9: test-tokenizer-1-falcon .............   Passed    0.76 sec
      Start 10: test-tokenizer-1-aquila
10/22 Test #10: test-tokenizer-1-aquila .............   Passed    1.14 sec
      Start 11: test-tokenizer-1-mpt
11/22 Test #11: test-tokenizer-1-mpt ................   Passed    0.63 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/22 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    0.64 sec
      Start 13: test-tokenizer-1-gpt-neox
13/22 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    0.62 sec
      Start 14: test-tokenizer-1-refact
14/22 Test #14: test-tokenizer-1-refact .............   Passed    0.61 sec
      Start 15: test-tokenizer-1-starcoder
15/22 Test #15: test-tokenizer-1-starcoder ..........   Passed    0.60 sec
      Start 16: test-tokenizer-1-gpt2
16/22 Test #16: test-tokenizer-1-gpt2 ...............   Passed    0.62 sec
      Start 17: test-grammar-parser
17/22 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/22 Test #18: test-llama-grammar ..................   Passed    0.00 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 ..........................   Passed    2.34 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops ....................   Passed    0.00 sec
      Start 21: test-rope
21/22 Test #21: test-rope ...........................   Passed    0.05 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar .........   Passed    1.11 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    =  28.02 sec*proc (22 tests)

Total Test time (real) =  28.03 sec

real	0m28.051s
user	0m40.883s
sys	0m3.582s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/embd_bge_small.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model:
2024-03-23 17:03:00 ERROR 404: Not Found.
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:00 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:01 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:01 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.458s
user	0m0.367s
sys	0m0.073s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/embd_bge_small-make.log
+ make -j
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Built target build_info
[  4%] Built target ggml
[  4%] Built target json-schema-to-grammar
[  5%] Linking C static library libggml_static.a
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  7%] Linking CXX executable ../../bin/gguf
[  7%] Linking CXX static library libllama.a
[  7%] Built target ggml_static
[  7%] Built target llama
[  7%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[  8%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[  8%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[  8%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[  9%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[  9%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Linking CXX executable ../bin/test-c
[ 13%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 14%] Linking CXX executable ../../bin/benchmark
[ 15%] Linking CXX executable ../../bin/quantize
[ 15%] Built target llava
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Linking CXX static library libcommon.a
[ 18%] Linking CXX static library libllava_static.a
[ 18%] Linking CXX executable ../../bin/quantize-stats
[ 18%] Built target gguf
[ 18%] Built target common
[ 18%] Built target test-c
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 21%] Built target llava_static
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 26%] Linking CXX executable ../bin/test-chat-template
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 26%] Linking CXX executable ../bin/test-quantize-perf
[ 27%] Linking CXX executable ../bin/test-sampling
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 28%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 29%] Linking CXX executable ../bin/test-quantize-fns
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 31%] Built target benchmark
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Built target quantize
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Built target quantize-stats
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 50%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 50%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 50%] Built target test-quantize-perf
[ 50%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-sampling
[ 53%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 55%] Built target test-chat-template
[ 55%] Built target test-tokenizer-0-llama
[ 55%] Built target test-tokenizer-1-llama
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-rope
[ 58%] Linking CXX executable ../bin/test-grad0
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 58%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 58%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 60%] Built target test-grammar-parser
[ 61%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 65%] Linking CXX executable ../../bin/batched
[ 66%] Linking CXX executable ../../bin/baby-llama
[ 67%] Linking CXX executable ../bin/test-autorelease
[ 68%] Linking CXX executable ../bin/test-model-load-cancel
[ 68%] Built target test-tokenizer-1-gpt-neox
[ 68%] Linking CXX executable ../../bin/batched-bench
[ 68%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 68%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 68%] Built target test-quantize-fns
[ 68%] Linking CXX executable ../../bin/beam-search
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 69%] Built target test-tokenizer-0-falcon
[ 70%] Linking CXX executable ../../bin/finetune
[ 71%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/embedding
[ 71%] Built target test-tokenizer-1-gpt2
[ 71%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 72%] Linking CXX executable ../../bin/infill
[ 72%] Built target test-tokenizer-1-baichuan
[ 72%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 72%] Built target test-llama-grammar
[ 72%] Linking CXX executable ../../bin/gguf-split
[ 72%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 73%] Linking CXX executable ../../bin/llava-cli
[ 73%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 73%] Built target test-tokenizer-1-aquila
[ 74%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 74%] Built target test-tokenizer-1-falcon
[ 75%] Linking CXX executable ../../bin/gritlm
[ 75%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 76%] Built target batched-bench
[ 76%] Built target test-rope
[ 77%] Linking CXX executable ../../bin/main
[ 78%] Linking CXX executable ../../bin/llama-bench
[ 78%] Built target test-tokenizer-1-starcoder
[ 78%] Built target test-tokenizer-1-refact
[ 78%] Built target beam-search
[ 78%] Built target finetune
[ 79%] Linking CXX executable ../../bin/tokenize
[ 80%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/parallel
[ 80%] Built target test-grad0
[ 80%] Built target test-tokenizer-1-mpt
[ 81%] Linking CXX executable ../../bin/perplexity
[ 81%] Built target test-json-schema-to-grammar
[ 81%] Built target baby-llama
[ 81%] Built target embedding
[ 81%] Built target convert-llama2c-to-ggml
[ 81%] Built target test-backend-ops
[ 81%] Built target infill
[ 81%] Built target llava-cli
[ 81%] Built target test-autorelease
[ 81%] Built target test-model-load-cancel
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 82%] Built target gguf-split
[ 82%] Linking CXX executable ../../bin/save-load-state
[ 82%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 82%] Built target batched
[ 82%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 83%] Built target gritlm
[ 83%] Built target tokenize
[ 83%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 84%] Linking CXX executable ../../bin/simple
[ 84%] Built target main
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 86%] Linking CXX executable ../../bin/speculative
[ 87%] Linking CXX executable ../../bin/lookahead
[ 87%] Linking CXX executable ../../bin/passkey
[ 88%] Linking CXX executable ../../bin/lookup
[ 88%] Linking CXX executable ../../bin/lookup-create
[ 88%] Built target llama-bench
[ 88%] Built target parallel
[ 89%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 91%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 91%] Built target perplexity
[ 92%] Linking CXX executable ../../bin/lookup-stats
[ 92%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 92%] Linking CXX executable ../../bin/lookup-merge
[ 92%] Linking CXX executable ../../bin/train-text-from-scratch
[ 92%] Built target save-load-state
[ 93%] Linking CXX executable ../../bin/imatrix
[ 94%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 94%] Built target simple
[ 94%] Built target passkey
[ 94%] Built target speculative
[ 94%] Built target lookahead
[ 94%] Built target lookup-create
[ 95%] Linking CXX executable ../../bin/server
[ 95%] Built target lookup
[ 96%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Linking CXX executable ../../bin/export-lora
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target lookup-merge
[100%] Built target lookup-stats
[100%] Built target imatrix
[100%] Built target train-text-from-scratch
[100%] Built target q8dot
[100%] Built target vdot
[100%] Built target export-lora
[100%] Built target server

real	0m1.175s
user	0m6.721s
sys	0m1.750s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
gguf: context length = 512
gguf: embedding length = 384
gguf: feed forward length = 1536
gguf: head count = 12
gguf: layer norm epsilon = 1e-12
gguf: file type = 1
Set model tokenizer
fname_tokenizer: ../models-mnt/bge-small
gguf: Setting special token type pad to 0
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llama_model_quantize_internal ============ Strange model: n_attention_wv = 12, n_ffn_down = 24, hparams.n_layer = 12
llama_model_quantize_internal: meta size = 760800 bytes
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f16, size =    0.375 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.46 MB
llama_model_quantize_internal: quant size  =    34.00 MB

main: quantize time =   127.72 ms
main:    total time =   127.72 ms
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213384
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =   241.45 MiB
llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =      61.15 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       6.18 ms /     9 tokens (    0.69 ms per token,  1455.60 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =      55.03 ms /    10 tokens

embedding 0: -0.044003 -0.019931  0.007665 -0.000818  0.001378 -0.037049  0.109437  0.042565  0.092037 -0.015907  0.006781 -0.035718 -0.017895  0.015016  0.018091  0.015880 

cosine similarity matrix:

  1.00 

real	0m0.176s
user	0m0.117s
sys	0m0.084s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213384
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    34.00 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =   241.45 MiB
llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =      56.06 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       4.79 ms /     9 tokens (    0.53 ms per token,  1879.31 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =      53.13 ms /    10 tokens

embedding 0: -0.044902 -0.019894  0.008398 -0.001235  0.001822 -0.037097  0.108956  0.043308  0.091165 -0.014905  0.006446 -0.035863 -0.018616  0.013880  0.017721  0.014506 

cosine similarity matrix:

  1.00 

real	0m0.169s
user	0m0.112s
sys	0m0.077s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_3b_v2
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:05 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json [506/506] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:05 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:05 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:03:05 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json [137/137] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/open-llama/3B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_QKK_64=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.456s
user	0m0.315s
sys	0m0.125s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Built target ggml
[  4%] Built target json-schema-to-grammar
[  4%] Built target build_info
[  4%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  5%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Linking C static library libggml_static.a
[  7%] Linking CXX executable ../../bin/gguf
[  7%] Linking CXX static library libllama.a
[  7%] Built target ggml_static
[  7%] Built target llama
[  8%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[  8%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[  8%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[  9%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[  9%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[  9%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 10%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Linking CXX executable ../bin/test-c
[ 15%] Linking CXX executable ../../bin/quantize
[ 16%] Linking CXX executable ../../bin/benchmark
[ 16%] Linking CXX executable ../../bin/quantize-stats
[ 16%] Built target llava
[ 17%] Linking CXX static library libcommon.a
[ 18%] Linking CXX static library libllava_static.a
[ 18%] Built target gguf
[ 18%] Built target llava_static
[ 18%] Built target common
[ 18%] Built target test-c
[ 18%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 19%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 23%] Linking CXX executable ../bin/test-sampling
[ 24%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Linking CXX executable ../bin/test-quantize-fns
[ 27%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 28%] Linking CXX executable ../bin/test-chat-template
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 29%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-quantize-perf
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 36%] Built target benchmark
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 36%] Built target quantize
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 41%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 41%] Built target quantize-stats
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-1-llama
[ 47%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 49%] Built target test-sampling
[ 49%] Built target test-quantize-fns
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 51%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 52%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Linking CXX executable ../bin/test-llama-grammar
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-grammar-parser
[ 55%] Built target test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Built target test-quantize-perf
[ 57%] Linking CXX executable ../bin/test-grad0
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Built target test-tokenizer-0-falcon
[ 58%] Built target test-tokenizer-1-baichuan
[ 58%] Linking CXX executable ../bin/test-rope
[ 59%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 59%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 59%] Built target test-tokenizer-1-mpt
[ 60%] Linking CXX executable ../../bin/baby-llama
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 63%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 63%] Built target test-tokenizer-1-aquila
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Linking CXX executable ../../bin/batched
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 65%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 65%] Built target test-tokenizer-1-starcoder
[ 66%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 66%] Built target test-tokenizer-1-gpt-neox
[ 66%] Built target test-tokenizer-1-refact
[ 66%] Built target test-tokenizer-0-llama
[ 66%] Linking CXX executable ../../bin/beam-search
[ 66%] Linking CXX executable ../../bin/batched-bench
[ 66%] Built target test-tokenizer-1-falcon
[ 66%] Built target test-grammar-parser
[ 66%] Built target test-llama-grammar
[ 66%] Built target test-grad0
[ 66%] Built target test-rope
[ 67%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 68%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 68%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 68%] Linking CXX executable ../../bin/embedding
[ 69%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 70%] Linking CXX executable ../../bin/finetune
[ 70%] Built target test-backend-ops
[ 71%] Linking CXX executable ../../bin/gritlm
[ 71%] Built target test-tokenizer-1-gpt2
[ 72%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 72%] Built target baby-llama
[ 72%] Linking CXX executable ../../bin/gguf-split
[ 72%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 74%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/infill
[ 75%] Built target test-model-load-cancel
[ 75%] Built target test-autorelease
[ 75%] Built target test-json-schema-to-grammar
[ 75%] Built target batched
[ 75%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 75%] Built target convert-llama2c-to-ggml
[ 76%] Linking CXX executable ../../bin/main
[ 76%] Built target embedding
[ 76%] Built target beam-search
[ 77%] Linking CXX executable ../../bin/llava-cli
[ 78%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 78%] Built target batched-bench
[ 78%] Linking CXX executable ../../bin/parallel
[ 79%] Linking CXX executable ../../bin/tokenize
[ 79%] Built target gritlm
[ 80%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/perplexity
[ 81%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 83%] Linking CXX executable ../../bin/passkey
[ 83%] Built target finetune
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Built target gguf-split
[ 83%] Built target llama-bench
[ 84%] Linking CXX executable ../../bin/speculative
[ 85%] Linking CXX executable ../../bin/simple
[ 85%] Built target infill
[ 85%] Built target main
[ 85%] Linking CXX executable ../../bin/save-load-state
[ 85%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 85%] Built target tokenize
[ 85%] Built target parallel
[ 85%] Built target llava-cli
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 87%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 88%] Linking CXX executable ../../bin/lookahead
[ 88%] Linking CXX executable ../../bin/lookup-create
[ 88%] Linking CXX executable ../../bin/lookup-merge
[ 89%] Linking CXX executable ../../bin/lookup
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 91%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 91%] Built target perplexity
[ 92%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 92%] Linking CXX executable ../../bin/train-text-from-scratch
[ 93%] Linking CXX executable ../../bin/lookup-stats
[ 93%] Built target passkey
[ 93%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/imatrix
[ 96%] Built target simple
[ 97%] Linking CXX executable ../../bin/q8dot
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target speculative
[100%] Linking CXX executable ../../bin/server
[100%] Built target lookahead
[100%] Built target save-load-state
[100%] Built target lookup-create
[100%] Built target lookup-merge
[100%] Built target lookup
[100%] Built target train-text-from-scratch
[100%] Built target vdot
[100%] Built target export-lora
[100%] Built target q8dot
[100%] Built target lookup-stats
[100%] Built target imatrix
[100%] Built target server

real	0m1.161s
user	0m6.452s
sys	0m1.810s
+ python3 ../convert.py ../models-mnt/open-llama/3B-v2
Loading model file ../models-mnt/open-llama/3B-v2/pytorch_model.bin
params = Params(n_vocab=32000, n_embd=3200, n_layer=26, n_ctx=2048, n_ff=8640, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/3B-v2'))
Found vocab files: {'spm': PosixPath('../models-mnt/open-llama/3B-v2/tokenizer.model'), 'bpe': None, 'hfft': None}
Loading vocab file PosixPath('../models-mnt/open-llama/3B-v2/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 3200]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [3200]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [3200]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [3200]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [3200]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [3200]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [3200]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [3200]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [3200]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [3200]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [3200]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [3200]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [3200]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [3200]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [3200]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [3200]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [3200]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [3200]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [3200]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [3200]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [3200]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [3200]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [3200]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [3200]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [3200]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [3200]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [3200]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [3200]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [3200]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [3200]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [3200]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [3200]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [3200]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [3200]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [3200]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [3200]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [3200]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [3200]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [3200]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [3200]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [3200]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [3200]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [3200]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [3200]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [3200]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [3200]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [3200]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [3200]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [3200]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [3200]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [3200]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [3200]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [3200]
model.norm.weight                                -> output_norm.weight                       | F16    | [3200]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 3200]
Writing ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/237] Writing tensor token_embd.weight                      | size  32000 x   3200  | type F16  | T+   0
[  2/237] Writing tensor blk.0.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[  3/237] Writing tensor blk.0.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[  4/237] Writing tensor blk.0.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[  5/237] Writing tensor blk.0.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[  6/237] Writing tensor blk.0.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[  7/237] Writing tensor blk.0.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[  8/237] Writing tensor blk.0.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[  9/237] Writing tensor blk.0.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 10/237] Writing tensor blk.0.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 11/237] Writing tensor blk.1.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 12/237] Writing tensor blk.1.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 13/237] Writing tensor blk.1.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 14/237] Writing tensor blk.1.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 15/237] Writing tensor blk.1.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 16/237] Writing tensor blk.1.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 17/237] Writing tensor blk.1.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 18/237] Writing tensor blk.1.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 19/237] Writing tensor blk.1.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 20/237] Writing tensor blk.2.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 21/237] Writing tensor blk.2.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 22/237] Writing tensor blk.2.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 23/237] Writing tensor blk.2.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 24/237] Writing tensor blk.2.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 25/237] Writing tensor blk.2.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 26/237] Writing tensor blk.2.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 27/237] Writing tensor blk.2.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 28/237] Writing tensor blk.2.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 29/237] Writing tensor blk.3.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 30/237] Writing tensor blk.3.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 31/237] Writing tensor blk.3.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 32/237] Writing tensor blk.3.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 33/237] Writing tensor blk.3.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 34/237] Writing tensor blk.3.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 35/237] Writing tensor blk.3.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 36/237] Writing tensor blk.3.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 37/237] Writing tensor blk.3.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 38/237] Writing tensor blk.4.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 39/237] Writing tensor blk.4.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 40/237] Writing tensor blk.4.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 41/237] Writing tensor blk.4.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 42/237] Writing tensor blk.4.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 43/237] Writing tensor blk.4.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 44/237] Writing tensor blk.4.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 45/237] Writing tensor blk.4.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 46/237] Writing tensor blk.4.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 47/237] Writing tensor blk.5.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 48/237] Writing tensor blk.5.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 49/237] Writing tensor blk.5.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 50/237] Writing tensor blk.5.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 51/237] Writing tensor blk.5.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 52/237] Writing tensor blk.5.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 53/237] Writing tensor blk.5.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 54/237] Writing tensor blk.5.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 55/237] Writing tensor blk.5.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 56/237] Writing tensor blk.6.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 57/237] Writing tensor blk.6.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 58/237] Writing tensor blk.6.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 59/237] Writing tensor blk.6.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 60/237] Writing tensor blk.6.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 61/237] Writing tensor blk.6.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 62/237] Writing tensor blk.6.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 63/237] Writing tensor blk.6.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 64/237] Writing tensor blk.6.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 65/237] Writing tensor blk.7.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 66/237] Writing tensor blk.7.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 67/237] Writing tensor blk.7.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 68/237] Writing tensor blk.7.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 69/237] Writing tensor blk.7.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 70/237] Writing tensor blk.7.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 71/237] Writing tensor blk.7.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 72/237] Writing tensor blk.7.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 73/237] Writing tensor blk.7.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 74/237] Writing tensor blk.8.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 75/237] Writing tensor blk.8.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 76/237] Writing tensor blk.8.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 77/237] Writing tensor blk.8.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 78/237] Writing tensor blk.8.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 79/237] Writing tensor blk.8.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 80/237] Writing tensor blk.8.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 81/237] Writing tensor blk.8.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 82/237] Writing tensor blk.8.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 83/237] Writing tensor blk.9.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 84/237] Writing tensor blk.9.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 85/237] Writing tensor blk.9.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 86/237] Writing tensor blk.9.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 87/237] Writing tensor blk.9.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 88/237] Writing tensor blk.9.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 89/237] Writing tensor blk.9.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 90/237] Writing tensor blk.9.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 91/237] Writing tensor blk.9.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 92/237] Writing tensor blk.10.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   1
[ 93/237] Writing tensor blk.10.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   1
[ 94/237] Writing tensor blk.10.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   1
[ 95/237] Writing tensor blk.10.attn_output.weight              | size   3200 x   3200  | type F16  | T+   1
[ 96/237] Writing tensor blk.10.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[ 97/237] Writing tensor blk.10.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[ 98/237] Writing tensor blk.10.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[ 99/237] Writing tensor blk.10.attn_norm.weight                | size   3200           | type F32  | T+   2
[100/237] Writing tensor blk.10.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[101/237] Writing tensor blk.11.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[102/237] Writing tensor blk.11.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[103/237] Writing tensor blk.11.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[104/237] Writing tensor blk.11.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[105/237] Writing tensor blk.11.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[106/237] Writing tensor blk.11.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[107/237] Writing tensor blk.11.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[108/237] Writing tensor blk.11.attn_norm.weight                | size   3200           | type F32  | T+   2
[109/237] Writing tensor blk.11.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[110/237] Writing tensor blk.12.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[111/237] Writing tensor blk.12.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[112/237] Writing tensor blk.12.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[113/237] Writing tensor blk.12.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[114/237] Writing tensor blk.12.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[115/237] Writing tensor blk.12.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[116/237] Writing tensor blk.12.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[117/237] Writing tensor blk.12.attn_norm.weight                | size   3200           | type F32  | T+   2
[118/237] Writing tensor blk.12.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[119/237] Writing tensor blk.13.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[120/237] Writing tensor blk.13.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[121/237] Writing tensor blk.13.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[122/237] Writing tensor blk.13.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[123/237] Writing tensor blk.13.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[124/237] Writing tensor blk.13.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[125/237] Writing tensor blk.13.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[126/237] Writing tensor blk.13.attn_norm.weight                | size   3200           | type F32  | T+   2
[127/237] Writing tensor blk.13.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[128/237] Writing tensor blk.14.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[129/237] Writing tensor blk.14.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[130/237] Writing tensor blk.14.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[131/237] Writing tensor blk.14.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[132/237] Writing tensor blk.14.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[133/237] Writing tensor blk.14.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[134/237] Writing tensor blk.14.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[135/237] Writing tensor blk.14.attn_norm.weight                | size   3200           | type F32  | T+   2
[136/237] Writing tensor blk.14.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[137/237] Writing tensor blk.15.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[138/237] Writing tensor blk.15.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[139/237] Writing tensor blk.15.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[140/237] Writing tensor blk.15.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[141/237] Writing tensor blk.15.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[142/237] Writing tensor blk.15.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[143/237] Writing tensor blk.15.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[144/237] Writing tensor blk.15.attn_norm.weight                | size   3200           | type F32  | T+   2
[145/237] Writing tensor blk.15.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[146/237] Writing tensor blk.16.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[147/237] Writing tensor blk.16.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[148/237] Writing tensor blk.16.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[149/237] Writing tensor blk.16.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[150/237] Writing tensor blk.16.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[151/237] Writing tensor blk.16.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[152/237] Writing tensor blk.16.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[153/237] Writing tensor blk.16.attn_norm.weight                | size   3200           | type F32  | T+   3
[154/237] Writing tensor blk.16.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[155/237] Writing tensor blk.17.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[156/237] Writing tensor blk.17.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[157/237] Writing tensor blk.17.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[158/237] Writing tensor blk.17.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[159/237] Writing tensor blk.17.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[160/237] Writing tensor blk.17.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[161/237] Writing tensor blk.17.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[162/237] Writing tensor blk.17.attn_norm.weight                | size   3200           | type F32  | T+   3
[163/237] Writing tensor blk.17.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[164/237] Writing tensor blk.18.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[165/237] Writing tensor blk.18.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[166/237] Writing tensor blk.18.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[167/237] Writing tensor blk.18.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[168/237] Writing tensor blk.18.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[169/237] Writing tensor blk.18.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[170/237] Writing tensor blk.18.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[171/237] Writing tensor blk.18.attn_norm.weight                | size   3200           | type F32  | T+   3
[172/237] Writing tensor blk.18.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[173/237] Writing tensor blk.19.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[174/237] Writing tensor blk.19.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[175/237] Writing tensor blk.19.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[176/237] Writing tensor blk.19.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[177/237] Writing tensor blk.19.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[178/237] Writing tensor blk.19.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[179/237] Writing tensor blk.19.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[180/237] Writing tensor blk.19.attn_norm.weight                | size   3200           | type F32  | T+   3
[181/237] Writing tensor blk.19.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[182/237] Writing tensor blk.20.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[183/237] Writing tensor blk.20.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[184/237] Writing tensor blk.20.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[185/237] Writing tensor blk.20.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[186/237] Writing tensor blk.20.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[187/237] Writing tensor blk.20.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[188/237] Writing tensor blk.20.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[189/237] Writing tensor blk.20.attn_norm.weight                | size   3200           | type F32  | T+   3
[190/237] Writing tensor blk.20.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[191/237] Writing tensor blk.21.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[192/237] Writing tensor blk.21.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[193/237] Writing tensor blk.21.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[194/237] Writing tensor blk.21.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[195/237] Writing tensor blk.21.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[196/237] Writing tensor blk.21.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[197/237] Writing tensor blk.21.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[198/237] Writing tensor blk.21.attn_norm.weight                | size   3200           | type F32  | T+   3
[199/237] Writing tensor blk.21.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[200/237] Writing tensor blk.22.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[201/237] Writing tensor blk.22.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[202/237] Writing tensor blk.22.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[203/237] Writing tensor blk.22.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[204/237] Writing tensor blk.22.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[205/237] Writing tensor blk.22.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[206/237] Writing tensor blk.22.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[207/237] Writing tensor blk.22.attn_norm.weight                | size   3200           | type F32  | T+   3
[208/237] Writing tensor blk.22.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[209/237] Writing tensor blk.23.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[210/237] Writing tensor blk.23.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[211/237] Writing tensor blk.23.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[212/237] Writing tensor blk.23.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[213/237] Writing tensor blk.23.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[214/237] Writing tensor blk.23.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[215/237] Writing tensor blk.23.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[216/237] Writing tensor blk.23.attn_norm.weight                | size   3200           | type F32  | T+   4
[217/237] Writing tensor blk.23.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[218/237] Writing tensor blk.24.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[219/237] Writing tensor blk.24.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[220/237] Writing tensor blk.24.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[221/237] Writing tensor blk.24.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[222/237] Writing tensor blk.24.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[223/237] Writing tensor blk.24.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[224/237] Writing tensor blk.24.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[225/237] Writing tensor blk.24.attn_norm.weight                | size   3200           | type F32  | T+   4
[226/237] Writing tensor blk.24.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[227/237] Writing tensor blk.25.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[228/237] Writing tensor blk.25.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[229/237] Writing tensor blk.25.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[230/237] Writing tensor blk.25.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[231/237] Writing tensor blk.25.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[232/237] Writing tensor blk.25.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[233/237] Writing tensor blk.25.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[234/237] Writing tensor blk.25.attn_norm.weight                | size   3200           | type F32  | T+   4
[235/237] Writing tensor blk.25.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[236/237] Writing tensor output_norm.weight                     | size   3200           | type F32  | T+   4
[237/237] Writing tensor output.weight                          | size  32000 x   3200  | type F16  | T+   4
Wrote ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf q8_0
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  3472.45 MB

main: quantize time = 10961.84 ms
main:    total time = 10961.84 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf q4_0
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_0 .. size =   195.31 MiB ->    54.93 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1866.13 MB

main: quantize time =  6152.12 ms
main:    total time =  6152.12 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf q4_1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_1 .. size =   195.31 MiB ->    61.04 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2064.25 MB

main: quantize time =  6296.98 ms
main:    total time =  6296.98 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf q5_0
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_0 .. size =   195.31 MiB ->    67.14 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2262.37 MB

main: quantize time =  6939.14 ms
main:    total time =  6939.14 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf q5_1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_1 .. size =   195.31 MiB ->    73.24 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2460.49 MB

main: quantize time =  7223.24 ms
main:    total time =  7223.24 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf q2_k
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q2_K .. size =   195.31 MiB ->    36.62 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1346.35 MB

main: quantize time = 32388.22 ms
main:    total time = 32388.22 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf q3_k
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q3_K .. size =   195.31 MiB ->    42.72 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1662.08 MB

main: quantize time = 29392.30 ms
main:    total time = 29392.30 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf q4_k
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_K .. size =   195.31 MiB ->    57.98 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2082.62 MB

main: quantize time = 53025.79 ms
main:    total time = 53025.79 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf q5_k
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_K .. size =   195.31 MiB ->    70.19 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2420.14 MB

main: quantize time = 35132.41 ms
main:    total time = 35132.41 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf q6_k
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llama_model_quantize_internal: meta size = 749824 bytes
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2757.67 MB

main: quantize time = 34194.87 ms
main:    total time = 34194.87 ms
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. To love life, to love yourself, to love others. To be able to love others, you must first be able to love yourself, and to love yourself you must first love.
The world is a complex place, the universe is even more. The world is not perfect, nor is the universe
llama_print_timings:        load time =     671.69 ms
llama_print_timings:      sample time =       2.66 ms /    64 runs   (    0.04 ms per token, 24069.20 tokens per second)
llama_print_timings: prompt eval time =     531.82 ms /     8 tokens (   66.48 ms per token,    15.04 tokens per second)
llama_print_timings:        eval time =   10100.08 ms /    63 runs   (  160.32 ms per token,     6.24 tokens per second)
llama_print_timings:       total time =   10654.36 ms /    71 tokens
Log end

real	0m11.660s
user	0m43.203s
sys	0m0.844s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. To love life, to love each other, to love our world. We are all connected. We are all interwoven. We are all the same. We are all one.
I believe that all the things you need in this world are already inside of you. You are already whole. You
llama_print_timings:        load time =     432.06 ms
llama_print_timings:      sample time =       2.26 ms /    64 runs   (    0.04 ms per token, 28306.06 tokens per second)
llama_print_timings: prompt eval time =     364.23 ms /     8 tokens (   45.53 ms per token,    21.96 tokens per second)
llama_print_timings:        eval time =    6382.42 ms /    63 runs   (  101.31 ms per token,     9.87 tokens per second)
llama_print_timings:       total time =    6766.55 ms /    71 tokens
Log end

real	0m7.524s
user	0m27.364s
sys	0m0.676s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to experience it, not to have it explained to you.
So, I am going to share with you the journey that I have been on in my life. My life is a story of adventure and discovery of what the real truth is.
My journey started at the age of 12. I started with
llama_print_timings:        load time =     295.33 ms
llama_print_timings:      sample time =       2.28 ms /    64 runs   (    0.04 ms per token, 28033.29 tokens per second)
llama_print_timings: prompt eval time =     372.09 ms /     8 tokens (   46.51 ms per token,    21.50 tokens per second)
llama_print_timings:        eval time =    4513.06 ms /    63 runs   (   71.64 ms per token,    13.96 tokens per second)
llama_print_timings:       total time =    4905.17 ms /    71 tokens
Log end

real	0m5.392s
user	0m19.851s
sys	0m0.412s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make the most of the time that you have. I believe the meaning of life is to be happy. I believe the meaning of life is to enjoy whatever you do. I believe the meaning of life is to love. I believe the meaning of life is to enjoy the moment. I believe the meaning of life is
llama_print_timings:        load time =     317.41 ms
llama_print_timings:      sample time =       2.27 ms /    64 runs   (    0.04 ms per token, 28131.87 tokens per second)
llama_print_timings: prompt eval time =     401.43 ms /     8 tokens (   50.18 ms per token,    19.93 tokens per second)
llama_print_timings:        eval time =    4945.12 ms /    63 runs   (   78.49 ms per token,    12.74 tokens per second)
llama_print_timings:       total time =    5365.89 ms /    71 tokens
Log end

real	0m5.894s
user	0m21.712s
sys	0m0.447s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find our purpose, live it, and inspire others to do the same.
I am a mom, wife, and entrepreneur. I'm a writer, a public speaker, and a woman who believes in the power of women and in the strength of faith. I live in the Pacific Northwest, where I spend
llama_print_timings:        load time =     345.35 ms
llama_print_timings:      sample time =       2.30 ms /    64 runs   (    0.04 ms per token, 27777.78 tokens per second)
llama_print_timings: prompt eval time =     454.54 ms /     8 tokens (   56.82 ms per token,    17.60 tokens per second)
llama_print_timings:        eval time =    5454.83 ms /    63 runs   (   86.58 ms per token,    11.55 tokens per second)
llama_print_timings:       total time =    5928.41 ms /    71 tokens
Log end

real	0m6.498s
user	0m24.044s
sys	0m0.464s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to live and die as the best version of ourselves.
I believe that a life well lived is a life well lived.
I believe we are all born with gifts and talents,
and we all have the capacity to be leaders, teachers and
co-creators of the world.
I believe in the
llama_print_timings:        load time =     360.25 ms
llama_print_timings:      sample time =       2.27 ms /    64 runs   (    0.04 ms per token, 28156.62 tokens per second)
llama_print_timings: prompt eval time =     459.06 ms /     8 tokens (   57.38 ms per token,    17.43 tokens per second)
llama_print_timings:        eval time =    5663.81 ms /    63 runs   (   89.90 ms per token,    11.12 tokens per second)
llama_print_timings:       total time =    6142.58 ms /    71 tokens
Log end

real	0m6.744s
user	0m24.968s
sys	0m0.432s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make your own, and to create your own meaning.
I am a writer, a teacher, and an artisan. I have an eclectic collection of interests that I love to share with the world.
My goal in life is to teach and inspire people to make their own meaning.
I am
llama_print_timings:        load time =     317.75 ms
llama_print_timings:      sample time =       2.30 ms /    64 runs   (    0.04 ms per token, 27789.84 tokens per second)
llama_print_timings: prompt eval time =     642.02 ms /     8 tokens (   80.25 ms per token,    12.46 tokens per second)
llama_print_timings:        eval time =    5796.32 ms /    63 runs   (   92.01 ms per token,    10.87 tokens per second)
llama_print_timings:       total time =    6457.81 ms /    71 tokens
Log end

real	0m6.926s
user	0m26.104s
sys	0m0.515s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to help others and not to be helped.
I believe that love is the greatest thing in the world and is not to be taken for granted.
I believe that happiness is the greatest goal in life and is not to be taken for granted.
I believe that the power of words are amazing and can change lives
llama_print_timings:        load time =     306.67 ms
llama_print_timings:      sample time =       2.27 ms /    64 runs   (    0.04 ms per token, 28231.14 tokens per second)
llama_print_timings: prompt eval time =     489.64 ms /     8 tokens (   61.21 ms per token,    16.34 tokens per second)
llama_print_timings:        eval time =    5099.86 ms /    63 runs   (   80.95 ms per token,    12.35 tokens per second)
llama_print_timings:       total time =    5609.13 ms /    71 tokens
Log end

real	0m6.092s
user	0m22.659s
sys	0m0.488s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love, and Im not sure Ive ever met anyone who doesnt love. Ive had this belief since I was a little girl, so Im not sure when I first really understood this to mean something. I know that I believe in love, and I know that I believe
llama_print_timings:        load time =     321.34 ms
llama_print_timings:      sample time =       2.25 ms /    64 runs   (    0.04 ms per token, 28381.37 tokens per second)
llama_print_timings: prompt eval time =     417.00 ms /     8 tokens (   52.13 ms per token,    19.18 tokens per second)
llama_print_timings:        eval time =    5069.68 ms /    63 runs   (   80.47 ms per token,    12.43 tokens per second)
llama_print_timings:       total time =    5506.55 ms /    71 tokens
Log end

real	0m6.038s
user	0m22.219s
sys	0m0.530s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find our passion, then to live our passion.
I believe in the power of a positive attitude.
I believe that success is not about the things you have, its about the things you do.
I believe in the value of relationships, not the value of possessions.
I believe in
llama_print_timings:        load time =     366.47 ms
llama_print_timings:      sample time =       2.26 ms /    64 runs   (    0.04 ms per token, 28306.06 tokens per second)
llama_print_timings: prompt eval time =     492.89 ms /     8 tokens (   61.61 ms per token,    16.23 tokens per second)
llama_print_timings:        eval time =    5835.40 ms /    63 runs   (   92.63 ms per token,    10.80 tokens per second)
llama_print_timings:       total time =    6347.62 ms /    71 tokens
Log end

real	0m6.952s
user	0m25.492s
sys	0m0.751s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. To be a loving person and to love others. I believe the meaning of life is to love. To be a loving person and to love others. I believe in the power of love. Love is the only reason I wake up in the morning. Love is the only reason Im breathing today.
llama_print_timings:        load time =     403.14 ms
llama_print_timings:      sample time =       2.25 ms /    64 runs   (    0.04 ms per token, 28419.18 tokens per second)
llama_print_timings: prompt eval time =     545.52 ms /     8 tokens (   68.19 ms per token,    14.66 tokens per second)
llama_print_timings:        eval time =    6436.92 ms /    63 runs   (  102.17 ms per token,     9.79 tokens per second)
llama_print_timings:       total time =    7002.18 ms /    71 tokens
Log end

real	0m7.670s
user	0m28.222s
sys	0m0.756s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213700
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.024 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.52 seconds per pass - ETA 0.12 minutes
[1]4.2453,
llama_print_timings:        load time =     635.72 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7516.41 ms /   128 tokens (   58.72 ms per token,    17.03 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7526.47 ms /   129 tokens

Final estimate: PPL = 4.2453 +/- 1.01618

real	0m8.492s
user	0m30.757s
sys	0m0.924s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213708
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.934 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.13 seconds per pass - ETA 0.08 minutes
[1]4.2539,
llama_print_timings:        load time =     395.01 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5129.83 ms /   128 tokens (   40.08 ms per token,    24.95 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5139.38 ms /   129 tokens

Final estimate: PPL = 4.2539 +/- 1.02214

real	0m5.745s
user	0m21.052s
sys	0m0.508s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213714
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.984 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.63 seconds per pass - ETA 0.08 minutes
[1]4.1757,
llama_print_timings:        load time =     262.97 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5629.58 ms /   128 tokens (   43.98 ms per token,    22.74 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5639.53 ms /   129 tokens

Final estimate: PPL = 4.1757 +/- 0.97989

real	0m6.032s
user	0m22.893s
sys	0m0.396s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213720
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.887 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.09 seconds per pass - ETA 0.10 minutes
[1]4.3727,
llama_print_timings:        load time =     279.87 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6093.07 ms /   128 tokens (   47.60 ms per token,    21.01 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    6103.47 ms /   129 tokens

Final estimate: PPL = 4.3727 +/- 1.04539

real	0m6.522s
user	0m24.777s
sys	0m0.400s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213727
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.912 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.99 seconds per pass - ETA 0.10 minutes
[1]4.3332,
llama_print_timings:        load time =     309.28 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6993.44 ms /   128 tokens (   54.64 ms per token,    18.30 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7003.74 ms /   129 tokens

Final estimate: PPL = 4.3332 +/- 1.03830

real	0m7.460s
user	0m28.495s
sys	0m0.372s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213734
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.853 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.07 seconds per pass - ETA 0.12 minutes
[1]4.2597,
llama_print_timings:        load time =     327.48 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7069.69 ms /   128 tokens (   55.23 ms per token,    18.11 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7079.90 ms /   129 tokens

Final estimate: PPL = 4.2597 +/- 1.02241

real	0m7.565s
user	0m28.690s
sys	0m0.528s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213742
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.983 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 10.16 seconds per pass - ETA 0.17 minutes
[1]5.6444,
llama_print_timings:        load time =     284.48 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   10156.81 ms /   128 tokens (   79.35 ms per token,    12.60 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   10166.82 ms /   129 tokens

Final estimate: PPL = 5.6444 +/- 1.37843

real	0m10.558s
user	0m40.964s
sys	0m0.592s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213752
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.917 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.65 seconds per pass - ETA 0.12 minutes
[1]4.4045,
llama_print_timings:        load time =     272.18 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7646.97 ms /   128 tokens (   59.74 ms per token,    16.74 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7656.71 ms /   129 tokens

Final estimate: PPL = 4.4045 +/- 1.04658

real	0m8.049s
user	0m30.749s
sys	0m0.672s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213760
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.882 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.34 seconds per pass - ETA 0.10 minutes
[1]4.1832,
llama_print_timings:        load time =     287.03 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6336.92 ms /   128 tokens (   49.51 ms per token,    20.20 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    6346.70 ms /   129 tokens

Final estimate: PPL = 4.1832 +/- 0.99347

real	0m6.772s
user	0m25.287s
sys	0m0.892s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213767
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.851 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.48 seconds per pass - ETA 0.12 minutes
[1]4.2451,
llama_print_timings:        load time =     329.75 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7476.54 ms /   128 tokens (   58.41 ms per token,    17.12 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7486.91 ms /   129 tokens

Final estimate: PPL = 4.2451 +/- 1.00899

real	0m7.974s
user	0m30.071s
sys	0m0.783s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213775
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 7.986 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 8.37 seconds per pass - ETA 0.13 minutes
[1]4.2445,
llama_print_timings:        load time =     373.81 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    8367.17 ms /   128 tokens (   65.37 ms per token,    15.30 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    8377.34 ms /   129 tokens

Final estimate: PPL = 4.2445 +/- 1.02325

real	0m8.929s
user	0m33.634s
sys	0m0.904s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213784
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 7.947 ms
compute_imatrix: computing over 1 chunks with batch_size 128
compute_imatrix: 7.59 seconds per pass - ETA 0.12 minutes
[1]4.2453,
save_imatrix: stored collected data after 1 chunks in imatrix.dat

llama_print_timings:        load time =    8029.36 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7594.57 ms /   128 tokens (   59.33 ms per token,    16.85 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    8033.79 ms /   129 tokens

Final estimate: PPL = 4.2453 +/- 1.01618

real	0m8.363s
user	0m30.172s
sys	0m0.861s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1
main : serialized state into 67206811 out of a maximum of 236003404 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1
main : deserialized state from 67206811 out of a maximum of 236003404 bytes

main : success

first run: The quick brown fox is a classic 55 filler puzzle. It does not use numbers


second run: The quick brown fox is a classic 55 filler puzzle. It does not use numbers

real	0m3.240s
user	0m10.700s
sys	0m0.459s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]4.2453,'
+ qnt=f16
++ echo '[1]4.2453,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2453
++ echo '4.2453 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 4.2453
+ return 0
  - f16 @ 4.2453 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]4.2539,'
+ qnt=q8_0
++ echo '[1]4.2539,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2539
++ echo '4.2539 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 4.2539
+ return 0
  - q8_0 @ 4.2539 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]4.1757,'
+ qnt=q4_0
++ echo '[1]4.1757,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.1757
++ echo '4.1757 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 4.1757
+ return 0
  - q4_0 @ 4.1757 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]4.3727,'
+ qnt=q4_1
++ echo '[1]4.3727,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.3727
++ echo '4.3727 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 4.3727
+ return 0
  - q4_1 @ 4.3727 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]4.3332,'
+ qnt=q5_0
++ echo '[1]4.3332,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.3332
++ echo '4.3332 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 4.3332
+ return 0
  - q5_0 @ 4.3332 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]4.2597,'
+ qnt=q5_1
++ echo '[1]4.2597,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2597
++ echo '4.2597 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 4.2597
+ return 0
  - q5_1 @ 4.2597 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
++ grep '^\[1\]'
+ check_ppl q2_k '[1]5.6444,'
+ qnt=q2_k
++ echo '[1]5.6444,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=5.6444
++ echo '5.6444 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 5.6444
+ return 0
  - q2_k @ 5.6444 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]4.4045,'
+ qnt=q3_k
++ echo '[1]4.4045,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.4045
++ echo '4.4045 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 4.4045
+ return 0
  - q3_k @ 4.4045 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]4.1832,'
+ qnt=q4_k
++ echo '[1]4.1832,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.1832
++ echo '4.1832 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 4.1832
+ return 0
  - q4_k @ 4.1832 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]4.2451,'
+ qnt=q5_k
++ echo '[1]4.2451,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2451
++ echo '4.2451 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 4.2451
+ return 0
  - q5_k @ 4.2451 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]4.2445,'
+ qnt=q6_k
++ echo '[1]4.2445,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2445
++ echo '4.2445 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 4.2445
+ return 0
  - q6_k @ 4.2445 OK
+ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-imatrix.log
+ grep Final
+ path_lora=../models-mnt/open-llama/3B-v2/lora
+ path_shakespeare=../models-mnt/shakespeare
+ shakespeare=../models-mnt/shakespeare/shakespeare.txt
+ lora_shakespeare=../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin
+ gg_wget ../models-mnt/open-llama/3B-v2/lora https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
+ local out=../models-mnt/open-llama/3B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/3B-v2/lora
+ cd ../models-mnt/open-llama/3B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:09:56 URL:https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_config.json [457/457] -> "adapter_config.json" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/open-llama/3B-v2/lora https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ local out=../models-mnt/open-llama/3B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/3B-v2/lora
+ cd ../models-mnt/open-llama/3B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/shakespeare https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
+ local out=../models-mnt/shakespeare
+ local url=https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/shakespeare
+ cd ../models-mnt/shakespeare
+ wget -nv -N https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:09:56 URL:https://huggingface.co/slaren/open_llama_3b_v2_shakespeare_lora/resolve/main/shakespeare.txt [94275/94275] -> "shakespeare.txt" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ python3 ../convert-lora-to-ggml.py ../models-mnt/open-llama/3B-v2/lora
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (3200, 64) float32 0.78MB
Converted ../models-mnt/open-llama/3B-v2/lora/adapter_config.json and ../models-mnt/open-llama/3B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213797
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 47.019 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.57 seconds per pass - ETA 0.12 minutes
[1]9.1010,
llama_print_timings:        load time =     632.21 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7566.51 ms /   128 tokens (   59.11 ms per token,    16.92 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7616.14 ms /   129 tokens

Final estimate: PPL = 9.1010 +/- 2.25269

real	0m8.573s
user	0m31.092s
sys	0m0.812s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213806
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
............. done (2650.08 ms)

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 45.451 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.43 seconds per pass - ETA 0.12 minutes
[1]7.0168,
llama_print_timings:        load time =    5570.83 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7429.89 ms /   128 tokens (   58.05 ms per token,    17.23 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7477.39 ms /   129 tokens

Final estimate: PPL = 7.0168 +/- 1.75912

real	0m13.212s
user	0m38.398s
sys	0m4.532s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-f16.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-f16.log
++ grep '^\[1\]'
+ compare_ppl 'f16 shakespeare' '[1]9.1010,' '[1]7.0168,'
+ qnt='f16 shakespeare'
++ echo '[1]9.1010,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=9.1010
++ echo '[1]7.0168,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=7.0168
++ echo '9.1010 < 7.0168'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'f16 shakespeare' 9.1010 7.0168
+ return 0
  - f16 shakespeare @ 9.1010 7.0168 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213819
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 47.37 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.12 seconds per pass - ETA 0.08 minutes
[1]9.2039,
llama_print_timings:        load time =     389.90 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5118.88 ms /   128 tokens (   39.99 ms per token,    25.01 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5168.20 ms /   129 tokens

Final estimate: PPL = 9.2039 +/- 2.28720

real	0m5.766s
user	0m21.042s
sys	0m0.508s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213825
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base
............. done (2548.64 ms)

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 44.652 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.11 seconds per pass - ETA 0.08 minutes
[1]7.0066,
llama_print_timings:        load time =    3944.95 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5112.48 ms /   128 tokens (   39.94 ms per token,    25.04 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5159.14 ms /   129 tokens

Final estimate: PPL = 7.0066 +/- 1.74769

real	0m9.189s
user	0m28.735s
sys	0m2.964s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0.log
++ grep '^\[1\]'
+ compare_ppl 'q8_0 shakespeare' '[1]9.2039,' '[1]7.0066,'
+ qnt='q8_0 shakespeare'
++ echo '[1]9.2039,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=9.2039
++ echo '[1]7.0066,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=7.0066
++ echo '9.2039 < 7.0066'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'q8_0 shakespeare' 9.2039 7.0066
+ return 0
  - q8_0 shakespeare @ 9.2039 7.0066 OK
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin --lora-base ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -c 128 -b 128 --chunks 1
main: build = 2513 (1997577d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711213834
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =    15.63 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 862
llama_new_context_with_model: graph splits = 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: loading base model from '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf'
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
............. done (2657.25 ms)

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 44.271 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.66 seconds per pass - ETA 0.08 minutes
[1]6.9704,
llama_print_timings:        load time =    4102.39 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5658.55 ms /   128 tokens (   44.21 ms per token,    22.62 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5705.42 ms /   129 tokens

Final estimate: PPL = 6.9704 +/- 1.74024

real	0m9.894s
user	0m30.811s
sys	0m3.167s
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-q8_0.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/open_llama_3b_v2-ppl-shakespeare-lora-q8_0-f16.log
++ grep '^\[1\]'
+ compare_ppl 'q8_0 / f16 base shakespeare' '[1]9.2039,' '[1]6.9704,'
+ qnt='q8_0 / f16 base shakespeare'
++ echo '[1]9.2039,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=9.2039
++ echo '[1]6.9704,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=6.9704
++ echo '9.2039 < 6.9704'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'q8_0 / f16 base shakespeare' 9.2039 6.9704
+ return 0
  - q8_0 / f16 base shakespeare @ 9.2039 6.9704 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_with_model_debug.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.14 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    0.92 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.06 sec*proc (2 tests)

Total Test time (real) =   1.06 sec
0.29user 0.78system 0:01.08elapsed 100%CPU (0avgtext+0avgdata 6942548maxresident)k
0inputs+56outputs (0major+113150minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ tee /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_with_model_release.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/19/97577d5e121568ae39f538021733ccd4278c23/ggml-2-x86-cpu/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.04 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    0.79 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.82 sec*proc (2 tests)

Total Test time (real) =   0.83 sec
0.10user 0.74system 0:00.84elapsed 100%CPU (0avgtext+0avgdata 6942232maxresident)k
0inputs+40outputs (0major+112619minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
