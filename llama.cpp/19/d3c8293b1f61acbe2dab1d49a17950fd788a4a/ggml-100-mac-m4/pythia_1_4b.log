Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.662s
user	0m0.909s
sys	0m1.294s
++ nproc
+ make -j10
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Linking CXX executable ../bin/test-llama-grammar
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Built target test-llama-grammar
[ 47%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Linking CXX executable ../bin/test-log
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Built target test-sampling
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Built target test-log
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-quantize-perf
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookup
[ 79%] Built target llama-bench
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-lookup-create
[ 79%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-parallel
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-cli
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-passkey
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-retrieval
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-save-load-state
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.119s
user	0m6.542s
sys	0m9.962s

main: quantize time =  5260.94 ms
main:    total time =  5260.94 ms

main: quantize time =  3265.99 ms
main:    total time =  3265.99 ms

main: quantize time =  3637.86 ms
main:    total time =  3637.86 ms

main: quantize time =  3630.91 ms
main:    total time =  3630.91 ms

main: quantize time =  2507.75 ms
main:    total time =  2507.75 ms

main: quantize time =  6232.21 ms
main:    total time =  6232.21 ms

main: quantize time =  6097.05 ms
main:    total time =  6097.05 ms

main: quantize time =  6745.14 ms
main:    total time =  6745.14 ms

main: quantize time =  6202.29 ms
main:    total time =  6202.29 ms

main: quantize time =  4554.78 ms
main:    total time =  4554.78 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.227 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.387 I main: llama backend init
0.00.000.393 I main: load the model and apply lora adapter, if any
0.00.084.986 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.097.329 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.097.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.097.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.097.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.097.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.097.352 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.097.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.097.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.097.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.097.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.097.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.097.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.097.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.097.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.097.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.097.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.097.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.104.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.106.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.115.236 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.115.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.115.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.115.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.115.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.115.248 I llama_model_loader: - type  f32:  194 tensors
0.00.115.248 I llama_model_loader: - type  f16:   98 tensors
0.00.115.250 I print_info: file format = GGUF V3 (latest)
0.00.115.256 I print_info: file type   = all F32 (guessed)
0.00.115.264 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.132.508 I load: special tokens cache size = 25
0.00.142.290 I load: token to piece cache size = 0.2984 MB
0.00.142.294 I print_info: arch             = gptneox
0.00.142.294 I print_info: vocab_only       = 0
0.00.142.294 I print_info: n_ctx_train      = 2048
0.00.142.294 I print_info: n_embd           = 2048
0.00.142.294 I print_info: n_layer          = 24
0.00.142.298 I print_info: n_head           = 16
0.00.142.299 I print_info: n_head_kv        = 16
0.00.142.300 I print_info: n_rot            = 32
0.00.142.300 I print_info: n_swa            = 0
0.00.142.300 I print_info: n_embd_head_k    = 128
0.00.142.300 I print_info: n_embd_head_v    = 128
0.00.142.301 I print_info: n_gqa            = 1
0.00.142.302 I print_info: n_embd_k_gqa     = 2048
0.00.142.303 I print_info: n_embd_v_gqa     = 2048
0.00.142.304 I print_info: f_norm_eps       = 1.0e-05
0.00.142.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.142.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.142.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.142.305 I print_info: f_logit_scale    = 0.0e+00
0.00.142.306 I print_info: n_ff             = 8192
0.00.142.306 I print_info: n_expert         = 0
0.00.142.307 I print_info: n_expert_used    = 0
0.00.142.307 I print_info: causal attn      = 1
0.00.142.307 I print_info: pooling type     = 0
0.00.142.307 I print_info: rope type        = 2
0.00.142.308 I print_info: rope scaling     = linear
0.00.142.308 I print_info: freq_base_train  = 10000.0
0.00.142.309 I print_info: freq_scale_train = 1
0.00.142.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.142.309 I print_info: rope_finetuned   = unknown
0.00.142.309 I print_info: ssm_d_conv       = 0
0.00.142.309 I print_info: ssm_d_inner      = 0
0.00.142.310 I print_info: ssm_d_state      = 0
0.00.142.310 I print_info: ssm_dt_rank      = 0
0.00.142.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.142.310 I print_info: model type       = 1.4B
0.00.142.311 I print_info: model params     = 1.41 B
0.00.142.311 I print_info: general.name     = 1.4B
0.00.142.311 I print_info: vocab type       = BPE
0.00.142.312 I print_info: n_vocab          = 50304
0.00.142.314 I print_info: n_merges         = 50009
0.00.142.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.142.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.142.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.142.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.142.315 I print_info: LF token         = 187 'Ċ'
0.00.142.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.142.316 I print_info: max token length = 1024
0.00.142.317 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.196.990 I load_tensors: offloading 24 repeating layers to GPU
0.00.196.995 I load_tensors: offloading output layer to GPU
0.00.196.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.197.019 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.197.021 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.197.579 I llama_init_from_model: n_seq_max     = 1
0.00.197.580 I llama_init_from_model: n_ctx         = 2048
0.00.197.580 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.197.580 I llama_init_from_model: n_batch       = 2048
0.00.197.580 I llama_init_from_model: n_ubatch      = 512
0.00.197.581 I llama_init_from_model: flash_attn    = 0
0.00.197.581 I llama_init_from_model: freq_base     = 10000.0
0.00.197.581 I llama_init_from_model: freq_scale    = 1
0.00.197.582 I ggml_metal_init: allocating
0.00.197.618 I ggml_metal_init: found device: Apple M4
0.00.197.625 I ggml_metal_init: picking default device: Apple M4
0.00.198.292 I ggml_metal_init: using embedded metal library
0.00.225.997 I ggml_metal_init: GPU name:   Apple M4
0.00.225.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.226.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.226.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.226.001 I ggml_metal_init: simdgroup reduction   = true
0.00.226.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.226.001 I ggml_metal_init: has residency sets    = true
0.00.226.001 I ggml_metal_init: has bfloat            = true
0.00.226.001 I ggml_metal_init: use bfloat            = true
0.00.226.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.226.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.587 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.403.436 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.403.443 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.403.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.407.948 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.407.951 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.407.951 I llama_init_from_model: graph nodes  = 967
0.00.407.951 I llama_init_from_model: graph splits = 2
0.00.407.958 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.408.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.408.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.563 I main: llama threadpool init, n_threads = 4
0.00.474.607 I 
0.00.474.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.639 I 
0.00.474.815 I sampler seed: 1234
0.00.474.820 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.474.844 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.474.846 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.474.846 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.306.311 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.02.306.311 I llama_perf_context_print:        load time =     388.70 ms
0.02.306.312 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.34 tokens per second)
0.02.306.313 I llama_perf_context_print:        eval time =    1784.85 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.306.313 I llama_perf_context_print:       total time =    1832.62 ms /    70 tokens
0.02.306.501 I ggml_metal_free: deallocating

real	0m2.616s
user	0m0.136s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.995 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.131 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.113 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.113 I llama_model_loader: - type  f32:  194 tensors
0.00.036.114 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.114 I print_info: file format = GGUF V3 (latest)
0.00.036.115 I print_info: file type   = Q8_0
0.00.036.121 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.305 I load: special tokens cache size = 25
0.00.050.365 I load: token to piece cache size = 0.2984 MB
0.00.050.372 I print_info: arch             = gptneox
0.00.050.372 I print_info: vocab_only       = 0
0.00.050.373 I print_info: n_ctx_train      = 2048
0.00.050.373 I print_info: n_embd           = 2048
0.00.050.373 I print_info: n_layer          = 24
0.00.050.380 I print_info: n_head           = 16
0.00.050.381 I print_info: n_head_kv        = 16
0.00.050.381 I print_info: n_rot            = 32
0.00.050.381 I print_info: n_swa            = 0
0.00.050.381 I print_info: n_embd_head_k    = 128
0.00.050.382 I print_info: n_embd_head_v    = 128
0.00.050.383 I print_info: n_gqa            = 1
0.00.050.383 I print_info: n_embd_k_gqa     = 2048
0.00.050.384 I print_info: n_embd_v_gqa     = 2048
0.00.050.385 I print_info: f_norm_eps       = 1.0e-05
0.00.050.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.386 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.386 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.386 I print_info: f_logit_scale    = 0.0e+00
0.00.050.387 I print_info: n_ff             = 8192
0.00.050.387 I print_info: n_expert         = 0
0.00.050.387 I print_info: n_expert_used    = 0
0.00.050.387 I print_info: causal attn      = 1
0.00.050.388 I print_info: pooling type     = 0
0.00.050.388 I print_info: rope type        = 2
0.00.050.388 I print_info: rope scaling     = linear
0.00.050.388 I print_info: freq_base_train  = 10000.0
0.00.050.389 I print_info: freq_scale_train = 1
0.00.050.389 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.389 I print_info: rope_finetuned   = unknown
0.00.050.389 I print_info: ssm_d_conv       = 0
0.00.050.389 I print_info: ssm_d_inner      = 0
0.00.050.389 I print_info: ssm_d_state      = 0
0.00.050.389 I print_info: ssm_dt_rank      = 0
0.00.050.390 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.390 I print_info: model type       = 1.4B
0.00.050.390 I print_info: model params     = 1.41 B
0.00.050.390 I print_info: general.name     = 1.4B
0.00.050.391 I print_info: vocab type       = BPE
0.00.050.391 I print_info: n_vocab          = 50304
0.00.050.391 I print_info: n_merges         = 50009
0.00.050.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: LF token         = 187 'Ċ'
0.00.050.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: max token length = 1024
0.00.050.393 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.155.523 I load_tensors: offloading 24 repeating layers to GPU
0.01.155.528 I load_tensors: offloading output layer to GPU
0.01.155.529 I load_tensors: offloaded 25/25 layers to GPU
0.01.155.550 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.155.552 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.156.477 I llama_init_from_model: n_seq_max     = 1
0.01.156.479 I llama_init_from_model: n_ctx         = 2048
0.01.156.479 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.156.479 I llama_init_from_model: n_batch       = 2048
0.01.156.480 I llama_init_from_model: n_ubatch      = 512
0.01.156.480 I llama_init_from_model: flash_attn    = 0
0.01.156.481 I llama_init_from_model: freq_base     = 10000.0
0.01.156.481 I llama_init_from_model: freq_scale    = 1
0.01.156.482 I ggml_metal_init: allocating
0.01.156.491 I ggml_metal_init: found device: Apple M4
0.01.156.497 I ggml_metal_init: picking default device: Apple M4
0.01.157.713 I ggml_metal_init: using embedded metal library
0.01.163.319 I ggml_metal_init: GPU name:   Apple M4
0.01.163.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.163.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.163.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.163.324 I ggml_metal_init: simdgroup reduction   = true
0.01.163.324 I ggml_metal_init: simdgroup matrix mul. = true
0.01.163.325 I ggml_metal_init: has residency sets    = true
0.01.163.325 I ggml_metal_init: has bfloat            = true
0.01.163.325 I ggml_metal_init: use bfloat            = true
0.01.163.326 I ggml_metal_init: hasUnifiedMemory      = true
0.01.163.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.179.431 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.233.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.233.371 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.233.392 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.238.151 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.238.152 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.238.153 I llama_init_from_model: graph nodes  = 967
0.01.238.153 I llama_init_from_model: graph splits = 2
0.01.238.158 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.238.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.238.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.294.887 I main: llama threadpool init, n_threads = 4
0.01.294.928 I 
0.01.294.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.294.951 I 
0.01.295.101 I sampler seed: 1234
0.01.295.106 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.295.129 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.295.131 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.295.131 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.386.869 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54281.35 tokens per second)
0.02.386.870 I llama_perf_context_print:        load time =    1284.18 ms
0.02.386.871 I llama_perf_context_print: prompt eval time =      49.29 ms /     7 tokens (    7.04 ms per token,   142.03 tokens per second)
0.02.386.872 I llama_perf_context_print:        eval time =    1039.47 ms /    63 runs   (   16.50 ms per token,    60.61 tokens per second)
0.02.386.873 I llama_perf_context_print:       total time =    1092.69 ms /    70 tokens
0.02.387.124 I ggml_metal_free: deallocating

real	0m2.408s
user	0m0.108s
sys	0m0.266s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.542 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.546 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.551 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.552 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.255 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.257 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.258 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.259 I llama_model_loader: - type  f32:  194 tensors
0.00.027.259 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.259 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.260 I print_info: file format = GGUF V3 (latest)
0.00.027.261 I print_info: file type   = Q4_0
0.00.027.262 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.612 I load: special tokens cache size = 25
0.00.041.706 I load: token to piece cache size = 0.2984 MB
0.00.041.710 I print_info: arch             = gptneox
0.00.041.710 I print_info: vocab_only       = 0
0.00.041.710 I print_info: n_ctx_train      = 2048
0.00.041.710 I print_info: n_embd           = 2048
0.00.041.710 I print_info: n_layer          = 24
0.00.041.715 I print_info: n_head           = 16
0.00.041.716 I print_info: n_head_kv        = 16
0.00.041.716 I print_info: n_rot            = 32
0.00.041.719 I print_info: n_swa            = 0
0.00.041.719 I print_info: n_embd_head_k    = 128
0.00.041.719 I print_info: n_embd_head_v    = 128
0.00.041.720 I print_info: n_gqa            = 1
0.00.041.721 I print_info: n_embd_k_gqa     = 2048
0.00.041.723 I print_info: n_embd_v_gqa     = 2048
0.00.041.724 I print_info: f_norm_eps       = 1.0e-05
0.00.041.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.733 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.735 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.736 I print_info: f_logit_scale    = 0.0e+00
0.00.041.737 I print_info: n_ff             = 8192
0.00.041.738 I print_info: n_expert         = 0
0.00.041.738 I print_info: n_expert_used    = 0
0.00.041.738 I print_info: causal attn      = 1
0.00.041.739 I print_info: pooling type     = 0
0.00.041.739 I print_info: rope type        = 2
0.00.041.739 I print_info: rope scaling     = linear
0.00.041.739 I print_info: freq_base_train  = 10000.0
0.00.041.739 I print_info: freq_scale_train = 1
0.00.041.740 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.740 I print_info: rope_finetuned   = unknown
0.00.041.740 I print_info: ssm_d_conv       = 0
0.00.041.740 I print_info: ssm_d_inner      = 0
0.00.041.740 I print_info: ssm_d_state      = 0
0.00.041.740 I print_info: ssm_dt_rank      = 0
0.00.041.740 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.741 I print_info: model type       = 1.4B
0.00.041.741 I print_info: model params     = 1.41 B
0.00.041.741 I print_info: general.name     = 1.4B
0.00.041.742 I print_info: vocab type       = BPE
0.00.041.742 I print_info: n_vocab          = 50304
0.00.041.742 I print_info: n_merges         = 50009
0.00.041.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.743 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.743 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.743 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.743 I print_info: LF token         = 187 'Ċ'
0.00.041.744 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.744 I print_info: max token length = 1024
0.00.041.744 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.364 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.378 I load_tensors: offloading output layer to GPU
0.00.592.380 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.412 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.592.413 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.593.813 I llama_init_from_model: n_seq_max     = 1
0.00.593.816 I llama_init_from_model: n_ctx         = 2048
0.00.593.816 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.817 I llama_init_from_model: n_batch       = 2048
0.00.593.817 I llama_init_from_model: n_ubatch      = 512
0.00.593.818 I llama_init_from_model: flash_attn    = 0
0.00.593.820 I llama_init_from_model: freq_base     = 10000.0
0.00.593.821 I llama_init_from_model: freq_scale    = 1
0.00.593.823 I ggml_metal_init: allocating
0.00.593.898 I ggml_metal_init: found device: Apple M4
0.00.593.912 I ggml_metal_init: picking default device: Apple M4
0.00.595.733 I ggml_metal_init: using embedded metal library
0.00.601.096 I ggml_metal_init: GPU name:   Apple M4
0.00.601.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.103 I ggml_metal_init: simdgroup reduction   = true
0.00.601.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.103 I ggml_metal_init: has residency sets    = true
0.00.601.104 I ggml_metal_init: has bfloat            = true
0.00.601.104 I ggml_metal_init: use bfloat            = true
0.00.601.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.760 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.102 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.679.108 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.679.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.752 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.755 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.755 I llama_init_from_model: graph nodes  = 967
0.00.684.755 I llama_init_from_model: graph splits = 2
0.00.684.760 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.369 I main: llama threadpool init, n_threads = 4
0.00.739.414 I 
0.00.739.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.437 I 
0.00.739.615 I sampler seed: 1234
0.00.739.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.640 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.641 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.641 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.416.975 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.416.976 I llama_perf_context_print:        load time =     727.72 ms
0.01.416.977 I llama_perf_context_print: prompt eval time =      47.27 ms /     7 tokens (    6.75 ms per token,   148.08 tokens per second)
0.01.416.978 I llama_perf_context_print:        eval time =     627.30 ms /    63 runs   (    9.96 ms per token,   100.43 tokens per second)
0.01.416.978 I llama_perf_context_print:       total time =     678.31 ms /    70 tokens
0.01.417.237 I ggml_metal_free: deallocating

real	0m1.435s
user	0m0.111s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.008.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.636 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.497 I llama_model_loader: - type  f32:  194 tensors
0.00.025.497 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.497 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.498 I print_info: file format = GGUF V3 (latest)
0.00.025.499 I print_info: file type   = Q4_1
0.00.025.504 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.836 I load: special tokens cache size = 25
0.00.040.097 I load: token to piece cache size = 0.2984 MB
0.00.040.102 I print_info: arch             = gptneox
0.00.040.102 I print_info: vocab_only       = 0
0.00.040.102 I print_info: n_ctx_train      = 2048
0.00.040.103 I print_info: n_embd           = 2048
0.00.040.103 I print_info: n_layer          = 24
0.00.040.107 I print_info: n_head           = 16
0.00.040.108 I print_info: n_head_kv        = 16
0.00.040.108 I print_info: n_rot            = 32
0.00.040.108 I print_info: n_swa            = 0
0.00.040.109 I print_info: n_embd_head_k    = 128
0.00.040.109 I print_info: n_embd_head_v    = 128
0.00.040.110 I print_info: n_gqa            = 1
0.00.040.110 I print_info: n_embd_k_gqa     = 2048
0.00.040.111 I print_info: n_embd_v_gqa     = 2048
0.00.040.112 I print_info: f_norm_eps       = 1.0e-05
0.00.040.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.116 I print_info: f_logit_scale    = 0.0e+00
0.00.040.117 I print_info: n_ff             = 8192
0.00.040.117 I print_info: n_expert         = 0
0.00.040.117 I print_info: n_expert_used    = 0
0.00.040.117 I print_info: causal attn      = 1
0.00.040.117 I print_info: pooling type     = 0
0.00.040.117 I print_info: rope type        = 2
0.00.040.119 I print_info: rope scaling     = linear
0.00.040.119 I print_info: freq_base_train  = 10000.0
0.00.040.120 I print_info: freq_scale_train = 1
0.00.040.120 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.120 I print_info: rope_finetuned   = unknown
0.00.040.120 I print_info: ssm_d_conv       = 0
0.00.040.120 I print_info: ssm_d_inner      = 0
0.00.040.120 I print_info: ssm_d_state      = 0
0.00.040.120 I print_info: ssm_dt_rank      = 0
0.00.040.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.121 I print_info: model type       = 1.4B
0.00.040.121 I print_info: model params     = 1.41 B
0.00.040.122 I print_info: general.name     = 1.4B
0.00.040.123 I print_info: vocab type       = BPE
0.00.040.123 I print_info: n_vocab          = 50304
0.00.040.123 I print_info: n_merges         = 50009
0.00.040.123 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: LF token         = 187 'Ċ'
0.00.040.124 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.124 I print_info: max token length = 1024
0.00.040.125 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.892 I load_tensors: offloading output layer to GPU
0.00.619.893 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.928 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.619.929 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.621.672 I llama_init_from_model: n_seq_max     = 1
0.00.621.674 I llama_init_from_model: n_ctx         = 2048
0.00.621.674 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.675 I llama_init_from_model: n_batch       = 2048
0.00.621.676 I llama_init_from_model: n_ubatch      = 512
0.00.621.676 I llama_init_from_model: flash_attn    = 0
0.00.621.678 I llama_init_from_model: freq_base     = 10000.0
0.00.621.679 I llama_init_from_model: freq_scale    = 1
0.00.621.681 I ggml_metal_init: allocating
0.00.621.761 I ggml_metal_init: found device: Apple M4
0.00.621.775 I ggml_metal_init: picking default device: Apple M4
0.00.623.645 I ggml_metal_init: using embedded metal library
0.00.630.491 I ggml_metal_init: GPU name:   Apple M4
0.00.630.497 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.498 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.499 I ggml_metal_init: simdgroup reduction   = true
0.00.630.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.499 I ggml_metal_init: has residency sets    = true
0.00.630.500 I ggml_metal_init: has bfloat            = true
0.00.630.500 I ggml_metal_init: use bfloat            = true
0.00.630.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.226 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.397 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.420 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.384 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.386 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.386 I llama_init_from_model: graph nodes  = 967
0.00.703.386 I llama_init_from_model: graph splits = 2
0.00.703.392 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.591 I main: llama threadpool init, n_threads = 4
0.00.756.634 I 
0.00.756.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.655 I 
0.00.756.808 I sampler seed: 1234
0.00.756.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.823 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.825 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.482.945 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.482.945 I llama_perf_context_print:        load time =     746.99 ms
0.01.482.946 I llama_perf_context_print: prompt eval time =      48.88 ms /     7 tokens (    6.98 ms per token,   143.20 tokens per second)
0.01.482.947 I llama_perf_context_print:        eval time =     674.36 ms /    63 runs   (   10.70 ms per token,    93.42 tokens per second)
0.01.482.947 I llama_perf_context_print:       total time =     727.07 ms /    70 tokens
0.01.483.180 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.112s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.716 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.376 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.393 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.393 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.394 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.859 I llama_model_loader: - type  f32:  194 tensors
0.00.025.859 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.860 I print_info: file format = GGUF V3 (latest)
0.00.025.860 I print_info: file type   = Q5_0
0.00.025.861 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.606 I load: special tokens cache size = 25
0.00.039.742 I load: token to piece cache size = 0.2984 MB
0.00.039.745 I print_info: arch             = gptneox
0.00.039.745 I print_info: vocab_only       = 0
0.00.039.745 I print_info: n_ctx_train      = 2048
0.00.039.745 I print_info: n_embd           = 2048
0.00.039.745 I print_info: n_layer          = 24
0.00.039.748 I print_info: n_head           = 16
0.00.039.749 I print_info: n_head_kv        = 16
0.00.039.749 I print_info: n_rot            = 32
0.00.039.749 I print_info: n_swa            = 0
0.00.039.749 I print_info: n_embd_head_k    = 128
0.00.039.751 I print_info: n_embd_head_v    = 128
0.00.039.752 I print_info: n_gqa            = 1
0.00.039.753 I print_info: n_embd_k_gqa     = 2048
0.00.039.754 I print_info: n_embd_v_gqa     = 2048
0.00.039.754 I print_info: f_norm_eps       = 1.0e-05
0.00.039.755 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.755 I print_info: f_logit_scale    = 0.0e+00
0.00.039.756 I print_info: n_ff             = 8192
0.00.039.756 I print_info: n_expert         = 0
0.00.039.756 I print_info: n_expert_used    = 0
0.00.039.756 I print_info: causal attn      = 1
0.00.039.757 I print_info: pooling type     = 0
0.00.039.757 I print_info: rope type        = 2
0.00.039.757 I print_info: rope scaling     = linear
0.00.039.757 I print_info: freq_base_train  = 10000.0
0.00.039.758 I print_info: freq_scale_train = 1
0.00.039.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.758 I print_info: rope_finetuned   = unknown
0.00.039.760 I print_info: ssm_d_conv       = 0
0.00.039.760 I print_info: ssm_d_inner      = 0
0.00.039.760 I print_info: ssm_d_state      = 0
0.00.039.760 I print_info: ssm_dt_rank      = 0
0.00.039.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.761 I print_info: model type       = 1.4B
0.00.039.761 I print_info: model params     = 1.41 B
0.00.039.761 I print_info: general.name     = 1.4B
0.00.039.762 I print_info: vocab type       = BPE
0.00.039.762 I print_info: n_vocab          = 50304
0.00.039.762 I print_info: n_merges         = 50009
0.00.039.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.762 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: LF token         = 187 'Ċ'
0.00.039.763 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: max token length = 1024
0.00.039.764 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.690.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.062 I load_tensors: offloading output layer to GPU
0.00.690.063 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.093 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.690.100 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.690.992 I llama_init_from_model: n_seq_max     = 1
0.00.690.994 I llama_init_from_model: n_ctx         = 2048
0.00.690.995 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.690.995 I llama_init_from_model: n_batch       = 2048
0.00.690.996 I llama_init_from_model: n_ubatch      = 512
0.00.690.996 I llama_init_from_model: flash_attn    = 0
0.00.690.997 I llama_init_from_model: freq_base     = 10000.0
0.00.690.997 I llama_init_from_model: freq_scale    = 1
0.00.690.999 I ggml_metal_init: allocating
0.00.691.037 I ggml_metal_init: found device: Apple M4
0.00.691.048 I ggml_metal_init: picking default device: Apple M4
0.00.692.216 I ggml_metal_init: using embedded metal library
0.00.697.879 I ggml_metal_init: GPU name:   Apple M4
0.00.697.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.885 I ggml_metal_init: simdgroup reduction   = true
0.00.697.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.885 I ggml_metal_init: has residency sets    = true
0.00.697.885 I ggml_metal_init: has bfloat            = true
0.00.697.886 I ggml_metal_init: use bfloat            = true
0.00.697.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.124 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.285 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.761.293 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.761.316 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.766.248 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.766.250 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.766.250 I llama_init_from_model: graph nodes  = 967
0.00.766.250 I llama_init_from_model: graph splits = 2
0.00.766.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.766.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.766.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.277 I main: llama threadpool init, n_threads = 4
0.00.815.321 I 
0.00.815.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.346 I 
0.00.815.466 I sampler seed: 1234
0.00.815.471 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.506 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.508 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.509 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.598.378 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.598.379 I llama_perf_context_print:        load time =     804.84 ms
0.01.598.380 I llama_perf_context_print: prompt eval time =      43.32 ms /     7 tokens (    6.19 ms per token,   161.59 tokens per second)
0.01.598.384 I llama_perf_context_print:        eval time =     736.62 ms /    63 runs   (   11.69 ms per token,    85.53 tokens per second)
0.01.598.384 I llama_perf_context_print:       total time =     783.82 ms /    70 tokens
0.01.598.670 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.107s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.125 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.825 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.826 I llama_model_loader: - type  f32:  194 tensors
0.00.025.826 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.826 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.827 I print_info: file format = GGUF V3 (latest)
0.00.025.827 I print_info: file type   = Q5_1
0.00.025.828 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.604 I load: special tokens cache size = 25
0.00.039.711 I load: token to piece cache size = 0.2984 MB
0.00.039.713 I print_info: arch             = gptneox
0.00.039.713 I print_info: vocab_only       = 0
0.00.039.714 I print_info: n_ctx_train      = 2048
0.00.039.714 I print_info: n_embd           = 2048
0.00.039.714 I print_info: n_layer          = 24
0.00.039.717 I print_info: n_head           = 16
0.00.039.717 I print_info: n_head_kv        = 16
0.00.039.718 I print_info: n_rot            = 32
0.00.039.718 I print_info: n_swa            = 0
0.00.039.718 I print_info: n_embd_head_k    = 128
0.00.039.718 I print_info: n_embd_head_v    = 128
0.00.039.719 I print_info: n_gqa            = 1
0.00.039.720 I print_info: n_embd_k_gqa     = 2048
0.00.039.720 I print_info: n_embd_v_gqa     = 2048
0.00.039.723 I print_info: f_norm_eps       = 1.0e-05
0.00.039.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.724 I print_info: f_logit_scale    = 0.0e+00
0.00.039.725 I print_info: n_ff             = 8192
0.00.039.725 I print_info: n_expert         = 0
0.00.039.725 I print_info: n_expert_used    = 0
0.00.039.725 I print_info: causal attn      = 1
0.00.039.725 I print_info: pooling type     = 0
0.00.039.726 I print_info: rope type        = 2
0.00.039.726 I print_info: rope scaling     = linear
0.00.039.726 I print_info: freq_base_train  = 10000.0
0.00.039.726 I print_info: freq_scale_train = 1
0.00.039.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.727 I print_info: rope_finetuned   = unknown
0.00.039.727 I print_info: ssm_d_conv       = 0
0.00.039.727 I print_info: ssm_d_inner      = 0
0.00.039.729 I print_info: ssm_d_state      = 0
0.00.039.729 I print_info: ssm_dt_rank      = 0
0.00.039.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.729 I print_info: model type       = 1.4B
0.00.039.730 I print_info: model params     = 1.41 B
0.00.039.730 I print_info: general.name     = 1.4B
0.00.039.730 I print_info: vocab type       = BPE
0.00.039.730 I print_info: n_vocab          = 50304
0.00.039.731 I print_info: n_merges         = 50009
0.00.039.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.732 I print_info: LF token         = 187 'Ċ'
0.00.039.732 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.732 I print_info: max token length = 1024
0.00.039.732 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.746.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.878 I load_tensors: offloading output layer to GPU
0.00.746.879 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.902 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.746.905 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.748.060 I llama_init_from_model: n_seq_max     = 1
0.00.748.062 I llama_init_from_model: n_ctx         = 2048
0.00.748.062 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.748.062 I llama_init_from_model: n_batch       = 2048
0.00.748.063 I llama_init_from_model: n_ubatch      = 512
0.00.748.063 I llama_init_from_model: flash_attn    = 0
0.00.748.064 I llama_init_from_model: freq_base     = 10000.0
0.00.748.064 I llama_init_from_model: freq_scale    = 1
0.00.748.065 I ggml_metal_init: allocating
0.00.748.103 I ggml_metal_init: found device: Apple M4
0.00.748.114 I ggml_metal_init: picking default device: Apple M4
0.00.749.521 I ggml_metal_init: using embedded metal library
0.00.754.975 I ggml_metal_init: GPU name:   Apple M4
0.00.754.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.980 I ggml_metal_init: simdgroup reduction   = true
0.00.754.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.980 I ggml_metal_init: has residency sets    = true
0.00.754.981 I ggml_metal_init: has bfloat            = true
0.00.754.981 I ggml_metal_init: use bfloat            = true
0.00.754.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.822 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.824.079 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.824.091 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.824.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.828.820 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.828.822 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.828.822 I llama_init_from_model: graph nodes  = 967
0.00.828.823 I llama_init_from_model: graph splits = 2
0.00.828.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.828.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.828.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.880.310 I main: llama threadpool init, n_threads = 4
0.00.880.355 I 
0.00.880.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.880.381 I 
0.00.880.510 I sampler seed: 1234
0.00.880.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.880.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.880.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.880.526 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.723.108 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51824.82 tokens per second)
0.01.723.108 I llama_perf_context_print:        load time =     870.40 ms
0.01.723.109 I llama_perf_context_print: prompt eval time =      52.56 ms /     7 tokens (    7.51 ms per token,   133.17 tokens per second)
0.01.723.110 I llama_perf_context_print:        eval time =     787.02 ms /    63 runs   (   12.49 ms per token,    80.05 tokens per second)
0.01.723.110 I llama_perf_context_print:       total time =     843.58 ms /    70 tokens
0.01.723.390 I ggml_metal_free: deallocating

real	0m1.739s
user	0m0.106s
sys	0m0.253s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.318 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.324 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.324 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.326 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.328 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.333 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.711 I llama_model_loader: - type  f32:  194 tensors
0.00.024.711 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.711 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.712 I print_info: file format = GGUF V3 (latest)
0.00.024.713 I print_info: file type   = Q2_K - Medium
0.00.024.713 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.425 I load: special tokens cache size = 25
0.00.038.542 I load: token to piece cache size = 0.2984 MB
0.00.038.544 I print_info: arch             = gptneox
0.00.038.545 I print_info: vocab_only       = 0
0.00.038.545 I print_info: n_ctx_train      = 2048
0.00.038.545 I print_info: n_embd           = 2048
0.00.038.545 I print_info: n_layer          = 24
0.00.038.548 I print_info: n_head           = 16
0.00.038.548 I print_info: n_head_kv        = 16
0.00.038.549 I print_info: n_rot            = 32
0.00.038.549 I print_info: n_swa            = 0
0.00.038.549 I print_info: n_embd_head_k    = 128
0.00.038.549 I print_info: n_embd_head_v    = 128
0.00.038.550 I print_info: n_gqa            = 1
0.00.038.551 I print_info: n_embd_k_gqa     = 2048
0.00.038.551 I print_info: n_embd_v_gqa     = 2048
0.00.038.552 I print_info: f_norm_eps       = 1.0e-05
0.00.038.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.555 I print_info: f_logit_scale    = 0.0e+00
0.00.038.555 I print_info: n_ff             = 8192
0.00.038.555 I print_info: n_expert         = 0
0.00.038.555 I print_info: n_expert_used    = 0
0.00.038.556 I print_info: causal attn      = 1
0.00.038.556 I print_info: pooling type     = 0
0.00.038.557 I print_info: rope type        = 2
0.00.038.557 I print_info: rope scaling     = linear
0.00.038.558 I print_info: freq_base_train  = 10000.0
0.00.038.558 I print_info: freq_scale_train = 1
0.00.038.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.558 I print_info: rope_finetuned   = unknown
0.00.038.559 I print_info: ssm_d_conv       = 0
0.00.038.559 I print_info: ssm_d_inner      = 0
0.00.038.559 I print_info: ssm_d_state      = 0
0.00.038.559 I print_info: ssm_dt_rank      = 0
0.00.038.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.559 I print_info: model type       = 1.4B
0.00.038.560 I print_info: model params     = 1.41 B
0.00.038.560 I print_info: general.name     = 1.4B
0.00.038.560 I print_info: vocab type       = BPE
0.00.038.560 I print_info: n_vocab          = 50304
0.00.038.561 I print_info: n_merges         = 50009
0.00.038.561 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.567 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.568 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.568 I print_info: LF token         = 187 'Ċ'
0.00.038.569 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.569 I print_info: max token length = 1024
0.00.038.570 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.404.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.404.248 I load_tensors: offloading output layer to GPU
0.00.404.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.404.276 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.404.278 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.405.717 I llama_init_from_model: n_seq_max     = 1
0.00.405.720 I llama_init_from_model: n_ctx         = 2048
0.00.405.721 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.405.722 I llama_init_from_model: n_batch       = 2048
0.00.405.722 I llama_init_from_model: n_ubatch      = 512
0.00.405.722 I llama_init_from_model: flash_attn    = 0
0.00.405.724 I llama_init_from_model: freq_base     = 10000.0
0.00.405.725 I llama_init_from_model: freq_scale    = 1
0.00.405.727 I ggml_metal_init: allocating
0.00.405.793 I ggml_metal_init: found device: Apple M4
0.00.405.806 I ggml_metal_init: picking default device: Apple M4
0.00.407.651 I ggml_metal_init: using embedded metal library
0.00.413.700 I ggml_metal_init: GPU name:   Apple M4
0.00.413.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.413.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.413.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.413.708 I ggml_metal_init: simdgroup reduction   = true
0.00.413.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.413.709 I ggml_metal_init: has residency sets    = true
0.00.413.709 I ggml_metal_init: has bfloat            = true
0.00.413.709 I ggml_metal_init: use bfloat            = true
0.00.413.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.413.712 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.432.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.148 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.485.154 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.485.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.490.203 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.490.205 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.490.205 I llama_init_from_model: graph nodes  = 967
0.00.490.206 I llama_init_from_model: graph splits = 2
0.00.490.210 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.490.344 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.490.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.667 I main: llama threadpool init, n_threads = 4
0.00.540.712 I 
0.00.540.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.738 I 
0.00.540.868 I sampler seed: 1234
0.00.540.872 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.882 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.884 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.884 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.237.833 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.237.834 I llama_perf_context_print:        load time =     530.03 ms
0.01.237.835 I llama_perf_context_print: prompt eval time =      44.36 ms /     7 tokens (    6.34 ms per token,   157.80 tokens per second)
0.01.237.835 I llama_perf_context_print:        eval time =     649.74 ms /    63 runs   (   10.31 ms per token,    96.96 tokens per second)
0.01.237.836 I llama_perf_context_print:       total time =     697.91 ms /    70 tokens
0.01.238.125 I ggml_metal_free: deallocating

real	0m1.257s
user	0m0.109s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.915 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.939 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.939 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.760 I llama_model_loader: - type  f32:  194 tensors
0.00.025.760 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.760 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.760 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.761 I print_info: file format = GGUF V3 (latest)
0.00.025.761 I print_info: file type   = Q3_K - Medium
0.00.025.762 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.854 I load: special tokens cache size = 25
0.00.039.874 I load: token to piece cache size = 0.2984 MB
0.00.039.879 I print_info: arch             = gptneox
0.00.039.880 I print_info: vocab_only       = 0
0.00.039.880 I print_info: n_ctx_train      = 2048
0.00.039.880 I print_info: n_embd           = 2048
0.00.039.880 I print_info: n_layer          = 24
0.00.039.883 I print_info: n_head           = 16
0.00.039.884 I print_info: n_head_kv        = 16
0.00.039.884 I print_info: n_rot            = 32
0.00.039.885 I print_info: n_swa            = 0
0.00.039.886 I print_info: n_embd_head_k    = 128
0.00.039.886 I print_info: n_embd_head_v    = 128
0.00.039.887 I print_info: n_gqa            = 1
0.00.039.888 I print_info: n_embd_k_gqa     = 2048
0.00.039.888 I print_info: n_embd_v_gqa     = 2048
0.00.039.889 I print_info: f_norm_eps       = 1.0e-05
0.00.039.889 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.891 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.892 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.892 I print_info: f_logit_scale    = 0.0e+00
0.00.039.892 I print_info: n_ff             = 8192
0.00.039.893 I print_info: n_expert         = 0
0.00.039.893 I print_info: n_expert_used    = 0
0.00.039.893 I print_info: causal attn      = 1
0.00.039.893 I print_info: pooling type     = 0
0.00.039.893 I print_info: rope type        = 2
0.00.039.895 I print_info: rope scaling     = linear
0.00.039.896 I print_info: freq_base_train  = 10000.0
0.00.039.896 I print_info: freq_scale_train = 1
0.00.039.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.896 I print_info: rope_finetuned   = unknown
0.00.039.896 I print_info: ssm_d_conv       = 0
0.00.039.896 I print_info: ssm_d_inner      = 0
0.00.039.897 I print_info: ssm_d_state      = 0
0.00.039.897 I print_info: ssm_dt_rank      = 0
0.00.039.897 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.897 I print_info: model type       = 1.4B
0.00.039.898 I print_info: model params     = 1.41 B
0.00.039.898 I print_info: general.name     = 1.4B
0.00.039.898 I print_info: vocab type       = BPE
0.00.039.900 I print_info: n_vocab          = 50304
0.00.039.900 I print_info: n_merges         = 50009
0.00.039.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: LF token         = 187 'Ċ'
0.00.039.901 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: max token length = 1024
0.00.039.902 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.497.359 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.364 I load_tensors: offloading output layer to GPU
0.00.497.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.388 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.497.391 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.498.541 I llama_init_from_model: n_seq_max     = 1
0.00.498.543 I llama_init_from_model: n_ctx         = 2048
0.00.498.544 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.498.545 I llama_init_from_model: n_batch       = 2048
0.00.498.545 I llama_init_from_model: n_ubatch      = 512
0.00.498.545 I llama_init_from_model: flash_attn    = 0
0.00.498.547 I llama_init_from_model: freq_base     = 10000.0
0.00.498.547 I llama_init_from_model: freq_scale    = 1
0.00.498.548 I ggml_metal_init: allocating
0.00.498.566 I ggml_metal_init: found device: Apple M4
0.00.498.576 I ggml_metal_init: picking default device: Apple M4
0.00.500.006 I ggml_metal_init: using embedded metal library
0.00.506.405 I ggml_metal_init: GPU name:   Apple M4
0.00.506.408 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.506.409 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.506.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.506.410 I ggml_metal_init: simdgroup reduction   = true
0.00.506.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.506.411 I ggml_metal_init: has residency sets    = true
0.00.506.411 I ggml_metal_init: has bfloat            = true
0.00.506.411 I ggml_metal_init: use bfloat            = true
0.00.506.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.506.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.523.455 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.581.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.581.014 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.581.081 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.586.287 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.586.292 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.586.292 I llama_init_from_model: graph nodes  = 967
0.00.586.292 I llama_init_from_model: graph splits = 2
0.00.586.297 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.586.416 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.586.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.446 I main: llama threadpool init, n_threads = 4
0.00.634.489 I 
0.00.634.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.513 I 
0.00.634.718 I sampler seed: 1234
0.00.634.725 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.634.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.634.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.634.743 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.383.331 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.383.331 I llama_perf_context_print:        load time =     624.84 ms
0.01.383.332 I llama_perf_context_print: prompt eval time =      50.49 ms /     7 tokens (    7.21 ms per token,   138.63 tokens per second)
0.01.383.333 I llama_perf_context_print:        eval time =     695.00 ms /    63 runs   (   11.03 ms per token,    90.65 tokens per second)
0.01.383.334 I llama_perf_context_print:       total time =     749.57 ms /    70 tokens
0.01.383.588 I ggml_metal_free: deallocating

real	0m1.399s
user	0m0.109s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.173 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.885 I llama_model_loader: - type  f32:  194 tensors
0.00.025.885 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.885 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.885 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.886 I print_info: file format = GGUF V3 (latest)
0.00.025.886 I print_info: file type   = Q4_K - Medium
0.00.025.887 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.980 I load: special tokens cache size = 25
0.00.040.035 I load: token to piece cache size = 0.2984 MB
0.00.040.038 I print_info: arch             = gptneox
0.00.040.038 I print_info: vocab_only       = 0
0.00.040.039 I print_info: n_ctx_train      = 2048
0.00.040.039 I print_info: n_embd           = 2048
0.00.040.039 I print_info: n_layer          = 24
0.00.040.042 I print_info: n_head           = 16
0.00.040.044 I print_info: n_head_kv        = 16
0.00.040.044 I print_info: n_rot            = 32
0.00.040.044 I print_info: n_swa            = 0
0.00.040.045 I print_info: n_embd_head_k    = 128
0.00.040.045 I print_info: n_embd_head_v    = 128
0.00.040.045 I print_info: n_gqa            = 1
0.00.040.046 I print_info: n_embd_k_gqa     = 2048
0.00.040.047 I print_info: n_embd_v_gqa     = 2048
0.00.040.048 I print_info: f_norm_eps       = 1.0e-05
0.00.040.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.050 I print_info: f_logit_scale    = 0.0e+00
0.00.040.051 I print_info: n_ff             = 8192
0.00.040.051 I print_info: n_expert         = 0
0.00.040.051 I print_info: n_expert_used    = 0
0.00.040.051 I print_info: causal attn      = 1
0.00.040.052 I print_info: pooling type     = 0
0.00.040.053 I print_info: rope type        = 2
0.00.040.053 I print_info: rope scaling     = linear
0.00.040.054 I print_info: freq_base_train  = 10000.0
0.00.040.054 I print_info: freq_scale_train = 1
0.00.040.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.055 I print_info: rope_finetuned   = unknown
0.00.040.055 I print_info: ssm_d_conv       = 0
0.00.040.055 I print_info: ssm_d_inner      = 0
0.00.040.055 I print_info: ssm_d_state      = 0
0.00.040.055 I print_info: ssm_dt_rank      = 0
0.00.040.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.055 I print_info: model type       = 1.4B
0.00.040.056 I print_info: model params     = 1.41 B
0.00.040.056 I print_info: general.name     = 1.4B
0.00.040.057 I print_info: vocab type       = BPE
0.00.040.057 I print_info: n_vocab          = 50304
0.00.040.057 I print_info: n_merges         = 50009
0.00.040.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: LF token         = 187 'Ċ'
0.00.040.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.060 I print_info: max token length = 1024
0.00.040.060 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.297 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.300 I load_tensors: offloading output layer to GPU
0.00.584.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.322 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.584.323 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.585.463 I llama_init_from_model: n_seq_max     = 1
0.00.585.465 I llama_init_from_model: n_ctx         = 2048
0.00.585.465 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.585.465 I llama_init_from_model: n_batch       = 2048
0.00.585.466 I llama_init_from_model: n_ubatch      = 512
0.00.585.466 I llama_init_from_model: flash_attn    = 0
0.00.585.467 I llama_init_from_model: freq_base     = 10000.0
0.00.585.468 I llama_init_from_model: freq_scale    = 1
0.00.585.469 I ggml_metal_init: allocating
0.00.585.488 I ggml_metal_init: found device: Apple M4
0.00.585.495 I ggml_metal_init: picking default device: Apple M4
0.00.586.827 I ggml_metal_init: using embedded metal library
0.00.592.818 I ggml_metal_init: GPU name:   Apple M4
0.00.592.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.824 I ggml_metal_init: simdgroup reduction   = true
0.00.592.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.825 I ggml_metal_init: has residency sets    = true
0.00.592.825 I ggml_metal_init: has bfloat            = true
0.00.592.825 I ggml_metal_init: use bfloat            = true
0.00.592.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.827 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.391 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.809 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.660.816 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.660.838 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.064 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.666.066 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.666.066 I llama_init_from_model: graph nodes  = 967
0.00.666.066 I llama_init_from_model: graph splits = 2
0.00.666.070 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.666.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.666.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.319 I main: llama threadpool init, n_threads = 4
0.00.716.373 I 
0.00.716.394 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.395 I 
0.00.716.528 I sampler seed: 1234
0.00.716.532 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.542 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.542 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.543 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.483.370 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.483.371 I llama_perf_context_print:        load time =     706.42 ms
0.01.483.372 I llama_perf_context_print: prompt eval time =      58.18 ms /     7 tokens (    8.31 ms per token,   120.32 tokens per second)
0.01.483.373 I llama_perf_context_print:        eval time =     705.65 ms /    63 runs   (   11.20 ms per token,    89.28 tokens per second)
0.01.483.373 I llama_perf_context_print:       total time =     767.77 ms /    70 tokens
0.01.483.614 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.108s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.392 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.393 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.394 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.394 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.394 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.395 I llama_model_loader: - type  f32:  194 tensors
0.00.026.395 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.396 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.396 I print_info: file format = GGUF V3 (latest)
0.00.026.397 I print_info: file type   = Q5_K - Medium
0.00.026.397 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.538 I load: special tokens cache size = 25
0.00.040.449 I load: token to piece cache size = 0.2984 MB
0.00.040.452 I print_info: arch             = gptneox
0.00.040.452 I print_info: vocab_only       = 0
0.00.040.452 I print_info: n_ctx_train      = 2048
0.00.040.453 I print_info: n_embd           = 2048
0.00.040.453 I print_info: n_layer          = 24
0.00.040.455 I print_info: n_head           = 16
0.00.040.456 I print_info: n_head_kv        = 16
0.00.040.456 I print_info: n_rot            = 32
0.00.040.456 I print_info: n_swa            = 0
0.00.040.457 I print_info: n_embd_head_k    = 128
0.00.040.457 I print_info: n_embd_head_v    = 128
0.00.040.458 I print_info: n_gqa            = 1
0.00.040.458 I print_info: n_embd_k_gqa     = 2048
0.00.040.459 I print_info: n_embd_v_gqa     = 2048
0.00.040.460 I print_info: f_norm_eps       = 1.0e-05
0.00.040.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.462 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.462 I print_info: f_logit_scale    = 0.0e+00
0.00.040.463 I print_info: n_ff             = 8192
0.00.040.463 I print_info: n_expert         = 0
0.00.040.463 I print_info: n_expert_used    = 0
0.00.040.463 I print_info: causal attn      = 1
0.00.040.463 I print_info: pooling type     = 0
0.00.040.463 I print_info: rope type        = 2
0.00.040.465 I print_info: rope scaling     = linear
0.00.040.465 I print_info: freq_base_train  = 10000.0
0.00.040.466 I print_info: freq_scale_train = 1
0.00.040.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.466 I print_info: rope_finetuned   = unknown
0.00.040.466 I print_info: ssm_d_conv       = 0
0.00.040.466 I print_info: ssm_d_inner      = 0
0.00.040.467 I print_info: ssm_d_state      = 0
0.00.040.467 I print_info: ssm_dt_rank      = 0
0.00.040.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.467 I print_info: model type       = 1.4B
0.00.040.467 I print_info: model params     = 1.41 B
0.00.040.469 I print_info: general.name     = 1.4B
0.00.040.469 I print_info: vocab type       = BPE
0.00.040.470 I print_info: n_vocab          = 50304
0.00.040.470 I print_info: n_merges         = 50009
0.00.040.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: LF token         = 187 'Ċ'
0.00.040.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: max token length = 1024
0.00.040.471 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.957 I load_tensors: offloading output layer to GPU
0.00.669.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.980 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.669.982 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.671.033 I llama_init_from_model: n_seq_max     = 1
0.00.671.034 I llama_init_from_model: n_ctx         = 2048
0.00.671.035 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.671.035 I llama_init_from_model: n_batch       = 2048
0.00.671.036 I llama_init_from_model: n_ubatch      = 512
0.00.671.036 I llama_init_from_model: flash_attn    = 0
0.00.671.037 I llama_init_from_model: freq_base     = 10000.0
0.00.671.037 I llama_init_from_model: freq_scale    = 1
0.00.671.038 I ggml_metal_init: allocating
0.00.671.067 I ggml_metal_init: found device: Apple M4
0.00.671.079 I ggml_metal_init: picking default device: Apple M4
0.00.672.411 I ggml_metal_init: using embedded metal library
0.00.677.762 I ggml_metal_init: GPU name:   Apple M4
0.00.677.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.766 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.767 I ggml_metal_init: simdgroup reduction   = true
0.00.677.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.767 I ggml_metal_init: has residency sets    = true
0.00.677.768 I ggml_metal_init: has bfloat            = true
0.00.677.768 I ggml_metal_init: use bfloat            = true
0.00.677.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.769 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.243 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.747.632 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.752.504 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.752.506 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.752.506 I llama_init_from_model: graph nodes  = 967
0.00.752.507 I llama_init_from_model: graph splits = 2
0.00.752.511 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.752.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.726 I main: llama threadpool init, n_threads = 4
0.00.805.770 I 
0.00.805.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.795 I 
0.00.805.928 I sampler seed: 1234
0.00.805.933 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.943 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.947 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.647.621 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.647.622 I llama_perf_context_print:        load time =     794.93 ms
0.01.647.623 I llama_perf_context_print: prompt eval time =      51.16 ms /     7 tokens (    7.31 ms per token,   136.82 tokens per second)
0.01.647.625 I llama_perf_context_print:        eval time =     787.55 ms /    63 runs   (   12.50 ms per token,    80.00 tokens per second)
0.01.647.625 I llama_perf_context_print:       total time =     842.61 ms /    70 tokens
0.01.647.914 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.105s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.426 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.553 I llama_model_loader: - type  f32:  194 tensors
0.00.024.553 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.554 I print_info: file format = GGUF V3 (latest)
0.00.024.554 I print_info: file type   = Q6_K
0.00.024.555 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.272 I load: special tokens cache size = 25
0.00.038.280 I load: token to piece cache size = 0.2984 MB
0.00.038.283 I print_info: arch             = gptneox
0.00.038.283 I print_info: vocab_only       = 0
0.00.038.283 I print_info: n_ctx_train      = 2048
0.00.038.283 I print_info: n_embd           = 2048
0.00.038.284 I print_info: n_layer          = 24
0.00.038.286 I print_info: n_head           = 16
0.00.038.287 I print_info: n_head_kv        = 16
0.00.038.287 I print_info: n_rot            = 32
0.00.038.287 I print_info: n_swa            = 0
0.00.038.288 I print_info: n_embd_head_k    = 128
0.00.038.288 I print_info: n_embd_head_v    = 128
0.00.038.288 I print_info: n_gqa            = 1
0.00.038.289 I print_info: n_embd_k_gqa     = 2048
0.00.038.290 I print_info: n_embd_v_gqa     = 2048
0.00.038.290 I print_info: f_norm_eps       = 1.0e-05
0.00.038.292 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.293 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.293 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.293 I print_info: f_logit_scale    = 0.0e+00
0.00.038.294 I print_info: n_ff             = 8192
0.00.038.294 I print_info: n_expert         = 0
0.00.038.294 I print_info: n_expert_used    = 0
0.00.038.294 I print_info: causal attn      = 1
0.00.038.294 I print_info: pooling type     = 0
0.00.038.294 I print_info: rope type        = 2
0.00.038.295 I print_info: rope scaling     = linear
0.00.038.295 I print_info: freq_base_train  = 10000.0
0.00.038.295 I print_info: freq_scale_train = 1
0.00.038.296 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.296 I print_info: rope_finetuned   = unknown
0.00.038.296 I print_info: ssm_d_conv       = 0
0.00.038.296 I print_info: ssm_d_inner      = 0
0.00.038.296 I print_info: ssm_d_state      = 0
0.00.038.296 I print_info: ssm_dt_rank      = 0
0.00.038.296 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.297 I print_info: model type       = 1.4B
0.00.038.297 I print_info: model params     = 1.41 B
0.00.038.297 I print_info: general.name     = 1.4B
0.00.038.298 I print_info: vocab type       = BPE
0.00.038.298 I print_info: n_vocab          = 50304
0.00.038.299 I print_info: n_merges         = 50009
0.00.038.299 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.300 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.301 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.301 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.301 I print_info: LF token         = 187 'Ċ'
0.00.038.301 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.301 I print_info: max token length = 1024
0.00.038.302 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.722.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.722.099 I load_tensors: offloading output layer to GPU
0.00.722.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.722.120 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.722.122 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.723.113 I llama_init_from_model: n_seq_max     = 1
0.00.723.115 I llama_init_from_model: n_ctx         = 2048
0.00.723.116 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.723.116 I llama_init_from_model: n_batch       = 2048
0.00.723.117 I llama_init_from_model: n_ubatch      = 512
0.00.723.117 I llama_init_from_model: flash_attn    = 0
0.00.723.118 I llama_init_from_model: freq_base     = 10000.0
0.00.723.118 I llama_init_from_model: freq_scale    = 1
0.00.723.119 I ggml_metal_init: allocating
0.00.723.138 I ggml_metal_init: found device: Apple M4
0.00.723.145 I ggml_metal_init: picking default device: Apple M4
0.00.724.348 I ggml_metal_init: using embedded metal library
0.00.729.984 I ggml_metal_init: GPU name:   Apple M4
0.00.729.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.988 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.989 I ggml_metal_init: simdgroup reduction   = true
0.00.729.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.989 I ggml_metal_init: has residency sets    = true
0.00.729.989 I ggml_metal_init: has bfloat            = true
0.00.729.990 I ggml_metal_init: use bfloat            = true
0.00.729.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.745.266 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.493 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.493 I llama_init_from_model: graph nodes  = 967
0.00.801.493 I llama_init_from_model: graph splits = 2
0.00.801.498 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.859.826 I main: llama threadpool init, n_threads = 4
0.00.859.868 I 
0.00.859.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.859.891 I 
0.00.860.026 I sampler seed: 1234
0.00.860.031 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.860.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.860.068 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.860.068 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.725.652 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.725.653 I llama_perf_context_print:        load time =     850.71 ms
0.01.725.654 I llama_perf_context_print: prompt eval time =      54.54 ms /     7 tokens (    7.79 ms per token,   128.35 tokens per second)
0.01.725.655 I llama_perf_context_print:        eval time =     808.12 ms /    63 runs   (   12.83 ms per token,    77.96 tokens per second)
0.01.725.656 I llama_perf_context_print:       total time =     866.52 ms /    70 tokens
0.01.725.925 I ggml_metal_free: deallocating

real	0m1.742s
user	0m0.104s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.772 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.183 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.232 I llama_model_loader: - type  f32:  194 tensors
0.00.049.232 I llama_model_loader: - type  f16:   98 tensors
0.00.049.233 I print_info: file format = GGUF V3 (latest)
0.00.049.234 I print_info: file type   = all F32 (guessed)
0.00.049.235 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.060.351 I load: special tokens cache size = 25
0.00.068.175 I load: token to piece cache size = 0.2984 MB
0.00.068.178 I print_info: arch             = gptneox
0.00.068.179 I print_info: vocab_only       = 0
0.00.068.179 I print_info: n_ctx_train      = 2048
0.00.068.179 I print_info: n_embd           = 2048
0.00.068.179 I print_info: n_layer          = 24
0.00.068.182 I print_info: n_head           = 16
0.00.068.183 I print_info: n_head_kv        = 16
0.00.068.183 I print_info: n_rot            = 32
0.00.068.183 I print_info: n_swa            = 0
0.00.068.183 I print_info: n_embd_head_k    = 128
0.00.068.183 I print_info: n_embd_head_v    = 128
0.00.068.184 I print_info: n_gqa            = 1
0.00.068.185 I print_info: n_embd_k_gqa     = 2048
0.00.068.186 I print_info: n_embd_v_gqa     = 2048
0.00.068.187 I print_info: f_norm_eps       = 1.0e-05
0.00.068.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.189 I print_info: f_logit_scale    = 0.0e+00
0.00.068.189 I print_info: n_ff             = 8192
0.00.068.189 I print_info: n_expert         = 0
0.00.068.190 I print_info: n_expert_used    = 0
0.00.068.190 I print_info: causal attn      = 1
0.00.068.190 I print_info: pooling type     = 0
0.00.068.190 I print_info: rope type        = 2
0.00.068.190 I print_info: rope scaling     = linear
0.00.068.191 I print_info: freq_base_train  = 10000.0
0.00.068.191 I print_info: freq_scale_train = 1
0.00.068.191 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.191 I print_info: rope_finetuned   = unknown
0.00.068.192 I print_info: ssm_d_conv       = 0
0.00.068.192 I print_info: ssm_d_inner      = 0
0.00.068.192 I print_info: ssm_d_state      = 0
0.00.068.193 I print_info: ssm_dt_rank      = 0
0.00.068.193 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.193 I print_info: model type       = 1.4B
0.00.068.194 I print_info: model params     = 1.41 B
0.00.068.194 I print_info: general.name     = 1.4B
0.00.068.194 I print_info: vocab type       = BPE
0.00.068.195 I print_info: n_vocab          = 50304
0.00.068.195 I print_info: n_merges         = 50009
0.00.068.195 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.195 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.195 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.196 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.196 I print_info: LF token         = 187 'Ċ'
0.00.068.196 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.196 I print_info: max token length = 1024
0.00.068.197 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.549.915 I load_tensors: offloading 24 repeating layers to GPU
0.01.549.923 I load_tensors: offloading output layer to GPU
0.01.549.925 I load_tensors: offloaded 25/25 layers to GPU
0.01.549.951 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.549.952 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.550.597 I llama_init_from_model: n_seq_max     = 1
0.01.550.597 I llama_init_from_model: n_ctx         = 128
0.01.550.598 I llama_init_from_model: n_ctx_per_seq = 128
0.01.550.598 I llama_init_from_model: n_batch       = 128
0.01.550.598 I llama_init_from_model: n_ubatch      = 128
0.01.550.598 I llama_init_from_model: flash_attn    = 0
0.01.550.599 I llama_init_from_model: freq_base     = 10000.0
0.01.550.599 I llama_init_from_model: freq_scale    = 1
0.01.550.599 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.550.600 I ggml_metal_init: allocating
0.01.550.637 I ggml_metal_init: found device: Apple M4
0.01.550.644 I ggml_metal_init: picking default device: Apple M4
0.01.551.555 I ggml_metal_init: using embedded metal library
0.01.555.029 I ggml_metal_init: GPU name:   Apple M4
0.01.555.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.555.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.555.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.555.032 I ggml_metal_init: simdgroup reduction   = true
0.01.555.032 I ggml_metal_init: simdgroup matrix mul. = true
0.01.555.032 I ggml_metal_init: has residency sets    = true
0.01.555.033 I ggml_metal_init: has bfloat            = true
0.01.555.033 I ggml_metal_init: use bfloat            = true
0.01.555.033 I ggml_metal_init: hasUnifiedMemory      = true
0.01.555.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.564.840 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.566.515 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.566.517 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.566.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.568.074 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.568.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.568.075 I llama_init_from_model: graph nodes  = 967
0.01.568.076 I llama_init_from_model: graph splits = 2
0.01.568.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.568.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.601.012 I 
0.01.601.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.601.047 I perplexity: tokenizing the input ..
0.01.605.292 I perplexity: tokenization took 4.244 ms
0.01.605.297 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.723.381 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.724.670 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.724.700 I llama_perf_context_print:        load time =    1582.02 ms
0.01.724.701 I llama_perf_context_print: prompt eval time =     117.82 ms /   128 tokens (    0.92 ms per token,  1086.41 tokens per second)
0.01.724.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.724.702 I llama_perf_context_print:       total time =     123.69 ms /   129 tokens
0.01.725.054 I ggml_metal_free: deallocating

real	0m1.921s
user	0m0.096s
sys	0m0.361s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.163 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.650 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.471 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.504 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.359 I llama_model_loader: - type  f32:  194 tensors
0.00.026.360 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.360 I print_info: file format = GGUF V3 (latest)
0.00.026.361 I print_info: file type   = Q8_0
0.00.026.362 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.423 I load: special tokens cache size = 25
0.00.040.465 I load: token to piece cache size = 0.2984 MB
0.00.040.469 I print_info: arch             = gptneox
0.00.040.469 I print_info: vocab_only       = 0
0.00.040.470 I print_info: n_ctx_train      = 2048
0.00.040.470 I print_info: n_embd           = 2048
0.00.040.470 I print_info: n_layer          = 24
0.00.040.474 I print_info: n_head           = 16
0.00.040.475 I print_info: n_head_kv        = 16
0.00.040.475 I print_info: n_rot            = 32
0.00.040.475 I print_info: n_swa            = 0
0.00.040.476 I print_info: n_embd_head_k    = 128
0.00.040.476 I print_info: n_embd_head_v    = 128
0.00.040.476 I print_info: n_gqa            = 1
0.00.040.477 I print_info: n_embd_k_gqa     = 2048
0.00.040.478 I print_info: n_embd_v_gqa     = 2048
0.00.040.478 I print_info: f_norm_eps       = 1.0e-05
0.00.040.479 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.482 I print_info: f_logit_scale    = 0.0e+00
0.00.040.482 I print_info: n_ff             = 8192
0.00.040.483 I print_info: n_expert         = 0
0.00.040.483 I print_info: n_expert_used    = 0
0.00.040.483 I print_info: causal attn      = 1
0.00.040.483 I print_info: pooling type     = 0
0.00.040.483 I print_info: rope type        = 2
0.00.040.487 I print_info: rope scaling     = linear
0.00.040.488 I print_info: freq_base_train  = 10000.0
0.00.040.488 I print_info: freq_scale_train = 1
0.00.040.488 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.488 I print_info: rope_finetuned   = unknown
0.00.040.489 I print_info: ssm_d_conv       = 0
0.00.040.489 I print_info: ssm_d_inner      = 0
0.00.040.489 I print_info: ssm_d_state      = 0
0.00.040.489 I print_info: ssm_dt_rank      = 0
0.00.040.489 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.490 I print_info: model type       = 1.4B
0.00.040.491 I print_info: model params     = 1.41 B
0.00.040.491 I print_info: general.name     = 1.4B
0.00.040.492 I print_info: vocab type       = BPE
0.00.040.492 I print_info: n_vocab          = 50304
0.00.040.492 I print_info: n_merges         = 50009
0.00.040.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: LF token         = 187 'Ċ'
0.00.040.493 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: max token length = 1024
0.00.040.494 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.908.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.908.826 I load_tensors: offloading output layer to GPU
0.00.908.827 I load_tensors: offloaded 25/25 layers to GPU
0.00.908.853 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.908.857 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.909.920 I llama_init_from_model: n_seq_max     = 1
0.00.909.922 I llama_init_from_model: n_ctx         = 128
0.00.909.922 I llama_init_from_model: n_ctx_per_seq = 128
0.00.909.922 I llama_init_from_model: n_batch       = 128
0.00.909.923 I llama_init_from_model: n_ubatch      = 128
0.00.909.923 I llama_init_from_model: flash_attn    = 0
0.00.909.924 I llama_init_from_model: freq_base     = 10000.0
0.00.909.924 I llama_init_from_model: freq_scale    = 1
0.00.909.925 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.909.927 I ggml_metal_init: allocating
0.00.909.983 I ggml_metal_init: found device: Apple M4
0.00.909.992 I ggml_metal_init: picking default device: Apple M4
0.00.911.252 I ggml_metal_init: using embedded metal library
0.00.916.273 I ggml_metal_init: GPU name:   Apple M4
0.00.916.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.916.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.916.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.916.278 I ggml_metal_init: simdgroup reduction   = true
0.00.916.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.916.278 I ggml_metal_init: has residency sets    = true
0.00.916.279 I ggml_metal_init: has bfloat            = true
0.00.916.279 I ggml_metal_init: use bfloat            = true
0.00.916.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.916.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.930.464 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.933.854 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.933.861 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.933.885 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.936.847 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.936.849 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.936.849 I llama_init_from_model: graph nodes  = 967
0.00.936.850 I llama_init_from_model: graph splits = 2
0.00.936.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.936.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.087 I 
0.00.963.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.179 I perplexity: tokenizing the input ..
0.00.970.861 I perplexity: tokenization took 7.677 ms
0.00.970.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.109.585 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.110.916 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.110.937 I llama_perf_context_print:        load time =     952.92 ms
0.01.110.938 I llama_perf_context_print: prompt eval time =     137.73 ms /   128 tokens (    1.08 ms per token,   929.35 tokens per second)
0.01.110.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.110.939 I llama_perf_context_print:       total time =     147.85 ms /   129 tokens
0.01.111.334 I ggml_metal_free: deallocating

real	0m1.125s
user	0m0.076s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.784 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.882 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.567 I llama_model_loader: - type  f32:  194 tensors
0.00.025.568 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.568 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.569 I print_info: file format = GGUF V3 (latest)
0.00.025.569 I print_info: file type   = Q4_0
0.00.025.570 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.802 I load: special tokens cache size = 25
0.00.040.042 I load: token to piece cache size = 0.2984 MB
0.00.040.047 I print_info: arch             = gptneox
0.00.040.047 I print_info: vocab_only       = 0
0.00.040.047 I print_info: n_ctx_train      = 2048
0.00.040.048 I print_info: n_embd           = 2048
0.00.040.048 I print_info: n_layer          = 24
0.00.040.052 I print_info: n_head           = 16
0.00.040.053 I print_info: n_head_kv        = 16
0.00.040.053 I print_info: n_rot            = 32
0.00.040.053 I print_info: n_swa            = 0
0.00.040.053 I print_info: n_embd_head_k    = 128
0.00.040.055 I print_info: n_embd_head_v    = 128
0.00.040.056 I print_info: n_gqa            = 1
0.00.040.057 I print_info: n_embd_k_gqa     = 2048
0.00.040.057 I print_info: n_embd_v_gqa     = 2048
0.00.040.058 I print_info: f_norm_eps       = 1.0e-05
0.00.040.058 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.059 I print_info: f_logit_scale    = 0.0e+00
0.00.040.059 I print_info: n_ff             = 8192
0.00.040.060 I print_info: n_expert         = 0
0.00.040.062 I print_info: n_expert_used    = 0
0.00.040.062 I print_info: causal attn      = 1
0.00.040.062 I print_info: pooling type     = 0
0.00.040.062 I print_info: rope type        = 2
0.00.040.062 I print_info: rope scaling     = linear
0.00.040.063 I print_info: freq_base_train  = 10000.0
0.00.040.063 I print_info: freq_scale_train = 1
0.00.040.063 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.064 I print_info: rope_finetuned   = unknown
0.00.040.064 I print_info: ssm_d_conv       = 0
0.00.040.064 I print_info: ssm_d_inner      = 0
0.00.040.065 I print_info: ssm_d_state      = 0
0.00.040.066 I print_info: ssm_dt_rank      = 0
0.00.040.066 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.066 I print_info: model type       = 1.4B
0.00.040.066 I print_info: model params     = 1.41 B
0.00.040.066 I print_info: general.name     = 1.4B
0.00.040.067 I print_info: vocab type       = BPE
0.00.040.067 I print_info: n_vocab          = 50304
0.00.040.067 I print_info: n_merges         = 50009
0.00.040.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: LF token         = 187 'Ċ'
0.00.040.068 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.069 I print_info: max token length = 1024
0.00.040.069 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.487 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.491 I load_tensors: offloading output layer to GPU
0.00.644.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.515 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.644.517 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.645.717 I llama_init_from_model: n_seq_max     = 1
0.00.645.719 I llama_init_from_model: n_ctx         = 128
0.00.645.720 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.720 I llama_init_from_model: n_batch       = 128
0.00.645.721 I llama_init_from_model: n_ubatch      = 128
0.00.645.721 I llama_init_from_model: flash_attn    = 0
0.00.645.722 I llama_init_from_model: freq_base     = 10000.0
0.00.645.723 I llama_init_from_model: freq_scale    = 1
0.00.645.724 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.727 I ggml_metal_init: allocating
0.00.645.775 I ggml_metal_init: found device: Apple M4
0.00.645.787 I ggml_metal_init: picking default device: Apple M4
0.00.647.173 I ggml_metal_init: using embedded metal library
0.00.653.101 I ggml_metal_init: GPU name:   Apple M4
0.00.653.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.108 I ggml_metal_init: simdgroup reduction   = true
0.00.653.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.109 I ggml_metal_init: has residency sets    = true
0.00.653.109 I ggml_metal_init: has bfloat            = true
0.00.653.109 I ggml_metal_init: use bfloat            = true
0.00.653.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.300 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.325 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.394 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.396 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.397 I llama_init_from_model: graph nodes  = 967
0.00.676.397 I llama_init_from_model: graph splits = 2
0.00.676.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.085 I 
0.00.701.154 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.161 I perplexity: tokenizing the input ..
0.00.708.083 I perplexity: tokenization took 6.919 ms
0.00.708.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.001 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.846.265 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.846.287 I llama_perf_context_print:        load time =     691.29 ms
0.00.846.288 I llama_perf_context_print: prompt eval time =     135.96 ms /   128 tokens (    1.06 ms per token,   941.43 tokens per second)
0.00.846.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.289 I llama_perf_context_print:       total time =     145.21 ms /   129 tokens
0.00.846.651 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.079s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.825 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.700 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.487 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.204 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.208 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.209 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q4_1
0.00.025.211 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.204 I load: special tokens cache size = 25
0.00.039.007 I load: token to piece cache size = 0.2984 MB
0.00.039.011 I print_info: arch             = gptneox
0.00.039.012 I print_info: vocab_only       = 0
0.00.039.012 I print_info: n_ctx_train      = 2048
0.00.039.012 I print_info: n_embd           = 2048
0.00.039.012 I print_info: n_layer          = 24
0.00.039.017 I print_info: n_head           = 16
0.00.039.018 I print_info: n_head_kv        = 16
0.00.039.018 I print_info: n_rot            = 32
0.00.039.018 I print_info: n_swa            = 0
0.00.039.018 I print_info: n_embd_head_k    = 128
0.00.039.018 I print_info: n_embd_head_v    = 128
0.00.039.019 I print_info: n_gqa            = 1
0.00.039.020 I print_info: n_embd_k_gqa     = 2048
0.00.039.021 I print_info: n_embd_v_gqa     = 2048
0.00.039.021 I print_info: f_norm_eps       = 1.0e-05
0.00.039.022 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.022 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.025 I print_info: f_logit_scale    = 0.0e+00
0.00.039.026 I print_info: n_ff             = 8192
0.00.039.026 I print_info: n_expert         = 0
0.00.039.026 I print_info: n_expert_used    = 0
0.00.039.026 I print_info: causal attn      = 1
0.00.039.027 I print_info: pooling type     = 0
0.00.039.027 I print_info: rope type        = 2
0.00.039.027 I print_info: rope scaling     = linear
0.00.039.027 I print_info: freq_base_train  = 10000.0
0.00.039.028 I print_info: freq_scale_train = 1
0.00.039.028 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.028 I print_info: rope_finetuned   = unknown
0.00.039.028 I print_info: ssm_d_conv       = 0
0.00.039.029 I print_info: ssm_d_inner      = 0
0.00.039.029 I print_info: ssm_d_state      = 0
0.00.039.029 I print_info: ssm_dt_rank      = 0
0.00.039.029 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.029 I print_info: model type       = 1.4B
0.00.039.030 I print_info: model params     = 1.41 B
0.00.039.030 I print_info: general.name     = 1.4B
0.00.039.030 I print_info: vocab type       = BPE
0.00.039.030 I print_info: n_vocab          = 50304
0.00.039.031 I print_info: n_merges         = 50009
0.00.039.031 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.031 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.031 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: LF token         = 187 'Ċ'
0.00.039.032 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: max token length = 1024
0.00.039.034 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.098 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.105 I load_tensors: offloading output layer to GPU
0.00.693.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.133 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.693.136 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.694.251 I llama_init_from_model: n_seq_max     = 1
0.00.694.253 I llama_init_from_model: n_ctx         = 128
0.00.694.253 I llama_init_from_model: n_ctx_per_seq = 128
0.00.694.254 I llama_init_from_model: n_batch       = 128
0.00.694.254 I llama_init_from_model: n_ubatch      = 128
0.00.694.255 I llama_init_from_model: flash_attn    = 0
0.00.694.256 I llama_init_from_model: freq_base     = 10000.0
0.00.694.256 I llama_init_from_model: freq_scale    = 1
0.00.694.257 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.694.258 I ggml_metal_init: allocating
0.00.694.297 I ggml_metal_init: found device: Apple M4
0.00.694.307 I ggml_metal_init: picking default device: Apple M4
0.00.695.600 I ggml_metal_init: using embedded metal library
0.00.701.570 I ggml_metal_init: GPU name:   Apple M4
0.00.701.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.576 I ggml_metal_init: simdgroup reduction   = true
0.00.701.576 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.576 I ggml_metal_init: has residency sets    = true
0.00.701.577 I ggml_metal_init: has bfloat            = true
0.00.701.577 I ggml_metal_init: use bfloat            = true
0.00.701.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.273 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.721.495 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.519 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.724.411 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.724.412 I llama_init_from_model: graph nodes  = 967
0.00.724.412 I llama_init_from_model: graph splits = 2
0.00.724.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.943 I 
0.00.752.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.017 I perplexity: tokenizing the input ..
0.00.758.459 I perplexity: tokenization took 6.438 ms
0.00.758.468 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.948 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.893.207 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.893.239 I llama_perf_context_print:        load time =     742.11 ms
0.00.893.239 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   962.96 tokens per second)
0.00.893.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.241 I llama_perf_context_print:       total time =     141.30 ms /   129 tokens
0.00.893.614 I ggml_metal_free: deallocating

real	0m0.907s
user	0m0.077s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.351 I llama_model_loader: - type  f32:  194 tensors
0.00.026.351 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.352 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.352 I print_info: file format = GGUF V3 (latest)
0.00.026.353 I print_info: file type   = Q5_0
0.00.026.354 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.603 I load: special tokens cache size = 25
0.00.040.679 I load: token to piece cache size = 0.2984 MB
0.00.040.684 I print_info: arch             = gptneox
0.00.040.684 I print_info: vocab_only       = 0
0.00.040.684 I print_info: n_ctx_train      = 2048
0.00.040.684 I print_info: n_embd           = 2048
0.00.040.684 I print_info: n_layer          = 24
0.00.040.689 I print_info: n_head           = 16
0.00.040.690 I print_info: n_head_kv        = 16
0.00.040.690 I print_info: n_rot            = 32
0.00.040.690 I print_info: n_swa            = 0
0.00.040.690 I print_info: n_embd_head_k    = 128
0.00.040.691 I print_info: n_embd_head_v    = 128
0.00.040.691 I print_info: n_gqa            = 1
0.00.040.692 I print_info: n_embd_k_gqa     = 2048
0.00.040.693 I print_info: n_embd_v_gqa     = 2048
0.00.040.693 I print_info: f_norm_eps       = 1.0e-05
0.00.040.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.694 I print_info: f_logit_scale    = 0.0e+00
0.00.040.695 I print_info: n_ff             = 8192
0.00.040.695 I print_info: n_expert         = 0
0.00.040.695 I print_info: n_expert_used    = 0
0.00.040.695 I print_info: causal attn      = 1
0.00.040.695 I print_info: pooling type     = 0
0.00.040.696 I print_info: rope type        = 2
0.00.040.696 I print_info: rope scaling     = linear
0.00.040.696 I print_info: freq_base_train  = 10000.0
0.00.040.697 I print_info: freq_scale_train = 1
0.00.040.697 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.697 I print_info: rope_finetuned   = unknown
0.00.040.697 I print_info: ssm_d_conv       = 0
0.00.040.697 I print_info: ssm_d_inner      = 0
0.00.040.697 I print_info: ssm_d_state      = 0
0.00.040.698 I print_info: ssm_dt_rank      = 0
0.00.040.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.699 I print_info: model type       = 1.4B
0.00.040.702 I print_info: model params     = 1.41 B
0.00.040.702 I print_info: general.name     = 1.4B
0.00.040.703 I print_info: vocab type       = BPE
0.00.040.703 I print_info: n_vocab          = 50304
0.00.040.703 I print_info: n_merges         = 50009
0.00.040.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.704 I print_info: LF token         = 187 'Ċ'
0.00.040.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.704 I print_info: max token length = 1024
0.00.040.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.694.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.694.033 I load_tensors: offloading output layer to GPU
0.00.694.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.694.058 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.694.064 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.695.268 I llama_init_from_model: n_seq_max     = 1
0.00.695.270 I llama_init_from_model: n_ctx         = 128
0.00.695.270 I llama_init_from_model: n_ctx_per_seq = 128
0.00.695.271 I llama_init_from_model: n_batch       = 128
0.00.695.271 I llama_init_from_model: n_ubatch      = 128
0.00.695.271 I llama_init_from_model: flash_attn    = 0
0.00.695.272 I llama_init_from_model: freq_base     = 10000.0
0.00.695.273 I llama_init_from_model: freq_scale    = 1
0.00.695.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.695.275 I ggml_metal_init: allocating
0.00.695.291 I ggml_metal_init: found device: Apple M4
0.00.695.299 I ggml_metal_init: picking default device: Apple M4
0.00.696.494 I ggml_metal_init: using embedded metal library
0.00.702.444 I ggml_metal_init: GPU name:   Apple M4
0.00.702.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.450 I ggml_metal_init: simdgroup reduction   = true
0.00.702.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.451 I ggml_metal_init: has residency sets    = true
0.00.702.451 I ggml_metal_init: has bfloat            = true
0.00.702.451 I ggml_metal_init: use bfloat            = true
0.00.702.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.634 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.168 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.723.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.723.210 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.305 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.726.309 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.726.309 I llama_init_from_model: graph nodes  = 967
0.00.726.310 I llama_init_from_model: graph splits = 2
0.00.726.313 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.726.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.157 I 
0.00.752.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.240 I perplexity: tokenizing the input ..
0.00.757.547 I perplexity: tokenization took 5.305 ms
0.00.757.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.892.186 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.893.451 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.893.487 I llama_perf_context_print:        load time =     741.28 ms
0.00.893.488 I llama_perf_context_print: prompt eval time =     134.10 ms /   128 tokens (    1.05 ms per token,   954.54 tokens per second)
0.00.893.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.489 I llama_perf_context_print:       total time =     141.34 ms /   129 tokens
0.00.893.907 I ggml_metal_free: deallocating

real	0m0.910s
user	0m0.076s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.131 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.136 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.673 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.674 I llama_model_loader: - type  f32:  194 tensors
0.00.025.674 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.675 I print_info: file format = GGUF V3 (latest)
0.00.025.675 I print_info: file type   = Q5_1
0.00.025.676 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.308 I load: special tokens cache size = 25
0.00.039.185 I load: token to piece cache size = 0.2984 MB
0.00.039.188 I print_info: arch             = gptneox
0.00.039.188 I print_info: vocab_only       = 0
0.00.039.189 I print_info: n_ctx_train      = 2048
0.00.039.189 I print_info: n_embd           = 2048
0.00.039.189 I print_info: n_layer          = 24
0.00.039.192 I print_info: n_head           = 16
0.00.039.192 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.193 I print_info: n_swa            = 0
0.00.039.193 I print_info: n_embd_head_k    = 128
0.00.039.193 I print_info: n_embd_head_v    = 128
0.00.039.194 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.195 I print_info: n_embd_v_gqa     = 2048
0.00.039.196 I print_info: f_norm_eps       = 1.0e-05
0.00.039.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.196 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.197 I print_info: f_logit_scale    = 0.0e+00
0.00.039.198 I print_info: n_ff             = 8192
0.00.039.198 I print_info: n_expert         = 0
0.00.039.198 I print_info: n_expert_used    = 0
0.00.039.198 I print_info: causal attn      = 1
0.00.039.199 I print_info: pooling type     = 0
0.00.039.200 I print_info: rope type        = 2
0.00.039.201 I print_info: rope scaling     = linear
0.00.039.201 I print_info: freq_base_train  = 10000.0
0.00.039.201 I print_info: freq_scale_train = 1
0.00.039.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.202 I print_info: rope_finetuned   = unknown
0.00.039.202 I print_info: ssm_d_conv       = 0
0.00.039.202 I print_info: ssm_d_inner      = 0
0.00.039.202 I print_info: ssm_d_state      = 0
0.00.039.202 I print_info: ssm_dt_rank      = 0
0.00.039.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.203 I print_info: model type       = 1.4B
0.00.039.203 I print_info: model params     = 1.41 B
0.00.039.203 I print_info: general.name     = 1.4B
0.00.039.204 I print_info: vocab type       = BPE
0.00.039.205 I print_info: n_vocab          = 50304
0.00.039.206 I print_info: n_merges         = 50009
0.00.039.206 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.207 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.207 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.207 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.207 I print_info: LF token         = 187 'Ċ'
0.00.039.208 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.208 I print_info: max token length = 1024
0.00.039.208 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.747.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.747.881 I load_tensors: offloading output layer to GPU
0.00.747.883 I load_tensors: offloaded 25/25 layers to GPU
0.00.747.906 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.747.908 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.749.053 I llama_init_from_model: n_seq_max     = 1
0.00.749.055 I llama_init_from_model: n_ctx         = 128
0.00.749.055 I llama_init_from_model: n_ctx_per_seq = 128
0.00.749.056 I llama_init_from_model: n_batch       = 128
0.00.749.056 I llama_init_from_model: n_ubatch      = 128
0.00.749.056 I llama_init_from_model: flash_attn    = 0
0.00.749.057 I llama_init_from_model: freq_base     = 10000.0
0.00.749.058 I llama_init_from_model: freq_scale    = 1
0.00.749.058 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.749.059 I ggml_metal_init: allocating
0.00.749.076 I ggml_metal_init: found device: Apple M4
0.00.749.085 I ggml_metal_init: picking default device: Apple M4
0.00.750.289 I ggml_metal_init: using embedded metal library
0.00.755.621 I ggml_metal_init: GPU name:   Apple M4
0.00.755.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.755.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.755.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.755.626 I ggml_metal_init: simdgroup reduction   = true
0.00.755.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.755.627 I ggml_metal_init: has residency sets    = true
0.00.755.627 I ggml_metal_init: has bfloat            = true
0.00.755.627 I ggml_metal_init: use bfloat            = true
0.00.755.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.755.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.982 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.774.391 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.774.395 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.774.419 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.261 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.777.262 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.777.263 I llama_init_from_model: graph nodes  = 967
0.00.777.263 I llama_init_from_model: graph splits = 2
0.00.777.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.777.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.590 I 
0.00.806.664 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.672 I perplexity: tokenizing the input ..
0.00.813.614 I perplexity: tokenization took 6.938 ms
0.00.813.621 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.962.043 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.963.308 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.963.339 I llama_perf_context_print:        load time =     796.78 ms
0.00.963.340 I llama_perf_context_print: prompt eval time =     147.51 ms /   128 tokens (    1.15 ms per token,   867.73 tokens per second)
0.00.963.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.963.341 I llama_perf_context_print:       total time =     156.75 ms /   129 tokens
0.00.963.764 I ggml_metal_free: deallocating

real	0m0.978s
user	0m0.076s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.943 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.943 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.944 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.945 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.946 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.950 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.620 I llama_model_loader: - type  f32:  194 tensors
0.00.026.620 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.620 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.621 I print_info: file format = GGUF V3 (latest)
0.00.026.621 I print_info: file type   = Q2_K - Medium
0.00.026.622 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.691 I load: special tokens cache size = 25
0.00.040.858 I load: token to piece cache size = 0.2984 MB
0.00.040.862 I print_info: arch             = gptneox
0.00.040.862 I print_info: vocab_only       = 0
0.00.040.862 I print_info: n_ctx_train      = 2048
0.00.040.862 I print_info: n_embd           = 2048
0.00.040.862 I print_info: n_layer          = 24
0.00.040.866 I print_info: n_head           = 16
0.00.040.867 I print_info: n_head_kv        = 16
0.00.040.867 I print_info: n_rot            = 32
0.00.040.867 I print_info: n_swa            = 0
0.00.040.867 I print_info: n_embd_head_k    = 128
0.00.040.869 I print_info: n_embd_head_v    = 128
0.00.040.869 I print_info: n_gqa            = 1
0.00.040.870 I print_info: n_embd_k_gqa     = 2048
0.00.040.871 I print_info: n_embd_v_gqa     = 2048
0.00.040.871 I print_info: f_norm_eps       = 1.0e-05
0.00.040.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.872 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.872 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.872 I print_info: f_logit_scale    = 0.0e+00
0.00.040.873 I print_info: n_ff             = 8192
0.00.040.873 I print_info: n_expert         = 0
0.00.040.873 I print_info: n_expert_used    = 0
0.00.040.873 I print_info: causal attn      = 1
0.00.040.874 I print_info: pooling type     = 0
0.00.040.876 I print_info: rope type        = 2
0.00.040.876 I print_info: rope scaling     = linear
0.00.040.877 I print_info: freq_base_train  = 10000.0
0.00.040.877 I print_info: freq_scale_train = 1
0.00.040.877 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.877 I print_info: rope_finetuned   = unknown
0.00.040.877 I print_info: ssm_d_conv       = 0
0.00.040.878 I print_info: ssm_d_inner      = 0
0.00.040.878 I print_info: ssm_d_state      = 0
0.00.040.878 I print_info: ssm_dt_rank      = 0
0.00.040.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.878 I print_info: model type       = 1.4B
0.00.040.879 I print_info: model params     = 1.41 B
0.00.040.879 I print_info: general.name     = 1.4B
0.00.040.879 I print_info: vocab type       = BPE
0.00.040.879 I print_info: n_vocab          = 50304
0.00.040.880 I print_info: n_merges         = 50009
0.00.040.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.886 I print_info: LF token         = 187 'Ċ'
0.00.040.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.886 I print_info: max token length = 1024
0.00.040.886 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.408.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.408.182 I load_tensors: offloading output layer to GPU
0.00.408.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.408.212 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.408.214 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.409.916 I llama_init_from_model: n_seq_max     = 1
0.00.409.919 I llama_init_from_model: n_ctx         = 128
0.00.409.920 I llama_init_from_model: n_ctx_per_seq = 128
0.00.409.921 I llama_init_from_model: n_batch       = 128
0.00.409.921 I llama_init_from_model: n_ubatch      = 128
0.00.409.922 I llama_init_from_model: flash_attn    = 0
0.00.409.924 I llama_init_from_model: freq_base     = 10000.0
0.00.409.924 I llama_init_from_model: freq_scale    = 1
0.00.409.925 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.409.930 I ggml_metal_init: allocating
0.00.409.989 I ggml_metal_init: found device: Apple M4
0.00.410.004 I ggml_metal_init: picking default device: Apple M4
0.00.411.952 I ggml_metal_init: using embedded metal library
0.00.418.296 I ggml_metal_init: GPU name:   Apple M4
0.00.418.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.418.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.418.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.418.310 I ggml_metal_init: simdgroup reduction   = true
0.00.418.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.418.311 I ggml_metal_init: has residency sets    = true
0.00.418.311 I ggml_metal_init: has bfloat            = true
0.00.418.312 I ggml_metal_init: use bfloat            = true
0.00.418.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.418.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.439.603 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.443.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.443.144 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.443.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.446.660 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.446.662 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.446.663 I llama_init_from_model: graph nodes  = 967
0.00.446.663 I llama_init_from_model: graph splits = 2
0.00.446.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.446.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.673 I 
0.00.476.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.749 I perplexity: tokenizing the input ..
0.00.483.436 I perplexity: tokenization took 6.684 ms
0.00.483.442 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.629.679 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.630.960 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.630.989 I llama_perf_context_print:        load time =     465.62 ms
0.00.630.990 I llama_perf_context_print: prompt eval time =     145.36 ms /   128 tokens (    1.14 ms per token,   880.55 tokens per second)
0.00.630.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.630.991 I llama_perf_context_print:       total time =     154.32 ms /   129 tokens
0.00.631.403 I ggml_metal_free: deallocating

real	0m0.647s
user	0m0.083s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.606 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.607 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.975 I llama_model_loader: - type  f32:  194 tensors
0.00.024.975 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.976 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.976 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.976 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.977 I print_info: file format = GGUF V3 (latest)
0.00.024.977 I print_info: file type   = Q3_K - Medium
0.00.024.978 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.906 I load: special tokens cache size = 25
0.00.038.917 I load: token to piece cache size = 0.2984 MB
0.00.038.920 I print_info: arch             = gptneox
0.00.038.920 I print_info: vocab_only       = 0
0.00.038.920 I print_info: n_ctx_train      = 2048
0.00.038.920 I print_info: n_embd           = 2048
0.00.038.921 I print_info: n_layer          = 24
0.00.038.924 I print_info: n_head           = 16
0.00.038.925 I print_info: n_head_kv        = 16
0.00.038.925 I print_info: n_rot            = 32
0.00.038.925 I print_info: n_swa            = 0
0.00.038.925 I print_info: n_embd_head_k    = 128
0.00.038.925 I print_info: n_embd_head_v    = 128
0.00.038.926 I print_info: n_gqa            = 1
0.00.038.927 I print_info: n_embd_k_gqa     = 2048
0.00.038.927 I print_info: n_embd_v_gqa     = 2048
0.00.038.932 I print_info: f_norm_eps       = 1.0e-05
0.00.038.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.932 I print_info: f_logit_scale    = 0.0e+00
0.00.038.933 I print_info: n_ff             = 8192
0.00.038.933 I print_info: n_expert         = 0
0.00.038.934 I print_info: n_expert_used    = 0
0.00.038.934 I print_info: causal attn      = 1
0.00.038.934 I print_info: pooling type     = 0
0.00.038.934 I print_info: rope type        = 2
0.00.038.934 I print_info: rope scaling     = linear
0.00.038.935 I print_info: freq_base_train  = 10000.0
0.00.038.935 I print_info: freq_scale_train = 1
0.00.038.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.935 I print_info: rope_finetuned   = unknown
0.00.038.936 I print_info: ssm_d_conv       = 0
0.00.038.936 I print_info: ssm_d_inner      = 0
0.00.038.937 I print_info: ssm_d_state      = 0
0.00.038.937 I print_info: ssm_dt_rank      = 0
0.00.038.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.938 I print_info: model type       = 1.4B
0.00.038.938 I print_info: model params     = 1.41 B
0.00.038.938 I print_info: general.name     = 1.4B
0.00.038.939 I print_info: vocab type       = BPE
0.00.038.939 I print_info: n_vocab          = 50304
0.00.038.939 I print_info: n_merges         = 50009
0.00.038.939 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.940 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.940 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.940 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.940 I print_info: LF token         = 187 'Ċ'
0.00.038.941 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.941 I print_info: max token length = 1024
0.00.038.941 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.492.559 I load_tensors: offloading 24 repeating layers to GPU
0.00.492.565 I load_tensors: offloading output layer to GPU
0.00.492.566 I load_tensors: offloaded 25/25 layers to GPU
0.00.492.590 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.492.591 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.493.848 I llama_init_from_model: n_seq_max     = 1
0.00.493.850 I llama_init_from_model: n_ctx         = 128
0.00.493.851 I llama_init_from_model: n_ctx_per_seq = 128
0.00.493.851 I llama_init_from_model: n_batch       = 128
0.00.493.852 I llama_init_from_model: n_ubatch      = 128
0.00.493.852 I llama_init_from_model: flash_attn    = 0
0.00.493.854 I llama_init_from_model: freq_base     = 10000.0
0.00.493.854 I llama_init_from_model: freq_scale    = 1
0.00.493.855 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.493.856 I ggml_metal_init: allocating
0.00.493.872 I ggml_metal_init: found device: Apple M4
0.00.493.883 I ggml_metal_init: picking default device: Apple M4
0.00.495.263 I ggml_metal_init: using embedded metal library
0.00.501.402 I ggml_metal_init: GPU name:   Apple M4
0.00.501.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.501.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.501.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.501.407 I ggml_metal_init: simdgroup reduction   = true
0.00.501.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.501.408 I ggml_metal_init: has residency sets    = true
0.00.501.408 I ggml_metal_init: has bfloat            = true
0.00.501.409 I ggml_metal_init: use bfloat            = true
0.00.501.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.501.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.518.960 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.522.407 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.522.431 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.525.463 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.525.464 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.525.465 I llama_init_from_model: graph nodes  = 967
0.00.525.465 I llama_init_from_model: graph splits = 2
0.00.525.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.525.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.137 I 
0.00.550.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.217 I perplexity: tokenizing the input ..
0.00.556.073 I perplexity: tokenization took 5.851 ms
0.00.556.081 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.860 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.689.131 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.689.160 I llama_perf_context_print:        load time =     540.42 ms
0.00.689.161 I llama_perf_context_print: prompt eval time =     130.89 ms /   128 tokens (    1.02 ms per token,   977.90 tokens per second)
0.00.689.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.164 I llama_perf_context_print:       total time =     139.03 ms /   129 tokens
0.00.689.578 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.077s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.654 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.947 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.949 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.950 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.950 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.950 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.951 I llama_model_loader: - type  f32:  194 tensors
0.00.024.951 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.952 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.952 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.952 I print_info: file format = GGUF V3 (latest)
0.00.024.953 I print_info: file type   = Q4_K - Medium
0.00.024.954 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.949 I load: special tokens cache size = 25
0.00.039.092 I load: token to piece cache size = 0.2984 MB
0.00.039.096 I print_info: arch             = gptneox
0.00.039.096 I print_info: vocab_only       = 0
0.00.039.096 I print_info: n_ctx_train      = 2048
0.00.039.096 I print_info: n_embd           = 2048
0.00.039.097 I print_info: n_layer          = 24
0.00.039.101 I print_info: n_head           = 16
0.00.039.101 I print_info: n_head_kv        = 16
0.00.039.101 I print_info: n_rot            = 32
0.00.039.101 I print_info: n_swa            = 0
0.00.039.103 I print_info: n_embd_head_k    = 128
0.00.039.103 I print_info: n_embd_head_v    = 128
0.00.039.104 I print_info: n_gqa            = 1
0.00.039.105 I print_info: n_embd_k_gqa     = 2048
0.00.039.105 I print_info: n_embd_v_gqa     = 2048
0.00.039.106 I print_info: f_norm_eps       = 1.0e-05
0.00.039.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.107 I print_info: f_logit_scale    = 0.0e+00
0.00.039.107 I print_info: n_ff             = 8192
0.00.039.108 I print_info: n_expert         = 0
0.00.039.108 I print_info: n_expert_used    = 0
0.00.039.108 I print_info: causal attn      = 1
0.00.039.108 I print_info: pooling type     = 0
0.00.039.108 I print_info: rope type        = 2
0.00.039.108 I print_info: rope scaling     = linear
0.00.039.111 I print_info: freq_base_train  = 10000.0
0.00.039.111 I print_info: freq_scale_train = 1
0.00.039.111 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.113 I print_info: rope_finetuned   = unknown
0.00.039.113 I print_info: ssm_d_conv       = 0
0.00.039.113 I print_info: ssm_d_inner      = 0
0.00.039.113 I print_info: ssm_d_state      = 0
0.00.039.113 I print_info: ssm_dt_rank      = 0
0.00.039.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.113 I print_info: model type       = 1.4B
0.00.039.115 I print_info: model params     = 1.41 B
0.00.039.115 I print_info: general.name     = 1.4B
0.00.039.116 I print_info: vocab type       = BPE
0.00.039.116 I print_info: n_vocab          = 50304
0.00.039.116 I print_info: n_merges         = 50009
0.00.039.117 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: LF token         = 187 'Ċ'
0.00.039.118 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.118 I print_info: max token length = 1024
0.00.039.118 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.434 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.440 I load_tensors: offloading output layer to GPU
0.00.581.441 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.464 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.581.467 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.582.506 I llama_init_from_model: n_seq_max     = 1
0.00.582.507 I llama_init_from_model: n_ctx         = 128
0.00.582.508 I llama_init_from_model: n_ctx_per_seq = 128
0.00.582.508 I llama_init_from_model: n_batch       = 128
0.00.582.509 I llama_init_from_model: n_ubatch      = 128
0.00.582.509 I llama_init_from_model: flash_attn    = 0
0.00.582.510 I llama_init_from_model: freq_base     = 10000.0
0.00.582.510 I llama_init_from_model: freq_scale    = 1
0.00.582.511 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.582.512 I ggml_metal_init: allocating
0.00.582.529 I ggml_metal_init: found device: Apple M4
0.00.582.537 I ggml_metal_init: picking default device: Apple M4
0.00.583.775 I ggml_metal_init: using embedded metal library
0.00.589.947 I ggml_metal_init: GPU name:   Apple M4
0.00.589.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.953 I ggml_metal_init: simdgroup reduction   = true
0.00.589.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.954 I ggml_metal_init: has residency sets    = true
0.00.589.954 I ggml_metal_init: has bfloat            = true
0.00.589.954 I ggml_metal_init: use bfloat            = true
0.00.589.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.606.639 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.929 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.775 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.612.776 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.612.777 I llama_init_from_model: graph nodes  = 967
0.00.612.777 I llama_init_from_model: graph splits = 2
0.00.612.779 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.612.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.954 I 
0.00.641.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.036 I perplexity: tokenizing the input ..
0.00.647.796 I perplexity: tokenization took 6.758 ms
0.00.647.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.792.602 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.792.638 I llama_perf_context_print:        load time =     631.04 ms
0.00.792.639 I llama_perf_context_print: prompt eval time =     143.01 ms /   128 tokens (    1.12 ms per token,   895.07 tokens per second)
0.00.792.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.640 I llama_perf_context_print:       total time =     151.69 ms /   129 tokens
0.00.793.038 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.309 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.310 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.311 I llama_model_loader: - type  f32:  194 tensors
0.00.026.311 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.311 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.312 I print_info: file format = GGUF V3 (latest)
0.00.026.312 I print_info: file type   = Q5_K - Medium
0.00.026.317 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.281 I load: special tokens cache size = 25
0.00.040.441 I load: token to piece cache size = 0.2984 MB
0.00.040.444 I print_info: arch             = gptneox
0.00.040.444 I print_info: vocab_only       = 0
0.00.040.444 I print_info: n_ctx_train      = 2048
0.00.040.445 I print_info: n_embd           = 2048
0.00.040.445 I print_info: n_layer          = 24
0.00.040.448 I print_info: n_head           = 16
0.00.040.449 I print_info: n_head_kv        = 16
0.00.040.449 I print_info: n_rot            = 32
0.00.040.449 I print_info: n_swa            = 0
0.00.040.450 I print_info: n_embd_head_k    = 128
0.00.040.450 I print_info: n_embd_head_v    = 128
0.00.040.450 I print_info: n_gqa            = 1
0.00.040.451 I print_info: n_embd_k_gqa     = 2048
0.00.040.452 I print_info: n_embd_v_gqa     = 2048
0.00.040.452 I print_info: f_norm_eps       = 1.0e-05
0.00.040.453 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.454 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.454 I print_info: f_logit_scale    = 0.0e+00
0.00.040.455 I print_info: n_ff             = 8192
0.00.040.455 I print_info: n_expert         = 0
0.00.040.455 I print_info: n_expert_used    = 0
0.00.040.455 I print_info: causal attn      = 1
0.00.040.460 I print_info: pooling type     = 0
0.00.040.461 I print_info: rope type        = 2
0.00.040.461 I print_info: rope scaling     = linear
0.00.040.461 I print_info: freq_base_train  = 10000.0
0.00.040.462 I print_info: freq_scale_train = 1
0.00.040.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.462 I print_info: rope_finetuned   = unknown
0.00.040.464 I print_info: ssm_d_conv       = 0
0.00.040.465 I print_info: ssm_d_inner      = 0
0.00.040.465 I print_info: ssm_d_state      = 0
0.00.040.465 I print_info: ssm_dt_rank      = 0
0.00.040.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.465 I print_info: model type       = 1.4B
0.00.040.465 I print_info: model params     = 1.41 B
0.00.040.466 I print_info: general.name     = 1.4B
0.00.040.466 I print_info: vocab type       = BPE
0.00.040.466 I print_info: n_vocab          = 50304
0.00.040.467 I print_info: n_merges         = 50009
0.00.040.468 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.468 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.468 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.468 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.468 I print_info: LF token         = 187 'Ċ'
0.00.040.469 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.469 I print_info: max token length = 1024
0.00.040.469 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.825 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.829 I load_tensors: offloading output layer to GPU
0.00.668.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.853 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.668.857 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.669.964 I llama_init_from_model: n_seq_max     = 1
0.00.669.966 I llama_init_from_model: n_ctx         = 128
0.00.669.966 I llama_init_from_model: n_ctx_per_seq = 128
0.00.669.967 I llama_init_from_model: n_batch       = 128
0.00.669.967 I llama_init_from_model: n_ubatch      = 128
0.00.669.967 I llama_init_from_model: flash_attn    = 0
0.00.669.968 I llama_init_from_model: freq_base     = 10000.0
0.00.669.969 I llama_init_from_model: freq_scale    = 1
0.00.669.970 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.669.971 I ggml_metal_init: allocating
0.00.669.983 I ggml_metal_init: found device: Apple M4
0.00.669.992 I ggml_metal_init: picking default device: Apple M4
0.00.671.160 I ggml_metal_init: using embedded metal library
0.00.676.513 I ggml_metal_init: GPU name:   Apple M4
0.00.676.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.676.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.676.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.676.518 I ggml_metal_init: simdgroup reduction   = true
0.00.676.518 I ggml_metal_init: simdgroup matrix mul. = true
0.00.676.519 I ggml_metal_init: has residency sets    = true
0.00.676.519 I ggml_metal_init: has bfloat            = true
0.00.676.519 I ggml_metal_init: use bfloat            = true
0.00.676.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.676.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.829 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.196 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.205 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.235 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.268 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.268 I llama_init_from_model: graph nodes  = 967
0.00.698.268 I llama_init_from_model: graph splits = 2
0.00.698.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.646 I 
0.00.733.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.725 I perplexity: tokenizing the input ..
0.00.740.263 I perplexity: tokenization took 6.536 ms
0.00.740.269 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.451 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.890.688 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.890.724 I llama_perf_context_print:        load time =     722.58 ms
0.00.890.725 I llama_perf_context_print: prompt eval time =     148.64 ms /   128 tokens (    1.16 ms per token,   861.14 tokens per second)
0.00.890.725 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.726 I llama_perf_context_print:       total time =     157.08 ms /   129 tokens
0.00.891.149 I ggml_metal_free: deallocating

real	0m0.907s
user	0m0.076s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.422 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.456 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.457 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.242 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.243 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.244 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.244 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.244 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.244 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.245 I llama_model_loader: - type  f32:  194 tensors
0.00.026.245 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.246 I print_info: file format = GGUF V3 (latest)
0.00.026.246 I print_info: file type   = Q6_K
0.00.026.247 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.285 I load: special tokens cache size = 25
0.00.040.585 I load: token to piece cache size = 0.2984 MB
0.00.040.589 I print_info: arch             = gptneox
0.00.040.589 I print_info: vocab_only       = 0
0.00.040.589 I print_info: n_ctx_train      = 2048
0.00.040.589 I print_info: n_embd           = 2048
0.00.040.589 I print_info: n_layer          = 24
0.00.040.592 I print_info: n_head           = 16
0.00.040.593 I print_info: n_head_kv        = 16
0.00.040.593 I print_info: n_rot            = 32
0.00.040.594 I print_info: n_swa            = 0
0.00.040.594 I print_info: n_embd_head_k    = 128
0.00.040.594 I print_info: n_embd_head_v    = 128
0.00.040.595 I print_info: n_gqa            = 1
0.00.040.595 I print_info: n_embd_k_gqa     = 2048
0.00.040.596 I print_info: n_embd_v_gqa     = 2048
0.00.040.597 I print_info: f_norm_eps       = 1.0e-05
0.00.040.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.597 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.597 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.598 I print_info: f_logit_scale    = 0.0e+00
0.00.040.598 I print_info: n_ff             = 8192
0.00.040.598 I print_info: n_expert         = 0
0.00.040.599 I print_info: n_expert_used    = 0
0.00.040.599 I print_info: causal attn      = 1
0.00.040.599 I print_info: pooling type     = 0
0.00.040.599 I print_info: rope type        = 2
0.00.040.599 I print_info: rope scaling     = linear
0.00.040.600 I print_info: freq_base_train  = 10000.0
0.00.040.600 I print_info: freq_scale_train = 1
0.00.040.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.600 I print_info: rope_finetuned   = unknown
0.00.040.601 I print_info: ssm_d_conv       = 0
0.00.040.601 I print_info: ssm_d_inner      = 0
0.00.040.601 I print_info: ssm_d_state      = 0
0.00.040.601 I print_info: ssm_dt_rank      = 0
0.00.040.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.601 I print_info: model type       = 1.4B
0.00.040.602 I print_info: model params     = 1.41 B
0.00.040.602 I print_info: general.name     = 1.4B
0.00.040.602 I print_info: vocab type       = BPE
0.00.040.603 I print_info: n_vocab          = 50304
0.00.040.603 I print_info: n_merges         = 50009
0.00.040.603 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.603 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.603 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.606 I print_info: LF token         = 187 'Ċ'
0.00.040.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.606 I print_info: max token length = 1024
0.00.040.607 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.678.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.678.338 I load_tensors: offloading output layer to GPU
0.00.678.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.358 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.678.360 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.679.373 I llama_init_from_model: n_seq_max     = 1
0.00.679.375 I llama_init_from_model: n_ctx         = 128
0.00.679.375 I llama_init_from_model: n_ctx_per_seq = 128
0.00.679.376 I llama_init_from_model: n_batch       = 128
0.00.679.376 I llama_init_from_model: n_ubatch      = 128
0.00.679.376 I llama_init_from_model: flash_attn    = 0
0.00.679.377 I llama_init_from_model: freq_base     = 10000.0
0.00.679.378 I llama_init_from_model: freq_scale    = 1
0.00.679.378 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.679.379 I ggml_metal_init: allocating
0.00.679.402 I ggml_metal_init: found device: Apple M4
0.00.679.411 I ggml_metal_init: picking default device: Apple M4
0.00.680.734 I ggml_metal_init: using embedded metal library
0.00.686.269 I ggml_metal_init: GPU name:   Apple M4
0.00.686.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.274 I ggml_metal_init: simdgroup reduction   = true
0.00.686.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.274 I ggml_metal_init: has residency sets    = true
0.00.686.275 I ggml_metal_init: has bfloat            = true
0.00.686.275 I ggml_metal_init: use bfloat            = true
0.00.686.276 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.299 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.341 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.335 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.708.337 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.708.337 I llama_init_from_model: graph nodes  = 967
0.00.708.338 I llama_init_from_model: graph splits = 2
0.00.708.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.708.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.642 I 
0.00.745.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.732 I perplexity: tokenizing the input ..
0.00.752.333 I perplexity: tokenization took 6.596 ms
0.00.752.341 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.893 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.901.186 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.901.220 I llama_perf_context_print:        load time =     735.21 ms
0.00.901.222 I llama_perf_context_print: prompt eval time =     146.68 ms /   128 tokens (    1.15 ms per token,   872.68 tokens per second)
0.00.901.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.225 I llama_perf_context_print:       total time =     155.58 ms /   129 tokens
0.00.901.646 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.077s
sys	0m0.191s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4677 (19d3c829) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.012 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.244 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.252 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.163 I llama_model_loader: - type  f32:  194 tensors
0.00.051.163 I llama_model_loader: - type  f16:   98 tensors
0.00.051.163 I print_info: file format = GGUF V3 (latest)
0.00.051.164 I print_info: file type   = all F32 (guessed)
0.00.051.165 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.776 I load: special tokens cache size = 25
0.00.070.393 I load: token to piece cache size = 0.2984 MB
0.00.070.396 I print_info: arch             = gptneox
0.00.070.396 I print_info: vocab_only       = 0
0.00.070.397 I print_info: n_ctx_train      = 2048
0.00.070.397 I print_info: n_embd           = 2048
0.00.070.397 I print_info: n_layer          = 24
0.00.070.400 I print_info: n_head           = 16
0.00.070.401 I print_info: n_head_kv        = 16
0.00.070.401 I print_info: n_rot            = 32
0.00.070.401 I print_info: n_swa            = 0
0.00.070.401 I print_info: n_embd_head_k    = 128
0.00.070.402 I print_info: n_embd_head_v    = 128
0.00.070.403 I print_info: n_gqa            = 1
0.00.070.404 I print_info: n_embd_k_gqa     = 2048
0.00.070.405 I print_info: n_embd_v_gqa     = 2048
0.00.070.406 I print_info: f_norm_eps       = 1.0e-05
0.00.070.406 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.406 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.406 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.406 I print_info: f_logit_scale    = 0.0e+00
0.00.070.407 I print_info: n_ff             = 8192
0.00.070.407 I print_info: n_expert         = 0
0.00.070.407 I print_info: n_expert_used    = 0
0.00.070.408 I print_info: causal attn      = 1
0.00.070.408 I print_info: pooling type     = 0
0.00.070.408 I print_info: rope type        = 2
0.00.070.408 I print_info: rope scaling     = linear
0.00.070.409 I print_info: freq_base_train  = 10000.0
0.00.070.409 I print_info: freq_scale_train = 1
0.00.070.409 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.409 I print_info: rope_finetuned   = unknown
0.00.070.411 I print_info: ssm_d_conv       = 0
0.00.070.411 I print_info: ssm_d_inner      = 0
0.00.070.411 I print_info: ssm_d_state      = 0
0.00.070.412 I print_info: ssm_dt_rank      = 0
0.00.070.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.412 I print_info: model type       = 1.4B
0.00.070.412 I print_info: model params     = 1.41 B
0.00.070.412 I print_info: general.name     = 1.4B
0.00.070.413 I print_info: vocab type       = BPE
0.00.070.413 I print_info: n_vocab          = 50304
0.00.070.413 I print_info: n_merges         = 50009
0.00.070.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.414 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.414 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.414 I print_info: LF token         = 187 'Ċ'
0.00.070.415 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.415 I print_info: max token length = 1024
0.00.070.419 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.443.871 I load_tensors: offloading 24 repeating layers to GPU
0.01.443.878 I load_tensors: offloading output layer to GPU
0.01.443.879 I load_tensors: offloaded 25/25 layers to GPU
0.01.443.904 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.443.905 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.444.638 I llama_init_from_model: n_seq_max     = 1
0.01.444.639 I llama_init_from_model: n_ctx         = 128
0.01.444.639 I llama_init_from_model: n_ctx_per_seq = 128
0.01.444.639 I llama_init_from_model: n_batch       = 128
0.01.444.640 I llama_init_from_model: n_ubatch      = 128
0.01.444.640 I llama_init_from_model: flash_attn    = 0
0.01.444.640 I llama_init_from_model: freq_base     = 10000.0
0.01.444.640 I llama_init_from_model: freq_scale    = 1
0.01.444.641 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.444.642 I ggml_metal_init: allocating
0.01.444.675 I ggml_metal_init: found device: Apple M4
0.01.444.679 I ggml_metal_init: picking default device: Apple M4
0.01.445.602 I ggml_metal_init: using embedded metal library
0.01.448.878 I ggml_metal_init: GPU name:   Apple M4
0.01.448.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.448.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.448.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.448.881 I ggml_metal_init: simdgroup reduction   = true
0.01.448.881 I ggml_metal_init: simdgroup matrix mul. = true
0.01.448.881 I ggml_metal_init: has residency sets    = true
0.01.448.881 I ggml_metal_init: has bfloat            = true
0.01.448.881 I ggml_metal_init: use bfloat            = true
0.01.448.882 I ggml_metal_init: hasUnifiedMemory      = true
0.01.448.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.458.582 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.460.173 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.460.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.460.192 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.566 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.567 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.567 I llama_init_from_model: graph nodes  = 967
0.01.461.568 I llama_init_from_model: graph splits = 2
0.01.461.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.569 I 
0.01.461.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.461.606 I compute_imatrix: tokenizing the input ..
0.01.465.058 I compute_imatrix: tokenization took 3.452 ms
0.01.465.060 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.727.548 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.730.699 I llama_perf_context_print:        load time =    1708.54 ms
0.01.730.700 I llama_perf_context_print: prompt eval time =     261.35 ms /   128 tokens (    2.04 ms per token,   489.76 tokens per second)
0.01.730.701 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.730.701 I llama_perf_context_print:       total time =    1711.68 ms /   129 tokens
0.01.731.250 I ggml_metal_free: deallocating

real	0m1.925s
user	0m0.126s
sys	0m0.349s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4677 (19d3c829)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15710b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15710be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15710c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15710c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15710cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15710d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15710da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15710e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15710e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15710eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15710eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15710f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157110010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1571107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157110fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1571116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157111e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157112530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157112c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157113420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157113b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157114260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157114980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157115220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157115940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157115c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157116210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157116e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1571173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157117680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157117b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157117de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157118670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157118bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157118e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157119310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1571197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157119c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15711a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15711a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15711aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15711aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15711b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15711b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15711bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15711c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15711c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15711d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15711d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15711dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15711e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15711e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15711ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15711f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15711fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157120100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1571205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157120860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157120e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157121660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157121920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157121dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157122260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157122700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157122ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157123040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1571234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157123980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157123e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1571242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157124760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157124c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1571250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1571255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157125b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157126090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1571265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157126b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157127080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1571275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157127b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157128070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1571285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157128b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157129060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1571295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157129b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15712a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15712a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15712aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15712b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15712b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15712bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15712c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15712c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15712cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15712d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15711cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15712d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15712dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15712e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15712e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15712ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15712f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15712f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15712fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157130170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1571306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157130c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157131160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1571316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157131c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157132150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1571325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157132a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157132f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1571333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157133870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157133d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1571341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157134650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157134af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157134f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157135430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1571358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157135d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157136210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1571366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157136b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157136ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157137490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157137930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157137dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157138270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157138710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157138bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157139050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1571394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157139990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157139e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15713a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15713a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15713ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15713b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15713b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15713b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15713be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15713c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15713c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15713cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15713d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15713d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15713da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15713def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15713e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15713e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15713ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15713f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15713f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15713fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15713ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1571403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157140890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157140d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1571411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157141670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157141b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157141fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157142450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1571428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157142d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157143230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1571436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157143b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157144010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1571444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157144950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157144df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157145290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157145730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157145bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157146070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157146510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1571469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157146e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1571472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157147790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157147c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1571480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157148570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157148a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157148eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157149350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1571498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157149df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15714a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15714a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15714ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15714b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15714b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15714bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15714c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15714ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15714ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15714d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15714d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15714e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15714e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15714ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15714eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15714f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15714fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157150110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157150660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157150bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157151100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157151650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157151ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1571520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157152640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157152b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1571530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157153630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157153b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1571540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157154620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157154b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1571550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157155610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157155b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1571560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157156600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157156b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1571570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1571575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157157b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157158090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1571585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157158b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157159080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1571595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157159b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15715a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15715a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15715ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15715b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15715b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15715bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15715c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15715c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15715caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15715d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15715d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15715dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15715e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15715e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15715ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15715f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15715f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15715fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157160010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157160560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157160ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157161000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157161550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157161aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157161ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157162490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157162930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157162dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157163270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157163710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157163bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157164050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1571644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157164990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157164e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1571652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157165770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157165c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1571660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157166550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157166aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1571671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1571678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157168000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157168720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1571689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1571691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157169490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157169aa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.747.862 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157169750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15714b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15714ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15714ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15711eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15711e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157120b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15714d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157115ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15711c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15711d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15711d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15711bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15711def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157114ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157121130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15712d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157168ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1571180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157118360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15714dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15714c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1571164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157116790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157116a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157169f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15716a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15716a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15716a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15716aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15716acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15716af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15716b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15716b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15716b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15716ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15716bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15716c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15716c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15716c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15716c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15716cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15716cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15716d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15716d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15716d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15716d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15716db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15716de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15716e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15716e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15716e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15716e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15716ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15716eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15716f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15716f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15716f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15716f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15716fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15716ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157170200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1571704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157170780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157170a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157170d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157170fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157171280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157171540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157171800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157171ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157171d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157172040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157172300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1571725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157172880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157172b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157172e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1571730c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157173380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157173640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157173900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157173bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157173e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157174140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157174400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1571746c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157174980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157174c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157174f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1571751c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157175480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157175740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157175a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157175cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157175f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157176240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157176500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1571767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157176a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157176d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157177000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1571772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157177580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157177840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157177b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157177dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157178080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157178340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157178600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1571788c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157178b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157178e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157179100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1571793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157179680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157179940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157179c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157179ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15717a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15717a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15717a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15717a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15717ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15717af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15717b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15717b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15717b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15717ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15717bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15717bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15717c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15717c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15717c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15717cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15717cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15717d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15717d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15717d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15717d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15717db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15717de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15717e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15717e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15717e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15717e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15717ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15717ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15717f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15717f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15717f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15717f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15717fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15717ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1571801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157180480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157180740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157180a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157180cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157180f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157181240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157181500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1571817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157181a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157181d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157182000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1571822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157182580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157182840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157182b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157182dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157183080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157183340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157183600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1571838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157183b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157183e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157184100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1571843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157184680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157184940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157184c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157184ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157185180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157185440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157185700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1571859c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157185c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157185f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157186200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1571864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157186780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157186a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157186d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157186fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157187280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157187540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157187800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157187ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157187d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157188040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157188300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1571885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157188880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157188dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157189300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1571895c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157189a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157189f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15718a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15718ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15718ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15718b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15718b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15718b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15718be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15718c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15718c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15718cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15718cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15718d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15718d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15718dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15718e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15718e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15718ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15718eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15718f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15718f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15718fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1571900b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157190520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157190990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157190e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157191270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1571916e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157191b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157191fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157192430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1571928a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157192d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157193180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1571935f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157193a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157193ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157194340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1571947b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157194c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157195090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157195500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157195970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157195de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157196250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1571966c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157196b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157196fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157197410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157197880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157197cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157198160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1571985d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157198a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157198eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157199320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157199790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157199c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15719a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15719a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15719a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15719adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15719b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15719b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15719bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15719bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15719c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15719c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15719ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15719d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15719d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15719da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15719de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15719e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15719e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15719f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15719f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1571a0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1571a0740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1571a0a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1571a11f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1571a14b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1571a1ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157204c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157205070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1572054e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157205950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157205dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157206230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1572066a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157206b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157206f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1572073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157207860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157207f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157208a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157209210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157209a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15720a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15720a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15720af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15720b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15720be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15720c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15720ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15720d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15720daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15720e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15720e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15720e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15720ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15720f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15720f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15720f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15720fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1572102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1572105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157210a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157210e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157211300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157211770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157211be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157212050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1572124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157212930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157212da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157213210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157213680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157213af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157213f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1572143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157214840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157214cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157215120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157215590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157215a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157215e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1572162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157216750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157216cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1572171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157217630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157217aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157217f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157218380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1572187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157218c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1572190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157219540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1572199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157219e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15721a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15721a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15721ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15721afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15721b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15721b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15721bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15721c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15721c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15721ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15721cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15721d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15721d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15721dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15721e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15721e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15721e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15721ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15721f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15721f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15721fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15721ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157220430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1572208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157220d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157221180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1572215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157221a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157221ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157222340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1572227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157222c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157223090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157223500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157223970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157224200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1572244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157224930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157224da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157225210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157225680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157225af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157225f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1572263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157226840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157226cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157227120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157227590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157227a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157227e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1572282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157228750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157228bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157229030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1572294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157229910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157229d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15722a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15722a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15722aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15722af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15722b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15722b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15722bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15722c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15722c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15722c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15722ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15722d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15722d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15722dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15722e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15722e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15722e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15722ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15722f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15722f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15722fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15722ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157230390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157230800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157230c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1572310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157231550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1572319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157231e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1572322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157232710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157232b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157232ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157233460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1572338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157233d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1572341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157234620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157234a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157234f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157235370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1572357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157235c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1572360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157236530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1572369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157236e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157237280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1572376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157237b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157237fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157238440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1572388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157238d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157239190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157239600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157239a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157239ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15723a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15723a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15723ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15723b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15723b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15723b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15723bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15723c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15723c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15723cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15723cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15723d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15723d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15723dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15723e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15723e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15723ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15723eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15723f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15723f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15723fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157240080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1572404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157240960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157240dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157241240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1572416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157242230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1572424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1572427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157242c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157243090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157243500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157243970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157243de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157244250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1572446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157244b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157244fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157245410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157245880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157245cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157246160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1572465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157246a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157246eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157247320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157247790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157247c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157248070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1572484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157248950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157248dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157249230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1572496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157249b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157249f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15724a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15724a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15724acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15724b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15724b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15724ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15724be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15724c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15724c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15724cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15724d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15724d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15724d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15724dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15724e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15724e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15724eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15724ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15724f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15724f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15724fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157250120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157250590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157250a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157250e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1572512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157251750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157251bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157252030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1572524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157252910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157252d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1572531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157253660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157253ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157253f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1572543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157254820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157254c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157255100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157255570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1572559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157255e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1572568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157256fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157257700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157257e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1572580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157258550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157258b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157259160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.779s
user	0m0.277s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4677 (19d3c829)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b907850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b907f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b908510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b908ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b909070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b909620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b909bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b90a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b90a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b90ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b90b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b90b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b90c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b90c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b90d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b90d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b90df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b90e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b90ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b90f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b90fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b9103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b910ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b911360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b911a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b911d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b912350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b912fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b913500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b9137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b913c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b913f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b9147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b914cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b914fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b915450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b9158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b915d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b916230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b9166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b916b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b917010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b9174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b917950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b917c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b918220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b918830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b919150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b919760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b919d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b91a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b91a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b91afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b91b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b91bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b91c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b91c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b91c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b91cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b91d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b91da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b91df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b91e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b91e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b91ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b91f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b91f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b91fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b91ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b920400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b9208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b920d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b9211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b921730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b921c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b9221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b922720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b922c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b9231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b923710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b923c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b9241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b924700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b924c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b9251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b9256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b925c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b926190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b9266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b926c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b927180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b9276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b927c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b928170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b9286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b928c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b929160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b918e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b9295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b929d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b92a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b92a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b92ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b92b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b92b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b92bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b92c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b92c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b92cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b92d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b92d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b92dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b92e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b92e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b92ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b92f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b92f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b92f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b92fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b9302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b930790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b930c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b9310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b931570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b931a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b931eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b932350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b9327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b932c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b933130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b9335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b933a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b933f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b9343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b934850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b934cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b935190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b935630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b935ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b935f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b936410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b9368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b936d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b9371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b937690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b937b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b937fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b938470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b938910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b938db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b939250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b9396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b939b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b93a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b93a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b93a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b93ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b93b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b93b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b93bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b93c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b93c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b93c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b93ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b93d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b93d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b93dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b93e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b93e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b93ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b93eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b93f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b93f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b93fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b940150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b9405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b940a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b940f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b9413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b941870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b941d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b9421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b942650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b942af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b942f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b943430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b9438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b943d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b944210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b9446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b944b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b944ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b945490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b9459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b945f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b946480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b9469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b946c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b9472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b9478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b947ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b9486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b948b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b948e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b949420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b949a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b94a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b94a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b94ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b94b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b94b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b94bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b94c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b94c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b94ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b94d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b94d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b94dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b94e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b94e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b94ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b94f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b94f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b94fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b950210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b950760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b950cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b951200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b951750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b951ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b9521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b952740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b952c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b9531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b953730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b953c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b9541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b954720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b954c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b9551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b955710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b955c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b9561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b956700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b956c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b9571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b9576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b957c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b958190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b9586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b958c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b959180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b9596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b959c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b95a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b95a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b95ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b95b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b95b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b95bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b95c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b95c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b95cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b95d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b95d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b95dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b95e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b95e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b95ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b95ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b95f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b95f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b95fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b960190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b960630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b960ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b960f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b961410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b9618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b961d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b9621f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b962690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b962be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b963300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b963a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b964140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b964860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b964b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b965310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b9655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b965be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.095.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a708880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a708cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a709160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a70b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a70b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a70be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a70c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a70c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a70cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a70d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a70dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a70e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a70ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a70f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a70fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a7104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a710c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a711320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a711a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a712170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a712890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a712fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a7136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a713df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a7140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a714370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a7147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a714c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a7150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a715a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a715ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a716190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a716600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a716a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a716ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a717350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a7177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a717c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a7180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a718510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a718df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a719260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a7196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a719b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a719fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a71a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a71a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a71ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a71b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a71b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a71ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a71bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a71c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a71c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a71cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a71d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a71d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a71daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a71df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a71e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a71e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a71ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a71f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a71f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a71fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a71fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a7202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a720750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a720bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a721030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a7214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a721910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a721d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a7221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a722660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a722ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a722f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a7233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a723c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a724100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a724570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a7249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a724e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a7252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a725730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a725ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a726010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a726480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a7268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a726d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a7271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a727640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a727ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a727f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a728390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a728800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a728c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a7290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a729550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a7299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a729e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a72a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a72a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a72ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a72aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a72b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a72b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a72bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a72c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a72c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a72ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a72cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a72d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a72d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a72dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a72e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a72e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a72e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a72ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a72f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a72f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a72fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a72ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a730440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a7308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a730d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a731190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a731600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a731a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a731ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a732350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a7327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a732c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a7330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a733510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a733980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a733df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a734260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a7346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a734b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a734fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a735420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a735890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a735d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a736170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a7365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a736a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a736ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a737330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a7377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a737c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a738080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a7384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a738960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a739240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a7396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a739b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a739f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a73a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a73a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a73ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a73b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a73bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a73be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a73c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a73c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a73cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a73d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a73d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a73d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a73dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a73e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a73e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a73eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a73ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a73f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a73f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a73fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a740120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a740590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a740a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a740e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a7412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a741750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a741bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a742030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a7424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a742d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a7431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a743660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a743ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a7443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a744820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a745100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a745660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a745b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a745fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a746450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a7468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a746d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a747250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a7482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a748590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a748b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a7496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a749c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a74a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a74a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a74add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a74b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a74b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a74bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a74c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a74ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a74d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a74d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a74dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a74e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a74e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a74ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a74f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a74f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a74fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a750410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a7509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a750f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a751550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a751b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a7520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a752690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a752c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a753210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a7537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a753d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a754350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a754910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a755490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a755a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a756010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a7565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a757150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a757710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a757cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a758290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a758e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a7593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a759990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a759f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a75a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a75aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a75b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a75b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a75bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a75c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a75c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a75cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a75d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a75d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a75db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a75e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a75e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a75ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a75ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a75f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a75f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a75fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a760390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a760890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a760d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a761290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a761ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a7623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a762ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a763200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a7634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a763cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a763f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a764580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a6087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a608c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a6090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a609e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a60a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a60a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a60ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a60b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a60b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a60bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a60bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a60c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a60d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a60d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a60dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a60e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a60ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a60f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a60fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a6105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a610d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a611430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a611b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a612270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a612530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a612b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a613150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a6143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a6146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a614f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a615480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a615740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a616520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a6169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a616e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a6177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a617c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a6180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a6183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a6189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a6195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a619be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a61a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a61a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a61ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a61b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a61ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a61c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a61c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a61cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a61ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a61d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a61dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a61e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a61e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a61ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a61eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a61f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a61f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a61fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a620120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a6205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a620a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a6213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a621840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a621d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a6222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a622830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a622d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a6232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a6242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a624810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a6252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a625800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a6262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a6267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a6277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a628280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a6287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a6297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a62a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a62a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a62ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a62b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a62b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a62bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a62c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a62c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a62cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a62d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a62dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a62e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a62e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a62ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a62f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a62f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a62faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a62ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a6303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a630880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a630d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a6311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a631660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a631b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a631fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a632440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a6328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a632d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a633220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a6336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a633b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a6344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a636060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a636500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a6369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a636e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a6372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a6380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a638560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a6397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a63a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a63a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a63aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a63af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a63b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a63b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a63bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a63c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a63c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a63cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a63cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a63d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a63d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a63dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a63e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a63e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a63eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a63efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a63f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a63f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a63fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a640240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a6406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a640b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a641020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a6414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a641960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a6422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a642740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a6439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a644300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a6447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a644c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a6450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a646960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a647400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a6476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a647cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a6482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a6488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a6490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a649580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a649840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a64a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a64ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a64b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a64b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a64ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a64c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a64c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a64cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a64d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a64d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a64dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a64e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a64e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a64ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a64f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a64f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a64fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a6501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a6506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a650c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a651190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a6516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a651c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a652180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a6526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a652c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a653170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a6536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a653c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a654160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a6546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a654c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a655150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a6556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a655bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a656140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a656690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a656be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a657130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a657680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a657bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a658120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a658670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a658bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a659110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a659660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a659bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a65a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a65a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a65aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a65b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a65b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a65bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a65c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a65c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a65cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a65d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a65d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a65db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a65e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a65e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a65eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a65f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a65f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a65f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a65fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a660280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a660720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a660bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a661060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a661500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a6619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a661e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a6622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a662780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a662c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a6630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a663610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a663d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a664450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a664b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a665290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a665550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a666000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a666610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.910s
user	0m0.228s
sys	0m0.145s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
