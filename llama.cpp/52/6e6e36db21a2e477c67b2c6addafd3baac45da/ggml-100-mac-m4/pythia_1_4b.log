Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:303 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.656s
user	0m0.709s
sys	0m0.978s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Built target llama-gguf
[ 23%] Built target llama-gguf-hash
[ 23%] Built target llama
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Linking CXX executable ../../bin/llama-run
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Built target llava
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-run
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llama-simple-chat
[ 33%] Built target test-c
[ 33%] Linking CXX static library libllava_static.a
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-simple
[ 34%] Built target common
[ 34%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 38%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-sampling
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-log
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Built target test-chat-template
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-arg-parser
[ 50%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Built target test-llama-grammar
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 60%] Built target test-grammar-integration
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-backend-ops
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-autorelease
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-quantize-fns
[ 65%] Built target test-barrier
[ 65%] Built target test-rope
[ 65%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target test-model-load-cancel
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target llama-batched-bench
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target llama-batched
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target test-json-schema-to-grammar
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Built target llama-gritlm
[ 76%] Built target llama-gguf-split
[ 76%] Built target llama-gbnf-validator
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-eval-callback
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Built target llama-infill
[ 76%] Built target llama-bench
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating index.html.hpp
[ 85%] Linking CXX executable ../../bin/llama-perplexity
[ 86%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-lookup-stats
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 87%] Built target llama-parallel
[ 87%] Built target llama-cli
[ 87%] Built target llama-passkey
[ 87%] Built target llama-lookup-merge
[ 87%] Built target llama-quantize
[ 87%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 94%] Linking CXX executable ../../bin/llama-speculative-simple
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-save-load-state
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-cvector-generator
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-speculative
[ 95%] Built target llama-tts
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 98%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-llava-cli
[100%] Built target llama-minicpmv-cli
[100%] Built target llama-vdot
[100%] Built target llama-q8dot
[100%] Built target llama-server

real	0m2.401s
user	0m5.169s
sys	0m8.392s

main: quantize time =  4941.22 ms
main:    total time =  4941.22 ms

main: quantize time =  1371.74 ms
main:    total time =  1371.74 ms

main: quantize time =  1308.22 ms
main:    total time =  1308.22 ms

main: quantize time =  1510.10 ms
main:    total time =  1510.10 ms

main: quantize time =  1685.42 ms
main:    total time =  1685.42 ms

main: quantize time =  5012.27 ms
main:    total time =  5012.27 ms

main: quantize time =  5706.91 ms
main:    total time =  5706.91 ms

main: quantize time =  6838.88 ms
main:    total time =  6838.88 ms

main: quantize time =  5945.67 ms
main:    total time =  5945.67 ms

main: quantize time =  4513.18 ms
main:    total time =  4513.18 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.226 I main: llama backend init
0.00.000.232 I main: load the model and apply lora adapter, if any
0.00.046.260 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.057.420 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.451 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.452 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.456 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.457 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.457 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.027 I llama_model_loader: - type  f32:  194 tensors
0.00.076.028 I llama_model_loader: - type  f16:   98 tensors
0.00.110.311 I llm_load_vocab: special tokens cache size = 25
0.00.117.483 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.117.486 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.117.486 I llm_load_print_meta: arch             = gptneox
0.00.117.487 I llm_load_print_meta: vocab type       = BPE
0.00.117.487 I llm_load_print_meta: n_vocab          = 50304
0.00.117.487 I llm_load_print_meta: n_merges         = 50009
0.00.117.487 I llm_load_print_meta: vocab_only       = 0
0.00.117.487 I llm_load_print_meta: n_ctx_train      = 2048
0.00.117.487 I llm_load_print_meta: n_embd           = 2048
0.00.117.488 I llm_load_print_meta: n_layer          = 24
0.00.117.490 I llm_load_print_meta: n_head           = 16
0.00.117.491 I llm_load_print_meta: n_head_kv        = 16
0.00.117.491 I llm_load_print_meta: n_rot            = 32
0.00.117.491 I llm_load_print_meta: n_swa            = 0
0.00.117.492 I llm_load_print_meta: n_embd_head_k    = 128
0.00.117.492 I llm_load_print_meta: n_embd_head_v    = 128
0.00.117.492 I llm_load_print_meta: n_gqa            = 1
0.00.117.493 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.117.494 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.117.494 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.117.495 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.117.495 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.117.495 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.117.495 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.117.496 I llm_load_print_meta: n_ff             = 8192
0.00.117.496 I llm_load_print_meta: n_expert         = 0
0.00.117.496 I llm_load_print_meta: n_expert_used    = 0
0.00.117.496 I llm_load_print_meta: causal attn      = 1
0.00.117.497 I llm_load_print_meta: pooling type     = 0
0.00.117.497 I llm_load_print_meta: rope type        = 2
0.00.117.506 I llm_load_print_meta: rope scaling     = linear
0.00.117.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.117.508 I llm_load_print_meta: freq_scale_train = 1
0.00.117.508 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.117.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.117.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.117.509 I llm_load_print_meta: ssm_d_inner      = 0
0.00.117.509 I llm_load_print_meta: ssm_d_state      = 0
0.00.117.509 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.117.510 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.117.523 I llm_load_print_meta: model type       = 1.4B
0.00.117.524 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.117.524 I llm_load_print_meta: model params     = 1.41 B
0.00.117.525 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.117.525 I llm_load_print_meta: general.name     = 1.4B
0.00.117.525 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.117.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.117.526 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.117.526 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.117.526 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.117.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.117.527 I llm_load_print_meta: max token length = 1024
0.00.120.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.120.207 I llm_load_tensors: offloading output layer to GPU
0.00.120.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.120.225 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.120.226 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.121.193 I llama_new_context_with_model: n_seq_max     = 1
0.00.121.194 I llama_new_context_with_model: n_ctx         = 2048
0.00.121.195 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.121.195 I llama_new_context_with_model: n_batch       = 2048
0.00.121.195 I llama_new_context_with_model: n_ubatch      = 512
0.00.121.195 I llama_new_context_with_model: flash_attn    = 0
0.00.121.196 I llama_new_context_with_model: freq_base     = 10000.0
0.00.121.196 I llama_new_context_with_model: freq_scale    = 1
0.00.121.196 I ggml_metal_init: allocating
0.00.121.200 I ggml_metal_init: found device: Apple M4
0.00.121.202 I ggml_metal_init: picking default device: Apple M4
0.00.121.890 I ggml_metal_init: using embedded metal library
0.00.132.572 I ggml_metal_init: GPU name:   Apple M4
0.00.132.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.132.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.132.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.132.575 I ggml_metal_init: simdgroup reduction   = true
0.00.132.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.132.575 I ggml_metal_init: has bfloat            = true
0.00.132.576 I ggml_metal_init: use bfloat            = true
0.00.132.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.132.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.156.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.178.252 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.178.258 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.178.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.179.270 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.179.272 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.179.273 I llama_new_context_with_model: graph nodes  = 967
0.00.179.273 I llama_new_context_with_model: graph splits = 2
0.00.179.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.258.199 I main: llama threadpool init, n_threads = 4
0.00.258.232 I 
0.00.258.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.258.271 I 
0.00.258.343 I sampler seed: 1234
0.00.258.348 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.258.382 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.258.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.258.383 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.115.306 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.115.307 I llama_perf_context_print:        load time =     211.93 ms
0.02.115.308 I llama_perf_context_print: prompt eval time =      43.90 ms /     7 tokens (    6.27 ms per token,   159.45 tokens per second)
0.02.115.309 I llama_perf_context_print:        eval time =    1810.08 ms /    63 runs   (   28.73 ms per token,    34.81 tokens per second)
0.02.115.309 I llama_perf_context_print:       total time =    1857.11 ms /    70 tokens
0.02.115.495 I ggml_metal_free: deallocating

real	0m2.432s
user	0m0.147s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.083 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.034.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.089 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.089 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.090 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.090 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.091 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.091 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.091 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.092 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.092 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.095 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.123 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.253 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.254 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.043.254 I llama_model_loader: - type  f32:  194 tensors
0.00.043.255 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.811 I llm_load_vocab: special tokens cache size = 25
0.00.077.775 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.780 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.780 I llm_load_print_meta: arch             = gptneox
0.00.077.781 I llm_load_print_meta: vocab type       = BPE
0.00.077.781 I llm_load_print_meta: n_vocab          = 50304
0.00.077.782 I llm_load_print_meta: n_merges         = 50009
0.00.077.782 I llm_load_print_meta: vocab_only       = 0
0.00.077.782 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.782 I llm_load_print_meta: n_embd           = 2048
0.00.077.784 I llm_load_print_meta: n_layer          = 24
0.00.077.789 I llm_load_print_meta: n_head           = 16
0.00.077.790 I llm_load_print_meta: n_head_kv        = 16
0.00.077.790 I llm_load_print_meta: n_rot            = 32
0.00.077.790 I llm_load_print_meta: n_swa            = 0
0.00.077.790 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.790 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.791 I llm_load_print_meta: n_gqa            = 1
0.00.077.792 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.793 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.794 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.794 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.794 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.795 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.795 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.796 I llm_load_print_meta: n_ff             = 8192
0.00.077.796 I llm_load_print_meta: n_expert         = 0
0.00.077.796 I llm_load_print_meta: n_expert_used    = 0
0.00.077.797 I llm_load_print_meta: causal attn      = 1
0.00.077.797 I llm_load_print_meta: pooling type     = 0
0.00.077.797 I llm_load_print_meta: rope type        = 2
0.00.077.797 I llm_load_print_meta: rope scaling     = linear
0.00.077.798 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.798 I llm_load_print_meta: freq_scale_train = 1
0.00.077.798 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.799 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.799 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.799 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.799 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.799 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.800 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.814 I llm_load_print_meta: model type       = 1.4B
0.00.077.814 I llm_load_print_meta: model ftype      = Q8_0
0.00.077.815 I llm_load_print_meta: model params     = 1.41 B
0.00.077.815 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.077.815 I llm_load_print_meta: general.name     = 1.4B
0.00.077.816 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.816 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.816 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.817 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.817 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.077.817 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.818 I llm_load_print_meta: max token length = 1024
0.00.080.853 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.853 I llm_load_tensors: offloading output layer to GPU
0.00.080.854 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.865 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.080.867 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.082.223 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.224 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.225 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.225 I llama_new_context_with_model: n_batch       = 2048
0.00.082.225 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.226 I llama_new_context_with_model: flash_attn    = 0
0.00.082.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.227 I llama_new_context_with_model: freq_scale    = 1
0.00.082.227 I ggml_metal_init: allocating
0.00.082.237 I ggml_metal_init: found device: Apple M4
0.00.082.240 I ggml_metal_init: picking default device: Apple M4
0.00.083.179 I ggml_metal_init: using embedded metal library
0.00.087.090 I ggml_metal_init: GPU name:   Apple M4
0.00.087.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.094 I ggml_metal_init: simdgroup reduction   = true
0.00.087.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.094 I ggml_metal_init: has bfloat            = true
0.00.087.094 I ggml_metal_init: use bfloat            = true
0.00.087.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.957 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.124.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.588 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.684 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.686 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.686 I llama_new_context_with_model: graph nodes  = 967
0.00.125.687 I llama_new_context_with_model: graph splits = 2
0.00.125.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.326.041 I main: llama threadpool init, n_threads = 4
0.01.326.079 I 
0.01.326.120 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.326.121 I 
0.01.326.385 I sampler seed: 1234
0.01.326.389 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.326.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.326.451 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.326.451 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.419.838 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.02.419.839 I llama_perf_context_print:        load time =    1316.23 ms
0.02.419.840 I llama_perf_context_print: prompt eval time =      47.97 ms /     7 tokens (    6.85 ms per token,   145.92 tokens per second)
0.02.419.841 I llama_perf_context_print:        eval time =    1042.43 ms /    63 runs   (   16.55 ms per token,    60.44 tokens per second)
0.02.419.841 I llama_perf_context_print:       total time =    1093.80 ms /    70 tokens
0.02.420.026 I ggml_metal_free: deallocating

real	0m2.439s
user	0m0.126s
sys	0m0.243s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.016.604 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.326 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.327 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.327 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.328 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.330 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.330 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.055.107 I llama_model_loader: - type  f32:  194 tensors
0.00.055.107 I llama_model_loader: - type q4_0:   97 tensors
0.00.055.108 I llama_model_loader: - type q6_K:    1 tensors
0.00.098.169 I llm_load_vocab: special tokens cache size = 25
0.00.106.558 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.106.561 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.106.562 I llm_load_print_meta: arch             = gptneox
0.00.106.562 I llm_load_print_meta: vocab type       = BPE
0.00.106.562 I llm_load_print_meta: n_vocab          = 50304
0.00.106.563 I llm_load_print_meta: n_merges         = 50009
0.00.106.563 I llm_load_print_meta: vocab_only       = 0
0.00.106.563 I llm_load_print_meta: n_ctx_train      = 2048
0.00.106.563 I llm_load_print_meta: n_embd           = 2048
0.00.106.563 I llm_load_print_meta: n_layer          = 24
0.00.106.567 I llm_load_print_meta: n_head           = 16
0.00.106.568 I llm_load_print_meta: n_head_kv        = 16
0.00.106.568 I llm_load_print_meta: n_rot            = 32
0.00.106.568 I llm_load_print_meta: n_swa            = 0
0.00.106.568 I llm_load_print_meta: n_embd_head_k    = 128
0.00.106.568 I llm_load_print_meta: n_embd_head_v    = 128
0.00.106.569 I llm_load_print_meta: n_gqa            = 1
0.00.106.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.106.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.106.571 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.106.572 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.106.572 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.106.572 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.106.572 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.106.573 I llm_load_print_meta: n_ff             = 8192
0.00.106.573 I llm_load_print_meta: n_expert         = 0
0.00.106.573 I llm_load_print_meta: n_expert_used    = 0
0.00.106.574 I llm_load_print_meta: causal attn      = 1
0.00.106.574 I llm_load_print_meta: pooling type     = 0
0.00.106.574 I llm_load_print_meta: rope type        = 2
0.00.106.574 I llm_load_print_meta: rope scaling     = linear
0.00.106.574 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.106.583 I llm_load_print_meta: freq_scale_train = 1
0.00.106.585 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.106.585 I llm_load_print_meta: rope_finetuned   = unknown
0.00.106.585 I llm_load_print_meta: ssm_d_conv       = 0
0.00.106.586 I llm_load_print_meta: ssm_d_inner      = 0
0.00.106.586 I llm_load_print_meta: ssm_d_state      = 0
0.00.106.586 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.106.586 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.106.599 I llm_load_print_meta: model type       = 1.4B
0.00.106.600 I llm_load_print_meta: model ftype      = Q4_0
0.00.106.600 I llm_load_print_meta: model params     = 1.41 B
0.00.106.601 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.106.601 I llm_load_print_meta: general.name     = 1.4B
0.00.106.601 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.106.602 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.106.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.106.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.106.603 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.106.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.106.604 I llm_load_print_meta: max token length = 1024
0.00.108.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.108.827 I llm_load_tensors: offloading output layer to GPU
0.00.108.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.108.838 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.108.839 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.109.923 I llama_new_context_with_model: n_seq_max     = 1
0.00.109.924 I llama_new_context_with_model: n_ctx         = 2048
0.00.109.924 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.109.924 I llama_new_context_with_model: n_batch       = 2048
0.00.109.925 I llama_new_context_with_model: n_ubatch      = 512
0.00.109.925 I llama_new_context_with_model: flash_attn    = 0
0.00.109.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.109.926 I llama_new_context_with_model: freq_scale    = 1
0.00.109.927 I ggml_metal_init: allocating
0.00.109.935 I ggml_metal_init: found device: Apple M4
0.00.109.938 I ggml_metal_init: picking default device: Apple M4
0.00.110.783 I ggml_metal_init: using embedded metal library
0.00.114.007 I ggml_metal_init: GPU name:   Apple M4
0.00.114.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.114.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.114.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.114.011 I ggml_metal_init: simdgroup reduction   = true
0.00.114.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.114.011 I ggml_metal_init: has bfloat            = true
0.00.114.012 I ggml_metal_init: use bfloat            = true
0.00.114.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.114.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.685 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.148.368 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.148.375 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.148.399 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.149.483 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.149.484 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.149.485 I llama_new_context_with_model: graph nodes  = 967
0.00.149.485 I llama_new_context_with_model: graph splits = 2
0.00.149.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.081 I main: llama threadpool init, n_threads = 4
0.00.884.165 I 
0.00.884.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.257 I 
0.00.884.800 I sampler seed: 1234
0.00.884.806 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.884.851 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.884.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.884.853 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.579.867 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.579.867 I llama_perf_context_print:        load time =     867.46 ms
0.01.579.869 I llama_perf_context_print: prompt eval time =      50.88 ms /     7 tokens (    7.27 ms per token,   137.57 tokens per second)
0.01.579.869 I llama_perf_context_print:        eval time =     641.27 ms /    63 runs   (   10.18 ms per token,    98.24 tokens per second)
0.01.579.870 I llama_perf_context_print:       total time =     695.79 ms /    70 tokens
0.01.580.063 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.155s
sys	0m0.192s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.014.790 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.631 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.641 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.642 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.609 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.996 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.948 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.949 I llama_model_loader: - type  f32:  194 tensors
0.00.034.949 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.950 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.006 I llm_load_vocab: special tokens cache size = 25
0.00.079.706 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.712 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.715 I llm_load_print_meta: arch             = gptneox
0.00.079.715 I llm_load_print_meta: vocab type       = BPE
0.00.079.716 I llm_load_print_meta: n_vocab          = 50304
0.00.079.716 I llm_load_print_meta: n_merges         = 50009
0.00.079.716 I llm_load_print_meta: vocab_only       = 0
0.00.079.716 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.717 I llm_load_print_meta: n_embd           = 2048
0.00.079.717 I llm_load_print_meta: n_layer          = 24
0.00.079.722 I llm_load_print_meta: n_head           = 16
0.00.079.723 I llm_load_print_meta: n_head_kv        = 16
0.00.079.723 I llm_load_print_meta: n_rot            = 32
0.00.079.723 I llm_load_print_meta: n_swa            = 0
0.00.079.724 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.724 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.725 I llm_load_print_meta: n_gqa            = 1
0.00.079.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.729 I llm_load_print_meta: n_ff             = 8192
0.00.079.729 I llm_load_print_meta: n_expert         = 0
0.00.079.729 I llm_load_print_meta: n_expert_used    = 0
0.00.079.731 I llm_load_print_meta: causal attn      = 1
0.00.079.733 I llm_load_print_meta: pooling type     = 0
0.00.079.733 I llm_load_print_meta: rope type        = 2
0.00.079.733 I llm_load_print_meta: rope scaling     = linear
0.00.079.734 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.734 I llm_load_print_meta: freq_scale_train = 1
0.00.079.734 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.735 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.736 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.738 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.738 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.738 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.750 I llm_load_print_meta: model type       = 1.4B
0.00.079.751 I llm_load_print_meta: model ftype      = Q4_1
0.00.079.751 I llm_load_print_meta: model params     = 1.41 B
0.00.079.752 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.079.752 I llm_load_print_meta: general.name     = 1.4B
0.00.079.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.753 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.754 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.754 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.754 I llm_load_print_meta: max token length = 1024
0.00.082.537 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.537 I llm_load_tensors: offloading output layer to GPU
0.00.082.537 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.549 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.082.550 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.083.922 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.924 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.924 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.924 I llama_new_context_with_model: n_batch       = 2048
0.00.083.925 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.925 I llama_new_context_with_model: flash_attn    = 0
0.00.083.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.926 I llama_new_context_with_model: freq_scale    = 1
0.00.083.927 I ggml_metal_init: allocating
0.00.083.935 I ggml_metal_init: found device: Apple M4
0.00.083.938 I ggml_metal_init: picking default device: Apple M4
0.00.084.810 I ggml_metal_init: using embedded metal library
0.00.088.342 I ggml_metal_init: GPU name:   Apple M4
0.00.088.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.345 I ggml_metal_init: simdgroup reduction   = true
0.00.088.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.345 I ggml_metal_init: has bfloat            = true
0.00.088.347 I ggml_metal_init: use bfloat            = true
0.00.088.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.963 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.122.330 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.357 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.408 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.409 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.410 I llama_new_context_with_model: graph nodes  = 967
0.00.123.410 I llama_new_context_with_model: graph splits = 2
0.00.123.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.297 I main: llama threadpool init, n_threads = 4
0.00.803.377 I 
0.00.803.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.448 I 
0.00.803.830 I sampler seed: 1234
0.00.803.835 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.875 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.877 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.096 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63906.39 tokens per second)
0.01.533.097 I llama_perf_context_print:        load time =     788.50 ms
0.01.533.097 I llama_perf_context_print: prompt eval time =      44.91 ms /     7 tokens (    6.42 ms per token,   155.85 tokens per second)
0.01.533.098 I llama_perf_context_print:        eval time =     681.49 ms /    63 runs   (   10.82 ms per token,    92.44 tokens per second)
0.01.533.099 I llama_perf_context_print:       total time =     729.80 ms /    70 tokens
0.01.533.288 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.140s
sys	0m0.178s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.481 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.064 I llama_model_loader: - type  f32:  194 tensors
0.00.027.064 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.064 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.202 I llm_load_vocab: special tokens cache size = 25
0.00.054.248 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.250 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.251 I llm_load_print_meta: arch             = gptneox
0.00.054.251 I llm_load_print_meta: vocab type       = BPE
0.00.054.251 I llm_load_print_meta: n_vocab          = 50304
0.00.054.251 I llm_load_print_meta: n_merges         = 50009
0.00.054.252 I llm_load_print_meta: vocab_only       = 0
0.00.054.252 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.252 I llm_load_print_meta: n_embd           = 2048
0.00.054.252 I llm_load_print_meta: n_layer          = 24
0.00.054.255 I llm_load_print_meta: n_head           = 16
0.00.054.256 I llm_load_print_meta: n_head_kv        = 16
0.00.054.256 I llm_load_print_meta: n_rot            = 32
0.00.054.256 I llm_load_print_meta: n_swa            = 0
0.00.054.256 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.256 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.259 I llm_load_print_meta: n_gqa            = 1
0.00.054.260 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.262 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.263 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.263 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.263 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.263 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.264 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.264 I llm_load_print_meta: n_ff             = 8192
0.00.054.265 I llm_load_print_meta: n_expert         = 0
0.00.054.265 I llm_load_print_meta: n_expert_used    = 0
0.00.054.265 I llm_load_print_meta: causal attn      = 1
0.00.054.265 I llm_load_print_meta: pooling type     = 0
0.00.054.265 I llm_load_print_meta: rope type        = 2
0.00.054.265 I llm_load_print_meta: rope scaling     = linear
0.00.054.266 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.266 I llm_load_print_meta: freq_scale_train = 1
0.00.054.266 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.267 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.267 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.267 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.267 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.267 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.279 I llm_load_print_meta: model type       = 1.4B
0.00.054.279 I llm_load_print_meta: model ftype      = Q5_0
0.00.054.280 I llm_load_print_meta: model params     = 1.41 B
0.00.054.280 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.054.280 I llm_load_print_meta: general.name     = 1.4B
0.00.054.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.281 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.282 I llm_load_print_meta: max token length = 1024
0.00.056.346 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.346 I llm_load_tensors: offloading output layer to GPU
0.00.056.347 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.358 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.359 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.057.324 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.325 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.325 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.325 I llama_new_context_with_model: n_batch       = 2048
0.00.057.325 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.325 I llama_new_context_with_model: flash_attn    = 0
0.00.057.326 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.326 I llama_new_context_with_model: freq_scale    = 1
0.00.057.327 I ggml_metal_init: allocating
0.00.057.332 I ggml_metal_init: found device: Apple M4
0.00.057.334 I ggml_metal_init: picking default device: Apple M4
0.00.057.901 I ggml_metal_init: using embedded metal library
0.00.060.248 I ggml_metal_init: GPU name:   Apple M4
0.00.060.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.251 I ggml_metal_init: simdgroup reduction   = true
0.00.060.251 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.251 I ggml_metal_init: has bfloat            = true
0.00.060.251 I ggml_metal_init: use bfloat            = true
0.00.060.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.648 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.654 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.721 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.722 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.723 I llama_new_context_with_model: graph nodes  = 967
0.00.090.723 I llama_new_context_with_model: graph splits = 2
0.00.090.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.105 I main: llama threadpool init, n_threads = 4
0.00.785.150 I 
0.00.785.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.185 I 
0.00.785.443 I sampler seed: 1234
0.00.785.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.506 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.506 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.520 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.577.520 I llama_perf_context_print:        load time =     775.62 ms
0.01.577.521 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.30 tokens per second)
0.01.577.522 I llama_perf_context_print:        eval time =     745.92 ms /    63 runs   (   11.84 ms per token,    84.46 tokens per second)
0.01.577.522 I llama_perf_context_print:       total time =     792.42 ms /    70 tokens
0.01.577.702 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.657 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.085 I llama_model_loader: - type  f32:  194 tensors
0.00.025.085 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.086 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.258 I llm_load_vocab: special tokens cache size = 25
0.00.052.297 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.300 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.300 I llm_load_print_meta: arch             = gptneox
0.00.052.301 I llm_load_print_meta: vocab type       = BPE
0.00.052.301 I llm_load_print_meta: n_vocab          = 50304
0.00.052.301 I llm_load_print_meta: n_merges         = 50009
0.00.052.301 I llm_load_print_meta: vocab_only       = 0
0.00.052.301 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.301 I llm_load_print_meta: n_embd           = 2048
0.00.052.302 I llm_load_print_meta: n_layer          = 24
0.00.052.304 I llm_load_print_meta: n_head           = 16
0.00.052.305 I llm_load_print_meta: n_head_kv        = 16
0.00.052.305 I llm_load_print_meta: n_rot            = 32
0.00.052.305 I llm_load_print_meta: n_swa            = 0
0.00.052.306 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.306 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.306 I llm_load_print_meta: n_gqa            = 1
0.00.052.307 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.308 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.309 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.309 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.309 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.309 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.310 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.310 I llm_load_print_meta: n_ff             = 8192
0.00.052.311 I llm_load_print_meta: n_expert         = 0
0.00.052.311 I llm_load_print_meta: n_expert_used    = 0
0.00.052.311 I llm_load_print_meta: causal attn      = 1
0.00.052.311 I llm_load_print_meta: pooling type     = 0
0.00.052.311 I llm_load_print_meta: rope type        = 2
0.00.052.311 I llm_load_print_meta: rope scaling     = linear
0.00.052.312 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.312 I llm_load_print_meta: freq_scale_train = 1
0.00.052.312 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.313 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.313 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.313 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.313 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.313 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.313 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.320 I llm_load_print_meta: model type       = 1.4B
0.00.052.320 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.321 I llm_load_print_meta: model params     = 1.41 B
0.00.052.322 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.322 I llm_load_print_meta: general.name     = 1.4B
0.00.052.322 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.322 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.325 I llm_load_print_meta: max token length = 1024
0.00.054.182 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.182 I llm_load_tensors: offloading output layer to GPU
0.00.054.182 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.188 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.188 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.074 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.075 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.075 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.075 I llama_new_context_with_model: n_batch       = 2048
0.00.055.075 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.075 I llama_new_context_with_model: flash_attn    = 0
0.00.055.076 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.076 I llama_new_context_with_model: freq_scale    = 1
0.00.055.076 I ggml_metal_init: allocating
0.00.055.079 I ggml_metal_init: found device: Apple M4
0.00.055.081 I ggml_metal_init: picking default device: Apple M4
0.00.055.685 I ggml_metal_init: using embedded metal library
0.00.058.068 I ggml_metal_init: GPU name:   Apple M4
0.00.058.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.070 I ggml_metal_init: simdgroup reduction   = true
0.00.058.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.071 I ggml_metal_init: has bfloat            = true
0.00.058.071 I ggml_metal_init: use bfloat            = true
0.00.058.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.218 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.192 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.202 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.223 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.318 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.319 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.319 I llama_new_context_with_model: graph nodes  = 967
0.00.089.320 I llama_new_context_with_model: graph splits = 2
0.00.089.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.509 I main: llama threadpool init, n_threads = 4
0.00.720.547 I 
0.00.720.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.577 I 
0.00.720.802 I sampler seed: 1234
0.00.720.807 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.850 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.866 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.866 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.564.413 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.564.414 I llama_perf_context_print:        load time =     710.85 ms
0.01.564.415 I llama_perf_context_print: prompt eval time =      46.20 ms /     7 tokens (    6.60 ms per token,   151.52 tokens per second)
0.01.564.416 I llama_perf_context_print:        eval time =     794.45 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.564.416 I llama_perf_context_print:       total time =     843.91 ms /    70 tokens
0.01.564.609 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.112s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.085 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.711 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.290 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.291 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.291 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.292 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.292 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.293 I llama_model_loader: - type  f32:  194 tensors
0.00.023.293 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.293 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.293 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.679 I llm_load_vocab: special tokens cache size = 25
0.00.049.470 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.472 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.473 I llm_load_print_meta: arch             = gptneox
0.00.049.473 I llm_load_print_meta: vocab type       = BPE
0.00.049.473 I llm_load_print_meta: n_vocab          = 50304
0.00.049.474 I llm_load_print_meta: n_merges         = 50009
0.00.049.474 I llm_load_print_meta: vocab_only       = 0
0.00.049.474 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.474 I llm_load_print_meta: n_embd           = 2048
0.00.049.474 I llm_load_print_meta: n_layer          = 24
0.00.049.477 I llm_load_print_meta: n_head           = 16
0.00.049.478 I llm_load_print_meta: n_head_kv        = 16
0.00.049.478 I llm_load_print_meta: n_rot            = 32
0.00.049.479 I llm_load_print_meta: n_swa            = 0
0.00.049.479 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.479 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.480 I llm_load_print_meta: n_gqa            = 1
0.00.049.480 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.481 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.482 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.482 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.482 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.482 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.483 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.484 I llm_load_print_meta: n_ff             = 8192
0.00.049.484 I llm_load_print_meta: n_expert         = 0
0.00.049.484 I llm_load_print_meta: n_expert_used    = 0
0.00.049.484 I llm_load_print_meta: causal attn      = 1
0.00.049.484 I llm_load_print_meta: pooling type     = 0
0.00.049.484 I llm_load_print_meta: rope type        = 2
0.00.049.485 I llm_load_print_meta: rope scaling     = linear
0.00.049.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.485 I llm_load_print_meta: freq_scale_train = 1
0.00.049.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.497 I llm_load_print_meta: model type       = 1.4B
0.00.049.498 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.498 I llm_load_print_meta: model params     = 1.41 B
0.00.049.498 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.499 I llm_load_print_meta: general.name     = 1.4B
0.00.049.499 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.499 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.499 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.501 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.501 I llm_load_print_meta: max token length = 1024
0.00.051.070 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.070 I llm_load_tensors: offloading output layer to GPU
0.00.051.071 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.081 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.082 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.922 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.922 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.922 I llama_new_context_with_model: n_batch       = 2048
0.00.051.922 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.923 I llama_new_context_with_model: flash_attn    = 0
0.00.051.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.923 I llama_new_context_with_model: freq_scale    = 1
0.00.051.924 I ggml_metal_init: allocating
0.00.051.930 I ggml_metal_init: found device: Apple M4
0.00.051.933 I ggml_metal_init: picking default device: Apple M4
0.00.052.508 I ggml_metal_init: using embedded metal library
0.00.054.822 I ggml_metal_init: GPU name:   Apple M4
0.00.054.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.824 I ggml_metal_init: simdgroup reduction   = true
0.00.054.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.824 I ggml_metal_init: has bfloat            = true
0.00.054.825 I ggml_metal_init: use bfloat            = true
0.00.054.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.206 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.211 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.228 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.268 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.269 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.269 I llama_new_context_with_model: graph nodes  = 967
0.00.087.269 I llama_new_context_with_model: graph splits = 2
0.00.087.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.449.190 I main: llama threadpool init, n_threads = 4
0.00.449.233 I 
0.00.449.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.449.266 I 
0.00.449.490 I sampler seed: 1234
0.00.449.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.449.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.449.547 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.449.547 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.138.781 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.138.782 I llama_perf_context_print:        load time =     440.10 ms
0.01.138.783 I llama_perf_context_print: prompt eval time =      42.39 ms /     7 tokens (    6.06 ms per token,   165.12 tokens per second)
0.01.138.783 I llama_perf_context_print:        eval time =     643.93 ms /    63 runs   (   10.22 ms per token,    97.84 tokens per second)
0.01.138.787 I llama_perf_context_print:       total time =     689.59 ms /    70 tokens
0.01.138.997 I ggml_metal_free: deallocating

real	0m1.158s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.012.097 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.643 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.644 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.647 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.655 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.655 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.333 I llama_model_loader: - type  f32:  194 tensors
0.00.027.333 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.333 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.333 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.334 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.726 I llm_load_vocab: special tokens cache size = 25
0.00.053.716 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.719 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.719 I llm_load_print_meta: arch             = gptneox
0.00.053.720 I llm_load_print_meta: vocab type       = BPE
0.00.053.720 I llm_load_print_meta: n_vocab          = 50304
0.00.053.720 I llm_load_print_meta: n_merges         = 50009
0.00.053.720 I llm_load_print_meta: vocab_only       = 0
0.00.053.720 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.721 I llm_load_print_meta: n_embd           = 2048
0.00.053.721 I llm_load_print_meta: n_layer          = 24
0.00.053.723 I llm_load_print_meta: n_head           = 16
0.00.053.724 I llm_load_print_meta: n_head_kv        = 16
0.00.053.724 I llm_load_print_meta: n_rot            = 32
0.00.053.724 I llm_load_print_meta: n_swa            = 0
0.00.053.725 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.726 I llm_load_print_meta: n_gqa            = 1
0.00.053.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.729 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.729 I llm_load_print_meta: n_ff             = 8192
0.00.053.730 I llm_load_print_meta: n_expert         = 0
0.00.053.732 I llm_load_print_meta: n_expert_used    = 0
0.00.053.732 I llm_load_print_meta: causal attn      = 1
0.00.053.732 I llm_load_print_meta: pooling type     = 0
0.00.053.732 I llm_load_print_meta: rope type        = 2
0.00.053.732 I llm_load_print_meta: rope scaling     = linear
0.00.053.733 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.735 I llm_load_print_meta: freq_scale_train = 1
0.00.053.735 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.735 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.735 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.735 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.736 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.736 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.747 I llm_load_print_meta: model type       = 1.4B
0.00.053.747 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.748 I llm_load_print_meta: model params     = 1.41 B
0.00.053.749 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.749 I llm_load_print_meta: general.name     = 1.4B
0.00.053.750 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.750 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.750 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.751 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.751 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.751 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.751 I llm_load_print_meta: max token length = 1024
0.00.055.433 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.433 I llm_load_tensors: offloading output layer to GPU
0.00.055.433 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.443 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.444 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.395 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.396 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.396 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.396 I llama_new_context_with_model: n_batch       = 2048
0.00.056.396 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.396 I llama_new_context_with_model: flash_attn    = 0
0.00.056.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.397 I llama_new_context_with_model: freq_scale    = 1
0.00.056.398 I ggml_metal_init: allocating
0.00.056.401 I ggml_metal_init: found device: Apple M4
0.00.056.403 I ggml_metal_init: picking default device: Apple M4
0.00.056.993 I ggml_metal_init: using embedded metal library
0.00.059.614 I ggml_metal_init: GPU name:   Apple M4
0.00.059.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.617 I ggml_metal_init: simdgroup reduction   = true
0.00.059.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.617 I ggml_metal_init: has bfloat            = true
0.00.059.618 I ggml_metal_init: use bfloat            = true
0.00.059.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.433 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.815 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.821 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.882 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.884 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.884 I llama_new_context_with_model: graph nodes  = 967
0.00.090.884 I llama_new_context_with_model: graph splits = 2
0.00.090.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.984 I main: llama threadpool init, n_threads = 4
0.00.541.026 I 
0.00.541.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.067 I 
0.00.541.236 I sampler seed: 1234
0.00.541.240 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.541.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.541.255 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.541.255 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.309.354 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.309.355 I llama_perf_context_print:        load time =     528.88 ms
0.01.309.356 I llama_perf_context_print: prompt eval time =      40.56 ms /     7 tokens (    5.79 ms per token,   172.57 tokens per second)
0.01.309.356 I llama_perf_context_print:        eval time =     724.59 ms /    63 runs   (   11.50 ms per token,    86.95 tokens per second)
0.01.309.357 I llama_perf_context_print:       total time =     768.37 ms /    70 tokens
0.01.309.547 I ggml_metal_free: deallocating

real	0m1.324s
user	0m0.110s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.012.189 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.801 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.801 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.802 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.803 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.803 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.807 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.807 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.807 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.808 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.810 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.810 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.747 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.778 I llama_model_loader: - type  f32:  194 tensors
0.00.027.778 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.778 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.778 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.901 I llm_load_vocab: special tokens cache size = 25
0.00.054.689 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.692 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.692 I llm_load_print_meta: arch             = gptneox
0.00.054.692 I llm_load_print_meta: vocab type       = BPE
0.00.054.693 I llm_load_print_meta: n_vocab          = 50304
0.00.054.693 I llm_load_print_meta: n_merges         = 50009
0.00.054.693 I llm_load_print_meta: vocab_only       = 0
0.00.054.693 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.693 I llm_load_print_meta: n_embd           = 2048
0.00.054.694 I llm_load_print_meta: n_layer          = 24
0.00.054.697 I llm_load_print_meta: n_head           = 16
0.00.054.697 I llm_load_print_meta: n_head_kv        = 16
0.00.054.698 I llm_load_print_meta: n_rot            = 32
0.00.054.698 I llm_load_print_meta: n_swa            = 0
0.00.054.698 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.698 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.699 I llm_load_print_meta: n_gqa            = 1
0.00.054.700 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.700 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.701 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.701 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.703 I llm_load_print_meta: n_ff             = 8192
0.00.054.703 I llm_load_print_meta: n_expert         = 0
0.00.054.703 I llm_load_print_meta: n_expert_used    = 0
0.00.054.703 I llm_load_print_meta: causal attn      = 1
0.00.054.704 I llm_load_print_meta: pooling type     = 0
0.00.054.704 I llm_load_print_meta: rope type        = 2
0.00.054.704 I llm_load_print_meta: rope scaling     = linear
0.00.054.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.705 I llm_load_print_meta: freq_scale_train = 1
0.00.054.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.705 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.705 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.718 I llm_load_print_meta: model type       = 1.4B
0.00.054.718 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.719 I llm_load_print_meta: model params     = 1.41 B
0.00.054.719 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.719 I llm_load_print_meta: general.name     = 1.4B
0.00.054.719 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.720 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.722 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.722 I llm_load_print_meta: max token length = 1024
0.00.056.390 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.390 I llm_load_tensors: offloading output layer to GPU
0.00.056.390 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.401 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.402 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.057.233 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.234 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.234 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.234 I llama_new_context_with_model: n_batch       = 2048
0.00.057.234 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.235 I llama_new_context_with_model: flash_attn    = 0
0.00.057.235 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.235 I llama_new_context_with_model: freq_scale    = 1
0.00.057.236 I ggml_metal_init: allocating
0.00.057.242 I ggml_metal_init: found device: Apple M4
0.00.057.245 I ggml_metal_init: picking default device: Apple M4
0.00.057.818 I ggml_metal_init: using embedded metal library
0.00.060.491 I ggml_metal_init: GPU name:   Apple M4
0.00.060.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.494 I ggml_metal_init: simdgroup reduction   = true
0.00.060.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.494 I ggml_metal_init: has bfloat            = true
0.00.060.494 I ggml_metal_init: use bfloat            = true
0.00.060.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.096 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.092.839 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.844 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.863 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.960 I llama_new_context_with_model: graph nodes  = 967
0.00.093.960 I llama_new_context_with_model: graph splits = 2
0.00.093.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.202 I main: llama threadpool init, n_threads = 4
0.00.627.259 I 
0.00.627.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.302 I 
0.00.627.453 I sampler seed: 1234
0.00.627.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.627.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.627.495 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.627.495 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.419.670 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.01.419.671 I llama_perf_context_print:        load time =     615.01 ms
0.01.419.672 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.20 tokens per second)
0.01.419.673 I llama_perf_context_print:        eval time =     741.81 ms /    63 runs   (   11.77 ms per token,    84.93 tokens per second)
0.01.419.673 I llama_perf_context_print:       total time =     792.47 ms /    70 tokens
0.01.419.867 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.113s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.419 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.116 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.117 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.117 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.119 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.828 I llama_model_loader: - type  f32:  194 tensors
0.00.024.829 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.829 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.232 I llm_load_vocab: special tokens cache size = 25
0.00.051.099 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.101 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.102 I llm_load_print_meta: arch             = gptneox
0.00.051.102 I llm_load_print_meta: vocab type       = BPE
0.00.051.103 I llm_load_print_meta: n_vocab          = 50304
0.00.051.103 I llm_load_print_meta: n_merges         = 50009
0.00.051.103 I llm_load_print_meta: vocab_only       = 0
0.00.051.103 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.103 I llm_load_print_meta: n_embd           = 2048
0.00.051.103 I llm_load_print_meta: n_layer          = 24
0.00.051.106 I llm_load_print_meta: n_head           = 16
0.00.051.107 I llm_load_print_meta: n_head_kv        = 16
0.00.051.107 I llm_load_print_meta: n_rot            = 32
0.00.051.107 I llm_load_print_meta: n_swa            = 0
0.00.051.107 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.107 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.108 I llm_load_print_meta: n_gqa            = 1
0.00.051.109 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.110 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.110 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.111 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.112 I llm_load_print_meta: n_ff             = 8192
0.00.051.112 I llm_load_print_meta: n_expert         = 0
0.00.051.112 I llm_load_print_meta: n_expert_used    = 0
0.00.051.113 I llm_load_print_meta: causal attn      = 1
0.00.051.115 I llm_load_print_meta: pooling type     = 0
0.00.051.115 I llm_load_print_meta: rope type        = 2
0.00.051.115 I llm_load_print_meta: rope scaling     = linear
0.00.051.122 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.123 I llm_load_print_meta: freq_scale_train = 1
0.00.051.123 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.123 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.123 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.125 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.125 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.125 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.125 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.137 I llm_load_print_meta: model type       = 1.4B
0.00.051.138 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.138 I llm_load_print_meta: model params     = 1.41 B
0.00.051.139 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.139 I llm_load_print_meta: general.name     = 1.4B
0.00.051.139 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.140 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: max token length = 1024
0.00.052.919 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.920 I llm_load_tensors: offloading output layer to GPU
0.00.052.920 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.931 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.932 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.795 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.796 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.796 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.796 I llama_new_context_with_model: n_batch       = 2048
0.00.053.796 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.796 I llama_new_context_with_model: flash_attn    = 0
0.00.053.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.797 I llama_new_context_with_model: freq_scale    = 1
0.00.053.798 I ggml_metal_init: allocating
0.00.053.804 I ggml_metal_init: found device: Apple M4
0.00.053.808 I ggml_metal_init: picking default device: Apple M4
0.00.054.417 I ggml_metal_init: using embedded metal library
0.00.056.745 I ggml_metal_init: GPU name:   Apple M4
0.00.056.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.748 I ggml_metal_init: simdgroup reduction   = true
0.00.056.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.749 I ggml_metal_init: has bfloat            = true
0.00.056.749 I ggml_metal_init: use bfloat            = true
0.00.056.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.879 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.202 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.210 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.230 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.358 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.359 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.359 I llama_new_context_with_model: graph nodes  = 967
0.00.090.360 I llama_new_context_with_model: graph splits = 2
0.00.090.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.584 I main: llama threadpool init, n_threads = 4
0.00.726.620 I 
0.00.726.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.652 I 
0.00.726.879 I sampler seed: 1234
0.00.726.883 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.939 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.940 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.581.657 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64663.02 tokens per second)
0.01.581.658 I llama_perf_context_print:        load time =     717.16 ms
0.01.581.658 I llama_perf_context_print: prompt eval time =      55.55 ms /     7 tokens (    7.94 ms per token,   126.02 tokens per second)
0.01.581.660 I llama_perf_context_print:        eval time =     796.36 ms /    63 runs   (   12.64 ms per token,    79.11 tokens per second)
0.01.581.660 I llama_perf_context_print:       total time =     855.07 ms /    70 tokens
0.01.581.852 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.110s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.655 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.203 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.067 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.070 I llama_model_loader: - type  f32:  194 tensors
0.00.025.070 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.335 I llm_load_vocab: special tokens cache size = 25
0.00.052.277 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.281 I llm_load_print_meta: arch             = gptneox
0.00.052.281 I llm_load_print_meta: vocab type       = BPE
0.00.052.281 I llm_load_print_meta: n_vocab          = 50304
0.00.052.281 I llm_load_print_meta: n_merges         = 50009
0.00.052.282 I llm_load_print_meta: vocab_only       = 0
0.00.052.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.282 I llm_load_print_meta: n_embd           = 2048
0.00.052.282 I llm_load_print_meta: n_layer          = 24
0.00.052.285 I llm_load_print_meta: n_head           = 16
0.00.052.286 I llm_load_print_meta: n_head_kv        = 16
0.00.052.286 I llm_load_print_meta: n_rot            = 32
0.00.052.286 I llm_load_print_meta: n_swa            = 0
0.00.052.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.286 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.287 I llm_load_print_meta: n_gqa            = 1
0.00.052.288 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.289 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.289 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.290 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.290 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.290 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.291 I llm_load_print_meta: n_ff             = 8192
0.00.052.293 I llm_load_print_meta: n_expert         = 0
0.00.052.293 I llm_load_print_meta: n_expert_used    = 0
0.00.052.293 I llm_load_print_meta: causal attn      = 1
0.00.052.293 I llm_load_print_meta: pooling type     = 0
0.00.052.293 I llm_load_print_meta: rope type        = 2
0.00.052.294 I llm_load_print_meta: rope scaling     = linear
0.00.052.295 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.295 I llm_load_print_meta: freq_scale_train = 1
0.00.052.296 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.296 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.296 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.296 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.296 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.296 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.296 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.308 I llm_load_print_meta: model type       = 1.4B
0.00.052.308 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.309 I llm_load_print_meta: model params     = 1.41 B
0.00.052.309 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.309 I llm_load_print_meta: general.name     = 1.4B
0.00.052.309 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.310 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.310 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.310 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.310 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.311 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.311 I llm_load_print_meta: max token length = 1024
0.00.054.384 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.385 I llm_load_tensors: offloading output layer to GPU
0.00.054.385 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.395 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.397 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.342 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.343 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.344 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.344 I llama_new_context_with_model: n_batch       = 2048
0.00.055.344 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.344 I llama_new_context_with_model: flash_attn    = 0
0.00.055.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.345 I llama_new_context_with_model: freq_scale    = 1
0.00.055.345 I ggml_metal_init: allocating
0.00.055.351 I ggml_metal_init: found device: Apple M4
0.00.055.353 I ggml_metal_init: picking default device: Apple M4
0.00.055.934 I ggml_metal_init: using embedded metal library
0.00.058.321 I ggml_metal_init: GPU name:   Apple M4
0.00.058.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.323 I ggml_metal_init: simdgroup reduction   = true
0.00.058.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.323 I ggml_metal_init: has bfloat            = true
0.00.058.324 I ggml_metal_init: use bfloat            = true
0.00.058.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.437 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.442 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.459 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.424 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.425 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.425 I llama_new_context_with_model: graph nodes  = 967
0.00.089.425 I llama_new_context_with_model: graph splits = 2
0.00.089.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.903 I main: llama threadpool init, n_threads = 4
0.00.775.939 I 
0.00.775.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.978 I 
0.00.776.203 I sampler seed: 1234
0.00.776.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.223 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.223 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.657.408 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.657.408 I llama_perf_context_print:        load time =     766.24 ms
0.01.657.409 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.52 tokens per second)
0.01.657.410 I llama_perf_context_print:        eval time =     823.68 ms /    63 runs   (   13.07 ms per token,    76.49 tokens per second)
0.01.657.410 I llama_perf_context_print:       total time =     881.51 ms /    70 tokens
0.01.657.582 I ggml_metal_free: deallocating

real	0m1.678s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.507 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.019 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.987 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.084 I llama_model_loader: - type  f32:  194 tensors
0.00.054.085 I llama_model_loader: - type  f16:   98 tensors
0.00.084.002 I llm_load_vocab: special tokens cache size = 25
0.00.090.565 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.568 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.568 I llm_load_print_meta: arch             = gptneox
0.00.090.569 I llm_load_print_meta: vocab type       = BPE
0.00.090.569 I llm_load_print_meta: n_vocab          = 50304
0.00.090.569 I llm_load_print_meta: n_merges         = 50009
0.00.090.569 I llm_load_print_meta: vocab_only       = 0
0.00.090.569 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.569 I llm_load_print_meta: n_embd           = 2048
0.00.090.570 I llm_load_print_meta: n_layer          = 24
0.00.090.572 I llm_load_print_meta: n_head           = 16
0.00.090.573 I llm_load_print_meta: n_head_kv        = 16
0.00.090.573 I llm_load_print_meta: n_rot            = 32
0.00.090.573 I llm_load_print_meta: n_swa            = 0
0.00.090.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.574 I llm_load_print_meta: n_gqa            = 1
0.00.090.575 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.577 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.578 I llm_load_print_meta: n_ff             = 8192
0.00.090.578 I llm_load_print_meta: n_expert         = 0
0.00.090.579 I llm_load_print_meta: n_expert_used    = 0
0.00.090.579 I llm_load_print_meta: causal attn      = 1
0.00.090.579 I llm_load_print_meta: pooling type     = 0
0.00.090.579 I llm_load_print_meta: rope type        = 2
0.00.090.579 I llm_load_print_meta: rope scaling     = linear
0.00.090.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.580 I llm_load_print_meta: freq_scale_train = 1
0.00.090.580 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.580 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.580 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.580 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.591 I llm_load_print_meta: model type       = 1.4B
0.00.090.592 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.593 I llm_load_print_meta: model params     = 1.41 B
0.00.090.594 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.594 I llm_load_print_meta: general.name     = 1.4B
0.00.090.594 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.594 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.595 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.595 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.596 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.596 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.596 I llm_load_print_meta: max token length = 1024
0.00.092.370 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.370 I llm_load_tensors: offloading output layer to GPU
0.00.092.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.380 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.381 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.255 I llama_new_context_with_model: n_ctx         = 128
0.00.093.256 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.256 I llama_new_context_with_model: n_batch       = 128
0.00.093.256 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.256 I llama_new_context_with_model: flash_attn    = 0
0.00.093.257 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.257 I llama_new_context_with_model: freq_scale    = 1
0.00.093.257 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.258 I ggml_metal_init: allocating
0.00.093.261 I ggml_metal_init: found device: Apple M4
0.00.093.263 I ggml_metal_init: picking default device: Apple M4
0.00.093.922 I ggml_metal_init: using embedded metal library
0.00.096.610 I ggml_metal_init: GPU name:   Apple M4
0.00.096.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.612 I ggml_metal_init: simdgroup reduction   = true
0.00.096.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.613 I ggml_metal_init: has bfloat            = true
0.00.096.613 I ggml_metal_init: use bfloat            = true
0.00.096.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.501 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.850 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.864 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.772 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.773 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.773 I llama_new_context_with_model: graph nodes  = 967
0.00.108.774 I llama_new_context_with_model: graph splits = 2
0.00.108.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.003.022 I 
0.01.003.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.003.065 I perplexity: tokenizing the input ..
0.01.015.094 I perplexity: tokenization took 12.027 ms
0.01.015.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.148.753 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.150.583 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.150.608 I llama_perf_context_print:        load time =     979.99 ms
0.01.150.610 I llama_perf_context_print: prompt eval time =     133.24 ms /   128 tokens (    1.04 ms per token,   960.67 tokens per second)
0.01.150.611 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.150.612 I llama_perf_context_print:       total time =     147.59 ms /   129 tokens
0.01.151.342 I ggml_metal_free: deallocating

real	0m1.342s
user	0m0.124s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.350 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.353 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.353 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.354 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.354 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.354 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.355 I llama_model_loader: - type  f32:  194 tensors
0.00.032.355 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.872 I llm_load_vocab: special tokens cache size = 25
0.00.065.359 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.363 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.364 I llm_load_print_meta: arch             = gptneox
0.00.065.364 I llm_load_print_meta: vocab type       = BPE
0.00.065.364 I llm_load_print_meta: n_vocab          = 50304
0.00.065.364 I llm_load_print_meta: n_merges         = 50009
0.00.065.365 I llm_load_print_meta: vocab_only       = 0
0.00.065.365 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.365 I llm_load_print_meta: n_embd           = 2048
0.00.065.367 I llm_load_print_meta: n_layer          = 24
0.00.065.373 I llm_load_print_meta: n_head           = 16
0.00.065.374 I llm_load_print_meta: n_head_kv        = 16
0.00.065.374 I llm_load_print_meta: n_rot            = 32
0.00.065.375 I llm_load_print_meta: n_swa            = 0
0.00.065.375 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.376 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.376 I llm_load_print_meta: n_gqa            = 1
0.00.065.377 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.378 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.379 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.379 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.379 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.379 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.379 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.380 I llm_load_print_meta: n_ff             = 8192
0.00.065.381 I llm_load_print_meta: n_expert         = 0
0.00.065.381 I llm_load_print_meta: n_expert_used    = 0
0.00.065.381 I llm_load_print_meta: causal attn      = 1
0.00.065.381 I llm_load_print_meta: pooling type     = 0
0.00.065.381 I llm_load_print_meta: rope type        = 2
0.00.065.381 I llm_load_print_meta: rope scaling     = linear
0.00.065.382 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.382 I llm_load_print_meta: freq_scale_train = 1
0.00.065.382 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.382 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.382 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.383 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.383 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.383 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.383 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.395 I llm_load_print_meta: model type       = 1.4B
0.00.065.395 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.396 I llm_load_print_meta: model params     = 1.41 B
0.00.065.396 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.397 I llm_load_print_meta: general.name     = 1.4B
0.00.065.397 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.398 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.398 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.399 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.399 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.399 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.399 I llm_load_print_meta: max token length = 1024
0.00.067.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.342 I llm_load_tensors: offloading output layer to GPU
0.00.067.342 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.353 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.354 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.244 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.245 I llama_new_context_with_model: n_ctx         = 128
0.00.068.245 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.246 I llama_new_context_with_model: n_batch       = 128
0.00.068.246 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.246 I llama_new_context_with_model: flash_attn    = 0
0.00.068.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.247 I llama_new_context_with_model: freq_scale    = 1
0.00.068.247 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.248 I ggml_metal_init: allocating
0.00.068.251 I ggml_metal_init: found device: Apple M4
0.00.068.253 I ggml_metal_init: picking default device: Apple M4
0.00.068.960 I ggml_metal_init: using embedded metal library
0.00.072.589 I ggml_metal_init: GPU name:   Apple M4
0.00.072.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.592 I ggml_metal_init: simdgroup reduction   = true
0.00.072.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.592 I ggml_metal_init: has bfloat            = true
0.00.072.592 I ggml_metal_init: use bfloat            = true
0.00.072.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.008 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.505 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.508 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.524 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.569 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.570 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.570 I llama_new_context_with_model: graph nodes  = 967
0.00.084.570 I llama_new_context_with_model: graph splits = 2
0.00.084.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.939.180 I 
0.00.939.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.939.249 I perplexity: tokenizing the input ..
0.00.947.475 I perplexity: tokenization took 8.225 ms
0.00.947.488 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.071.654 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.072.823 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.072.845 I llama_perf_context_print:        load time =     927.50 ms
0.01.072.846 I llama_perf_context_print: prompt eval time =     123.91 ms /   128 tokens (    0.97 ms per token,  1033.03 tokens per second)
0.01.072.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.072.848 I llama_perf_context_print:       total time =     133.67 ms /   129 tokens
0.01.073.326 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.094s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.738 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.739 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.740 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.509 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.510 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.511 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.512 I llama_model_loader: - type  f32:  194 tensors
0.00.024.512 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.853 I llm_load_vocab: special tokens cache size = 25
0.00.050.558 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.561 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.561 I llm_load_print_meta: arch             = gptneox
0.00.050.561 I llm_load_print_meta: vocab type       = BPE
0.00.050.562 I llm_load_print_meta: n_vocab          = 50304
0.00.050.562 I llm_load_print_meta: n_merges         = 50009
0.00.050.562 I llm_load_print_meta: vocab_only       = 0
0.00.050.562 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.562 I llm_load_print_meta: n_embd           = 2048
0.00.050.562 I llm_load_print_meta: n_layer          = 24
0.00.050.564 I llm_load_print_meta: n_head           = 16
0.00.050.565 I llm_load_print_meta: n_head_kv        = 16
0.00.050.565 I llm_load_print_meta: n_rot            = 32
0.00.050.565 I llm_load_print_meta: n_swa            = 0
0.00.050.566 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.568 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.568 I llm_load_print_meta: n_gqa            = 1
0.00.050.569 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.570 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.582 I llm_load_print_meta: n_ff             = 8192
0.00.050.582 I llm_load_print_meta: n_expert         = 0
0.00.050.582 I llm_load_print_meta: n_expert_used    = 0
0.00.050.582 I llm_load_print_meta: causal attn      = 1
0.00.050.583 I llm_load_print_meta: pooling type     = 0
0.00.050.583 I llm_load_print_meta: rope type        = 2
0.00.050.584 I llm_load_print_meta: rope scaling     = linear
0.00.050.584 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.585 I llm_load_print_meta: freq_scale_train = 1
0.00.050.585 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.586 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.586 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.586 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.587 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.587 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.587 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.600 I llm_load_print_meta: model type       = 1.4B
0.00.050.600 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.600 I llm_load_print_meta: model params     = 1.41 B
0.00.050.601 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.601 I llm_load_print_meta: general.name     = 1.4B
0.00.050.601 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.601 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.602 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: max token length = 1024
0.00.052.483 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.483 I llm_load_tensors: offloading output layer to GPU
0.00.052.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.493 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.495 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.355 I llama_new_context_with_model: n_ctx         = 128
0.00.053.356 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.356 I llama_new_context_with_model: n_batch       = 128
0.00.053.356 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.356 I llama_new_context_with_model: flash_attn    = 0
0.00.053.357 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.357 I llama_new_context_with_model: freq_scale    = 1
0.00.053.357 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.358 I ggml_metal_init: allocating
0.00.053.363 I ggml_metal_init: found device: Apple M4
0.00.053.365 I ggml_metal_init: picking default device: Apple M4
0.00.053.928 I ggml_metal_init: using embedded metal library
0.00.056.242 I ggml_metal_init: GPU name:   Apple M4
0.00.056.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.245 I ggml_metal_init: simdgroup reduction   = true
0.00.056.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.245 I ggml_metal_init: has bfloat            = true
0.00.056.245 I ggml_metal_init: use bfloat            = true
0.00.056.246 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.547 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.556 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.578 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.501 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.502 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.503 I llama_new_context_with_model: graph nodes  = 967
0.00.068.503 I llama_new_context_with_model: graph splits = 2
0.00.068.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.165 I 
0.00.629.206 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.209 I perplexity: tokenizing the input ..
0.00.637.235 I perplexity: tokenization took 8.025 ms
0.00.637.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.726 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.760.876 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.760.892 I llama_perf_context_print:        load time =     619.29 ms
0.00.760.893 I llama_perf_context_print: prompt eval time =     122.25 ms /   128 tokens (    0.96 ms per token,  1047.00 tokens per second)
0.00.760.894 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.894 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.00.761.309 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.775 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.221 I llama_model_loader: - type  f32:  194 tensors
0.00.023.221 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.221 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.606 I llm_load_vocab: special tokens cache size = 25
0.00.049.445 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.448 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.449 I llm_load_print_meta: arch             = gptneox
0.00.049.449 I llm_load_print_meta: vocab type       = BPE
0.00.049.449 I llm_load_print_meta: n_vocab          = 50304
0.00.049.449 I llm_load_print_meta: n_merges         = 50009
0.00.049.450 I llm_load_print_meta: vocab_only       = 0
0.00.049.450 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.450 I llm_load_print_meta: n_embd           = 2048
0.00.049.450 I llm_load_print_meta: n_layer          = 24
0.00.049.453 I llm_load_print_meta: n_head           = 16
0.00.049.456 I llm_load_print_meta: n_head_kv        = 16
0.00.049.456 I llm_load_print_meta: n_rot            = 32
0.00.049.456 I llm_load_print_meta: n_swa            = 0
0.00.049.456 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.456 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.457 I llm_load_print_meta: n_gqa            = 1
0.00.049.458 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.459 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.459 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.460 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.460 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.461 I llm_load_print_meta: n_ff             = 8192
0.00.049.461 I llm_load_print_meta: n_expert         = 0
0.00.049.461 I llm_load_print_meta: n_expert_used    = 0
0.00.049.462 I llm_load_print_meta: causal attn      = 1
0.00.049.462 I llm_load_print_meta: pooling type     = 0
0.00.049.462 I llm_load_print_meta: rope type        = 2
0.00.049.462 I llm_load_print_meta: rope scaling     = linear
0.00.049.462 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.463 I llm_load_print_meta: freq_scale_train = 1
0.00.049.463 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.463 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.463 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.464 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.464 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.464 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.476 I llm_load_print_meta: model type       = 1.4B
0.00.049.476 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.476 I llm_load_print_meta: model params     = 1.41 B
0.00.049.477 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.477 I llm_load_print_meta: general.name     = 1.4B
0.00.049.477 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.477 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.477 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.478 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: max token length = 1024
0.00.051.471 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.471 I llm_load_tensors: offloading output layer to GPU
0.00.051.471 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.482 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.483 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.329 I llama_new_context_with_model: n_ctx         = 128
0.00.052.329 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.329 I llama_new_context_with_model: n_batch       = 128
0.00.052.330 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.330 I llama_new_context_with_model: flash_attn    = 0
0.00.052.330 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.331 I llama_new_context_with_model: freq_scale    = 1
0.00.052.331 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.331 I ggml_metal_init: allocating
0.00.052.337 I ggml_metal_init: found device: Apple M4
0.00.052.341 I ggml_metal_init: picking default device: Apple M4
0.00.052.890 I ggml_metal_init: using embedded metal library
0.00.055.380 I ggml_metal_init: GPU name:   Apple M4
0.00.055.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.382 I ggml_metal_init: simdgroup reduction   = true
0.00.055.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.382 I ggml_metal_init: has bfloat            = true
0.00.055.383 I ggml_metal_init: use bfloat            = true
0.00.055.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.001 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.277 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.279 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.174 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.176 I llama_new_context_with_model: graph nodes  = 967
0.00.067.176 I llama_new_context_with_model: graph splits = 2
0.00.067.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.839 I 
0.00.685.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.891 I perplexity: tokenizing the input ..
0.00.693.778 I perplexity: tokenization took 7.885 ms
0.00.693.789 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.473 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.817.627 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.817.644 I llama_perf_context_print:        load time =     677.06 ms
0.00.817.645 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.43 tokens per second)
0.00.817.646 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.646 I llama_perf_context_print:       total time =     131.81 ms /   129 tokens
0.00.818.099 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.268 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.272 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.033 I llama_model_loader: - type  f32:  194 tensors
0.00.024.033 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.405 I llm_load_vocab: special tokens cache size = 25
0.00.050.102 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.105 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.106 I llm_load_print_meta: arch             = gptneox
0.00.050.106 I llm_load_print_meta: vocab type       = BPE
0.00.050.106 I llm_load_print_meta: n_vocab          = 50304
0.00.050.106 I llm_load_print_meta: n_merges         = 50009
0.00.050.106 I llm_load_print_meta: vocab_only       = 0
0.00.050.107 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.107 I llm_load_print_meta: n_embd           = 2048
0.00.050.107 I llm_load_print_meta: n_layer          = 24
0.00.050.110 I llm_load_print_meta: n_head           = 16
0.00.050.110 I llm_load_print_meta: n_head_kv        = 16
0.00.050.111 I llm_load_print_meta: n_rot            = 32
0.00.050.111 I llm_load_print_meta: n_swa            = 0
0.00.050.111 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.111 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.112 I llm_load_print_meta: n_gqa            = 1
0.00.050.113 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.113 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.114 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.114 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.115 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.115 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.115 I llm_load_print_meta: n_ff             = 8192
0.00.050.116 I llm_load_print_meta: n_expert         = 0
0.00.050.116 I llm_load_print_meta: n_expert_used    = 0
0.00.050.116 I llm_load_print_meta: causal attn      = 1
0.00.050.116 I llm_load_print_meta: pooling type     = 0
0.00.050.116 I llm_load_print_meta: rope type        = 2
0.00.050.116 I llm_load_print_meta: rope scaling     = linear
0.00.050.119 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.119 I llm_load_print_meta: freq_scale_train = 1
0.00.050.119 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.120 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.120 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.120 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.120 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.120 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.120 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.132 I llm_load_print_meta: model type       = 1.4B
0.00.050.132 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.133 I llm_load_print_meta: model params     = 1.41 B
0.00.050.133 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.133 I llm_load_print_meta: general.name     = 1.4B
0.00.050.134 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.134 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.134 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.134 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.134 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.135 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.135 I llm_load_print_meta: max token length = 1024
0.00.052.118 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.118 I llm_load_tensors: offloading output layer to GPU
0.00.052.118 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.129 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.130 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.026 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.027 I llama_new_context_with_model: n_ctx         = 128
0.00.053.027 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.027 I llama_new_context_with_model: n_batch       = 128
0.00.053.027 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.028 I llama_new_context_with_model: flash_attn    = 0
0.00.053.028 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.028 I llama_new_context_with_model: freq_scale    = 1
0.00.053.029 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.029 I ggml_metal_init: allocating
0.00.053.032 I ggml_metal_init: found device: Apple M4
0.00.053.034 I ggml_metal_init: picking default device: Apple M4
0.00.053.606 I ggml_metal_init: using embedded metal library
0.00.055.914 I ggml_metal_init: GPU name:   Apple M4
0.00.055.916 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.917 I ggml_metal_init: simdgroup reduction   = true
0.00.055.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.917 I ggml_metal_init: has bfloat            = true
0.00.055.917 I ggml_metal_init: use bfloat            = true
0.00.055.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.550 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.938 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.941 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.954 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.883 I llama_new_context_with_model: graph nodes  = 967
0.00.067.883 I llama_new_context_with_model: graph splits = 2
0.00.067.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.118 I 
0.00.741.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.159 I perplexity: tokenizing the input ..
0.00.749.129 I perplexity: tokenization took 7.968 ms
0.00.749.140 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.352 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.885.624 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.885.637 I llama_perf_context_print:        load time =     731.43 ms
0.00.885.638 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.35 tokens per second)
0.00.885.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.640 I llama_perf_context_print:       total time =     144.52 ms /   129 tokens
0.00.885.986 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.077s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.149 I llama_model_loader: - type  f32:  194 tensors
0.00.023.149 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.585 I llm_load_vocab: special tokens cache size = 25
0.00.049.279 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.283 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.283 I llm_load_print_meta: arch             = gptneox
0.00.049.284 I llm_load_print_meta: vocab type       = BPE
0.00.049.285 I llm_load_print_meta: n_vocab          = 50304
0.00.049.286 I llm_load_print_meta: n_merges         = 50009
0.00.049.286 I llm_load_print_meta: vocab_only       = 0
0.00.049.286 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.286 I llm_load_print_meta: n_embd           = 2048
0.00.049.286 I llm_load_print_meta: n_layer          = 24
0.00.049.289 I llm_load_print_meta: n_head           = 16
0.00.049.289 I llm_load_print_meta: n_head_kv        = 16
0.00.049.290 I llm_load_print_meta: n_rot            = 32
0.00.049.290 I llm_load_print_meta: n_swa            = 0
0.00.049.290 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.290 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.291 I llm_load_print_meta: n_gqa            = 1
0.00.049.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.294 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.295 I llm_load_print_meta: n_ff             = 8192
0.00.049.295 I llm_load_print_meta: n_expert         = 0
0.00.049.295 I llm_load_print_meta: n_expert_used    = 0
0.00.049.295 I llm_load_print_meta: causal attn      = 1
0.00.049.295 I llm_load_print_meta: pooling type     = 0
0.00.049.295 I llm_load_print_meta: rope type        = 2
0.00.049.296 I llm_load_print_meta: rope scaling     = linear
0.00.049.296 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.298 I llm_load_print_meta: freq_scale_train = 1
0.00.049.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.311 I llm_load_print_meta: model type       = 1.4B
0.00.049.311 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.312 I llm_load_print_meta: model params     = 1.41 B
0.00.049.312 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.313 I llm_load_print_meta: general.name     = 1.4B
0.00.049.313 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.313 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.313 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.313 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.314 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.314 I llm_load_print_meta: max token length = 1024
0.00.051.345 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.345 I llm_load_tensors: offloading output layer to GPU
0.00.051.346 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.356 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.357 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.324 I llama_new_context_with_model: n_ctx         = 128
0.00.052.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.324 I llama_new_context_with_model: n_batch       = 128
0.00.052.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.324 I llama_new_context_with_model: flash_attn    = 0
0.00.052.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.325 I llama_new_context_with_model: freq_scale    = 1
0.00.052.325 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.326 I ggml_metal_init: allocating
0.00.052.332 I ggml_metal_init: found device: Apple M4
0.00.052.336 I ggml_metal_init: picking default device: Apple M4
0.00.052.885 I ggml_metal_init: using embedded metal library
0.00.055.249 I ggml_metal_init: GPU name:   Apple M4
0.00.055.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.251 I ggml_metal_init: simdgroup reduction   = true
0.00.055.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.252 I ggml_metal_init: has bfloat            = true
0.00.055.252 I ggml_metal_init: use bfloat            = true
0.00.055.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.926 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.278 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.282 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.297 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.206 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.207 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.207 I llama_new_context_with_model: graph nodes  = 967
0.00.067.208 I llama_new_context_with_model: graph splits = 2
0.00.067.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.199 I 
0.00.591.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.241 I perplexity: tokenizing the input ..
0.00.599.052 I perplexity: tokenization took 7.809 ms
0.00.599.063 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.733.726 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.734.988 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.735.006 I llama_perf_context_print:        load time =     582.21 ms
0.00.735.007 I llama_perf_context_print: prompt eval time =     134.44 ms /   128 tokens (    1.05 ms per token,   952.10 tokens per second)
0.00.735.008 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.009 I llama_perf_context_print:       total time =     143.81 ms /   129 tokens
0.00.735.438 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.308 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.311 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.009 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.010 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.010 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.011 I llama_model_loader: - type  f32:  194 tensors
0.00.024.011 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.011 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.011 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.401 I llm_load_vocab: special tokens cache size = 25
0.00.050.121 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.124 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.124 I llm_load_print_meta: arch             = gptneox
0.00.050.124 I llm_load_print_meta: vocab type       = BPE
0.00.050.125 I llm_load_print_meta: n_vocab          = 50304
0.00.050.125 I llm_load_print_meta: n_merges         = 50009
0.00.050.125 I llm_load_print_meta: vocab_only       = 0
0.00.050.125 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.125 I llm_load_print_meta: n_embd           = 2048
0.00.050.126 I llm_load_print_meta: n_layer          = 24
0.00.050.128 I llm_load_print_meta: n_head           = 16
0.00.050.129 I llm_load_print_meta: n_head_kv        = 16
0.00.050.129 I llm_load_print_meta: n_rot            = 32
0.00.050.129 I llm_load_print_meta: n_swa            = 0
0.00.050.129 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.130 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.130 I llm_load_print_meta: n_gqa            = 1
0.00.050.131 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.132 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.132 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.133 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.133 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.133 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.133 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.134 I llm_load_print_meta: n_ff             = 8192
0.00.050.134 I llm_load_print_meta: n_expert         = 0
0.00.050.134 I llm_load_print_meta: n_expert_used    = 0
0.00.050.134 I llm_load_print_meta: causal attn      = 1
0.00.050.135 I llm_load_print_meta: pooling type     = 0
0.00.050.135 I llm_load_print_meta: rope type        = 2
0.00.050.136 I llm_load_print_meta: rope scaling     = linear
0.00.050.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.139 I llm_load_print_meta: freq_scale_train = 1
0.00.050.139 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.140 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.140 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.140 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.140 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.140 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.140 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.147 I llm_load_print_meta: model type       = 1.4B
0.00.050.147 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.148 I llm_load_print_meta: model params     = 1.41 B
0.00.050.150 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.150 I llm_load_print_meta: general.name     = 1.4B
0.00.050.150 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.151 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.152 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: max token length = 1024
0.00.051.955 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.956 I llm_load_tensors: offloading output layer to GPU
0.00.051.956 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.961 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.962 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.973 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.974 I llama_new_context_with_model: n_ctx         = 128
0.00.052.975 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.975 I llama_new_context_with_model: n_batch       = 128
0.00.052.975 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.975 I llama_new_context_with_model: flash_attn    = 0
0.00.052.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.976 I llama_new_context_with_model: freq_scale    = 1
0.00.052.976 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.977 I ggml_metal_init: allocating
0.00.052.980 I ggml_metal_init: found device: Apple M4
0.00.052.982 I ggml_metal_init: picking default device: Apple M4
0.00.053.611 I ggml_metal_init: using embedded metal library
0.00.056.010 I ggml_metal_init: GPU name:   Apple M4
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.013 I ggml_metal_init: simdgroup reduction   = true
0.00.056.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.013 I ggml_metal_init: has bfloat            = true
0.00.056.013 I ggml_metal_init: use bfloat            = true
0.00.056.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.086 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.110 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.048 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.049 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.049 I llama_new_context_with_model: graph nodes  = 967
0.00.068.049 I llama_new_context_with_model: graph splits = 2
0.00.068.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.544 I 
0.00.391.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.581 I perplexity: tokenizing the input ..
0.00.399.223 I perplexity: tokenization took 7.64 ms
0.00.399.235 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.531.671 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.532.827 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.532.848 I llama_perf_context_print:        load time =     381.77 ms
0.00.532.849 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.17 tokens per second)
0.00.532.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.532.850 I llama_perf_context_print:       total time =     141.30 ms /   129 tokens
0.00.533.377 I ggml_metal_free: deallocating

real	0m0.549s
user	0m0.077s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.566 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.568 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.569 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.378 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.172 I llama_model_loader: - type  f32:  194 tensors
0.00.023.173 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.173 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.173 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.571 I llm_load_vocab: special tokens cache size = 25
0.00.049.424 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.427 I llm_load_print_meta: arch             = gptneox
0.00.049.427 I llm_load_print_meta: vocab type       = BPE
0.00.049.427 I llm_load_print_meta: n_vocab          = 50304
0.00.049.428 I llm_load_print_meta: n_merges         = 50009
0.00.049.428 I llm_load_print_meta: vocab_only       = 0
0.00.049.428 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.428 I llm_load_print_meta: n_embd           = 2048
0.00.049.428 I llm_load_print_meta: n_layer          = 24
0.00.049.431 I llm_load_print_meta: n_head           = 16
0.00.049.432 I llm_load_print_meta: n_head_kv        = 16
0.00.049.432 I llm_load_print_meta: n_rot            = 32
0.00.049.432 I llm_load_print_meta: n_swa            = 0
0.00.049.432 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.432 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.433 I llm_load_print_meta: n_gqa            = 1
0.00.049.434 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.435 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.435 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.436 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.436 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.436 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.436 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.437 I llm_load_print_meta: n_ff             = 8192
0.00.049.437 I llm_load_print_meta: n_expert         = 0
0.00.049.437 I llm_load_print_meta: n_expert_used    = 0
0.00.049.437 I llm_load_print_meta: causal attn      = 1
0.00.049.438 I llm_load_print_meta: pooling type     = 0
0.00.049.438 I llm_load_print_meta: rope type        = 2
0.00.049.438 I llm_load_print_meta: rope scaling     = linear
0.00.049.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.439 I llm_load_print_meta: freq_scale_train = 1
0.00.049.441 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.441 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.441 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.441 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.442 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.442 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.442 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.453 I llm_load_print_meta: model type       = 1.4B
0.00.049.454 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.454 I llm_load_print_meta: model params     = 1.41 B
0.00.049.455 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.455 I llm_load_print_meta: general.name     = 1.4B
0.00.049.455 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.456 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: max token length = 1024
0.00.051.383 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.383 I llm_load_tensors: offloading output layer to GPU
0.00.051.384 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.394 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.395 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.323 I llama_new_context_with_model: n_ctx         = 128
0.00.052.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.324 I llama_new_context_with_model: n_batch       = 128
0.00.052.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.324 I llama_new_context_with_model: flash_attn    = 0
0.00.052.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.325 I llama_new_context_with_model: freq_scale    = 1
0.00.052.325 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.325 I ggml_metal_init: allocating
0.00.052.329 I ggml_metal_init: found device: Apple M4
0.00.052.331 I ggml_metal_init: picking default device: Apple M4
0.00.052.920 I ggml_metal_init: using embedded metal library
0.00.055.391 I ggml_metal_init: GPU name:   Apple M4
0.00.055.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.394 I ggml_metal_init: simdgroup reduction   = true
0.00.055.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.394 I ggml_metal_init: has bfloat            = true
0.00.055.394 I ggml_metal_init: use bfloat            = true
0.00.055.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.059 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.371 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.374 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.387 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.295 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.296 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.296 I llama_new_context_with_model: graph nodes  = 967
0.00.067.297 I llama_new_context_with_model: graph splits = 2
0.00.067.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.599 I 
0.00.524.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.637 I perplexity: tokenizing the input ..
0.00.532.484 I perplexity: tokenization took 7.846 ms
0.00.532.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.664.703 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.875 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.896 I llama_perf_context_print:        load time =     515.91 ms
0.00.665.897 I llama_perf_context_print: prompt eval time =     131.98 ms /   128 tokens (    1.03 ms per token,   969.85 tokens per second)
0.00.665.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.899 I llama_perf_context_print:       total time =     141.30 ms /   129 tokens
0.00.666.374 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.594 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.272 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.107 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.140 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.018 I llama_model_loader: - type  f32:  194 tensors
0.00.025.018 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.018 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.019 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.100 I llm_load_vocab: special tokens cache size = 25
0.00.051.970 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.973 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.974 I llm_load_print_meta: arch             = gptneox
0.00.051.974 I llm_load_print_meta: vocab type       = BPE
0.00.051.974 I llm_load_print_meta: n_vocab          = 50304
0.00.051.974 I llm_load_print_meta: n_merges         = 50009
0.00.051.974 I llm_load_print_meta: vocab_only       = 0
0.00.051.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.975 I llm_load_print_meta: n_embd           = 2048
0.00.051.975 I llm_load_print_meta: n_layer          = 24
0.00.051.978 I llm_load_print_meta: n_head           = 16
0.00.051.979 I llm_load_print_meta: n_head_kv        = 16
0.00.051.979 I llm_load_print_meta: n_rot            = 32
0.00.051.979 I llm_load_print_meta: n_swa            = 0
0.00.051.979 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.979 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.981 I llm_load_print_meta: n_gqa            = 1
0.00.051.982 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.983 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.983 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.984 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.984 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.985 I llm_load_print_meta: n_ff             = 8192
0.00.051.985 I llm_load_print_meta: n_expert         = 0
0.00.051.985 I llm_load_print_meta: n_expert_used    = 0
0.00.051.986 I llm_load_print_meta: causal attn      = 1
0.00.051.986 I llm_load_print_meta: pooling type     = 0
0.00.051.986 I llm_load_print_meta: rope type        = 2
0.00.051.986 I llm_load_print_meta: rope scaling     = linear
0.00.051.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.987 I llm_load_print_meta: freq_scale_train = 1
0.00.051.987 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.988 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.003 I llm_load_print_meta: model type       = 1.4B
0.00.052.004 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.004 I llm_load_print_meta: model params     = 1.41 B
0.00.052.004 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.006 I llm_load_print_meta: general.name     = 1.4B
0.00.052.006 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.007 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: max token length = 1024
0.00.054.040 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.041 I llm_load_tensors: offloading output layer to GPU
0.00.054.041 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.052 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.053 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.995 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.995 I llama_new_context_with_model: n_ctx         = 128
0.00.054.996 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.996 I llama_new_context_with_model: n_batch       = 128
0.00.054.996 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.996 I llama_new_context_with_model: flash_attn    = 0
0.00.054.997 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.997 I llama_new_context_with_model: freq_scale    = 1
0.00.054.997 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.998 I ggml_metal_init: allocating
0.00.055.004 I ggml_metal_init: found device: Apple M4
0.00.055.007 I ggml_metal_init: picking default device: Apple M4
0.00.055.565 I ggml_metal_init: using embedded metal library
0.00.057.950 I ggml_metal_init: GPU name:   Apple M4
0.00.057.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.953 I ggml_metal_init: simdgroup reduction   = true
0.00.057.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.953 I ggml_metal_init: has bfloat            = true
0.00.057.953 I ggml_metal_init: use bfloat            = true
0.00.057.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.684 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.944 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.947 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.961 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.916 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.917 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.918 I llama_new_context_with_model: graph nodes  = 967
0.00.069.918 I llama_new_context_with_model: graph splits = 2
0.00.069.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.194 I 
0.00.478.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.231 I perplexity: tokenizing the input ..
0.00.485.675 I perplexity: tokenization took 7.442 ms
0.00.485.688 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.620.253 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.621.483 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.621.505 I llama_perf_context_print:        load time =     467.60 ms
0.00.621.506 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.91 tokens per second)
0.00.621.506 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.507 I llama_perf_context_print:       total time =     143.31 ms /   129 tokens
0.00.621.988 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.079s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.205 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.092 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.094 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.094 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.097 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.896 I llama_model_loader: - type  f32:  194 tensors
0.00.023.896 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.896 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.382 I llm_load_vocab: special tokens cache size = 25
0.00.050.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.419 I llm_load_print_meta: arch             = gptneox
0.00.050.420 I llm_load_print_meta: vocab type       = BPE
0.00.050.420 I llm_load_print_meta: n_vocab          = 50304
0.00.050.420 I llm_load_print_meta: n_merges         = 50009
0.00.050.420 I llm_load_print_meta: vocab_only       = 0
0.00.050.420 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.421 I llm_load_print_meta: n_embd           = 2048
0.00.050.421 I llm_load_print_meta: n_layer          = 24
0.00.050.424 I llm_load_print_meta: n_head           = 16
0.00.050.425 I llm_load_print_meta: n_head_kv        = 16
0.00.050.425 I llm_load_print_meta: n_rot            = 32
0.00.050.425 I llm_load_print_meta: n_swa            = 0
0.00.050.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.426 I llm_load_print_meta: n_gqa            = 1
0.00.050.427 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.428 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.429 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.430 I llm_load_print_meta: n_ff             = 8192
0.00.050.431 I llm_load_print_meta: n_expert         = 0
0.00.050.431 I llm_load_print_meta: n_expert_used    = 0
0.00.050.432 I llm_load_print_meta: causal attn      = 1
0.00.050.433 I llm_load_print_meta: pooling type     = 0
0.00.050.433 I llm_load_print_meta: rope type        = 2
0.00.050.433 I llm_load_print_meta: rope scaling     = linear
0.00.050.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.434 I llm_load_print_meta: freq_scale_train = 1
0.00.050.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.435 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.446 I llm_load_print_meta: model type       = 1.4B
0.00.050.447 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.447 I llm_load_print_meta: model params     = 1.41 B
0.00.050.448 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.448 I llm_load_print_meta: general.name     = 1.4B
0.00.050.448 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.448 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.448 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.448 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.449 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.449 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.449 I llm_load_print_meta: max token length = 1024
0.00.052.500 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.500 I llm_load_tensors: offloading output layer to GPU
0.00.052.500 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.511 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.512 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.457 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.458 I llama_new_context_with_model: n_ctx         = 128
0.00.053.458 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.458 I llama_new_context_with_model: n_batch       = 128
0.00.053.458 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.458 I llama_new_context_with_model: flash_attn    = 0
0.00.053.459 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.459 I llama_new_context_with_model: freq_scale    = 1
0.00.053.460 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.460 I ggml_metal_init: allocating
0.00.053.466 I ggml_metal_init: found device: Apple M4
0.00.053.468 I ggml_metal_init: picking default device: Apple M4
0.00.054.025 I ggml_metal_init: using embedded metal library
0.00.056.409 I ggml_metal_init: GPU name:   Apple M4
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.411 I ggml_metal_init: simdgroup reduction   = true
0.00.056.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.411 I ggml_metal_init: has bfloat            = true
0.00.056.412 I ggml_metal_init: use bfloat            = true
0.00.056.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.197 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.513 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.518 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.535 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.419 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.420 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.421 I llama_new_context_with_model: graph nodes  = 967
0.00.068.421 I llama_new_context_with_model: graph splits = 2
0.00.068.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.866 I 
0.00.643.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.907 I perplexity: tokenizing the input ..
0.00.652.023 I perplexity: tokenization took 8.114 ms
0.00.652.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.426 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.612 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.630 I llama_perf_context_print:        load time =     634.65 ms
0.00.793.631 I llama_perf_context_print: prompt eval time =     140.17 ms /   128 tokens (    1.10 ms per token,   913.19 tokens per second)
0.00.793.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.632 I llama_perf_context_print:       total time =     149.77 ms /   129 tokens
0.00.794.065 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.078s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.737 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.561 I llama_model_loader: - type  f32:  194 tensors
0.00.024.561 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.675 I llm_load_vocab: special tokens cache size = 25
0.00.051.657 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.660 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.660 I llm_load_print_meta: arch             = gptneox
0.00.051.661 I llm_load_print_meta: vocab type       = BPE
0.00.051.661 I llm_load_print_meta: n_vocab          = 50304
0.00.051.661 I llm_load_print_meta: n_merges         = 50009
0.00.051.661 I llm_load_print_meta: vocab_only       = 0
0.00.051.661 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.661 I llm_load_print_meta: n_embd           = 2048
0.00.051.662 I llm_load_print_meta: n_layer          = 24
0.00.051.664 I llm_load_print_meta: n_head           = 16
0.00.051.665 I llm_load_print_meta: n_head_kv        = 16
0.00.051.665 I llm_load_print_meta: n_rot            = 32
0.00.051.667 I llm_load_print_meta: n_swa            = 0
0.00.051.667 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.667 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.668 I llm_load_print_meta: n_gqa            = 1
0.00.051.669 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.675 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.676 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.677 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.677 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.678 I llm_load_print_meta: n_ff             = 8192
0.00.051.681 I llm_load_print_meta: n_expert         = 0
0.00.051.682 I llm_load_print_meta: n_expert_used    = 0
0.00.051.682 I llm_load_print_meta: causal attn      = 1
0.00.051.683 I llm_load_print_meta: pooling type     = 0
0.00.051.683 I llm_load_print_meta: rope type        = 2
0.00.051.683 I llm_load_print_meta: rope scaling     = linear
0.00.051.684 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.684 I llm_load_print_meta: freq_scale_train = 1
0.00.051.684 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.684 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.685 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.685 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.685 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.685 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.685 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.692 I llm_load_print_meta: model type       = 1.4B
0.00.051.692 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.693 I llm_load_print_meta: model params     = 1.41 B
0.00.051.693 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.693 I llm_load_print_meta: general.name     = 1.4B
0.00.051.693 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.694 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: max token length = 1024
0.00.053.531 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.532 I llm_load_tensors: offloading output layer to GPU
0.00.053.532 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.538 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.538 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.127 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.128 I llama_new_context_with_model: n_ctx         = 128
0.00.055.128 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.128 I llama_new_context_with_model: n_batch       = 128
0.00.055.128 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.128 I llama_new_context_with_model: flash_attn    = 0
0.00.055.133 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.133 I llama_new_context_with_model: freq_scale    = 1
0.00.055.134 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.137 I ggml_metal_init: allocating
0.00.055.179 I ggml_metal_init: found device: Apple M4
0.00.055.182 I ggml_metal_init: picking default device: Apple M4
0.00.055.825 I ggml_metal_init: using embedded metal library
0.00.058.139 I ggml_metal_init: GPU name:   Apple M4
0.00.058.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.141 I ggml_metal_init: simdgroup reduction   = true
0.00.058.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.141 I ggml_metal_init: has bfloat            = true
0.00.058.142 I ggml_metal_init: use bfloat            = true
0.00.058.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.861 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.874 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.715 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.716 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.716 I llama_new_context_with_model: graph nodes  = 967
0.00.069.717 I llama_new_context_with_model: graph splits = 2
0.00.069.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.219.317 I 
0.00.219.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.219.349 I perplexity: tokenizing the input ..
0.00.227.156 I perplexity: tokenization took 7.806 ms
0.00.227.167 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.367.638 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.368.800 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.368.813 I llama_perf_context_print:        load time =     209.58 ms
0.00.368.814 I llama_perf_context_print: prompt eval time =     140.20 ms /   128 tokens (    1.10 ms per token,   912.96 tokens per second)
0.00.368.817 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.368.818 I llama_perf_context_print:       total time =     149.50 ms /   129 tokens
0.00.369.274 I ggml_metal_free: deallocating

real	0m0.385s
user	0m0.079s
sys	0m0.048s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.252 I build: 4317 (526e6e36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.107 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.118 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.152 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.034 I llama_model_loader: - type  f32:  194 tensors
0.00.050.035 I llama_model_loader: - type  f16:   98 tensors
0.00.078.918 I llm_load_vocab: special tokens cache size = 25
0.00.085.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.408 I llm_load_print_meta: arch             = gptneox
0.00.085.408 I llm_load_print_meta: vocab type       = BPE
0.00.085.409 I llm_load_print_meta: n_vocab          = 50304
0.00.085.409 I llm_load_print_meta: n_merges         = 50009
0.00.085.409 I llm_load_print_meta: vocab_only       = 0
0.00.085.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.409 I llm_load_print_meta: n_embd           = 2048
0.00.085.409 I llm_load_print_meta: n_layer          = 24
0.00.085.412 I llm_load_print_meta: n_head           = 16
0.00.085.416 I llm_load_print_meta: n_head_kv        = 16
0.00.085.416 I llm_load_print_meta: n_rot            = 32
0.00.085.416 I llm_load_print_meta: n_swa            = 0
0.00.085.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.421 I llm_load_print_meta: n_gqa            = 1
0.00.085.421 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.422 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.431 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.433 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.433 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.434 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.434 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.437 I llm_load_print_meta: n_ff             = 8192
0.00.085.438 I llm_load_print_meta: n_expert         = 0
0.00.085.438 I llm_load_print_meta: n_expert_used    = 0
0.00.085.438 I llm_load_print_meta: causal attn      = 1
0.00.085.438 I llm_load_print_meta: pooling type     = 0
0.00.085.438 I llm_load_print_meta: rope type        = 2
0.00.085.438 I llm_load_print_meta: rope scaling     = linear
0.00.085.439 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.439 I llm_load_print_meta: freq_scale_train = 1
0.00.085.439 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.439 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.439 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.440 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.440 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.441 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.441 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.453 I llm_load_print_meta: model type       = 1.4B
0.00.085.454 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.455 I llm_load_print_meta: model params     = 1.41 B
0.00.085.455 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.455 I llm_load_print_meta: general.name     = 1.4B
0.00.085.455 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.456 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.456 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.456 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.458 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.458 I llm_load_print_meta: max token length = 1024
0.00.088.179 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.179 I llm_load_tensors: offloading output layer to GPU
0.00.088.179 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.190 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.192 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.251 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.252 I llama_new_context_with_model: n_ctx         = 128
0.00.089.252 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.252 I llama_new_context_with_model: n_batch       = 128
0.00.089.253 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.253 I llama_new_context_with_model: flash_attn    = 0
0.00.089.253 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.254 I llama_new_context_with_model: freq_scale    = 1
0.00.089.254 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.254 I ggml_metal_init: allocating
0.00.089.261 I ggml_metal_init: found device: Apple M4
0.00.089.263 I ggml_metal_init: picking default device: Apple M4
0.00.089.909 I ggml_metal_init: using embedded metal library
0.00.092.456 I ggml_metal_init: GPU name:   Apple M4
0.00.092.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.458 I ggml_metal_init: simdgroup reduction   = true
0.00.092.459 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.459 I ggml_metal_init: has bfloat            = true
0.00.092.459 I ggml_metal_init: use bfloat            = true
0.00.092.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.323 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.103.629 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.631 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.653 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.513 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.514 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.514 I llama_new_context_with_model: graph nodes  = 967
0.00.104.514 I llama_new_context_with_model: graph splits = 2
0.00.104.527 I 
0.00.104.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.561 I compute_imatrix: tokenizing the input ..
0.00.111.738 I compute_imatrix: tokenization took 7.176 ms
0.00.111.739 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.611.524 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.614.084 I llama_perf_context_print:        load time =    1589.50 ms
0.01.614.084 I llama_perf_context_print: prompt eval time =    1499.14 ms /   128 tokens (   11.71 ms per token,    85.38 tokens per second)
0.01.614.085 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.614.086 I llama_perf_context_print:       total time =    1592.05 ms /   129 tokens
0.01.614.651 I ggml_metal_free: deallocating

real	0m1.798s
user	0m0.165s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4317 (526e6e36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de07590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de08800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de13240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de65310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de65920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de09bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de0a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de0bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de0c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de0d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de0e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de0e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de11e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de12420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de17670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de1be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de1c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de1d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de1e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de1ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de20b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de21b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de29fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de2a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de2aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de2c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de2da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de2fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de31710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de32990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de32e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de3e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de46180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de4bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de4d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de4df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de4f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de51450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de52990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de53ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de54970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de55410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de55eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de56400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de56950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de57e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de58930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de5a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de5a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de5ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de5b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de5b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de5be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de5c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de5c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de5d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de5de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de5e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de5ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de5f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de5f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de5f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de5fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de60c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de61110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de61ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de62390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de63720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de64560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de64820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de65010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de658e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de0b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de0bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de0c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de0f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de0ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de10650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de11f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de12370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de12c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de16450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de17610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de17ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de28050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de30100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de35550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de36710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de39370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de3ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de42ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de43c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de44c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de47d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de48620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de48a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de4b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de4bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de4c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de4c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de4cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de4dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de4e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de50fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de52170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de52a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de52ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de53330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de54960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de54dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de55240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de55b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de56400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de57ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de58e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de59c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de5b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de5b500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.292s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4317 (526e6e36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143f0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143f0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143f10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143f10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143f11060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143f11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143f11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143f120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143f125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143f135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143f13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143f145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143f14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143f153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143f15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143f16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143f169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143f17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143f17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143f17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143f187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143f18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143f191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143f197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143f1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143f1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143f1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143f1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143f1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143f1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143f1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143f1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143f1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143f1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143f1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143f1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143f1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143f1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143f1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143f1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143f1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143f1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143f20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143f21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143f21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143f21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143f22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143f22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143f23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143f236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143f23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143f23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143f24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143f24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143f24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143f25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143f25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143f25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143f26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143f26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143f26f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143f27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143f27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143f28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143f28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143f29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143f29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143f2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143f2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143f2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143f2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143f2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143f2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143f2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143f2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143f2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143f2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143f2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143f2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143f2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143f300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143f305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143f202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143f31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143f31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143f33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143f33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143f34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143f35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143f36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143f36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143f369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143f36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143f372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143f37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143f37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143f380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143f38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143f38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143f38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143f39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143f397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143f39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143f3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143f3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143f3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143f3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143f3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143f3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143f3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143f3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143f3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143f3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143f3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143f3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143f3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143f3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143f3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143f3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143f3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143f3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143f3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143f40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143f406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143f40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143f41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143f414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143f41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143f41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143f422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143f42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143f42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143f43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143f43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143f439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143f44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143f447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143f44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143f450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143f45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143f45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143f45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143f46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143f47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143f475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143f47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143f47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143f483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143f48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143f48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143f491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143f49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143f49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143f49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143f4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143f4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143f4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143f4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143f4b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143f4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143f4bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143f4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143f4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143f4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143f4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143f4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143f4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143f4ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143f4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143f4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143f502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143f508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143f50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143f51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143f51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143f52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143f53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143f546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143f54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143f55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143f556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143f55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143f56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143f56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143f57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143f58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143f58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143f58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143f59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143f59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143f59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143f5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143f5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143f5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143f5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143f5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143f5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143f5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143f5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143f5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143f5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143f5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143f5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143f5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143f5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143f5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143f5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143f5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143f60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143f60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143f61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143f61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143f620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143f62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143f63090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143f635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143f63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143f64080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143f645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143f64b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143f65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143f655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143f65a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143f65f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143f663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143f66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143f66ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143f67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143f67620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143f67ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143f67f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143f68400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143f688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143f68d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143f691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143f69680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143f69b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143f6a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143f6a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143f6aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143f6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143f6bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143f6bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143f6c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143f6ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143f6d070 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.314 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145a356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145a35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145a35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145a36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145a36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145a36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145a37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145a375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145a37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145a37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145a38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145a38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145a38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145a39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145a394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145a39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145a39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145a3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145a3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145a3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145a3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145a3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145a3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145a3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145a3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145a3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145a3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145a3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145a3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145a3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145a3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145a3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145a3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145a3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145a3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145a3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145a3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145a3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145a3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145a403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145a40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145a40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145a41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145a416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145a421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145a424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145a42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145a42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145a43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145a434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145a43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145a43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145a44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145a44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145a44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145a44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145a453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145a45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145a45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145a46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145a46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145a46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145a46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145a472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145a47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145a47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145a48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145a484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145a48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145a48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145a491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145a49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145a49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145a49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145a4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145a4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145a4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145a4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145a4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145a4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145a4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145a4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145a4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145a4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145a4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145a4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145a4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145a4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145a4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145a4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145a4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145a4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145a4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145a4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145a500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145a50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145a509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145a50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145a512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145a51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145a51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145a51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145a52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145a528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145a52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145a531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145a53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145a53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145a53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145a54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145a547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145a54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145a550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145a55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145a559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145a55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145a56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145a56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145a576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145a57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145a580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145a58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145a58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145a59120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143e1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143e1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143e1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143e1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143e1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143e1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143e1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143e1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143e1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143e1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143e1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143e1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143e1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143e20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143e20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143e21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143e21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143e219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143e21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143e222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143e22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143e22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143e23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143e238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143e241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143e24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143e24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143e25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143e25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143e25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143e260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143e26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143e269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143e26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143e3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143e40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e45af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e47a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e4a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e4aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143e524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143e52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e53310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e55690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143e58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143e592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143e59760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.243s
sys	0m0.150s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
