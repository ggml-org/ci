Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.509s
user	0m0.839s
sys	0m1.219s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-sampling
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Built target test-arg-parser
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-gguf
[ 61%] Built target test-backend-ops
[ 61%] Built target test-chat-template
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target test-quantize-fns
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Built target llama-imatrix
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookup
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-quantize
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Built target llama-parallel
[ 88%] Built target llama-passkey
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-retrieval
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Built target llama-speculative-simple
[ 89%] Built target llama-speculative
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-cvector-generator
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.065s
user	0m6.210s
sys	0m9.246s

main: quantize time =  5167.75 ms
main:    total time =  5167.75 ms

main: quantize time =  1664.88 ms
main:    total time =  1664.88 ms

main: quantize time =  1674.43 ms
main:    total time =  1674.43 ms

main: quantize time =  1488.72 ms
main:    total time =  1488.72 ms

main: quantize time =  2226.08 ms
main:    total time =  2226.08 ms

main: quantize time =  4974.85 ms
main:    total time =  4974.85 ms

main: quantize time =  5875.33 ms
main:    total time =  5875.33 ms

main: quantize time =  6998.96 ms
main:    total time =  6998.96 ms

main: quantize time =  5985.64 ms
main:    total time =  5985.64 ms

main: quantize time =  4561.92 ms
main:    total time =  4561.92 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.132 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.321 I main: llama backend init
0.00.000.328 I main: load the model and apply lora adapter, if any
0.00.046.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.059.843 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.865 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.873 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.873 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.875 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.877 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.920 I llama_model_loader: - type  f32:  194 tensors
0.00.078.921 I llama_model_loader: - type  f16:   98 tensors
0.00.078.922 I print_info: file format = GGUF V3 (latest)
0.00.078.924 I print_info: file type   = all F32 (guessed)
0.00.078.927 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.108.790 I load: special tokens cache size = 25
0.00.115.855 I load: token to piece cache size = 0.2984 MB
0.00.115.858 I print_info: arch             = gptneox
0.00.115.858 I print_info: vocab_only       = 0
0.00.115.858 I print_info: n_ctx_train      = 2048
0.00.115.858 I print_info: n_embd           = 2048
0.00.115.859 I print_info: n_layer          = 24
0.00.115.862 I print_info: n_head           = 16
0.00.115.863 I print_info: n_head_kv        = 16
0.00.115.863 I print_info: n_rot            = 32
0.00.115.863 I print_info: n_swa            = 0
0.00.115.864 I print_info: n_embd_head_k    = 128
0.00.115.864 I print_info: n_embd_head_v    = 128
0.00.115.864 I print_info: n_gqa            = 1
0.00.115.865 I print_info: n_embd_k_gqa     = 2048
0.00.115.866 I print_info: n_embd_v_gqa     = 2048
0.00.115.866 I print_info: f_norm_eps       = 1.0e-05
0.00.115.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.115.868 I print_info: f_clamp_kqv      = 0.0e+00
0.00.115.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.115.868 I print_info: f_logit_scale    = 0.0e+00
0.00.115.869 I print_info: n_ff             = 8192
0.00.115.869 I print_info: n_expert         = 0
0.00.115.870 I print_info: n_expert_used    = 0
0.00.115.871 I print_info: causal attn      = 1
0.00.115.871 I print_info: pooling type     = 0
0.00.115.871 I print_info: rope type        = 2
0.00.115.871 I print_info: rope scaling     = linear
0.00.115.871 I print_info: freq_base_train  = 10000.0
0.00.115.872 I print_info: freq_scale_train = 1
0.00.115.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.115.872 I print_info: rope_finetuned   = unknown
0.00.115.873 I print_info: ssm_d_conv       = 0
0.00.115.873 I print_info: ssm_d_inner      = 0
0.00.115.873 I print_info: ssm_d_state      = 0
0.00.115.874 I print_info: ssm_dt_rank      = 0
0.00.115.874 I print_info: ssm_dt_b_c_rms   = 0
0.00.115.874 I print_info: model type       = 1.4B
0.00.115.874 I print_info: model params     = 1.41 B
0.00.115.874 I print_info: general.name     = 1.4B
0.00.115.875 I print_info: vocab type       = BPE
0.00.115.875 I print_info: n_vocab          = 50304
0.00.115.875 I print_info: n_merges         = 50009
0.00.115.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.115.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.115.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.115.880 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.115.881 I print_info: LF token         = 128 'Ä'
0.00.115.881 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.115.881 I print_info: max token length = 1024
0.00.118.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.118.449 I load_tensors: offloading output layer to GPU
0.00.118.449 I load_tensors: offloaded 25/25 layers to GPU
0.00.118.468 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.118.469 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.118.769 I llama_init_from_model: n_seq_max     = 1
0.00.118.770 I llama_init_from_model: n_ctx         = 2048
0.00.118.770 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.118.770 I llama_init_from_model: n_batch       = 2048
0.00.118.771 I llama_init_from_model: n_ubatch      = 512
0.00.118.771 I llama_init_from_model: flash_attn    = 0
0.00.118.771 I llama_init_from_model: freq_base     = 10000.0
0.00.118.771 I llama_init_from_model: freq_scale    = 1
0.00.118.772 I ggml_metal_init: allocating
0.00.118.775 I ggml_metal_init: found device: Apple M4
0.00.118.777 I ggml_metal_init: picking default device: Apple M4
0.00.119.498 I ggml_metal_init: using embedded metal library
0.00.141.546 I ggml_metal_init: GPU name:   Apple M4
0.00.141.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.141.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.141.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.141.549 I ggml_metal_init: simdgroup reduction   = true
0.00.141.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.141.550 I ggml_metal_init: has bfloat            = true
0.00.141.550 I ggml_metal_init: use bfloat            = true
0.00.141.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.141.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.198.934 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.220.540 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.220.546 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.220.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.221.534 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.221.535 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.221.535 I llama_init_from_model: graph nodes  = 967
0.00.221.536 I llama_init_from_model: graph splits = 2
0.00.221.539 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.221.673 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.221.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.313.760 I main: llama threadpool init, n_threads = 4
0.00.313.794 I 
0.00.313.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.313.830 I 
0.00.313.891 I sampler seed: 1234
0.00.313.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.313.919 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.313.921 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.313.921 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.141.332 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.02.141.332 I llama_perf_context_print:        load time =     265.76 ms
0.02.141.335 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.79 tokens per second)
0.02.141.335 I llama_perf_context_print:        eval time =    1781.02 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.141.336 I llama_perf_context_print:       total time =    1828.57 ms /    70 tokens
0.02.141.567 I ggml_metal_free: deallocating

real	0m2.471s
user	0m0.148s
sys	0m0.106s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.289 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.634 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.635 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.635 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.636 I llama_model_loader: - type  f32:  194 tensors
0.00.034.636 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.637 I print_info: file format = GGUF V3 (latest)
0.00.034.638 I print_info: file type   = Q8_0
0.00.034.641 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.389 I load: special tokens cache size = 25
0.00.061.474 I load: token to piece cache size = 0.2984 MB
0.00.061.479 I print_info: arch             = gptneox
0.00.061.479 I print_info: vocab_only       = 0
0.00.061.479 I print_info: n_ctx_train      = 2048
0.00.061.481 I print_info: n_embd           = 2048
0.00.061.482 I print_info: n_layer          = 24
0.00.061.487 I print_info: n_head           = 16
0.00.061.488 I print_info: n_head_kv        = 16
0.00.061.488 I print_info: n_rot            = 32
0.00.061.488 I print_info: n_swa            = 0
0.00.061.489 I print_info: n_embd_head_k    = 128
0.00.061.489 I print_info: n_embd_head_v    = 128
0.00.061.489 I print_info: n_gqa            = 1
0.00.061.491 I print_info: n_embd_k_gqa     = 2048
0.00.061.491 I print_info: n_embd_v_gqa     = 2048
0.00.061.492 I print_info: f_norm_eps       = 1.0e-05
0.00.061.492 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.492 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.493 I print_info: f_logit_scale    = 0.0e+00
0.00.061.494 I print_info: n_ff             = 8192
0.00.061.494 I print_info: n_expert         = 0
0.00.061.494 I print_info: n_expert_used    = 0
0.00.061.494 I print_info: causal attn      = 1
0.00.061.494 I print_info: pooling type     = 0
0.00.061.494 I print_info: rope type        = 2
0.00.061.495 I print_info: rope scaling     = linear
0.00.061.495 I print_info: freq_base_train  = 10000.0
0.00.061.495 I print_info: freq_scale_train = 1
0.00.061.495 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.496 I print_info: rope_finetuned   = unknown
0.00.061.496 I print_info: ssm_d_conv       = 0
0.00.061.496 I print_info: ssm_d_inner      = 0
0.00.061.496 I print_info: ssm_d_state      = 0
0.00.061.496 I print_info: ssm_dt_rank      = 0
0.00.061.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.497 I print_info: model type       = 1.4B
0.00.061.497 I print_info: model params     = 1.41 B
0.00.061.497 I print_info: general.name     = 1.4B
0.00.061.498 I print_info: vocab type       = BPE
0.00.061.498 I print_info: n_vocab          = 50304
0.00.061.498 I print_info: n_merges         = 50009
0.00.061.499 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.499 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.499 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.500 I print_info: LF token         = 128 'Ä'
0.00.061.500 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.500 I print_info: max token length = 1024
0.00.063.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.866 I load_tensors: offloading output layer to GPU
0.00.063.867 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.879 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.880 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.211 I llama_init_from_model: n_seq_max     = 1
0.00.064.212 I llama_init_from_model: n_ctx         = 2048
0.00.064.212 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.212 I llama_init_from_model: n_batch       = 2048
0.00.064.212 I llama_init_from_model: n_ubatch      = 512
0.00.064.212 I llama_init_from_model: flash_attn    = 0
0.00.064.213 I llama_init_from_model: freq_base     = 10000.0
0.00.064.213 I llama_init_from_model: freq_scale    = 1
0.00.064.214 I ggml_metal_init: allocating
0.00.064.218 I ggml_metal_init: found device: Apple M4
0.00.064.220 I ggml_metal_init: picking default device: Apple M4
0.00.064.943 I ggml_metal_init: using embedded metal library
0.00.067.533 I ggml_metal_init: GPU name:   Apple M4
0.00.067.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.536 I ggml_metal_init: simdgroup reduction   = true
0.00.067.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.536 I ggml_metal_init: has bfloat            = true
0.00.067.536 I ggml_metal_init: use bfloat            = true
0.00.067.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.288 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.805 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.915 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.103.917 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.103.918 I llama_init_from_model: graph nodes  = 967
0.00.103.918 I llama_init_from_model: graph splits = 2
0.00.103.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.365.713 I main: llama threadpool init, n_threads = 4
0.01.365.746 I 
0.01.365.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.365.770 I 
0.01.365.994 I sampler seed: 1234
0.01.365.999 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.366.021 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.366.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.366.023 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.453.232 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.02.453.232 I llama_perf_context_print:        load time =    1354.95 ms
0.02.453.234 I llama_perf_context_print: prompt eval time =      39.87 ms /     7 tokens (    5.70 ms per token,   175.59 tokens per second)
0.02.453.235 I llama_perf_context_print:        eval time =    1044.51 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.453.236 I llama_perf_context_print:       total time =    1088.40 ms /    70 tokens
0.02.453.439 I ggml_metal_free: deallocating

real	0m2.483s
user	0m0.115s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.066 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.122 I main: llama backend init
0.00.000.124 I main: load the model and apply lora adapter, if any
0.00.068.334 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.139.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.139.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.139.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.139.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.139.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.139.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.139.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.139.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.139.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.139.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.139.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.139.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.139.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.139.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.139.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.139.556 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.139.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.146.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.148.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.155.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.155.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.155.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.155.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.155.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.155.749 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.155.750 I llama_model_loader: - type  f32:  194 tensors
0.00.155.751 I llama_model_loader: - type q4_0:   97 tensors
0.00.155.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.155.757 I print_info: file format = GGUF V3 (latest)
0.00.155.757 I print_info: file type   = Q4_0
0.00.155.760 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.179.751 I load: special tokens cache size = 25
0.00.185.895 I load: token to piece cache size = 0.2984 MB
0.00.185.899 I print_info: arch             = gptneox
0.00.185.900 I print_info: vocab_only       = 0
0.00.185.900 I print_info: n_ctx_train      = 2048
0.00.185.900 I print_info: n_embd           = 2048
0.00.185.900 I print_info: n_layer          = 24
0.00.185.905 I print_info: n_head           = 16
0.00.185.908 I print_info: n_head_kv        = 16
0.00.185.908 I print_info: n_rot            = 32
0.00.185.908 I print_info: n_swa            = 0
0.00.185.908 I print_info: n_embd_head_k    = 128
0.00.185.908 I print_info: n_embd_head_v    = 128
0.00.185.909 I print_info: n_gqa            = 1
0.00.185.910 I print_info: n_embd_k_gqa     = 2048
0.00.185.910 I print_info: n_embd_v_gqa     = 2048
0.00.185.911 I print_info: f_norm_eps       = 1.0e-05
0.00.185.911 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.185.911 I print_info: f_clamp_kqv      = 0.0e+00
0.00.185.911 I print_info: f_max_alibi_bias = 0.0e+00
0.00.185.911 I print_info: f_logit_scale    = 0.0e+00
0.00.185.912 I print_info: n_ff             = 8192
0.00.185.912 I print_info: n_expert         = 0
0.00.185.912 I print_info: n_expert_used    = 0
0.00.185.913 I print_info: causal attn      = 1
0.00.185.913 I print_info: pooling type     = 0
0.00.185.913 I print_info: rope type        = 2
0.00.185.913 I print_info: rope scaling     = linear
0.00.185.914 I print_info: freq_base_train  = 10000.0
0.00.185.914 I print_info: freq_scale_train = 1
0.00.185.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.185.914 I print_info: rope_finetuned   = unknown
0.00.185.915 I print_info: ssm_d_conv       = 0
0.00.185.915 I print_info: ssm_d_inner      = 0
0.00.185.915 I print_info: ssm_d_state      = 0
0.00.185.915 I print_info: ssm_dt_rank      = 0
0.00.185.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.185.915 I print_info: model type       = 1.4B
0.00.185.916 I print_info: model params     = 1.41 B
0.00.185.916 I print_info: general.name     = 1.4B
0.00.185.916 I print_info: vocab type       = BPE
0.00.185.916 I print_info: n_vocab          = 50304
0.00.185.917 I print_info: n_merges         = 50009
0.00.185.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.185.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.185.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.185.917 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.185.918 I print_info: LF token         = 128 'Ä'
0.00.185.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.185.918 I print_info: max token length = 1024
0.00.187.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.187.913 I load_tensors: offloading output layer to GPU
0.00.187.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.187.925 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.187.926 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.188.210 I llama_init_from_model: n_seq_max     = 1
0.00.188.211 I llama_init_from_model: n_ctx         = 2048
0.00.188.211 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.188.211 I llama_init_from_model: n_batch       = 2048
0.00.188.211 I llama_init_from_model: n_ubatch      = 512
0.00.188.212 I llama_init_from_model: flash_attn    = 0
0.00.188.212 I llama_init_from_model: freq_base     = 10000.0
0.00.188.212 I llama_init_from_model: freq_scale    = 1
0.00.188.213 I ggml_metal_init: allocating
0.00.188.216 I ggml_metal_init: found device: Apple M4
0.00.188.218 I ggml_metal_init: picking default device: Apple M4
0.00.188.906 I ggml_metal_init: using embedded metal library
0.00.191.379 I ggml_metal_init: GPU name:   Apple M4
0.00.191.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.191.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.191.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.191.382 I ggml_metal_init: simdgroup reduction   = true
0.00.191.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.191.382 I ggml_metal_init: has bfloat            = true
0.00.191.382 I ggml_metal_init: use bfloat            = true
0.00.191.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.191.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.202.084 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.222.108 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.222.117 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.222.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.223.042 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.223.044 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.223.044 I llama_init_from_model: graph nodes  = 967
0.00.223.044 I llama_init_from_model: graph splits = 2
0.00.223.047 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.223.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.223.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.391.184 I main: llama threadpool init, n_threads = 4
0.01.391.231 I 
0.01.391.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.391.267 I 
0.01.391.551 I sampler seed: 1234
0.01.391.557 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.391.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.391.585 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.391.585 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.02.072.513 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.02.072.514 I llama_perf_context_print:        load time =    1321.65 ms
0.02.072.517 I llama_perf_context_print: prompt eval time =      49.83 ms /     7 tokens (    7.12 ms per token,   140.49 tokens per second)
0.02.072.518 I llama_perf_context_print:        eval time =     628.22 ms /    63 runs   (    9.97 ms per token,   100.28 tokens per second)
0.02.072.520 I llama_perf_context_print:       total time =     682.52 ms /    70 tokens
0.02.072.778 I ggml_metal_free: deallocating

real	0m2.128s
user	0m0.135s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.068 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.474 I llama_model_loader: - type  f32:  194 tensors
0.00.026.474 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.475 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.475 I print_info: file format = GGUF V3 (latest)
0.00.026.476 I print_info: file type   = Q4_1
0.00.026.480 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.954 I load: special tokens cache size = 25
0.00.051.037 I load: token to piece cache size = 0.2984 MB
0.00.051.041 I print_info: arch             = gptneox
0.00.051.041 I print_info: vocab_only       = 0
0.00.051.041 I print_info: n_ctx_train      = 2048
0.00.051.041 I print_info: n_embd           = 2048
0.00.051.041 I print_info: n_layer          = 24
0.00.051.045 I print_info: n_head           = 16
0.00.051.046 I print_info: n_head_kv        = 16
0.00.051.046 I print_info: n_rot            = 32
0.00.051.046 I print_info: n_swa            = 0
0.00.051.047 I print_info: n_embd_head_k    = 128
0.00.051.047 I print_info: n_embd_head_v    = 128
0.00.051.047 I print_info: n_gqa            = 1
0.00.051.048 I print_info: n_embd_k_gqa     = 2048
0.00.051.049 I print_info: n_embd_v_gqa     = 2048
0.00.051.049 I print_info: f_norm_eps       = 1.0e-05
0.00.051.050 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.052 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.052 I print_info: f_logit_scale    = 0.0e+00
0.00.051.053 I print_info: n_ff             = 8192
0.00.051.053 I print_info: n_expert         = 0
0.00.051.053 I print_info: n_expert_used    = 0
0.00.051.054 I print_info: causal attn      = 1
0.00.051.054 I print_info: pooling type     = 0
0.00.051.055 I print_info: rope type        = 2
0.00.051.055 I print_info: rope scaling     = linear
0.00.051.056 I print_info: freq_base_train  = 10000.0
0.00.051.056 I print_info: freq_scale_train = 1
0.00.051.056 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.057 I print_info: rope_finetuned   = unknown
0.00.051.057 I print_info: ssm_d_conv       = 0
0.00.051.057 I print_info: ssm_d_inner      = 0
0.00.051.057 I print_info: ssm_d_state      = 0
0.00.051.057 I print_info: ssm_dt_rank      = 0
0.00.051.057 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.058 I print_info: model type       = 1.4B
0.00.051.058 I print_info: model params     = 1.41 B
0.00.051.058 I print_info: general.name     = 1.4B
0.00.051.059 I print_info: vocab type       = BPE
0.00.051.059 I print_info: n_vocab          = 50304
0.00.051.059 I print_info: n_merges         = 50009
0.00.051.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: LF token         = 128 'Ä'
0.00.051.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.061 I print_info: max token length = 1024
0.00.052.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.967 I load_tensors: offloading output layer to GPU
0.00.052.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.978 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.980 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.279 I llama_init_from_model: n_seq_max     = 1
0.00.053.280 I llama_init_from_model: n_ctx         = 2048
0.00.053.280 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.280 I llama_init_from_model: n_batch       = 2048
0.00.053.280 I llama_init_from_model: n_ubatch      = 512
0.00.053.281 I llama_init_from_model: flash_attn    = 0
0.00.053.281 I llama_init_from_model: freq_base     = 10000.0
0.00.053.281 I llama_init_from_model: freq_scale    = 1
0.00.053.282 I ggml_metal_init: allocating
0.00.053.285 I ggml_metal_init: found device: Apple M4
0.00.053.287 I ggml_metal_init: picking default device: Apple M4
0.00.053.882 I ggml_metal_init: using embedded metal library
0.00.056.207 I ggml_metal_init: GPU name:   Apple M4
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.209 I ggml_metal_init: simdgroup reduction   = true
0.00.056.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.209 I ggml_metal_init: has bfloat            = true
0.00.056.209 I ggml_metal_init: use bfloat            = true
0.00.056.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.041 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.741 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.746 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.770 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.772 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.772 I llama_init_from_model: graph nodes  = 967
0.00.087.773 I llama_init_from_model: graph splits = 2
0.00.087.776 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.321 I main: llama threadpool init, n_threads = 4
0.00.758.360 I 
0.00.758.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.384 I 
0.00.758.599 I sampler seed: 1234
0.00.758.603 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.614 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.614 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.614 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.478.425 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65317.39 tokens per second)
0.01.478.426 I llama_perf_context_print:        load time =     747.39 ms
0.01.478.427 I llama_perf_context_print: prompt eval time =      42.65 ms /     7 tokens (    6.09 ms per token,   164.12 tokens per second)
0.01.478.427 I llama_perf_context_print:        eval time =     674.32 ms /    63 runs   (   10.70 ms per token,    93.43 tokens per second)
0.01.478.428 I llama_perf_context_print:       total time =     720.96 ms /    70 tokens
0.01.478.633 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.108s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.990 I llama_model_loader: - type  f32:  194 tensors
0.00.024.990 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.991 I print_info: file format = GGUF V3 (latest)
0.00.024.991 I print_info: file type   = Q5_0
0.00.024.992 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.361 I load: special tokens cache size = 25
0.00.049.316 I load: token to piece cache size = 0.2984 MB
0.00.049.319 I print_info: arch             = gptneox
0.00.049.319 I print_info: vocab_only       = 0
0.00.049.320 I print_info: n_ctx_train      = 2048
0.00.049.320 I print_info: n_embd           = 2048
0.00.049.320 I print_info: n_layer          = 24
0.00.049.323 I print_info: n_head           = 16
0.00.049.324 I print_info: n_head_kv        = 16
0.00.049.324 I print_info: n_rot            = 32
0.00.049.324 I print_info: n_swa            = 0
0.00.049.326 I print_info: n_embd_head_k    = 128
0.00.049.326 I print_info: n_embd_head_v    = 128
0.00.049.327 I print_info: n_gqa            = 1
0.00.049.328 I print_info: n_embd_k_gqa     = 2048
0.00.049.329 I print_info: n_embd_v_gqa     = 2048
0.00.049.329 I print_info: f_norm_eps       = 1.0e-05
0.00.049.330 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.330 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.330 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.330 I print_info: f_logit_scale    = 0.0e+00
0.00.049.331 I print_info: n_ff             = 8192
0.00.049.331 I print_info: n_expert         = 0
0.00.049.331 I print_info: n_expert_used    = 0
0.00.049.331 I print_info: causal attn      = 1
0.00.049.331 I print_info: pooling type     = 0
0.00.049.331 I print_info: rope type        = 2
0.00.049.332 I print_info: rope scaling     = linear
0.00.049.332 I print_info: freq_base_train  = 10000.0
0.00.049.332 I print_info: freq_scale_train = 1
0.00.049.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.337 I print_info: rope_finetuned   = unknown
0.00.049.337 I print_info: ssm_d_conv       = 0
0.00.049.337 I print_info: ssm_d_inner      = 0
0.00.049.337 I print_info: ssm_d_state      = 0
0.00.049.337 I print_info: ssm_dt_rank      = 0
0.00.049.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.339 I print_info: model type       = 1.4B
0.00.049.339 I print_info: model params     = 1.41 B
0.00.049.339 I print_info: general.name     = 1.4B
0.00.049.340 I print_info: vocab type       = BPE
0.00.049.340 I print_info: n_vocab          = 50304
0.00.049.340 I print_info: n_merges         = 50009
0.00.049.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.342 I print_info: LF token         = 128 'Ä'
0.00.049.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.342 I print_info: max token length = 1024
0.00.050.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.894 I load_tensors: offloading output layer to GPU
0.00.050.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.905 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.050.906 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.191 I llama_init_from_model: n_seq_max     = 1
0.00.051.192 I llama_init_from_model: n_ctx         = 2048
0.00.051.192 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.193 I llama_init_from_model: n_batch       = 2048
0.00.051.193 I llama_init_from_model: n_ubatch      = 512
0.00.051.193 I llama_init_from_model: flash_attn    = 0
0.00.051.193 I llama_init_from_model: freq_base     = 10000.0
0.00.051.194 I llama_init_from_model: freq_scale    = 1
0.00.051.194 I ggml_metal_init: allocating
0.00.051.197 I ggml_metal_init: found device: Apple M4
0.00.051.199 I ggml_metal_init: picking default device: Apple M4
0.00.051.834 I ggml_metal_init: using embedded metal library
0.00.054.468 I ggml_metal_init: GPU name:   Apple M4
0.00.054.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.471 I ggml_metal_init: simdgroup reduction   = true
0.00.054.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.471 I ggml_metal_init: has bfloat            = true
0.00.054.472 I ggml_metal_init: use bfloat            = true
0.00.054.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.864 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.843 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.845 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.845 I llama_init_from_model: graph nodes  = 967
0.00.084.845 I llama_init_from_model: graph splits = 2
0.00.084.847 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.163 I main: llama threadpool init, n_threads = 4
0.00.788.201 I 
0.00.788.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.239 I 
0.00.788.460 I sampler seed: 1234
0.00.788.465 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.476 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.476 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.478 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.573.098 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.573.099 I llama_perf_context_print:        load time =     778.50 ms
0.01.573.099 I llama_perf_context_print: prompt eval time =      47.61 ms /     7 tokens (    6.80 ms per token,   147.03 tokens per second)
0.01.573.100 I llama_perf_context_print:        eval time =     733.99 ms /    63 runs   (   11.65 ms per token,    85.83 tokens per second)
0.01.573.103 I llama_perf_context_print:       total time =     785.83 ms /    70 tokens
0.01.573.365 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.668 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.129 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.129 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.131 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.131 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.660 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.661 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.662 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.662 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.662 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.662 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.663 I llama_model_loader: - type  f32:  194 tensors
0.00.026.663 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.663 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.664 I print_info: file format = GGUF V3 (latest)
0.00.026.664 I print_info: file type   = Q5_1
0.00.026.665 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.125 I load: special tokens cache size = 25
0.00.051.180 I load: token to piece cache size = 0.2984 MB
0.00.051.183 I print_info: arch             = gptneox
0.00.051.183 I print_info: vocab_only       = 0
0.00.051.184 I print_info: n_ctx_train      = 2048
0.00.051.184 I print_info: n_embd           = 2048
0.00.051.184 I print_info: n_layer          = 24
0.00.051.187 I print_info: n_head           = 16
0.00.051.188 I print_info: n_head_kv        = 16
0.00.051.188 I print_info: n_rot            = 32
0.00.051.188 I print_info: n_swa            = 0
0.00.051.188 I print_info: n_embd_head_k    = 128
0.00.051.188 I print_info: n_embd_head_v    = 128
0.00.051.189 I print_info: n_gqa            = 1
0.00.051.190 I print_info: n_embd_k_gqa     = 2048
0.00.051.191 I print_info: n_embd_v_gqa     = 2048
0.00.051.191 I print_info: f_norm_eps       = 1.0e-05
0.00.051.191 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.195 I print_info: f_logit_scale    = 0.0e+00
0.00.051.195 I print_info: n_ff             = 8192
0.00.051.195 I print_info: n_expert         = 0
0.00.051.195 I print_info: n_expert_used    = 0
0.00.051.196 I print_info: causal attn      = 1
0.00.051.196 I print_info: pooling type     = 0
0.00.051.196 I print_info: rope type        = 2
0.00.051.196 I print_info: rope scaling     = linear
0.00.051.197 I print_info: freq_base_train  = 10000.0
0.00.051.197 I print_info: freq_scale_train = 1
0.00.051.197 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.198 I print_info: rope_finetuned   = unknown
0.00.051.199 I print_info: ssm_d_conv       = 0
0.00.051.199 I print_info: ssm_d_inner      = 0
0.00.051.199 I print_info: ssm_d_state      = 0
0.00.051.199 I print_info: ssm_dt_rank      = 0
0.00.051.199 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.199 I print_info: model type       = 1.4B
0.00.051.200 I print_info: model params     = 1.41 B
0.00.051.200 I print_info: general.name     = 1.4B
0.00.051.200 I print_info: vocab type       = BPE
0.00.051.200 I print_info: n_vocab          = 50304
0.00.051.201 I print_info: n_merges         = 50009
0.00.051.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.201 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.201 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.201 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.202 I print_info: LF token         = 128 'Ä'
0.00.051.202 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.202 I print_info: max token length = 1024
0.00.053.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.160 I load_tensors: offloading output layer to GPU
0.00.053.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.171 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.172 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.457 I llama_init_from_model: n_seq_max     = 1
0.00.053.458 I llama_init_from_model: n_ctx         = 2048
0.00.053.458 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.459 I llama_init_from_model: n_batch       = 2048
0.00.053.459 I llama_init_from_model: n_ubatch      = 512
0.00.053.459 I llama_init_from_model: flash_attn    = 0
0.00.053.459 I llama_init_from_model: freq_base     = 10000.0
0.00.053.460 I llama_init_from_model: freq_scale    = 1
0.00.053.460 I ggml_metal_init: allocating
0.00.053.463 I ggml_metal_init: found device: Apple M4
0.00.053.465 I ggml_metal_init: picking default device: Apple M4
0.00.054.036 I ggml_metal_init: using embedded metal library
0.00.056.396 I ggml_metal_init: GPU name:   Apple M4
0.00.056.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.399 I ggml_metal_init: simdgroup reduction   = true
0.00.056.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.399 I ggml_metal_init: has bfloat            = true
0.00.056.399 I ggml_metal_init: use bfloat            = true
0.00.056.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.932 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.954 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.143 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.145 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.145 I llama_init_from_model: graph nodes  = 967
0.00.088.145 I llama_init_from_model: graph splits = 2
0.00.088.148 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.365 I main: llama threadpool init, n_threads = 4
0.00.728.406 I 
0.00.728.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.431 I 
0.00.728.650 I sampler seed: 1234
0.00.728.656 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.683 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.684 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.564.756 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.564.757 I llama_perf_context_print:        load time =     716.81 ms
0.01.564.758 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.69 tokens per second)
0.01.564.758 I llama_perf_context_print:        eval time =     790.74 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.564.759 I llama_perf_context_print:       total time =     837.28 ms /    70 tokens
0.01.565.011 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.109s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.336 I llama_model_loader: - type  f32:  194 tensors
0.00.025.337 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.337 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.338 I print_info: file format = GGUF V3 (latest)
0.00.025.338 I print_info: file type   = Q2_K - Medium
0.00.025.339 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.827 I load: special tokens cache size = 25
0.00.049.906 I load: token to piece cache size = 0.2984 MB
0.00.049.915 I print_info: arch             = gptneox
0.00.049.917 I print_info: vocab_only       = 0
0.00.049.917 I print_info: n_ctx_train      = 2048
0.00.049.917 I print_info: n_embd           = 2048
0.00.049.918 I print_info: n_layer          = 24
0.00.049.925 I print_info: n_head           = 16
0.00.049.926 I print_info: n_head_kv        = 16
0.00.049.927 I print_info: n_rot            = 32
0.00.049.927 I print_info: n_swa            = 0
0.00.049.927 I print_info: n_embd_head_k    = 128
0.00.049.927 I print_info: n_embd_head_v    = 128
0.00.049.928 I print_info: n_gqa            = 1
0.00.049.930 I print_info: n_embd_k_gqa     = 2048
0.00.049.931 I print_info: n_embd_v_gqa     = 2048
0.00.049.932 I print_info: f_norm_eps       = 1.0e-05
0.00.049.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.933 I print_info: f_logit_scale    = 0.0e+00
0.00.049.934 I print_info: n_ff             = 8192
0.00.049.934 I print_info: n_expert         = 0
0.00.049.934 I print_info: n_expert_used    = 0
0.00.049.934 I print_info: causal attn      = 1
0.00.049.934 I print_info: pooling type     = 0
0.00.049.934 I print_info: rope type        = 2
0.00.049.935 I print_info: rope scaling     = linear
0.00.049.935 I print_info: freq_base_train  = 10000.0
0.00.049.935 I print_info: freq_scale_train = 1
0.00.049.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.936 I print_info: rope_finetuned   = unknown
0.00.049.936 I print_info: ssm_d_conv       = 0
0.00.049.936 I print_info: ssm_d_inner      = 0
0.00.049.936 I print_info: ssm_d_state      = 0
0.00.049.936 I print_info: ssm_dt_rank      = 0
0.00.049.936 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.936 I print_info: model type       = 1.4B
0.00.049.937 I print_info: model params     = 1.41 B
0.00.049.937 I print_info: general.name     = 1.4B
0.00.049.937 I print_info: vocab type       = BPE
0.00.049.938 I print_info: n_vocab          = 50304
0.00.049.938 I print_info: n_merges         = 50009
0.00.049.938 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.938 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.939 I print_info: LF token         = 128 'Ä'
0.00.049.940 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: max token length = 1024
0.00.051.666 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.666 I load_tensors: offloading output layer to GPU
0.00.051.666 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.671 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.672 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.937 I llama_init_from_model: n_seq_max     = 1
0.00.051.938 I llama_init_from_model: n_ctx         = 2048
0.00.051.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.938 I llama_init_from_model: n_batch       = 2048
0.00.051.938 I llama_init_from_model: n_ubatch      = 512
0.00.051.938 I llama_init_from_model: flash_attn    = 0
0.00.051.939 I llama_init_from_model: freq_base     = 10000.0
0.00.051.939 I llama_init_from_model: freq_scale    = 1
0.00.051.939 I ggml_metal_init: allocating
0.00.051.942 I ggml_metal_init: found device: Apple M4
0.00.051.944 I ggml_metal_init: picking default device: Apple M4
0.00.052.526 I ggml_metal_init: using embedded metal library
0.00.054.853 I ggml_metal_init: GPU name:   Apple M4
0.00.054.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.856 I ggml_metal_init: simdgroup reduction   = true
0.00.054.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.856 I ggml_metal_init: has bfloat            = true
0.00.054.856 I ggml_metal_init: use bfloat            = true
0.00.054.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.044 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.841 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.860 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.958 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.959 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.959 I llama_init_from_model: graph nodes  = 967
0.00.086.959 I llama_init_from_model: graph splits = 2
0.00.086.962 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.847 I main: llama threadpool init, n_threads = 4
0.00.440.885 I 
0.00.440.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.908 I 
0.00.441.134 I sampler seed: 1234
0.00.441.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.180 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.132 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.116.133 I llama_perf_context_print:        load time =     430.09 ms
0.01.116.133 I llama_perf_context_print: prompt eval time =      35.83 ms /     7 tokens (    5.12 ms per token,   195.36 tokens per second)
0.01.116.134 I llama_perf_context_print:        eval time =     636.02 ms /    63 runs   (   10.10 ms per token,    99.05 tokens per second)
0.01.116.135 I llama_perf_context_print:       total time =     676.16 ms /    70 tokens
0.01.116.372 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.108s
sys	0m0.106s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.644 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.332 I llama_model_loader: - type  f32:  194 tensors
0.00.027.332 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.332 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.332 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.333 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.333 I print_info: file format = GGUF V3 (latest)
0.00.027.334 I print_info: file type   = Q3_K - Medium
0.00.027.334 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.926 I load: special tokens cache size = 25
0.00.052.187 I load: token to piece cache size = 0.2984 MB
0.00.052.190 I print_info: arch             = gptneox
0.00.052.190 I print_info: vocab_only       = 0
0.00.052.191 I print_info: n_ctx_train      = 2048
0.00.052.191 I print_info: n_embd           = 2048
0.00.052.191 I print_info: n_layer          = 24
0.00.052.194 I print_info: n_head           = 16
0.00.052.195 I print_info: n_head_kv        = 16
0.00.052.195 I print_info: n_rot            = 32
0.00.052.195 I print_info: n_swa            = 0
0.00.052.195 I print_info: n_embd_head_k    = 128
0.00.052.195 I print_info: n_embd_head_v    = 128
0.00.052.196 I print_info: n_gqa            = 1
0.00.052.197 I print_info: n_embd_k_gqa     = 2048
0.00.052.198 I print_info: n_embd_v_gqa     = 2048
0.00.052.198 I print_info: f_norm_eps       = 1.0e-05
0.00.052.199 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.199 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.199 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.199 I print_info: f_logit_scale    = 0.0e+00
0.00.052.200 I print_info: n_ff             = 8192
0.00.052.200 I print_info: n_expert         = 0
0.00.052.200 I print_info: n_expert_used    = 0
0.00.052.200 I print_info: causal attn      = 1
0.00.052.200 I print_info: pooling type     = 0
0.00.052.200 I print_info: rope type        = 2
0.00.052.201 I print_info: rope scaling     = linear
0.00.052.202 I print_info: freq_base_train  = 10000.0
0.00.052.202 I print_info: freq_scale_train = 1
0.00.052.203 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.203 I print_info: rope_finetuned   = unknown
0.00.052.203 I print_info: ssm_d_conv       = 0
0.00.052.203 I print_info: ssm_d_inner      = 0
0.00.052.203 I print_info: ssm_d_state      = 0
0.00.052.203 I print_info: ssm_dt_rank      = 0
0.00.052.204 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.204 I print_info: model type       = 1.4B
0.00.052.204 I print_info: model params     = 1.41 B
0.00.052.204 I print_info: general.name     = 1.4B
0.00.052.205 I print_info: vocab type       = BPE
0.00.052.205 I print_info: n_vocab          = 50304
0.00.052.205 I print_info: n_merges         = 50009
0.00.052.206 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: LF token         = 128 'Ä'
0.00.052.207 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.207 I print_info: max token length = 1024
0.00.054.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.135 I load_tensors: offloading output layer to GPU
0.00.054.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.145 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.147 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.494 I llama_init_from_model: n_seq_max     = 1
0.00.054.495 I llama_init_from_model: n_ctx         = 2048
0.00.054.495 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.495 I llama_init_from_model: n_batch       = 2048
0.00.054.496 I llama_init_from_model: n_ubatch      = 512
0.00.054.496 I llama_init_from_model: flash_attn    = 0
0.00.054.496 I llama_init_from_model: freq_base     = 10000.0
0.00.054.496 I llama_init_from_model: freq_scale    = 1
0.00.054.497 I ggml_metal_init: allocating
0.00.054.500 I ggml_metal_init: found device: Apple M4
0.00.054.501 I ggml_metal_init: picking default device: Apple M4
0.00.055.092 I ggml_metal_init: using embedded metal library
0.00.057.435 I ggml_metal_init: GPU name:   Apple M4
0.00.057.437 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.438 I ggml_metal_init: simdgroup reduction   = true
0.00.057.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.438 I ggml_metal_init: has bfloat            = true
0.00.057.438 I ggml_metal_init: use bfloat            = true
0.00.057.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.105 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.788 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.795 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.820 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.810 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.811 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.811 I llama_init_from_model: graph nodes  = 967
0.00.086.812 I llama_init_from_model: graph splits = 2
0.00.086.816 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.353 I main: llama threadpool init, n_threads = 4
0.00.603.390 I 
0.00.603.432 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.433 I 
0.00.603.651 I sampler seed: 1234
0.00.603.655 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.603.684 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.603.685 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.603.686 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.346.909 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62610.23 tokens per second)
0.01.346.910 I llama_perf_context_print:        load time =     591.83 ms
0.01.346.911 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.16 tokens per second)
0.01.346.911 I llama_perf_context_print:        eval time =     696.59 ms /    63 runs   (   11.06 ms per token,    90.44 tokens per second)
0.01.346.912 I llama_perf_context_print:       total time =     744.44 ms /    70 tokens
0.01.347.115 I ggml_metal_free: deallocating

real	0m1.364s
user	0m0.110s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.011.368 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.755 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.763 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.658 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.530 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.531 I llama_model_loader: - type  f32:  194 tensors
0.00.027.531 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.532 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.532 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.532 I print_info: file format = GGUF V3 (latest)
0.00.027.533 I print_info: file type   = Q4_K - Medium
0.00.027.535 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.788 I load: special tokens cache size = 25
0.00.053.030 I load: token to piece cache size = 0.2984 MB
0.00.053.033 I print_info: arch             = gptneox
0.00.053.033 I print_info: vocab_only       = 0
0.00.053.034 I print_info: n_ctx_train      = 2048
0.00.053.034 I print_info: n_embd           = 2048
0.00.053.034 I print_info: n_layer          = 24
0.00.053.037 I print_info: n_head           = 16
0.00.053.038 I print_info: n_head_kv        = 16
0.00.053.038 I print_info: n_rot            = 32
0.00.053.038 I print_info: n_swa            = 0
0.00.053.038 I print_info: n_embd_head_k    = 128
0.00.053.038 I print_info: n_embd_head_v    = 128
0.00.053.039 I print_info: n_gqa            = 1
0.00.053.040 I print_info: n_embd_k_gqa     = 2048
0.00.053.041 I print_info: n_embd_v_gqa     = 2048
0.00.053.041 I print_info: f_norm_eps       = 1.0e-05
0.00.053.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.042 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.042 I print_info: f_logit_scale    = 0.0e+00
0.00.053.043 I print_info: n_ff             = 8192
0.00.053.043 I print_info: n_expert         = 0
0.00.053.043 I print_info: n_expert_used    = 0
0.00.053.043 I print_info: causal attn      = 1
0.00.053.044 I print_info: pooling type     = 0
0.00.053.046 I print_info: rope type        = 2
0.00.053.047 I print_info: rope scaling     = linear
0.00.053.047 I print_info: freq_base_train  = 10000.0
0.00.053.047 I print_info: freq_scale_train = 1
0.00.053.048 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.048 I print_info: rope_finetuned   = unknown
0.00.053.048 I print_info: ssm_d_conv       = 0
0.00.053.048 I print_info: ssm_d_inner      = 0
0.00.053.048 I print_info: ssm_d_state      = 0
0.00.053.048 I print_info: ssm_dt_rank      = 0
0.00.053.048 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.049 I print_info: model type       = 1.4B
0.00.053.049 I print_info: model params     = 1.41 B
0.00.053.049 I print_info: general.name     = 1.4B
0.00.053.050 I print_info: vocab type       = BPE
0.00.053.050 I print_info: n_vocab          = 50304
0.00.053.051 I print_info: n_merges         = 50009
0.00.053.051 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.051 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.051 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.051 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.053 I print_info: LF token         = 128 'Ä'
0.00.053.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.053 I print_info: max token length = 1024
0.00.054.865 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.866 I load_tensors: offloading output layer to GPU
0.00.054.866 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.872 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.872 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.249 I llama_init_from_model: n_seq_max     = 1
0.00.055.249 I llama_init_from_model: n_ctx         = 2048
0.00.055.250 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.250 I llama_init_from_model: n_batch       = 2048
0.00.055.250 I llama_init_from_model: n_ubatch      = 512
0.00.055.250 I llama_init_from_model: flash_attn    = 0
0.00.055.250 I llama_init_from_model: freq_base     = 10000.0
0.00.055.251 I llama_init_from_model: freq_scale    = 1
0.00.055.251 I ggml_metal_init: allocating
0.00.055.254 I ggml_metal_init: found device: Apple M4
0.00.055.256 I ggml_metal_init: picking default device: Apple M4
0.00.055.858 I ggml_metal_init: using embedded metal library
0.00.058.225 I ggml_metal_init: GPU name:   Apple M4
0.00.058.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.228 I ggml_metal_init: simdgroup reduction   = true
0.00.058.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.228 I ggml_metal_init: has bfloat            = true
0.00.058.228 I ggml_metal_init: use bfloat            = true
0.00.058.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.096 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.125 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.132 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.151 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.243 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.244 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.244 I llama_init_from_model: graph nodes  = 967
0.00.088.245 I llama_init_from_model: graph splits = 2
0.00.088.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.502 I main: llama threadpool init, n_threads = 4
0.00.592.545 I 
0.00.592.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.565 I 
0.00.592.776 I sampler seed: 1234
0.00.592.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.592.825 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.592.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.592.828 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.356.390 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46864.69 tokens per second)
0.01.356.393 I llama_perf_context_print:        load time =     580.24 ms
0.01.356.394 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.55 tokens per second)
0.01.356.400 I llama_perf_context_print:        eval time =     713.87 ms /    63 runs   (   11.33 ms per token,    88.25 tokens per second)
0.01.356.401 I llama_perf_context_print:       total time =     764.78 ms /    70 tokens
0.01.356.672 I ggml_metal_free: deallocating

real	0m1.374s
user	0m0.110s
sys	0m0.114s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.734 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.735 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.882 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.047 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.049 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.050 I llama_model_loader: - type  f32:  194 tensors
0.00.027.050 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.050 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.051 I print_info: file format = GGUF V3 (latest)
0.00.027.052 I print_info: file type   = Q5_K - Medium
0.00.027.053 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.579 I load: special tokens cache size = 25
0.00.053.867 I load: token to piece cache size = 0.2984 MB
0.00.053.871 I print_info: arch             = gptneox
0.00.053.872 I print_info: vocab_only       = 0
0.00.053.872 I print_info: n_ctx_train      = 2048
0.00.053.872 I print_info: n_embd           = 2048
0.00.053.872 I print_info: n_layer          = 24
0.00.053.876 I print_info: n_head           = 16
0.00.053.877 I print_info: n_head_kv        = 16
0.00.053.877 I print_info: n_rot            = 32
0.00.053.878 I print_info: n_swa            = 0
0.00.053.878 I print_info: n_embd_head_k    = 128
0.00.053.878 I print_info: n_embd_head_v    = 128
0.00.053.879 I print_info: n_gqa            = 1
0.00.053.879 I print_info: n_embd_k_gqa     = 2048
0.00.053.880 I print_info: n_embd_v_gqa     = 2048
0.00.053.880 I print_info: f_norm_eps       = 1.0e-05
0.00.053.881 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.881 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.882 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.885 I print_info: f_logit_scale    = 0.0e+00
0.00.053.885 I print_info: n_ff             = 8192
0.00.053.885 I print_info: n_expert         = 0
0.00.053.885 I print_info: n_expert_used    = 0
0.00.053.885 I print_info: causal attn      = 1
0.00.053.886 I print_info: pooling type     = 0
0.00.053.886 I print_info: rope type        = 2
0.00.053.886 I print_info: rope scaling     = linear
0.00.053.886 I print_info: freq_base_train  = 10000.0
0.00.053.888 I print_info: freq_scale_train = 1
0.00.053.888 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.888 I print_info: rope_finetuned   = unknown
0.00.053.888 I print_info: ssm_d_conv       = 0
0.00.053.888 I print_info: ssm_d_inner      = 0
0.00.053.889 I print_info: ssm_d_state      = 0
0.00.053.889 I print_info: ssm_dt_rank      = 0
0.00.053.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.889 I print_info: model type       = 1.4B
0.00.053.889 I print_info: model params     = 1.41 B
0.00.053.889 I print_info: general.name     = 1.4B
0.00.053.890 I print_info: vocab type       = BPE
0.00.053.890 I print_info: n_vocab          = 50304
0.00.053.890 I print_info: n_merges         = 50009
0.00.053.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.891 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.891 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.891 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.891 I print_info: LF token         = 128 'Ä'
0.00.053.892 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.892 I print_info: max token length = 1024
0.00.055.925 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.925 I load_tensors: offloading output layer to GPU
0.00.055.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.956 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.958 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.255 I llama_init_from_model: n_seq_max     = 1
0.00.056.256 I llama_init_from_model: n_ctx         = 2048
0.00.056.256 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.256 I llama_init_from_model: n_batch       = 2048
0.00.056.257 I llama_init_from_model: n_ubatch      = 512
0.00.056.257 I llama_init_from_model: flash_attn    = 0
0.00.056.257 I llama_init_from_model: freq_base     = 10000.0
0.00.056.257 I llama_init_from_model: freq_scale    = 1
0.00.056.258 I ggml_metal_init: allocating
0.00.056.261 I ggml_metal_init: found device: Apple M4
0.00.056.263 I ggml_metal_init: picking default device: Apple M4
0.00.056.933 I ggml_metal_init: using embedded metal library
0.00.060.916 I ggml_metal_init: GPU name:   Apple M4
0.00.060.918 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.919 I ggml_metal_init: simdgroup reduction   = true
0.00.060.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.919 I ggml_metal_init: has bfloat            = true
0.00.060.919 I ggml_metal_init: use bfloat            = true
0.00.060.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.004 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.122 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.122 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.123 I llama_init_from_model: graph nodes  = 967
0.00.090.124 I llama_init_from_model: graph splits = 2
0.00.090.129 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.151 I main: llama threadpool init, n_threads = 4
0.00.673.187 I 
0.00.673.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.221 I 
0.00.673.440 I sampler seed: 1234
0.00.673.445 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.457 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.457 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.524.148 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.524.149 I llama_perf_context_print:        load time =     662.29 ms
0.01.524.150 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.72 tokens per second)
0.01.524.154 I llama_perf_context_print:        eval time =     796.02 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.524.154 I llama_perf_context_print:       total time =     851.89 ms /    70 tokens
0.01.524.416 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.112s
sys	0m0.138s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.904 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.906 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.906 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.907 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.990 I llama_model_loader: - type  f32:  194 tensors
0.00.026.990 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.991 I print_info: file format = GGUF V3 (latest)
0.00.026.991 I print_info: file type   = Q6_K
0.00.026.992 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.515 I load: special tokens cache size = 25
0.00.051.395 I load: token to piece cache size = 0.2984 MB
0.00.051.398 I print_info: arch             = gptneox
0.00.051.398 I print_info: vocab_only       = 0
0.00.051.398 I print_info: n_ctx_train      = 2048
0.00.051.398 I print_info: n_embd           = 2048
0.00.051.399 I print_info: n_layer          = 24
0.00.051.402 I print_info: n_head           = 16
0.00.051.403 I print_info: n_head_kv        = 16
0.00.051.403 I print_info: n_rot            = 32
0.00.051.403 I print_info: n_swa            = 0
0.00.051.403 I print_info: n_embd_head_k    = 128
0.00.051.403 I print_info: n_embd_head_v    = 128
0.00.051.404 I print_info: n_gqa            = 1
0.00.051.405 I print_info: n_embd_k_gqa     = 2048
0.00.051.406 I print_info: n_embd_v_gqa     = 2048
0.00.051.406 I print_info: f_norm_eps       = 1.0e-05
0.00.051.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.410 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.410 I print_info: f_logit_scale    = 0.0e+00
0.00.051.411 I print_info: n_ff             = 8192
0.00.051.411 I print_info: n_expert         = 0
0.00.051.411 I print_info: n_expert_used    = 0
0.00.051.411 I print_info: causal attn      = 1
0.00.051.413 I print_info: pooling type     = 0
0.00.051.413 I print_info: rope type        = 2
0.00.051.414 I print_info: rope scaling     = linear
0.00.051.416 I print_info: freq_base_train  = 10000.0
0.00.051.416 I print_info: freq_scale_train = 1
0.00.051.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.416 I print_info: rope_finetuned   = unknown
0.00.051.416 I print_info: ssm_d_conv       = 0
0.00.051.416 I print_info: ssm_d_inner      = 0
0.00.051.417 I print_info: ssm_d_state      = 0
0.00.051.417 I print_info: ssm_dt_rank      = 0
0.00.051.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.417 I print_info: model type       = 1.4B
0.00.051.417 I print_info: model params     = 1.41 B
0.00.051.421 I print_info: general.name     = 1.4B
0.00.051.421 I print_info: vocab type       = BPE
0.00.051.422 I print_info: n_vocab          = 50304
0.00.051.422 I print_info: n_merges         = 50009
0.00.051.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: LF token         = 128 'Ä'
0.00.051.424 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.424 I print_info: max token length = 1024
0.00.053.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.390 I load_tensors: offloading output layer to GPU
0.00.053.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.401 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.402 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.674 I llama_init_from_model: n_seq_max     = 1
0.00.053.674 I llama_init_from_model: n_ctx         = 2048
0.00.053.674 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.675 I llama_init_from_model: n_batch       = 2048
0.00.053.675 I llama_init_from_model: n_ubatch      = 512
0.00.053.675 I llama_init_from_model: flash_attn    = 0
0.00.053.675 I llama_init_from_model: freq_base     = 10000.0
0.00.053.675 I llama_init_from_model: freq_scale    = 1
0.00.053.676 I ggml_metal_init: allocating
0.00.053.679 I ggml_metal_init: found device: Apple M4
0.00.053.680 I ggml_metal_init: picking default device: Apple M4
0.00.054.278 I ggml_metal_init: using embedded metal library
0.00.056.617 I ggml_metal_init: GPU name:   Apple M4
0.00.056.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.619 I ggml_metal_init: simdgroup reduction   = true
0.00.056.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.619 I ggml_metal_init: has bfloat            = true
0.00.056.620 I ggml_metal_init: use bfloat            = true
0.00.056.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.621 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.477 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.639 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.646 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.665 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.748 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.749 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.750 I llama_init_from_model: graph nodes  = 967
0.00.086.750 I llama_init_from_model: graph splits = 2
0.00.086.753 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.899 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.673 I main: llama threadpool init, n_threads = 4
0.00.760.706 I 
0.00.760.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.731 I 
0.00.760.995 I sampler seed: 1234
0.00.760.999 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.028 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.029 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.029 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.635.053 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.635.054 I llama_perf_context_print:        load time =     749.83 ms
0.01.635.054 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.52 tokens per second)
0.01.635.055 I llama_perf_context_print:        eval time =     816.56 ms /    63 runs   (   12.96 ms per token,    77.15 tokens per second)
0.01.635.056 I llama_perf_context_print:       total time =     875.28 ms /    70 tokens
0.01.635.259 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.110s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.769 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.955 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.966 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.977 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.703 I llama_model_loader: - type  f32:  194 tensors
0.00.056.703 I llama_model_loader: - type  f16:   98 tensors
0.00.056.704 I print_info: file format = GGUF V3 (latest)
0.00.056.711 I print_info: file type   = all F32 (guessed)
0.00.056.712 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.447 I load: special tokens cache size = 25
0.00.090.352 I load: token to piece cache size = 0.2984 MB
0.00.090.356 I print_info: arch             = gptneox
0.00.090.356 I print_info: vocab_only       = 0
0.00.090.356 I print_info: n_ctx_train      = 2048
0.00.090.356 I print_info: n_embd           = 2048
0.00.090.356 I print_info: n_layer          = 24
0.00.090.359 I print_info: n_head           = 16
0.00.090.360 I print_info: n_head_kv        = 16
0.00.090.360 I print_info: n_rot            = 32
0.00.090.360 I print_info: n_swa            = 0
0.00.090.361 I print_info: n_embd_head_k    = 128
0.00.090.363 I print_info: n_embd_head_v    = 128
0.00.090.363 I print_info: n_gqa            = 1
0.00.090.364 I print_info: n_embd_k_gqa     = 2048
0.00.090.365 I print_info: n_embd_v_gqa     = 2048
0.00.090.365 I print_info: f_norm_eps       = 1.0e-05
0.00.090.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.366 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.366 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.366 I print_info: f_logit_scale    = 0.0e+00
0.00.090.367 I print_info: n_ff             = 8192
0.00.090.367 I print_info: n_expert         = 0
0.00.090.367 I print_info: n_expert_used    = 0
0.00.090.367 I print_info: causal attn      = 1
0.00.090.367 I print_info: pooling type     = 0
0.00.090.367 I print_info: rope type        = 2
0.00.090.367 I print_info: rope scaling     = linear
0.00.090.368 I print_info: freq_base_train  = 10000.0
0.00.090.368 I print_info: freq_scale_train = 1
0.00.090.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.368 I print_info: rope_finetuned   = unknown
0.00.090.368 I print_info: ssm_d_conv       = 0
0.00.090.368 I print_info: ssm_d_inner      = 0
0.00.090.369 I print_info: ssm_d_state      = 0
0.00.090.369 I print_info: ssm_dt_rank      = 0
0.00.090.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.369 I print_info: model type       = 1.4B
0.00.090.370 I print_info: model params     = 1.41 B
0.00.090.371 I print_info: general.name     = 1.4B
0.00.090.371 I print_info: vocab type       = BPE
0.00.090.371 I print_info: n_vocab          = 50304
0.00.090.371 I print_info: n_merges         = 50009
0.00.090.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.372 I print_info: LF token         = 128 'Ä'
0.00.090.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.373 I print_info: max token length = 1024
0.00.093.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.027 I load_tensors: offloading output layer to GPU
0.00.093.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.038 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.039 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.298 I llama_init_from_model: n_seq_max     = 1
0.00.093.299 I llama_init_from_model: n_ctx         = 128
0.00.093.299 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.299 I llama_init_from_model: n_batch       = 128
0.00.093.299 I llama_init_from_model: n_ubatch      = 128
0.00.093.299 I llama_init_from_model: flash_attn    = 0
0.00.093.300 I llama_init_from_model: freq_base     = 10000.0
0.00.093.300 I llama_init_from_model: freq_scale    = 1
0.00.093.300 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.301 I ggml_metal_init: allocating
0.00.093.303 I ggml_metal_init: found device: Apple M4
0.00.093.305 I ggml_metal_init: picking default device: Apple M4
0.00.093.910 I ggml_metal_init: using embedded metal library
0.00.096.484 I ggml_metal_init: GPU name:   Apple M4
0.00.096.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.487 I ggml_metal_init: simdgroup reduction   = true
0.00.096.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.487 I ggml_metal_init: has bfloat            = true
0.00.096.487 I ggml_metal_init: use bfloat            = true
0.00.096.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.998 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.285 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.299 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.173 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.174 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.175 I llama_init_from_model: graph nodes  = 967
0.00.108.175 I llama_init_from_model: graph splits = 2
0.00.108.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.998.961 I 
0.00.999.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.999.039 I perplexity: tokenizing the input ..
0.01.012.018 I perplexity: tokenization took 12.976 ms
0.01.012.049 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.135.066 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.136.996 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.137.022 I llama_perf_context_print:        load time =     974.75 ms
0.01.137.024 I llama_perf_context_print: prompt eval time =     122.17 ms /   128 tokens (    0.95 ms per token,  1047.72 tokens per second)
0.01.137.025 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.137.026 I llama_perf_context_print:       total time =     138.07 ms /   129 tokens
0.01.137.885 I ggml_metal_free: deallocating

real	0m1.333s
user	0m0.126s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.714 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.069 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.070 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.072 I llama_model_loader: - type  f32:  194 tensors
0.00.034.072 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.073 I print_info: file format = GGUF V3 (latest)
0.00.034.074 I print_info: file type   = Q8_0
0.00.034.075 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.390 I load: special tokens cache size = 25
0.00.063.843 I load: token to piece cache size = 0.2984 MB
0.00.063.846 I print_info: arch             = gptneox
0.00.063.846 I print_info: vocab_only       = 0
0.00.063.847 I print_info: n_ctx_train      = 2048
0.00.063.847 I print_info: n_embd           = 2048
0.00.063.847 I print_info: n_layer          = 24
0.00.063.851 I print_info: n_head           = 16
0.00.063.852 I print_info: n_head_kv        = 16
0.00.063.852 I print_info: n_rot            = 32
0.00.063.852 I print_info: n_swa            = 0
0.00.063.852 I print_info: n_embd_head_k    = 128
0.00.063.852 I print_info: n_embd_head_v    = 128
0.00.063.853 I print_info: n_gqa            = 1
0.00.063.854 I print_info: n_embd_k_gqa     = 2048
0.00.063.856 I print_info: n_embd_v_gqa     = 2048
0.00.063.857 I print_info: f_norm_eps       = 1.0e-05
0.00.063.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.858 I print_info: f_logit_scale    = 0.0e+00
0.00.063.858 I print_info: n_ff             = 8192
0.00.063.858 I print_info: n_expert         = 0
0.00.063.859 I print_info: n_expert_used    = 0
0.00.063.859 I print_info: causal attn      = 1
0.00.063.859 I print_info: pooling type     = 0
0.00.063.859 I print_info: rope type        = 2
0.00.063.859 I print_info: rope scaling     = linear
0.00.063.860 I print_info: freq_base_train  = 10000.0
0.00.063.860 I print_info: freq_scale_train = 1
0.00.063.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.860 I print_info: rope_finetuned   = unknown
0.00.063.861 I print_info: ssm_d_conv       = 0
0.00.063.861 I print_info: ssm_d_inner      = 0
0.00.063.861 I print_info: ssm_d_state      = 0
0.00.063.861 I print_info: ssm_dt_rank      = 0
0.00.063.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.861 I print_info: model type       = 1.4B
0.00.063.862 I print_info: model params     = 1.41 B
0.00.063.862 I print_info: general.name     = 1.4B
0.00.063.863 I print_info: vocab type       = BPE
0.00.063.863 I print_info: n_vocab          = 50304
0.00.063.863 I print_info: n_merges         = 50009
0.00.063.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.864 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.864 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.864 I print_info: LF token         = 128 'Ä'
0.00.063.864 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.865 I print_info: max token length = 1024
0.00.066.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.175 I load_tensors: offloading output layer to GPU
0.00.066.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.186 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.187 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.512 I llama_init_from_model: n_seq_max     = 1
0.00.066.513 I llama_init_from_model: n_ctx         = 128
0.00.066.513 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.514 I llama_init_from_model: n_batch       = 128
0.00.066.514 I llama_init_from_model: n_ubatch      = 128
0.00.066.514 I llama_init_from_model: flash_attn    = 0
0.00.066.514 I llama_init_from_model: freq_base     = 10000.0
0.00.066.515 I llama_init_from_model: freq_scale    = 1
0.00.066.515 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.515 I ggml_metal_init: allocating
0.00.066.518 I ggml_metal_init: found device: Apple M4
0.00.066.520 I ggml_metal_init: picking default device: Apple M4
0.00.067.155 I ggml_metal_init: using embedded metal library
0.00.069.604 I ggml_metal_init: GPU name:   Apple M4
0.00.069.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.606 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.606 I ggml_metal_init: simdgroup reduction   = true
0.00.069.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.607 I ggml_metal_init: has bfloat            = true
0.00.069.607 I ggml_metal_init: use bfloat            = true
0.00.069.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.774 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.013 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.987 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.988 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.988 I llama_init_from_model: graph nodes  = 967
0.00.081.989 I llama_init_from_model: graph splits = 2
0.00.081.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.923.893 I 
0.00.923.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.923.980 I perplexity: tokenizing the input ..
0.00.932.115 I perplexity: tokenization took 8.134 ms
0.00.932.124 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.056.448 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.057.765 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.057.781 I llama_perf_context_print:        load time =     912.17 ms
0.01.057.782 I llama_perf_context_print: prompt eval time =     124.08 ms /   128 tokens (    0.97 ms per token,  1031.62 tokens per second)
0.01.057.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.057.783 I llama_perf_context_print:       total time =     133.89 ms /   129 tokens
0.01.058.306 I ggml_metal_free: deallocating

real	0m1.078s
user	0m0.092s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.331 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.691 I llama_model_loader: - type  f32:  194 tensors
0.00.026.692 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.693 I print_info: file format = GGUF V3 (latest)
0.00.026.693 I print_info: file type   = Q4_0
0.00.026.694 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.936 I load: special tokens cache size = 25
0.00.050.885 I load: token to piece cache size = 0.2984 MB
0.00.050.888 I print_info: arch             = gptneox
0.00.050.888 I print_info: vocab_only       = 0
0.00.050.888 I print_info: n_ctx_train      = 2048
0.00.050.888 I print_info: n_embd           = 2048
0.00.050.889 I print_info: n_layer          = 24
0.00.050.892 I print_info: n_head           = 16
0.00.050.893 I print_info: n_head_kv        = 16
0.00.050.893 I print_info: n_rot            = 32
0.00.050.893 I print_info: n_swa            = 0
0.00.050.894 I print_info: n_embd_head_k    = 128
0.00.050.896 I print_info: n_embd_head_v    = 128
0.00.050.897 I print_info: n_gqa            = 1
0.00.050.898 I print_info: n_embd_k_gqa     = 2048
0.00.050.898 I print_info: n_embd_v_gqa     = 2048
0.00.050.899 I print_info: f_norm_eps       = 1.0e-05
0.00.050.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.900 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.900 I print_info: f_logit_scale    = 0.0e+00
0.00.050.901 I print_info: n_ff             = 8192
0.00.050.901 I print_info: n_expert         = 0
0.00.050.901 I print_info: n_expert_used    = 0
0.00.050.901 I print_info: causal attn      = 1
0.00.050.901 I print_info: pooling type     = 0
0.00.050.902 I print_info: rope type        = 2
0.00.050.902 I print_info: rope scaling     = linear
0.00.050.902 I print_info: freq_base_train  = 10000.0
0.00.050.903 I print_info: freq_scale_train = 1
0.00.050.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.903 I print_info: rope_finetuned   = unknown
0.00.050.903 I print_info: ssm_d_conv       = 0
0.00.050.905 I print_info: ssm_d_inner      = 0
0.00.050.905 I print_info: ssm_d_state      = 0
0.00.050.905 I print_info: ssm_dt_rank      = 0
0.00.050.905 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.905 I print_info: model type       = 1.4B
0.00.050.906 I print_info: model params     = 1.41 B
0.00.050.906 I print_info: general.name     = 1.4B
0.00.050.906 I print_info: vocab type       = BPE
0.00.050.907 I print_info: n_vocab          = 50304
0.00.050.907 I print_info: n_merges         = 50009
0.00.050.907 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.907 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: LF token         = 128 'Ä'
0.00.050.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.909 I print_info: max token length = 1024
0.00.052.614 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.614 I load_tensors: offloading output layer to GPU
0.00.052.614 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.619 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.620 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.919 I llama_init_from_model: n_seq_max     = 1
0.00.052.920 I llama_init_from_model: n_ctx         = 128
0.00.052.920 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.920 I llama_init_from_model: n_batch       = 128
0.00.052.921 I llama_init_from_model: n_ubatch      = 128
0.00.052.921 I llama_init_from_model: flash_attn    = 0
0.00.052.921 I llama_init_from_model: freq_base     = 10000.0
0.00.052.921 I llama_init_from_model: freq_scale    = 1
0.00.052.922 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.922 I ggml_metal_init: allocating
0.00.052.924 I ggml_metal_init: found device: Apple M4
0.00.052.926 I ggml_metal_init: picking default device: Apple M4
0.00.053.503 I ggml_metal_init: using embedded metal library
0.00.055.809 I ggml_metal_init: GPU name:   Apple M4
0.00.055.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.812 I ggml_metal_init: simdgroup reduction   = true
0.00.055.812 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.812 I ggml_metal_init: has bfloat            = true
0.00.055.812 I ggml_metal_init: use bfloat            = true
0.00.055.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.358 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.575 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.588 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.499 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.500 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.500 I llama_init_from_model: graph nodes  = 967
0.00.067.500 I llama_init_from_model: graph splits = 2
0.00.067.501 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.475 I 
0.00.607.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.531 I perplexity: tokenizing the input ..
0.00.615.784 I perplexity: tokenization took 8.252 ms
0.00.615.795 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.562 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.739.719 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.739.736 I llama_perf_context_print:        load time =     597.14 ms
0.00.739.737 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.57 tokens per second)
0.00.739.740 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.741 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.740.261 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.210 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.211 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.174 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.175 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.176 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.176 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.177 I llama_model_loader: - type  f32:  194 tensors
0.00.025.178 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.178 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.178 I print_info: file format = GGUF V3 (latest)
0.00.025.179 I print_info: file type   = Q4_1
0.00.025.179 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.363 I load: special tokens cache size = 25
0.00.050.123 I load: token to piece cache size = 0.2984 MB
0.00.050.126 I print_info: arch             = gptneox
0.00.050.126 I print_info: vocab_only       = 0
0.00.050.126 I print_info: n_ctx_train      = 2048
0.00.050.126 I print_info: n_embd           = 2048
0.00.050.126 I print_info: n_layer          = 24
0.00.050.129 I print_info: n_head           = 16
0.00.050.130 I print_info: n_head_kv        = 16
0.00.050.130 I print_info: n_rot            = 32
0.00.050.130 I print_info: n_swa            = 0
0.00.050.130 I print_info: n_embd_head_k    = 128
0.00.050.131 I print_info: n_embd_head_v    = 128
0.00.050.131 I print_info: n_gqa            = 1
0.00.050.132 I print_info: n_embd_k_gqa     = 2048
0.00.050.133 I print_info: n_embd_v_gqa     = 2048
0.00.050.136 I print_info: f_norm_eps       = 1.0e-05
0.00.050.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.138 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.138 I print_info: f_logit_scale    = 0.0e+00
0.00.050.139 I print_info: n_ff             = 8192
0.00.050.139 I print_info: n_expert         = 0
0.00.050.139 I print_info: n_expert_used    = 0
0.00.050.139 I print_info: causal attn      = 1
0.00.050.139 I print_info: pooling type     = 0
0.00.050.139 I print_info: rope type        = 2
0.00.050.140 I print_info: rope scaling     = linear
0.00.050.140 I print_info: freq_base_train  = 10000.0
0.00.050.141 I print_info: freq_scale_train = 1
0.00.050.142 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.142 I print_info: rope_finetuned   = unknown
0.00.050.142 I print_info: ssm_d_conv       = 0
0.00.050.142 I print_info: ssm_d_inner      = 0
0.00.050.142 I print_info: ssm_d_state      = 0
0.00.050.142 I print_info: ssm_dt_rank      = 0
0.00.050.142 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.143 I print_info: model type       = 1.4B
0.00.050.143 I print_info: model params     = 1.41 B
0.00.050.143 I print_info: general.name     = 1.4B
0.00.050.144 I print_info: vocab type       = BPE
0.00.050.144 I print_info: n_vocab          = 50304
0.00.050.144 I print_info: n_merges         = 50009
0.00.050.145 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.145 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.145 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.145 I print_info: LF token         = 128 'Ä'
0.00.050.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.146 I print_info: max token length = 1024
0.00.052.018 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.018 I load_tensors: offloading output layer to GPU
0.00.052.019 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.029 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.030 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.304 I llama_init_from_model: n_seq_max     = 1
0.00.052.305 I llama_init_from_model: n_ctx         = 128
0.00.052.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.305 I llama_init_from_model: n_batch       = 128
0.00.052.306 I llama_init_from_model: n_ubatch      = 128
0.00.052.306 I llama_init_from_model: flash_attn    = 0
0.00.052.306 I llama_init_from_model: freq_base     = 10000.0
0.00.052.306 I llama_init_from_model: freq_scale    = 1
0.00.052.307 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.307 I ggml_metal_init: allocating
0.00.052.310 I ggml_metal_init: found device: Apple M4
0.00.052.312 I ggml_metal_init: picking default device: Apple M4
0.00.052.880 I ggml_metal_init: using embedded metal library
0.00.055.200 I ggml_metal_init: GPU name:   Apple M4
0.00.055.201 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.202 I ggml_metal_init: simdgroup reduction   = true
0.00.055.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.202 I ggml_metal_init: has bfloat            = true
0.00.055.202 I ggml_metal_init: use bfloat            = true
0.00.055.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.472 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.688 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.702 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.548 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.549 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.549 I llama_init_from_model: graph nodes  = 967
0.00.066.549 I llama_init_from_model: graph splits = 2
0.00.066.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.900 I 
0.00.668.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.975 I perplexity: tokenizing the input ..
0.00.677.159 I perplexity: tokenization took 8.183 ms
0.00.677.170 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.918 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.801.419 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.801.441 I llama_perf_context_print:        load time =     660.08 ms
0.00.801.443 I llama_perf_context_print: prompt eval time =     122.49 ms /   128 tokens (    0.96 ms per token,  1044.98 tokens per second)
0.00.801.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.444 I llama_perf_context_print:       total time =     132.54 ms /   129 tokens
0.00.801.981 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.712 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.603 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.576 I llama_model_loader: - type  f32:  194 tensors
0.00.026.577 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.578 I print_info: file format = GGUF V3 (latest)
0.00.026.579 I print_info: file type   = Q5_0
0.00.026.581 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.536 I load: special tokens cache size = 25
0.00.052.725 I load: token to piece cache size = 0.2984 MB
0.00.052.730 I print_info: arch             = gptneox
0.00.052.731 I print_info: vocab_only       = 0
0.00.052.731 I print_info: n_ctx_train      = 2048
0.00.052.731 I print_info: n_embd           = 2048
0.00.052.731 I print_info: n_layer          = 24
0.00.052.736 I print_info: n_head           = 16
0.00.052.736 I print_info: n_head_kv        = 16
0.00.052.737 I print_info: n_rot            = 32
0.00.052.737 I print_info: n_swa            = 0
0.00.052.737 I print_info: n_embd_head_k    = 128
0.00.052.737 I print_info: n_embd_head_v    = 128
0.00.052.738 I print_info: n_gqa            = 1
0.00.052.739 I print_info: n_embd_k_gqa     = 2048
0.00.052.742 I print_info: n_embd_v_gqa     = 2048
0.00.052.743 I print_info: f_norm_eps       = 1.0e-05
0.00.052.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.744 I print_info: f_logit_scale    = 0.0e+00
0.00.052.745 I print_info: n_ff             = 8192
0.00.052.745 I print_info: n_expert         = 0
0.00.052.745 I print_info: n_expert_used    = 0
0.00.052.745 I print_info: causal attn      = 1
0.00.052.745 I print_info: pooling type     = 0
0.00.052.745 I print_info: rope type        = 2
0.00.052.746 I print_info: rope scaling     = linear
0.00.052.746 I print_info: freq_base_train  = 10000.0
0.00.052.746 I print_info: freq_scale_train = 1
0.00.052.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.747 I print_info: rope_finetuned   = unknown
0.00.052.747 I print_info: ssm_d_conv       = 0
0.00.052.747 I print_info: ssm_d_inner      = 0
0.00.052.748 I print_info: ssm_d_state      = 0
0.00.052.749 I print_info: ssm_dt_rank      = 0
0.00.052.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.749 I print_info: model type       = 1.4B
0.00.052.749 I print_info: model params     = 1.41 B
0.00.052.750 I print_info: general.name     = 1.4B
0.00.052.750 I print_info: vocab type       = BPE
0.00.052.751 I print_info: n_vocab          = 50304
0.00.052.751 I print_info: n_merges         = 50009
0.00.052.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.752 I print_info: LF token         = 128 'Ä'
0.00.052.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.753 I print_info: max token length = 1024
0.00.054.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.794 I load_tensors: offloading output layer to GPU
0.00.054.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.805 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.806 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.079 I llama_init_from_model: n_seq_max     = 1
0.00.055.080 I llama_init_from_model: n_ctx         = 128
0.00.055.080 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.080 I llama_init_from_model: n_batch       = 128
0.00.055.080 I llama_init_from_model: n_ubatch      = 128
0.00.055.081 I llama_init_from_model: flash_attn    = 0
0.00.055.081 I llama_init_from_model: freq_base     = 10000.0
0.00.055.081 I llama_init_from_model: freq_scale    = 1
0.00.055.082 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.082 I ggml_metal_init: allocating
0.00.055.085 I ggml_metal_init: found device: Apple M4
0.00.055.088 I ggml_metal_init: picking default device: Apple M4
0.00.055.717 I ggml_metal_init: using embedded metal library
0.00.058.096 I ggml_metal_init: GPU name:   Apple M4
0.00.058.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.098 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.099 I ggml_metal_init: simdgroup reduction   = true
0.00.058.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.099 I ggml_metal_init: has bfloat            = true
0.00.058.099 I ggml_metal_init: use bfloat            = true
0.00.058.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.100 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.285 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.574 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.590 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.447 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.448 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.448 I llama_init_from_model: graph nodes  = 967
0.00.070.448 I llama_init_from_model: graph splits = 2
0.00.070.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.960 I 
0.00.693.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.009 I perplexity: tokenizing the input ..
0.00.702.106 I perplexity: tokenization took 8.096 ms
0.00.702.117 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.163 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.838.334 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.838.344 I llama_perf_context_print:        load time =     683.24 ms
0.00.838.345 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.41 tokens per second)
0.00.838.346 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.347 I llama_perf_context_print:       total time =     144.39 ms /   129 tokens
0.00.838.705 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.080s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.615 I llama_model_loader: - type  f32:  194 tensors
0.00.024.615 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.616 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.616 I print_info: file format = GGUF V3 (latest)
0.00.024.616 I print_info: file type   = Q5_1
0.00.024.617 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.798 I load: special tokens cache size = 25
0.00.049.889 I load: token to piece cache size = 0.2984 MB
0.00.049.892 I print_info: arch             = gptneox
0.00.049.893 I print_info: vocab_only       = 0
0.00.049.893 I print_info: n_ctx_train      = 2048
0.00.049.893 I print_info: n_embd           = 2048
0.00.049.893 I print_info: n_layer          = 24
0.00.049.896 I print_info: n_head           = 16
0.00.049.897 I print_info: n_head_kv        = 16
0.00.049.897 I print_info: n_rot            = 32
0.00.049.897 I print_info: n_swa            = 0
0.00.049.897 I print_info: n_embd_head_k    = 128
0.00.049.897 I print_info: n_embd_head_v    = 128
0.00.049.898 I print_info: n_gqa            = 1
0.00.049.899 I print_info: n_embd_k_gqa     = 2048
0.00.049.900 I print_info: n_embd_v_gqa     = 2048
0.00.049.900 I print_info: f_norm_eps       = 1.0e-05
0.00.049.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.901 I print_info: f_logit_scale    = 0.0e+00
0.00.049.902 I print_info: n_ff             = 8192
0.00.049.902 I print_info: n_expert         = 0
0.00.049.904 I print_info: n_expert_used    = 0
0.00.049.904 I print_info: causal attn      = 1
0.00.049.904 I print_info: pooling type     = 0
0.00.049.905 I print_info: rope type        = 2
0.00.049.905 I print_info: rope scaling     = linear
0.00.049.905 I print_info: freq_base_train  = 10000.0
0.00.049.905 I print_info: freq_scale_train = 1
0.00.049.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.906 I print_info: rope_finetuned   = unknown
0.00.049.906 I print_info: ssm_d_conv       = 0
0.00.049.906 I print_info: ssm_d_inner      = 0
0.00.049.906 I print_info: ssm_d_state      = 0
0.00.049.906 I print_info: ssm_dt_rank      = 0
0.00.049.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.907 I print_info: model type       = 1.4B
0.00.049.907 I print_info: model params     = 1.41 B
0.00.049.907 I print_info: general.name     = 1.4B
0.00.049.908 I print_info: vocab type       = BPE
0.00.049.908 I print_info: n_vocab          = 50304
0.00.049.908 I print_info: n_merges         = 50009
0.00.049.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.909 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.909 I print_info: LF token         = 128 'Ä'
0.00.049.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.913 I print_info: max token length = 1024
0.00.051.892 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.892 I load_tensors: offloading output layer to GPU
0.00.051.892 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.903 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.904 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.174 I llama_init_from_model: n_seq_max     = 1
0.00.052.175 I llama_init_from_model: n_ctx         = 128
0.00.052.175 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.175 I llama_init_from_model: n_batch       = 128
0.00.052.175 I llama_init_from_model: n_ubatch      = 128
0.00.052.175 I llama_init_from_model: flash_attn    = 0
0.00.052.176 I llama_init_from_model: freq_base     = 10000.0
0.00.052.176 I llama_init_from_model: freq_scale    = 1
0.00.052.176 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.177 I ggml_metal_init: allocating
0.00.052.179 I ggml_metal_init: found device: Apple M4
0.00.052.181 I ggml_metal_init: picking default device: Apple M4
0.00.052.769 I ggml_metal_init: using embedded metal library
0.00.055.128 I ggml_metal_init: GPU name:   Apple M4
0.00.055.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.130 I ggml_metal_init: simdgroup reduction   = true
0.00.055.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.131 I ggml_metal_init: has bfloat            = true
0.00.055.131 I ggml_metal_init: use bfloat            = true
0.00.055.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.805 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.052 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.054 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.067 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.016 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.017 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.018 I llama_init_from_model: graph nodes  = 967
0.00.067.018 I llama_init_from_model: graph splits = 2
0.00.067.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.826 I 
0.00.716.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.871 I perplexity: tokenizing the input ..
0.00.724.981 I perplexity: tokenization took 8.109 ms
0.00.724.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.975 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.861.434 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.861.462 I llama_perf_context_print:        load time =     707.97 ms
0.00.861.463 I llama_perf_context_print: prompt eval time =     134.73 ms /   128 tokens (    1.05 ms per token,   950.03 tokens per second)
0.00.861.464 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.464 I llama_perf_context_print:       total time =     144.64 ms /   129 tokens
0.00.861.953 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.078s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.397 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.446 I llama_model_loader: - type  f32:  194 tensors
0.00.027.447 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.447 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.447 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.448 I print_info: file format = GGUF V3 (latest)
0.00.027.448 I print_info: file type   = Q2_K - Medium
0.00.027.449 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.578 I load: special tokens cache size = 25
0.00.053.823 I load: token to piece cache size = 0.2984 MB
0.00.053.827 I print_info: arch             = gptneox
0.00.053.827 I print_info: vocab_only       = 0
0.00.053.828 I print_info: n_ctx_train      = 2048
0.00.053.828 I print_info: n_embd           = 2048
0.00.053.828 I print_info: n_layer          = 24
0.00.053.832 I print_info: n_head           = 16
0.00.053.832 I print_info: n_head_kv        = 16
0.00.053.832 I print_info: n_rot            = 32
0.00.053.833 I print_info: n_swa            = 0
0.00.053.833 I print_info: n_embd_head_k    = 128
0.00.053.835 I print_info: n_embd_head_v    = 128
0.00.053.836 I print_info: n_gqa            = 1
0.00.053.836 I print_info: n_embd_k_gqa     = 2048
0.00.053.837 I print_info: n_embd_v_gqa     = 2048
0.00.053.837 I print_info: f_norm_eps       = 1.0e-05
0.00.053.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.839 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.839 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.840 I print_info: f_logit_scale    = 0.0e+00
0.00.053.840 I print_info: n_ff             = 8192
0.00.053.840 I print_info: n_expert         = 0
0.00.053.840 I print_info: n_expert_used    = 0
0.00.053.840 I print_info: causal attn      = 1
0.00.053.841 I print_info: pooling type     = 0
0.00.053.841 I print_info: rope type        = 2
0.00.053.841 I print_info: rope scaling     = linear
0.00.053.841 I print_info: freq_base_train  = 10000.0
0.00.053.842 I print_info: freq_scale_train = 1
0.00.053.842 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.842 I print_info: rope_finetuned   = unknown
0.00.053.842 I print_info: ssm_d_conv       = 0
0.00.053.842 I print_info: ssm_d_inner      = 0
0.00.053.843 I print_info: ssm_d_state      = 0
0.00.053.843 I print_info: ssm_dt_rank      = 0
0.00.053.843 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.843 I print_info: model type       = 1.4B
0.00.053.843 I print_info: model params     = 1.41 B
0.00.053.844 I print_info: general.name     = 1.4B
0.00.053.844 I print_info: vocab type       = BPE
0.00.053.844 I print_info: n_vocab          = 50304
0.00.053.844 I print_info: n_merges         = 50009
0.00.053.845 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.845 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.845 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.845 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.845 I print_info: LF token         = 128 'Ä'
0.00.053.846 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.846 I print_info: max token length = 1024
0.00.055.660 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.660 I load_tensors: offloading output layer to GPU
0.00.055.660 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.671 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.673 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.055.967 I llama_init_from_model: n_seq_max     = 1
0.00.055.968 I llama_init_from_model: n_ctx         = 128
0.00.055.968 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.968 I llama_init_from_model: n_batch       = 128
0.00.055.968 I llama_init_from_model: n_ubatch      = 128
0.00.055.968 I llama_init_from_model: flash_attn    = 0
0.00.055.969 I llama_init_from_model: freq_base     = 10000.0
0.00.055.972 I llama_init_from_model: freq_scale    = 1
0.00.055.973 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.973 I ggml_metal_init: allocating
0.00.055.977 I ggml_metal_init: found device: Apple M4
0.00.055.979 I ggml_metal_init: picking default device: Apple M4
0.00.056.603 I ggml_metal_init: using embedded metal library
0.00.059.279 I ggml_metal_init: GPU name:   Apple M4
0.00.059.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.282 I ggml_metal_init: simdgroup reduction   = true
0.00.059.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.282 I ggml_metal_init: has bfloat            = true
0.00.059.283 I ggml_metal_init: use bfloat            = true
0.00.059.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.430 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.911 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.924 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.948 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.796 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.797 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.798 I llama_init_from_model: graph nodes  = 967
0.00.070.798 I llama_init_from_model: graph splits = 2
0.00.070.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.772 I 
0.00.442.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.848 I perplexity: tokenizing the input ..
0.00.450.242 I perplexity: tokenization took 7.393 ms
0.00.450.253 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.582.527 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.583.688 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.583.705 I llama_perf_context_print:        load time =     431.16 ms
0.00.583.706 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.34 tokens per second)
0.00.583.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.583.710 I llama_perf_context_print:       total time =     140.94 ms /   129 tokens
0.00.584.162 I ggml_metal_free: deallocating

real	0m0.600s
user	0m0.079s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.678 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.470 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.471 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.606 I llama_model_loader: - type  f32:  194 tensors
0.00.024.606 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.607 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.607 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.607 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.608 I print_info: file format = GGUF V3 (latest)
0.00.024.608 I print_info: file type   = Q3_K - Medium
0.00.024.609 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.822 I load: special tokens cache size = 25
0.00.048.741 I load: token to piece cache size = 0.2984 MB
0.00.048.743 I print_info: arch             = gptneox
0.00.048.744 I print_info: vocab_only       = 0
0.00.048.744 I print_info: n_ctx_train      = 2048
0.00.048.744 I print_info: n_embd           = 2048
0.00.048.744 I print_info: n_layer          = 24
0.00.048.747 I print_info: n_head           = 16
0.00.048.748 I print_info: n_head_kv        = 16
0.00.048.748 I print_info: n_rot            = 32
0.00.048.749 I print_info: n_swa            = 0
0.00.048.749 I print_info: n_embd_head_k    = 128
0.00.048.751 I print_info: n_embd_head_v    = 128
0.00.048.751 I print_info: n_gqa            = 1
0.00.048.752 I print_info: n_embd_k_gqa     = 2048
0.00.048.753 I print_info: n_embd_v_gqa     = 2048
0.00.048.753 I print_info: f_norm_eps       = 1.0e-05
0.00.048.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.754 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.754 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.754 I print_info: f_logit_scale    = 0.0e+00
0.00.048.755 I print_info: n_ff             = 8192
0.00.048.755 I print_info: n_expert         = 0
0.00.048.755 I print_info: n_expert_used    = 0
0.00.048.755 I print_info: causal attn      = 1
0.00.048.756 I print_info: pooling type     = 0
0.00.048.757 I print_info: rope type        = 2
0.00.048.759 I print_info: rope scaling     = linear
0.00.048.759 I print_info: freq_base_train  = 10000.0
0.00.048.759 I print_info: freq_scale_train = 1
0.00.048.760 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.760 I print_info: rope_finetuned   = unknown
0.00.048.760 I print_info: ssm_d_conv       = 0
0.00.048.760 I print_info: ssm_d_inner      = 0
0.00.048.760 I print_info: ssm_d_state      = 0
0.00.048.760 I print_info: ssm_dt_rank      = 0
0.00.048.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.761 I print_info: model type       = 1.4B
0.00.048.762 I print_info: model params     = 1.41 B
0.00.048.765 I print_info: general.name     = 1.4B
0.00.048.766 I print_info: vocab type       = BPE
0.00.048.766 I print_info: n_vocab          = 50304
0.00.048.766 I print_info: n_merges         = 50009
0.00.048.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.769 I print_info: LF token         = 128 'Ä'
0.00.048.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.769 I print_info: max token length = 1024
0.00.050.632 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.633 I load_tensors: offloading output layer to GPU
0.00.050.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.644 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.645 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.928 I llama_init_from_model: n_seq_max     = 1
0.00.050.928 I llama_init_from_model: n_ctx         = 128
0.00.050.929 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.929 I llama_init_from_model: n_batch       = 128
0.00.050.929 I llama_init_from_model: n_ubatch      = 128
0.00.050.929 I llama_init_from_model: flash_attn    = 0
0.00.050.929 I llama_init_from_model: freq_base     = 10000.0
0.00.050.930 I llama_init_from_model: freq_scale    = 1
0.00.050.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.930 I ggml_metal_init: allocating
0.00.050.933 I ggml_metal_init: found device: Apple M4
0.00.050.935 I ggml_metal_init: picking default device: Apple M4
0.00.051.510 I ggml_metal_init: using embedded metal library
0.00.053.853 I ggml_metal_init: GPU name:   Apple M4
0.00.053.854 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.855 I ggml_metal_init: simdgroup reduction   = true
0.00.053.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.855 I ggml_metal_init: has bfloat            = true
0.00.053.855 I ggml_metal_init: use bfloat            = true
0.00.053.856 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.282 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.504 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.507 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.521 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.443 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.444 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.445 I llama_init_from_model: graph nodes  = 967
0.00.065.445 I llama_init_from_model: graph splits = 2
0.00.065.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.022 I 
0.00.476.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.069 I perplexity: tokenizing the input ..
0.00.484.287 I perplexity: tokenization took 8.217 ms
0.00.484.298 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.765 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.021 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.045 I llama_perf_context_print:        load time =     467.34 ms
0.00.618.046 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.97 tokens per second)
0.00.618.047 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.047 I llama_perf_context_print:       total time =     142.02 ms /   129 tokens
0.00.618.586 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.077s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.985 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.043 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.047 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.048 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.936 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.821 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.821 I llama_model_loader: - type  f32:  194 tensors
0.00.025.822 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.822 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.822 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.823 I print_info: file format = GGUF V3 (latest)
0.00.025.823 I print_info: file type   = Q4_K - Medium
0.00.025.824 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.087 I load: special tokens cache size = 25
0.00.051.134 I load: token to piece cache size = 0.2984 MB
0.00.051.137 I print_info: arch             = gptneox
0.00.051.138 I print_info: vocab_only       = 0
0.00.051.138 I print_info: n_ctx_train      = 2048
0.00.051.138 I print_info: n_embd           = 2048
0.00.051.138 I print_info: n_layer          = 24
0.00.051.141 I print_info: n_head           = 16
0.00.051.142 I print_info: n_head_kv        = 16
0.00.051.142 I print_info: n_rot            = 32
0.00.051.142 I print_info: n_swa            = 0
0.00.051.143 I print_info: n_embd_head_k    = 128
0.00.051.143 I print_info: n_embd_head_v    = 128
0.00.051.143 I print_info: n_gqa            = 1
0.00.051.144 I print_info: n_embd_k_gqa     = 2048
0.00.051.146 I print_info: n_embd_v_gqa     = 2048
0.00.051.146 I print_info: f_norm_eps       = 1.0e-05
0.00.051.147 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.149 I print_info: f_logit_scale    = 0.0e+00
0.00.051.149 I print_info: n_ff             = 8192
0.00.051.150 I print_info: n_expert         = 0
0.00.051.150 I print_info: n_expert_used    = 0
0.00.051.150 I print_info: causal attn      = 1
0.00.051.150 I print_info: pooling type     = 0
0.00.051.150 I print_info: rope type        = 2
0.00.051.150 I print_info: rope scaling     = linear
0.00.051.151 I print_info: freq_base_train  = 10000.0
0.00.051.151 I print_info: freq_scale_train = 1
0.00.051.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.151 I print_info: rope_finetuned   = unknown
0.00.051.152 I print_info: ssm_d_conv       = 0
0.00.051.152 I print_info: ssm_d_inner      = 0
0.00.051.152 I print_info: ssm_d_state      = 0
0.00.051.152 I print_info: ssm_dt_rank      = 0
0.00.051.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.152 I print_info: model type       = 1.4B
0.00.051.153 I print_info: model params     = 1.41 B
0.00.051.153 I print_info: general.name     = 1.4B
0.00.051.153 I print_info: vocab type       = BPE
0.00.051.154 I print_info: n_vocab          = 50304
0.00.051.154 I print_info: n_merges         = 50009
0.00.051.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.155 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.155 I print_info: LF token         = 128 'Ä'
0.00.051.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.156 I print_info: max token length = 1024
0.00.053.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.085 I load_tensors: offloading output layer to GPU
0.00.053.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.095 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.097 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.375 I llama_init_from_model: n_seq_max     = 1
0.00.053.375 I llama_init_from_model: n_ctx         = 128
0.00.053.375 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.376 I llama_init_from_model: n_batch       = 128
0.00.053.376 I llama_init_from_model: n_ubatch      = 128
0.00.053.376 I llama_init_from_model: flash_attn    = 0
0.00.053.376 I llama_init_from_model: freq_base     = 10000.0
0.00.053.376 I llama_init_from_model: freq_scale    = 1
0.00.053.377 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.377 I ggml_metal_init: allocating
0.00.053.380 I ggml_metal_init: found device: Apple M4
0.00.053.382 I ggml_metal_init: picking default device: Apple M4
0.00.053.937 I ggml_metal_init: using embedded metal library
0.00.056.250 I ggml_metal_init: GPU name:   Apple M4
0.00.056.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.252 I ggml_metal_init: simdgroup reduction   = true
0.00.056.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.252 I ggml_metal_init: has bfloat            = true
0.00.056.252 I ggml_metal_init: use bfloat            = true
0.00.056.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.709 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.910 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.915 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.929 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.787 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.788 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.788 I llama_init_from_model: graph nodes  = 967
0.00.067.788 I llama_init_from_model: graph splits = 2
0.00.067.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.082 I 
0.00.611.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.133 I perplexity: tokenizing the input ..
0.00.619.395 I perplexity: tokenization took 8.259 ms
0.00.619.410 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.504 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.711 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.754.728 I llama_perf_context_print:        load time =     601.09 ms
0.00.754.729 I llama_perf_context_print: prompt eval time =     133.87 ms /   128 tokens (    1.05 ms per token,   956.16 tokens per second)
0.00.754.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.730 I llama_perf_context_print:       total time =     143.65 ms /   129 tokens
0.00.755.198 I ggml_metal_free: deallocating

real	0m0.770s
user	0m0.078s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.884 I llama_model_loader: - type  f32:  194 tensors
0.00.024.885 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.885 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.885 I print_info: file format = GGUF V3 (latest)
0.00.024.886 I print_info: file type   = Q5_K - Medium
0.00.024.886 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.914 I load: special tokens cache size = 25
0.00.049.990 I load: token to piece cache size = 0.2984 MB
0.00.049.994 I print_info: arch             = gptneox
0.00.049.994 I print_info: vocab_only       = 0
0.00.049.994 I print_info: n_ctx_train      = 2048
0.00.049.994 I print_info: n_embd           = 2048
0.00.049.995 I print_info: n_layer          = 24
0.00.049.997 I print_info: n_head           = 16
0.00.049.998 I print_info: n_head_kv        = 16
0.00.049.998 I print_info: n_rot            = 32
0.00.049.999 I print_info: n_swa            = 0
0.00.049.999 I print_info: n_embd_head_k    = 128
0.00.049.999 I print_info: n_embd_head_v    = 128
0.00.050.002 I print_info: n_gqa            = 1
0.00.050.003 I print_info: n_embd_k_gqa     = 2048
0.00.050.003 I print_info: n_embd_v_gqa     = 2048
0.00.050.005 I print_info: f_norm_eps       = 1.0e-05
0.00.050.005 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.005 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.006 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.006 I print_info: f_logit_scale    = 0.0e+00
0.00.050.006 I print_info: n_ff             = 8192
0.00.050.007 I print_info: n_expert         = 0
0.00.050.007 I print_info: n_expert_used    = 0
0.00.050.007 I print_info: causal attn      = 1
0.00.050.007 I print_info: pooling type     = 0
0.00.050.007 I print_info: rope type        = 2
0.00.050.007 I print_info: rope scaling     = linear
0.00.050.009 I print_info: freq_base_train  = 10000.0
0.00.050.011 I print_info: freq_scale_train = 1
0.00.050.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.011 I print_info: rope_finetuned   = unknown
0.00.050.012 I print_info: ssm_d_conv       = 0
0.00.050.012 I print_info: ssm_d_inner      = 0
0.00.050.012 I print_info: ssm_d_state      = 0
0.00.050.012 I print_info: ssm_dt_rank      = 0
0.00.050.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.012 I print_info: model type       = 1.4B
0.00.050.013 I print_info: model params     = 1.41 B
0.00.050.017 I print_info: general.name     = 1.4B
0.00.050.017 I print_info: vocab type       = BPE
0.00.050.017 I print_info: n_vocab          = 50304
0.00.050.018 I print_info: n_merges         = 50009
0.00.050.018 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.018 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.018 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.018 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.019 I print_info: LF token         = 128 'Ä'
0.00.050.019 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.019 I print_info: max token length = 1024
0.00.052.042 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.042 I load_tensors: offloading output layer to GPU
0.00.052.042 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.052 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.053 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.339 I llama_init_from_model: n_seq_max     = 1
0.00.052.340 I llama_init_from_model: n_ctx         = 128
0.00.052.340 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.340 I llama_init_from_model: n_batch       = 128
0.00.052.340 I llama_init_from_model: n_ubatch      = 128
0.00.052.340 I llama_init_from_model: flash_attn    = 0
0.00.052.341 I llama_init_from_model: freq_base     = 10000.0
0.00.052.341 I llama_init_from_model: freq_scale    = 1
0.00.052.341 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.342 I ggml_metal_init: allocating
0.00.052.345 I ggml_metal_init: found device: Apple M4
0.00.052.346 I ggml_metal_init: picking default device: Apple M4
0.00.052.936 I ggml_metal_init: using embedded metal library
0.00.055.289 I ggml_metal_init: GPU name:   Apple M4
0.00.055.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.291 I ggml_metal_init: simdgroup reduction   = true
0.00.055.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.291 I ggml_metal_init: has bfloat            = true
0.00.055.292 I ggml_metal_init: use bfloat            = true
0.00.055.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.195 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.213 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.062 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.063 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.063 I llama_init_from_model: graph nodes  = 967
0.00.067.063 I llama_init_from_model: graph splits = 2
0.00.067.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.053 I 
0.00.637.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.093 I perplexity: tokenizing the input ..
0.00.645.132 I perplexity: tokenization took 8.038 ms
0.00.645.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.250 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.787.488 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.787.503 I llama_perf_context_print:        load time =     628.27 ms
0.00.787.504 I llama_perf_context_print: prompt eval time =     140.87 ms /   128 tokens (    1.10 ms per token,   908.65 tokens per second)
0.00.787.504 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.505 I llama_perf_context_print:       total time =     150.45 ms /   129 tokens
0.00.788.027 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.078s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.137 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.840 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.840 I llama_model_loader: - type  f32:  194 tensors
0.00.025.840 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.841 I print_info: file format = GGUF V3 (latest)
0.00.025.841 I print_info: file type   = Q6_K
0.00.025.842 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.148 I load: special tokens cache size = 25
0.00.049.876 I load: token to piece cache size = 0.2984 MB
0.00.049.879 I print_info: arch             = gptneox
0.00.049.880 I print_info: vocab_only       = 0
0.00.049.880 I print_info: n_ctx_train      = 2048
0.00.049.880 I print_info: n_embd           = 2048
0.00.049.880 I print_info: n_layer          = 24
0.00.049.883 I print_info: n_head           = 16
0.00.049.884 I print_info: n_head_kv        = 16
0.00.049.884 I print_info: n_rot            = 32
0.00.049.884 I print_info: n_swa            = 0
0.00.049.884 I print_info: n_embd_head_k    = 128
0.00.049.884 I print_info: n_embd_head_v    = 128
0.00.049.885 I print_info: n_gqa            = 1
0.00.049.886 I print_info: n_embd_k_gqa     = 2048
0.00.049.887 I print_info: n_embd_v_gqa     = 2048
0.00.049.887 I print_info: f_norm_eps       = 1.0e-05
0.00.049.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.889 I print_info: f_logit_scale    = 0.0e+00
0.00.049.890 I print_info: n_ff             = 8192
0.00.049.890 I print_info: n_expert         = 0
0.00.049.890 I print_info: n_expert_used    = 0
0.00.049.891 I print_info: causal attn      = 1
0.00.049.891 I print_info: pooling type     = 0
0.00.049.891 I print_info: rope type        = 2
0.00.049.891 I print_info: rope scaling     = linear
0.00.049.891 I print_info: freq_base_train  = 10000.0
0.00.049.892 I print_info: freq_scale_train = 1
0.00.049.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.892 I print_info: rope_finetuned   = unknown
0.00.049.892 I print_info: ssm_d_conv       = 0
0.00.049.893 I print_info: ssm_d_inner      = 0
0.00.049.893 I print_info: ssm_d_state      = 0
0.00.049.893 I print_info: ssm_dt_rank      = 0
0.00.049.893 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.893 I print_info: model type       = 1.4B
0.00.049.894 I print_info: model params     = 1.41 B
0.00.049.894 I print_info: general.name     = 1.4B
0.00.049.894 I print_info: vocab type       = BPE
0.00.049.897 I print_info: n_vocab          = 50304
0.00.049.897 I print_info: n_merges         = 50009
0.00.049.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.897 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.898 I print_info: LF token         = 128 'Ä'
0.00.049.898 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.898 I print_info: max token length = 1024
0.00.051.879 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.879 I load_tensors: offloading output layer to GPU
0.00.051.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.890 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.891 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.169 I llama_init_from_model: n_seq_max     = 1
0.00.052.170 I llama_init_from_model: n_ctx         = 128
0.00.052.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.170 I llama_init_from_model: n_batch       = 128
0.00.052.170 I llama_init_from_model: n_ubatch      = 128
0.00.052.170 I llama_init_from_model: flash_attn    = 0
0.00.052.171 I llama_init_from_model: freq_base     = 10000.0
0.00.052.171 I llama_init_from_model: freq_scale    = 1
0.00.052.171 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.172 I ggml_metal_init: allocating
0.00.052.174 I ggml_metal_init: found device: Apple M4
0.00.052.176 I ggml_metal_init: picking default device: Apple M4
0.00.052.746 I ggml_metal_init: using embedded metal library
0.00.055.050 I ggml_metal_init: GPU name:   Apple M4
0.00.055.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.052 I ggml_metal_init: simdgroup reduction   = true
0.00.055.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.052 I ggml_metal_init: has bfloat            = true
0.00.055.052 I ggml_metal_init: use bfloat            = true
0.00.055.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.410 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.655 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.657 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.574 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.575 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.575 I llama_init_from_model: graph nodes  = 967
0.00.066.575 I llama_init_from_model: graph splits = 2
0.00.066.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.266.877 I 
0.00.266.913 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.266.923 I perplexity: tokenizing the input ..
0.00.274.137 I perplexity: tokenization took 7.213 ms
0.00.274.148 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.414.487 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.415.697 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.415.713 I llama_perf_context_print:        load time =     256.19 ms
0.00.415.715 I llama_perf_context_print: prompt eval time =     140.12 ms /   128 tokens (    1.09 ms per token,   913.54 tokens per second)
0.00.415.715 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.415.716 I llama_perf_context_print:       total time =     148.84 ms /   129 tokens
0.00.416.133 I ggml_metal_free: deallocating

real	0m0.431s
user	0m0.076s
sys	0m0.048s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.395 I build: 4532 (5245729e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.467 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.291 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.308 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.954 I llama_model_loader: - type  f32:  194 tensors
0.00.054.954 I llama_model_loader: - type  f16:   98 tensors
0.00.054.955 I print_info: file format = GGUF V3 (latest)
0.00.054.955 I print_info: file type   = all F32 (guessed)
0.00.054.957 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.692 I load: special tokens cache size = 25
0.00.087.319 I load: token to piece cache size = 0.2984 MB
0.00.087.322 I print_info: arch             = gptneox
0.00.087.322 I print_info: vocab_only       = 0
0.00.087.323 I print_info: n_ctx_train      = 2048
0.00.087.323 I print_info: n_embd           = 2048
0.00.087.323 I print_info: n_layer          = 24
0.00.087.326 I print_info: n_head           = 16
0.00.087.327 I print_info: n_head_kv        = 16
0.00.087.328 I print_info: n_rot            = 32
0.00.087.328 I print_info: n_swa            = 0
0.00.087.328 I print_info: n_embd_head_k    = 128
0.00.087.328 I print_info: n_embd_head_v    = 128
0.00.087.329 I print_info: n_gqa            = 1
0.00.087.330 I print_info: n_embd_k_gqa     = 2048
0.00.087.332 I print_info: n_embd_v_gqa     = 2048
0.00.087.332 I print_info: f_norm_eps       = 1.0e-05
0.00.087.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.335 I print_info: f_logit_scale    = 0.0e+00
0.00.087.335 I print_info: n_ff             = 8192
0.00.087.336 I print_info: n_expert         = 0
0.00.087.336 I print_info: n_expert_used    = 0
0.00.087.337 I print_info: causal attn      = 1
0.00.087.338 I print_info: pooling type     = 0
0.00.087.338 I print_info: rope type        = 2
0.00.087.338 I print_info: rope scaling     = linear
0.00.087.338 I print_info: freq_base_train  = 10000.0
0.00.087.338 I print_info: freq_scale_train = 1
0.00.087.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.339 I print_info: rope_finetuned   = unknown
0.00.087.339 I print_info: ssm_d_conv       = 0
0.00.087.339 I print_info: ssm_d_inner      = 0
0.00.087.339 I print_info: ssm_d_state      = 0
0.00.087.339 I print_info: ssm_dt_rank      = 0
0.00.087.339 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.340 I print_info: model type       = 1.4B
0.00.087.340 I print_info: model params     = 1.41 B
0.00.087.340 I print_info: general.name     = 1.4B
0.00.087.341 I print_info: vocab type       = BPE
0.00.087.341 I print_info: n_vocab          = 50304
0.00.087.341 I print_info: n_merges         = 50009
0.00.087.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.344 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.345 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.346 I print_info: LF token         = 128 'Ä'
0.00.087.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.346 I print_info: max token length = 1024
0.00.089.910 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.910 I load_tensors: offloading output layer to GPU
0.00.089.910 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.921 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.923 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.204 I llama_init_from_model: n_seq_max     = 1
0.00.090.205 I llama_init_from_model: n_ctx         = 128
0.00.090.205 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.205 I llama_init_from_model: n_batch       = 128
0.00.090.206 I llama_init_from_model: n_ubatch      = 128
0.00.090.206 I llama_init_from_model: flash_attn    = 0
0.00.090.206 I llama_init_from_model: freq_base     = 10000.0
0.00.090.206 I llama_init_from_model: freq_scale    = 1
0.00.090.207 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.207 I ggml_metal_init: allocating
0.00.090.210 I ggml_metal_init: found device: Apple M4
0.00.090.212 I ggml_metal_init: picking default device: Apple M4
0.00.090.850 I ggml_metal_init: using embedded metal library
0.00.093.442 I ggml_metal_init: GPU name:   Apple M4
0.00.093.443 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.444 I ggml_metal_init: simdgroup reduction   = true
0.00.093.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.444 I ggml_metal_init: has bfloat            = true
0.00.093.445 I ggml_metal_init: use bfloat            = true
0.00.093.445 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.458 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.756 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.758 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.772 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.683 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.104.684 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.104.684 I llama_init_from_model: graph nodes  = 967
0.00.104.685 I llama_init_from_model: graph splits = 2
0.00.104.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.686 I 
0.00.104.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.723 I compute_imatrix: tokenizing the input ..
0.00.111.757 I compute_imatrix: tokenization took 7.032 ms
0.00.111.759 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.666.646 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.669.281 I llama_perf_context_print:        load time =    1644.18 ms
0.01.669.283 I llama_perf_context_print: prompt eval time =    1554.28 ms /   128 tokens (   12.14 ms per token,    82.35 tokens per second)
0.01.669.285 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.669.286 I llama_perf_context_print:       total time =    1646.81 ms /   129 tokens
0.01.669.879 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.170s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4532 (5245729e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be0a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be0ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be0b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be0ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be0d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be0d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be11c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be14a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be15c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be15ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be18de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be1d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be1dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be22fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be2a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be2b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be2c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be2d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be2d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be2e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be30e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be32a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be36f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be38fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be3ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be3f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be40ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be41f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be43640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be4c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be4c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be4dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be4f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be4fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be51930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be51e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be53910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be53e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be54e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be56e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be57380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be58e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be59e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be5b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be5c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be5cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be5d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be5e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be5edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be5f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be5f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be5fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be60840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be60ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be61620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be61ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be62400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be63680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be64da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be66130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be66850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be66f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be67230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be67a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be67ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be682f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.191.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.191.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be67fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be4a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be4c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be4a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be68a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be68cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be68f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be69250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be69510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be697d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be69a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be69d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be6a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be6a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be6a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be6ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be6add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be6b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be6b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be6b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be6bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be6c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be6c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be6c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be6c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be6cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be6ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be6d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be6d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be6d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be6dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be6df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be6e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be6e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be6e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be6ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be6ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be6efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be6f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be6f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be6f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be6fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be6fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be70050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be70310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be705d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be70890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be70b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be70e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be710d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be71390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be71650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be71910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be71bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be71e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be72150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be72410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be726d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be72990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be72c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be72f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be73490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be73750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be73a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be73cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be73f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be74250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be74510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be747d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be74a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be74d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be75010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be752d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be75590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be75850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be75b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be75dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be76090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be76350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be76610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be76b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be76e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be77110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be77690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be77950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be77c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be77ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be78190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be78450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be78710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be789d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be78f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be79210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be794d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be79790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be79a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be79d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be79fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be7a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be7a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be7a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be7aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be7ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be7b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be7b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be7b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be7b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be7bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be7be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be7c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be7c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be7c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be7c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be7cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be7ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be7d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be7d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be7d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be7d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be7dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be7df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be7e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be7e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be7e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be7ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be7ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be7ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be7f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be7fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be7fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be80010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be802d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be80590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be80850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be80b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be80dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be81090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be81350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be81610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be818d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be81b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be81e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be82110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be823d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be82690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be82950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be82c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be82ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be83190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be83450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be83710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be839d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be83c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be83f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be84210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be844d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be84790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be84a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be84d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be84fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be85290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be85550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be85810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be85ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be85d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be86050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be86310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be86890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be86b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be86e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be870d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be87390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be87650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be87910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be87bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be87e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be88330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be88ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be88da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be89060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be894d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be89940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be89db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be8a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be8a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be8ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be8af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be8b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be8b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be8bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be8c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be8c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be8ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be8ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be8d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be8d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be8dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be8e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be8e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be8e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be8ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be8f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be8f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be8fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be8ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be903c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be90830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be90ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be91110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be91580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be91e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be92740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be92bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be93020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be93490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be93900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be93d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be94650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be94ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be94f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be953a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be95810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be95c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be960f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be96560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be969d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be96e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be972b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be97720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be97b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be98000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be98470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be988e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be98d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be991c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be99630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be99aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be99f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be9a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be9a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be9ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be9b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be9b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be9b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be9be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be9c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be9c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be9d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be9d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be9dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be9e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be9e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be9f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be9f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be9fa50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13bd044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13bd04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13bd04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13bd05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13bd056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13bd05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13bd05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13bd063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13bd06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13bd06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13bd07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13bd07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13bd08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13bd08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13bd09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13bd09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13bd0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13bd0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13bd0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13bd0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13bd0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13bd0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13bd0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13bd0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13bd0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13bd0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13bd0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13bd0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13bd0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13bd0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13bd0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13bd0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13bd0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13bd0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13bd102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13bd10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13bd10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13bd10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13bd11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13bd118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13bd11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13bd121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13bd12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13bd12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13bd12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13bd13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13bd137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13bd13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13bd140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13bd14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13bd149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13bd14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13bd15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13bd156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13bd15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13bd15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13bd16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13bd16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13bd16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13bd17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13bd17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13bd17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13bd18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13bd184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13bd18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13bd18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13bd19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13bd196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13bd19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13bd19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13bd1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13bd1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13bd1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13bd1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13bd1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13bd1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13bd1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13bd1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13bd1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13bd1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13bd1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13bd1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13bd1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13bd1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13bd1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13bd1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13bd1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13bd1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13bd1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13bd1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13bd1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13bd20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13bd20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13bd20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13bd20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13bd212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13bd21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13bd21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13bd22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13bd224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13bd22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13bd22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13bd231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13bd23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13bd23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13bd241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13bd24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13bd24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13bd24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13bd25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13bd257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13bd25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13bd260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13bd26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13bd269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13bd26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13bd27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13bd276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13bd27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13bd27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13bd28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13bd288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13bd28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13bd29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13bd29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13bd29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13bd29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13bd2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13bd2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13bd2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13bd2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13bd2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13bd2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13bd2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13bd2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13bd2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13bd2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13bd2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13bd2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13bd2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13bd2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13bd2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13bd2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13bd2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13bd2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13bd2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13bd2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13bd2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13bd30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13bd304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13bd30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13bd30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13bd31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13bd316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13bd31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13bd31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13bd32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13bd32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13bd32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13bd33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13bd335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13bd33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13bd33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13bd34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13bd34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13bd34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13bd35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13bd354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13bd35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13bd35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13bd36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13bd36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13bd36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13bd36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13bd373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13bd37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13bd37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13bd38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13bd385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13bd38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13bd38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13bd392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13bd39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13bd39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13bd3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13bd3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13bd3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13bd3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13bd3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13bd3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13bd3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13bd3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13bd3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13bd3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13bd3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13bd3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13bd3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13bd3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13bd3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13bd3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13bd3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13bd3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13bd3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13bd3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13bd3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13bd3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13bd401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13bd40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13bd40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13bd40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13bd41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13bd41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13bd42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13bd424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13bd42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13bd42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13bd431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13bd43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13bd43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13bd43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13bd443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13bd44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13bd44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13bd45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13bd45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13bd459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13bd45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13bd462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13bd46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13bd46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13bd47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13bd47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13bd478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13bd47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13bd481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13bd48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13bd48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13bd48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13bd49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13bd49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13bd49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13bd4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13bd4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13bd4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13bd4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13bd4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13bd4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13bd4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13bd4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13bd4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13bd4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13bd4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13bd4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13bd4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13bd4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13bd4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13bd4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13bd4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13bd4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13bd4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13bd4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13bd4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13bd4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13bd50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13bd506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13bd50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13bd50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13bd51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13bd518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13bd51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13bd52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13bd52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13bd52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13bd52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13bd53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13bd537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13bd53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13bd540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13bd54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13bd54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13bd54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13bd55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13bd556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13bd56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13bd56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13bd56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13bd576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13bd57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13bd57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13bd583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13bd589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.980s
user	0m0.304s
sys	0m0.339s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4532 (5245729e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15bf0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15bf0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15bf0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15bf0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15bf0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15bf0f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15bf0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15bf0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15bf101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15bf106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15bf10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15bf110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15bf11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15bf123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15bf12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15bf132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15bf13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15bf14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15bf14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15bf15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15bf15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15bf15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15bf16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15bf16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15bf17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15bf177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15bf17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15bf18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15bf18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15bf19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15bf19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15bf199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15bf1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15bf1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15bf1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15bf1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15bf1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15bf1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15bf1bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15bf1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15bf1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15bf1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15bf1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15bf1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15bf1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15bf1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15bf1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15bf1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15bf1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15bf1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15bf1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15bf20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15bf20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15bf21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15bf21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15bf21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15bf22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15bf22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15bf22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15bf23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15bf23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15bf239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15bf23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15bf242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15bf24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15bf24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15bf250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15bf25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15bf25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15bf25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15bf26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15bf267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15bf26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15bf271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15bf27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15bf27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15bf281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15bf28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15bf28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15bf291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15bf29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15bf29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15bf2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15bf2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15bf2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15bf2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15bf2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15bf2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15bf2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15bf2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15bf2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15bf2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15bf2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15bf2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15bf2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15bf2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15bf2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15bf1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15bf2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15bf2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15bf2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15bf302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15bf30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15bf30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15bf312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15bf31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15bf31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15bf322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15bf32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15bf32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15bf332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15bf337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15bf33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15bf341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15bf34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15bf34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15bf34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15bf35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15bf35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15bf35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15bf36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15bf366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15bf36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15bf37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15bf374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15bf37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15bf37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15bf382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15bf38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15bf38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15bf39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15bf39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15bf399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15bf39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15bf3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15bf3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15bf3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15bf3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15bf3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15bf3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15bf3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15bf3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15bf3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15bf3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15bf3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15bf3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15bf3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15bf3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15bf3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15bf3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15bf3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15bf3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15bf3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15bf3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15bf3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15bf40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15bf408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15bf40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15bf41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15bf416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15bf41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15bf41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15bf42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15bf42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15bf42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15bf43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15bf43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15bf43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15bf44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15bf444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15bf44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15bf44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15bf452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15bf45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15bf45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15bf460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15bf46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15bf469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15bf46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15bf47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15bf477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15bf47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15bf48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15bf485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15bf48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15bf48ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15bf49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15bf49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15bf49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15bf4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15bf4a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15bf4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15bf4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15bf4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15bf4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15bf4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15bf4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15bf4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15bf4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15bf4d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15bf4d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15bf4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15bf4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15bf4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15bf4eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15bf4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15bf4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15bf50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15bf50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15bf50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15bf51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15bf517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15bf51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15bf52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15bf527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15bf52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15bf53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15bf53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15bf53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15bf54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15bf54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15bf54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15bf55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15bf55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15bf55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15bf56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15bf56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15bf56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15bf57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15bf57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15bf57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15bf581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15bf58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15bf58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15bf591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15bf59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15bf59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15bf5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15bf5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15bf5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15bf5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15bf5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15bf5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15bf5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15bf5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15bf5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15bf5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15bf5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15bf5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15bf5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15bf5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15bf5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15bf5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15bf5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15bf5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15bf60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15bf606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15bf60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15bf61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15bf616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15bf61c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15bf62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15bf626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15bf62bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15bf63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15bf63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15bf63be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15bf64080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15bf64520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15bf649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15bf64e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15bf65300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15bf657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15bf65c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15bf660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15bf66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15bf66a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15bf66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15bf67360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15bf67800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15bf67ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15bf68140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15bf68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15bf68db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15bf694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15bf69bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15bf6a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15bf6a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15bf6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15bf6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15bf6b690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.113.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d0090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d00a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d00a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d00ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d00b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d00bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d00c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d00cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d00d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d00d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d00e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d00e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d00e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d00eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d00ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d00f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d00f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d00fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d0101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d0123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d0139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d0142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d0161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d0170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d0186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15d019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15d019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15d01a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15d01a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15d01aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15d01aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15d01b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15d01b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15d01bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15d01c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15d01c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15d01c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15d01cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15d01d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15d01d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15d01db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15d01df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15d01e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15d01e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15d01ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15d01f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15d01f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15d01fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15d01fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15d020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15d020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15d020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15d021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15d0214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15d021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15d021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15d022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15d022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15d022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15d022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15d0233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15d023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15d023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15d024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15d0245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15d024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15d024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15d0252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15d025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15d025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15d026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15d0264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15d026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15d026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15d027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15d027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15d027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15d027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15d0283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15d028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15d028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15d029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15d029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15d0299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15d029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15d02a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15d02a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15d02abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15d02b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15d02b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15d02b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15d02bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15d02c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15d02c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15d02cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15d02cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15d02d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15d02d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15d02dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15d02e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15d02e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15d02e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15d02ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15d02f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15d02f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15d02fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15d030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15d030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15d0308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15d030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15d0311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15d031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15d031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15d031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15d032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15d0327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15d032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15d0330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15d033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15d0339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15d033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15d034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15d034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15d034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15d034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15d035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15d035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15d036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15d036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15d036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15d036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15d037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15d0377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15d037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15d0380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15d038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15d038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15d038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15d039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15d0396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15d039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15d039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15d03a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15d03a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15d03ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15d03b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15d03b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15d03ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15d03bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15d03c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15d03c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15d03cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15d03d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15d03d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15d03d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15d03ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15d03e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15d03e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15d03eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15d03ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15d03f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15d03f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15d03fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15d0402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15d040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15d040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15d041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15d041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15d041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15d0425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15d042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15d042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15d043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15d0439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15d043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15d044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15d044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15d0450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15d045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15d045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15d046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15d0467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15d046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15d047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15d047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15d047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15d048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15d048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15d049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15d0495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15d049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15d04a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15d04a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15d04acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15d04b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15d04b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15d04be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15d04c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15d04c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15d04cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15d04d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15d04dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15d04e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15d04e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15d04ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15d04f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15d04f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15d04fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15d050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15d0508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15d050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15d051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15d051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15d051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15d052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15d052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15d053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15d0536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15d053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15d054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15d054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15d054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15d055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15d055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15d055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15d0564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15d056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15d056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15d057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15d057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15d057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15d058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15d058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15d058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15d059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15d059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15d059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15d05a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15d05a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15d05ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15d05b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15d05b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15d05bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15d05c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15d05cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15d05d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15d05d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15d05dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15d05e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15d05e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15be088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15be08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15be091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15be09610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15be09a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15be09ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15be0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15be0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15be0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15be0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15be0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15be0bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15be0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15be0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15be0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15be0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15be0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15be0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15be0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15be0fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15be101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15be10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15be11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15be11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15be11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15be12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15be123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15be12850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15be12cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15be13130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15be13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15be13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15be13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15be14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15be146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15be14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15be150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15be155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15be15ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15be15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15be164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15be169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15be16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15be173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15be178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15be17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15be18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15be18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15be18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15be18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15be19350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15be197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15be19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15be1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15be1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15be1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15be1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15be1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15be1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15be1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15be1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15be1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15be1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15be1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15be1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15be1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15be1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15be1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15be1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15be1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15be1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15be1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15be1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15be203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15be20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15be20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15be213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15be218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15be21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15be22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15be228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15be22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15be23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15be238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15be23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15be24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15be248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15be24e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15be25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15be258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15be25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15be26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15be268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15be26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15be27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15be27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15be27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15be28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15be28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15be28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15be29320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15be29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15be29dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15be2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15be2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15be2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15be2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15be2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15be2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15be2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15be2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15be2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15be2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15be2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15be2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15be2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15be2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15be2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15be2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15be2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15be2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15be2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15be30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15be305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15be30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15be30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15be313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15be31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15be31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15be32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15be32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15be32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15be32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15be33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15be338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15be33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15be341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15be34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15be34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15be34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15be35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15be35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15be35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15be36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15be366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15be36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15be37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15be374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15be37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15be37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15be382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15be38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15be38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15be39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15be39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15be399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15be39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15be3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15be3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15be3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15be3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15be3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15be3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15be3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15be3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15be3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15be3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15be3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15be3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15be3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15be3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15be3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15be3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15be3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15be3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15be3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15be3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15be3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15be40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15be408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15be40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15be41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15be416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15be41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15be41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15be42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15be42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15be42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15be43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15be43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15be43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15be44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15be444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15be44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15be44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15be454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15be45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15be45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15be462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15be46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15be46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15be47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15be47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15be47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15be48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15be48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15be49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15be49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15be49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15be4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15be4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15be4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15be4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15be4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15be4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15be4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15be4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15be4cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15be4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15be4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15be4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15be4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15be4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15be4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15be4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15be4f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15be4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15be50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15be507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15be50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15be51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15be51790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15be51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15be52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15be52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15be52cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15be53220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15be53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15be53cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15be54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15be54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15be54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15be55200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15be55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15be55ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15be561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15be56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15be56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15be571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15be57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15be57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15be581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15be58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15be58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15be591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15be59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15be59c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15be5a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15be5a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15be5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15be5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15be5b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15be5bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15be5c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15be5c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15be5cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15be5d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15be5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15be5dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15be5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15be5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15be5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15be5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15be5f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15be5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15be5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15be5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15be60460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15be60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15be60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15be61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15be616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15be61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15be62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15be62a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15be63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15be638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15be63b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15be64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15be64620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15be64c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.949s
user	0m0.251s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
