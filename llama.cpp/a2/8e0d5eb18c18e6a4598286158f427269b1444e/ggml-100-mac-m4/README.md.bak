### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.45 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.30 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.00 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.69 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    1.11 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.47 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.15 sec*proc (29 tests)

Total Test time (real) = 164.16 sec

real	2m44.223s
user	4m39.381s
sys	0m5.704s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.86 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.48 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.39 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.28 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.47 sec*proc (29 tests)

Total Test time (real) =  48.48 sec

real	0m48.494s
user	0m54.928s
sys	0m5.136s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.212 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.791 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.799 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.801 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.803 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.803 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.804 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.805 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.806 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.807 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.807 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.808 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.811 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.812 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.812 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.813 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.814 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.814 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.815 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.081 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.083 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.084 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.084 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.085 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.085 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.086 I llama_model_loader: - type  f32:  124 tensors
0.00.033.086 I llama_model_loader: - type  f16:   73 tensors
0.00.033.087 I print_info: file format = GGUF V3 (latest)
0.00.033.088 I print_info: file type   = F16
0.00.033.089 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.357 I load: special tokens cache size = 5
0.00.039.549 I load: token to piece cache size = 0.2032 MB
0.00.039.553 I print_info: arch             = bert
0.00.039.553 I print_info: vocab_only       = 0
0.00.039.553 I print_info: n_ctx_train      = 512
0.00.039.554 I print_info: n_embd           = 384
0.00.039.554 I print_info: n_layer          = 12
0.00.039.557 I print_info: n_head           = 12
0.00.039.558 I print_info: n_head_kv        = 12
0.00.039.558 I print_info: n_rot            = 32
0.00.039.558 I print_info: n_swa            = 0
0.00.039.559 I print_info: n_embd_head_k    = 32
0.00.039.559 I print_info: n_embd_head_v    = 32
0.00.039.560 I print_info: n_gqa            = 1
0.00.039.561 I print_info: n_embd_k_gqa     = 384
0.00.039.561 I print_info: n_embd_v_gqa     = 384
0.00.039.562 I print_info: f_norm_eps       = 1.0e-12
0.00.039.563 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.563 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.563 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.564 I print_info: f_logit_scale    = 0.0e+00
0.00.039.565 I print_info: n_ff             = 1536
0.00.039.565 I print_info: n_expert         = 0
0.00.039.565 I print_info: n_expert_used    = 0
0.00.039.565 I print_info: causal attn      = 0
0.00.039.565 I print_info: pooling type     = 2
0.00.039.566 I print_info: rope type        = 2
0.00.039.569 I print_info: rope scaling     = linear
0.00.039.569 I print_info: freq_base_train  = 10000.0
0.00.039.570 I print_info: freq_scale_train = 1
0.00.039.570 I print_info: n_ctx_orig_yarn  = 512
0.00.039.570 I print_info: rope_finetuned   = unknown
0.00.039.570 I print_info: ssm_d_conv       = 0
0.00.039.571 I print_info: ssm_d_inner      = 0
0.00.039.571 I print_info: ssm_d_state      = 0
0.00.039.571 I print_info: ssm_dt_rank      = 0
0.00.039.571 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.571 I print_info: model type       = 33M
0.00.039.572 I print_info: model params     = 33.21 M
0.00.039.572 I print_info: general.name     = Bge Small
0.00.039.573 I print_info: vocab type       = WPM
0.00.039.573 I print_info: n_vocab          = 30522
0.00.039.573 I print_info: n_merges         = 0
0.00.039.573 I print_info: BOS token        = 101 '[CLS]'
0.00.039.574 I print_info: UNK token        = 100 '[UNK]'
0.00.039.574 I print_info: SEP token        = 102 '[SEP]'
0.00.039.574 I print_info: PAD token        = 0 '[PAD]'
0.00.039.574 I print_info: MASK token       = 103 '[MASK]'
0.00.039.575 I print_info: LF token         = 0 '[PAD]'
0.00.039.575 I print_info: max token length = 21
0.00.039.575 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.042.729 I load_tensors: offloading 12 repeating layers to GPU
0.00.042.731 I load_tensors: offloading output layer to GPU
0.00.042.731 I load_tensors: offloaded 13/13 layers to GPU
0.00.042.757 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.759 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.040 I llama_init_from_model: n_seq_max     = 1
0.00.043.041 I llama_init_from_model: n_ctx         = 512
0.00.043.042 I llama_init_from_model: n_ctx_per_seq = 512
0.00.043.042 I llama_init_from_model: n_batch       = 2048
0.00.043.042 I llama_init_from_model: n_ubatch      = 2048
0.00.043.042 I llama_init_from_model: flash_attn    = 0
0.00.043.043 I llama_init_from_model: freq_base     = 10000.0
0.00.043.043 I llama_init_from_model: freq_scale    = 1
0.00.043.044 I ggml_metal_init: allocating
0.00.043.049 I ggml_metal_init: found device: Apple M4
0.00.043.054 I ggml_metal_init: picking default device: Apple M4
0.00.043.836 I ggml_metal_init: using embedded metal library
0.00.047.924 I ggml_metal_init: GPU name:   Apple M4
0.00.047.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.928 I ggml_metal_init: simdgroup reduction   = true
0.00.047.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.929 I ggml_metal_init: has residency sets    = true
0.00.047.929 I ggml_metal_init: has bfloat            = true
0.00.047.929 I ggml_metal_init: use bfloat            = true
0.00.047.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.930 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.916 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.060.663 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.665 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.687 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.061.952 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.061.954 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.061.954 I llama_init_from_model: graph nodes  = 429
0.00.061.954 I llama_init_from_model: graph splits = 2
0.00.061.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.678 I 
0.00.067.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.333 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.430 I llama_perf_context_print:        load time =      46.48 ms
0.00.073.431 I llama_perf_context_print: prompt eval time =       4.93 ms /     9 tokens (    0.55 ms per token,  1824.82 tokens per second)
0.00.073.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.434 I llama_perf_context_print:       total time =       5.75 ms /    10 tokens
0.00.073.574 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.051s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.538 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.542 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.543 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.544 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.545 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.545 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.546 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.546 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.547 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.547 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.550 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.552 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.552 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.553 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.553 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.553 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.553 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.857 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.494 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.495 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.496 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.496 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.496 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.497 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.497 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.497 I llama_model_loader: - type  f32:  124 tensors
0.00.014.498 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.498 I print_info: file format = GGUF V3 (latest)
0.00.014.499 I print_info: file type   = Q8_0
0.00.014.500 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.869 I load: special tokens cache size = 5
0.00.018.155 I load: token to piece cache size = 0.2032 MB
0.00.018.158 I print_info: arch             = bert
0.00.018.158 I print_info: vocab_only       = 0
0.00.018.158 I print_info: n_ctx_train      = 512
0.00.018.159 I print_info: n_embd           = 384
0.00.018.159 I print_info: n_layer          = 12
0.00.018.162 I print_info: n_head           = 12
0.00.018.162 I print_info: n_head_kv        = 12
0.00.018.162 I print_info: n_rot            = 32
0.00.018.163 I print_info: n_swa            = 0
0.00.018.163 I print_info: n_embd_head_k    = 32
0.00.018.163 I print_info: n_embd_head_v    = 32
0.00.018.164 I print_info: n_gqa            = 1
0.00.018.164 I print_info: n_embd_k_gqa     = 384
0.00.018.165 I print_info: n_embd_v_gqa     = 384
0.00.018.165 I print_info: f_norm_eps       = 1.0e-12
0.00.018.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.166 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.168 I print_info: f_logit_scale    = 0.0e+00
0.00.018.169 I print_info: n_ff             = 1536
0.00.018.169 I print_info: n_expert         = 0
0.00.018.169 I print_info: n_expert_used    = 0
0.00.018.169 I print_info: causal attn      = 0
0.00.018.169 I print_info: pooling type     = 2
0.00.018.170 I print_info: rope type        = 2
0.00.018.170 I print_info: rope scaling     = linear
0.00.018.170 I print_info: freq_base_train  = 10000.0
0.00.018.170 I print_info: freq_scale_train = 1
0.00.018.171 I print_info: n_ctx_orig_yarn  = 512
0.00.018.171 I print_info: rope_finetuned   = unknown
0.00.018.171 I print_info: ssm_d_conv       = 0
0.00.018.171 I print_info: ssm_d_inner      = 0
0.00.018.171 I print_info: ssm_d_state      = 0
0.00.018.173 I print_info: ssm_dt_rank      = 0
0.00.018.173 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.173 I print_info: model type       = 33M
0.00.018.173 I print_info: model params     = 33.21 M
0.00.018.173 I print_info: general.name     = Bge Small
0.00.018.174 I print_info: vocab type       = WPM
0.00.018.174 I print_info: n_vocab          = 30522
0.00.018.174 I print_info: n_merges         = 0
0.00.018.174 I print_info: BOS token        = 101 '[CLS]'
0.00.018.175 I print_info: UNK token        = 100 '[UNK]'
0.00.018.175 I print_info: SEP token        = 102 '[SEP]'
0.00.018.175 I print_info: PAD token        = 0 '[PAD]'
0.00.018.175 I print_info: MASK token       = 103 '[MASK]'
0.00.018.175 I print_info: LF token         = 0 '[PAD]'
0.00.018.175 I print_info: max token length = 21
0.00.018.180 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.766 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.767 I load_tensors: offloading output layer to GPU
0.00.019.768 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.774 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.775 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.956 I llama_init_from_model: n_seq_max     = 1
0.00.019.957 I llama_init_from_model: n_ctx         = 512
0.00.019.958 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.958 I llama_init_from_model: n_batch       = 2048
0.00.019.958 I llama_init_from_model: n_ubatch      = 2048
0.00.019.958 I llama_init_from_model: flash_attn    = 0
0.00.019.959 I llama_init_from_model: freq_base     = 10000.0
0.00.019.959 I llama_init_from_model: freq_scale    = 1
0.00.019.960 I ggml_metal_init: allocating
0.00.019.966 I ggml_metal_init: found device: Apple M4
0.00.019.971 I ggml_metal_init: picking default device: Apple M4
0.00.020.470 I ggml_metal_init: using embedded metal library
0.00.022.832 I ggml_metal_init: GPU name:   Apple M4
0.00.022.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.834 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.835 I ggml_metal_init: simdgroup reduction   = true
0.00.022.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.835 I ggml_metal_init: has residency sets    = true
0.00.022.835 I ggml_metal_init: has bfloat            = true
0.00.022.835 I ggml_metal_init: use bfloat            = true
0.00.022.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.279 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.858 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.860 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.874 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.842 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.843 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.844 I llama_init_from_model: graph nodes  = 429
0.00.034.844 I llama_init_from_model: graph splits = 2
0.00.034.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.804 I 
0.00.038.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.320 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.832 I llama_perf_context_print:        load time =      29.87 ms
0.00.043.833 I llama_perf_context_print: prompt eval time =       4.39 ms /     9 tokens (    0.49 ms per token,  2051.05 tokens per second)
0.00.043.834 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.835 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.044.012 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.265 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.840 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.845 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.847 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.848 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.851 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.852 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.852 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.853 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.854 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.855 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.856 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.856 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.865 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.866 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.867 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.868 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.484 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.484 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.484 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.485 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.485 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.485 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.486 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.486 I llama_model_loader: - type  f32:   40 tensors
0.00.049.491 I llama_model_loader: - type  f16:   30 tensors
0.00.049.492 I print_info: file format = GGUF V3 (latest)
0.00.049.493 I print_info: file type   = F16
0.00.049.493 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.737 W load: empty token at index 5
0.00.058.774 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.288 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.322 I load: special tokens cache size = 5
0.00.324.099 I load: token to piece cache size = 1.5060 MB
0.00.324.105 I print_info: arch             = jina-bert-v2
0.00.324.105 I print_info: vocab_only       = 0
0.00.324.106 I print_info: n_ctx_train      = 8192
0.00.324.106 I print_info: n_embd           = 384
0.00.324.106 I print_info: n_layer          = 4
0.00.324.114 I print_info: n_head           = 12
0.00.324.119 I print_info: n_head_kv        = 12
0.00.324.121 I print_info: n_rot            = 32
0.00.324.121 I print_info: n_swa            = 0
0.00.324.121 I print_info: n_embd_head_k    = 32
0.00.324.121 I print_info: n_embd_head_v    = 32
0.00.324.122 I print_info: n_gqa            = 1
0.00.324.123 I print_info: n_embd_k_gqa     = 384
0.00.324.123 I print_info: n_embd_v_gqa     = 384
0.00.324.124 I print_info: f_norm_eps       = 1.0e-12
0.00.324.125 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.125 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.125 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.126 I print_info: f_logit_scale    = 0.0e+00
0.00.324.126 I print_info: n_ff             = 1536
0.00.324.126 I print_info: n_expert         = 0
0.00.324.126 I print_info: n_expert_used    = 0
0.00.324.126 I print_info: causal attn      = 0
0.00.324.127 I print_info: pooling type     = -1
0.00.324.127 I print_info: rope type        = -1
0.00.324.127 I print_info: rope scaling     = linear
0.00.324.127 I print_info: freq_base_train  = 10000.0
0.00.324.128 I print_info: freq_scale_train = 1
0.00.324.128 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.128 I print_info: rope_finetuned   = unknown
0.00.324.128 I print_info: ssm_d_conv       = 0
0.00.324.128 I print_info: ssm_d_inner      = 0
0.00.324.129 I print_info: ssm_d_state      = 0
0.00.324.129 I print_info: ssm_dt_rank      = 0
0.00.324.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.129 I print_info: model type       = 33M
0.00.324.130 I print_info: model params     = 32.90 M
0.00.324.130 I print_info: general.name     = Jina Bert Implementation
0.00.324.131 I print_info: vocab type       = BPE
0.00.324.131 I print_info: n_vocab          = 61056
0.00.324.131 I print_info: n_merges         = 39382
0.00.324.133 I print_info: BOS token        = 0 '<s>'
0.00.324.133 I print_info: EOS token        = 2 '</s>'
0.00.324.133 I print_info: UNK token        = 3 '<unk>'
0.00.324.133 I print_info: SEP token        = 2 '</s>'
0.00.324.133 I print_info: PAD token        = 1 '<pad>'
0.00.324.133 I print_info: MASK token       = 4 '<mask>'
0.00.324.134 I print_info: EOG token        = 2 '</s>'
0.00.324.134 I print_info: max token length = 45
0.00.324.134 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.326.084 I load_tensors: offloading 4 repeating layers to GPU
0.00.326.085 I load_tensors: offloading output layer to GPU
0.00.326.085 I load_tensors: offloaded 5/5 layers to GPU
0.00.326.109 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.110 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.326.439 I llama_init_from_model: n_seq_max     = 1
0.00.326.441 I llama_init_from_model: n_ctx         = 8192
0.00.326.441 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.441 I llama_init_from_model: n_batch       = 2048
0.00.326.441 I llama_init_from_model: n_ubatch      = 2048
0.00.326.442 I llama_init_from_model: flash_attn    = 0
0.00.326.442 I llama_init_from_model: freq_base     = 10000.0
0.00.326.442 I llama_init_from_model: freq_scale    = 1
0.00.326.444 I ggml_metal_init: allocating
0.00.326.457 I ggml_metal_init: found device: Apple M4
0.00.326.462 I ggml_metal_init: picking default device: Apple M4
0.00.327.421 I ggml_metal_init: using embedded metal library
0.00.330.215 I ggml_metal_init: GPU name:   Apple M4
0.00.330.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.217 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.217 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.218 I ggml_metal_init: simdgroup reduction   = true
0.00.330.218 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.218 I ggml_metal_init: has residency sets    = true
0.00.330.218 I ggml_metal_init: has bfloat            = true
0.00.330.218 I ggml_metal_init: use bfloat            = true
0.00.330.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.339.650 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.342.753 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.342.755 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.342.777 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.615 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.617 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.617 I llama_init_from_model: graph nodes  = 154
0.00.349.617 I llama_init_from_model: graph splits = 2
0.00.349.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.157 I 
0.00.357.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.283 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.284 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.287 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.287 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.289 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.290 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.843 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.346 I llama_perf_context_print:        load time =     333.79 ms
0.00.361.347 I llama_perf_context_print: prompt eval time =       3.49 ms /    62 tokens (    0.06 ms per token, 17744.71 tokens per second)
0.00.361.348 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.348 I llama_perf_context_print:       total time =       4.19 ms /    63 tokens
0.00.361.576 I ggml_metal_free: deallocating

real	0m1.061s
user	0m0.331s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.242 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.445 I main: llama backend init
0.00.000.452 I main: load the model and apply lora adapter, if any
0.00.044.009 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.056.758 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.789 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.791 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.791 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.792 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.792 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.075.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.075.463 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.075.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.075.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.075.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.075.465 I llama_model_loader: - type  f32:  194 tensors
0.00.075.466 I llama_model_loader: - type  f16:   98 tensors
0.00.075.467 I print_info: file format = GGUF V3 (latest)
0.00.075.468 I print_info: file type   = all F32 (guessed)
0.00.075.470 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.090.556 I load: special tokens cache size = 25
0.00.099.062 I load: token to piece cache size = 0.2984 MB
0.00.099.066 I print_info: arch             = gptneox
0.00.099.066 I print_info: vocab_only       = 0
0.00.099.066 I print_info: n_ctx_train      = 2048
0.00.099.066 I print_info: n_embd           = 2048
0.00.099.067 I print_info: n_layer          = 24
0.00.099.069 I print_info: n_head           = 16
0.00.099.070 I print_info: n_head_kv        = 16
0.00.099.071 I print_info: n_rot            = 32
0.00.099.071 I print_info: n_swa            = 0
0.00.099.071 I print_info: n_embd_head_k    = 128
0.00.099.071 I print_info: n_embd_head_v    = 128
0.00.099.073 I print_info: n_gqa            = 1
0.00.099.074 I print_info: n_embd_k_gqa     = 2048
0.00.099.077 I print_info: n_embd_v_gqa     = 2048
0.00.099.077 I print_info: f_norm_eps       = 1.0e-05
0.00.099.078 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.078 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.078 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.079 I print_info: f_logit_scale    = 0.0e+00
0.00.099.079 I print_info: n_ff             = 8192
0.00.099.081 I print_info: n_expert         = 0
0.00.099.081 I print_info: n_expert_used    = 0
0.00.099.081 I print_info: causal attn      = 1
0.00.099.081 I print_info: pooling type     = 0
0.00.099.082 I print_info: rope type        = 2
0.00.099.082 I print_info: rope scaling     = linear
0.00.099.082 I print_info: freq_base_train  = 10000.0
0.00.099.083 I print_info: freq_scale_train = 1
0.00.099.083 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.083 I print_info: rope_finetuned   = unknown
0.00.099.083 I print_info: ssm_d_conv       = 0
0.00.099.083 I print_info: ssm_d_inner      = 0
0.00.099.084 I print_info: ssm_d_state      = 0
0.00.099.084 I print_info: ssm_dt_rank      = 0
0.00.099.084 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.084 I print_info: model type       = 1.4B
0.00.099.084 I print_info: model params     = 1.41 B
0.00.099.085 I print_info: general.name     = 1.4B
0.00.099.085 I print_info: vocab type       = BPE
0.00.099.085 I print_info: n_vocab          = 50304
0.00.099.085 I print_info: n_merges         = 50009
0.00.099.086 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.088 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.088 I print_info: LF token         = 187 ''
0.00.099.089 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.089 I print_info: max token length = 1024
0.00.099.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.164 I load_tensors: offloading output layer to GPU
0.00.153.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.189 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.191 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.153.656 I llama_init_from_model: n_seq_max     = 1
0.00.153.657 I llama_init_from_model: n_ctx         = 2048
0.00.153.657 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.153.657 I llama_init_from_model: n_batch       = 2048
0.00.153.657 I llama_init_from_model: n_ubatch      = 512
0.00.153.657 I llama_init_from_model: flash_attn    = 0
0.00.153.658 I llama_init_from_model: freq_base     = 10000.0
0.00.153.658 I llama_init_from_model: freq_scale    = 1
0.00.153.659 I ggml_metal_init: allocating
0.00.153.700 I ggml_metal_init: found device: Apple M4
0.00.153.705 I ggml_metal_init: picking default device: Apple M4
0.00.154.428 I ggml_metal_init: using embedded metal library
0.00.164.397 I ggml_metal_init: GPU name:   Apple M4
0.00.164.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.164.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.164.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.164.399 I ggml_metal_init: simdgroup reduction   = true
0.00.164.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.164.399 I ggml_metal_init: has residency sets    = true
0.00.164.400 I ggml_metal_init: has bfloat            = true
0.00.164.400 I ggml_metal_init: use bfloat            = true
0.00.164.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.164.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.188.663 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.217.376 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.217.383 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.217.428 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.220.924 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.220.926 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.220.927 I llama_init_from_model: graph nodes  = 967
0.00.220.927 I llama_init_from_model: graph splits = 2
0.00.220.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.221.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.221.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.287.043 I main: llama threadpool init, n_threads = 4
0.00.287.086 I 
0.00.287.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.287.120 I 
0.00.287.297 I sampler seed: 1234
0.00.287.302 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.287.326 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.287.328 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.287.328 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.116.390 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.02.116.390 I llama_perf_context_print:        load time =     242.14 ms
0.02.116.391 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.00 tokens per second)
0.02.116.392 I llama_perf_context_print:        eval time =    1782.51 ms /    63 runs   (   28.29 ms per token,    35.34 tokens per second)
0.02.116.392 I llama_perf_context_print:       total time =    1830.22 ms /    70 tokens
0.02.116.606 I ggml_metal_free: deallocating

real	0m2.435s
user	0m0.132s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.590 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.186 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.653 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.666 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.649 I llama_model_loader: - type  f32:  194 tensors
0.00.055.649 I llama_model_loader: - type  f16:   98 tensors
0.00.055.650 I print_info: file format = GGUF V3 (latest)
0.00.055.651 I print_info: file type   = all F32 (guessed)
0.00.055.652 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.858 I load: special tokens cache size = 25
0.00.075.944 I load: token to piece cache size = 0.2984 MB
0.00.075.947 I print_info: arch             = gptneox
0.00.075.947 I print_info: vocab_only       = 0
0.00.075.947 I print_info: n_ctx_train      = 2048
0.00.075.948 I print_info: n_embd           = 2048
0.00.075.948 I print_info: n_layer          = 24
0.00.075.951 I print_info: n_head           = 16
0.00.075.952 I print_info: n_head_kv        = 16
0.00.075.952 I print_info: n_rot            = 32
0.00.075.952 I print_info: n_swa            = 0
0.00.075.953 I print_info: n_embd_head_k    = 128
0.00.075.953 I print_info: n_embd_head_v    = 128
0.00.075.953 I print_info: n_gqa            = 1
0.00.075.954 I print_info: n_embd_k_gqa     = 2048
0.00.075.955 I print_info: n_embd_v_gqa     = 2048
0.00.075.955 I print_info: f_norm_eps       = 1.0e-05
0.00.075.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.961 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.961 I print_info: f_logit_scale    = 0.0e+00
0.00.075.962 I print_info: n_ff             = 8192
0.00.075.962 I print_info: n_expert         = 0
0.00.075.962 I print_info: n_expert_used    = 0
0.00.075.962 I print_info: causal attn      = 1
0.00.075.962 I print_info: pooling type     = 0
0.00.075.962 I print_info: rope type        = 2
0.00.075.963 I print_info: rope scaling     = linear
0.00.075.963 I print_info: freq_base_train  = 10000.0
0.00.075.963 I print_info: freq_scale_train = 1
0.00.075.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.964 I print_info: rope_finetuned   = unknown
0.00.075.964 I print_info: ssm_d_conv       = 0
0.00.075.964 I print_info: ssm_d_inner      = 0
0.00.075.964 I print_info: ssm_d_state      = 0
0.00.075.964 I print_info: ssm_dt_rank      = 0
0.00.075.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.965 I print_info: model type       = 1.4B
0.00.075.967 I print_info: model params     = 1.41 B
0.00.075.967 I print_info: general.name     = 1.4B
0.00.075.967 I print_info: vocab type       = BPE
0.00.075.967 I print_info: n_vocab          = 50304
0.00.075.968 I print_info: n_merges         = 50009
0.00.075.968 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.968 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.968 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.968 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.969 I print_info: LF token         = 187 ''
0.00.075.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.969 I print_info: max token length = 1024
0.00.075.970 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.436.236 I load_tensors: offloading 24 repeating layers to GPU
0.01.436.241 I load_tensors: offloading output layer to GPU
0.01.436.242 I load_tensors: offloaded 25/25 layers to GPU
0.01.436.271 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.436.273 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.437.034 I llama_init_from_model: n_seq_max     = 1
0.01.437.035 I llama_init_from_model: n_ctx         = 128
0.01.437.035 I llama_init_from_model: n_ctx_per_seq = 128
0.01.437.036 I llama_init_from_model: n_batch       = 128
0.01.437.036 I llama_init_from_model: n_ubatch      = 128
0.01.437.036 I llama_init_from_model: flash_attn    = 0
0.01.437.037 I llama_init_from_model: freq_base     = 10000.0
0.01.437.037 I llama_init_from_model: freq_scale    = 1
0.01.437.037 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.437.042 I ggml_metal_init: allocating
0.01.437.130 I ggml_metal_init: found device: Apple M4
0.01.437.137 I ggml_metal_init: picking default device: Apple M4
0.01.438.351 I ggml_metal_init: using embedded metal library
0.01.442.255 I ggml_metal_init: GPU name:   Apple M4
0.01.442.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.442.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.442.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.442.259 I ggml_metal_init: simdgroup reduction   = true
0.01.442.259 I ggml_metal_init: simdgroup matrix mul. = true
0.01.442.259 I ggml_metal_init: has residency sets    = true
0.01.442.260 I ggml_metal_init: has bfloat            = true
0.01.442.260 I ggml_metal_init: use bfloat            = true
0.01.442.260 I ggml_metal_init: hasUnifiedMemory      = true
0.01.442.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.453.379 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.455.097 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.455.099 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.455.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.456.740 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.456.741 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.456.741 I llama_init_from_model: graph nodes  = 967
0.01.456.742 I llama_init_from_model: graph splits = 2
0.01.456.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.456.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.492.524 I 
0.01.492.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.492.568 I perplexity: tokenizing the input ..
0.01.497.837 I perplexity: tokenization took 5.268 ms
0.01.497.841 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.616.845 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.618.258 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.618.290 I llama_perf_context_print:        load time =    1468.33 ms
0.01.618.291 I llama_perf_context_print: prompt eval time =     118.72 ms /   128 tokens (    0.93 ms per token,  1078.13 tokens per second)
0.01.618.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.618.292 I llama_perf_context_print:       total time =     125.77 ms /   129 tokens
0.01.618.643 I ggml_metal_free: deallocating

real	0m1.804s
user	0m0.097s
sys	0m0.250s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.421 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.369 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.370 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.819 I llama_model_loader: - type  f32:  194 tensors
0.00.028.819 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.820 I print_info: file format = GGUF V3 (latest)
0.00.028.821 I print_info: file type   = Q8_0
0.00.028.822 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.137 I load: special tokens cache size = 25
0.00.042.978 I load: token to piece cache size = 0.2984 MB
0.00.042.983 I print_info: arch             = gptneox
0.00.042.984 I print_info: vocab_only       = 0
0.00.042.984 I print_info: n_ctx_train      = 2048
0.00.042.986 I print_info: n_embd           = 2048
0.00.042.986 I print_info: n_layer          = 24
0.00.042.992 I print_info: n_head           = 16
0.00.042.993 I print_info: n_head_kv        = 16
0.00.042.994 I print_info: n_rot            = 32
0.00.042.994 I print_info: n_swa            = 0
0.00.042.994 I print_info: n_embd_head_k    = 128
0.00.042.994 I print_info: n_embd_head_v    = 128
0.00.042.995 I print_info: n_gqa            = 1
0.00.042.995 I print_info: n_embd_k_gqa     = 2048
0.00.042.996 I print_info: n_embd_v_gqa     = 2048
0.00.042.997 I print_info: f_norm_eps       = 1.0e-05
0.00.042.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.001 I print_info: f_logit_scale    = 0.0e+00
0.00.043.001 I print_info: n_ff             = 8192
0.00.043.001 I print_info: n_expert         = 0
0.00.043.002 I print_info: n_expert_used    = 0
0.00.043.002 I print_info: causal attn      = 1
0.00.043.002 I print_info: pooling type     = 0
0.00.043.003 I print_info: rope type        = 2
0.00.043.003 I print_info: rope scaling     = linear
0.00.043.003 I print_info: freq_base_train  = 10000.0
0.00.043.004 I print_info: freq_scale_train = 1
0.00.043.005 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.005 I print_info: rope_finetuned   = unknown
0.00.043.005 I print_info: ssm_d_conv       = 0
0.00.043.005 I print_info: ssm_d_inner      = 0
0.00.043.006 I print_info: ssm_d_state      = 0
0.00.043.006 I print_info: ssm_dt_rank      = 0
0.00.043.006 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.006 I print_info: model type       = 1.4B
0.00.043.007 I print_info: model params     = 1.41 B
0.00.043.008 I print_info: general.name     = 1.4B
0.00.043.009 I print_info: vocab type       = BPE
0.00.043.009 I print_info: n_vocab          = 50304
0.00.043.009 I print_info: n_merges         = 50009
0.00.043.009 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.009 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.010 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.010 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.010 I print_info: LF token         = 187 ''
0.00.043.010 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.010 I print_info: max token length = 1024
0.00.043.011 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.230.384 I load_tensors: offloading 24 repeating layers to GPU
0.01.230.389 I load_tensors: offloading output layer to GPU
0.01.230.390 I load_tensors: offloaded 25/25 layers to GPU
0.01.230.410 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.230.412 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.231.266 I llama_init_from_model: n_seq_max     = 1
0.01.231.268 I llama_init_from_model: n_ctx         = 2048
0.01.231.268 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.231.269 I llama_init_from_model: n_batch       = 2048
0.01.231.269 I llama_init_from_model: n_ubatch      = 512
0.01.231.269 I llama_init_from_model: flash_attn    = 0
0.01.231.270 I llama_init_from_model: freq_base     = 10000.0
0.01.231.271 I llama_init_from_model: freq_scale    = 1
0.01.231.272 I ggml_metal_init: allocating
0.01.231.288 I ggml_metal_init: found device: Apple M4
0.01.231.296 I ggml_metal_init: picking default device: Apple M4
0.01.232.607 I ggml_metal_init: using embedded metal library
0.01.237.972 I ggml_metal_init: GPU name:   Apple M4
0.01.237.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.237.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.237.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.237.977 I ggml_metal_init: simdgroup reduction   = true
0.01.237.978 I ggml_metal_init: simdgroup matrix mul. = true
0.01.237.978 I ggml_metal_init: has residency sets    = true
0.01.237.978 I ggml_metal_init: has bfloat            = true
0.01.237.978 I ggml_metal_init: use bfloat            = true
0.01.237.979 I ggml_metal_init: hasUnifiedMemory      = true
0.01.237.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.254.767 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.302.562 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.302.568 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.302.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.307.875 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.307.877 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.307.877 I llama_init_from_model: graph nodes  = 967
0.01.307.878 I llama_init_from_model: graph splits = 2
0.01.307.881 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.307.995 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.307.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.364.312 I main: llama threadpool init, n_threads = 4
0.01.364.358 I 
0.01.364.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.364.380 I 
0.01.364.532 I sampler seed: 1234
0.01.364.537 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.364.581 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.364.582 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.364.582 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.449.956 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.02.449.956 I llama_perf_context_print:        load time =    1353.17 ms
0.02.449.957 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.39 tokens per second)
0.02.449.958 I llama_perf_context_print:        eval time =    1033.23 ms /    63 runs   (   16.40 ms per token,    60.97 tokens per second)
0.02.449.958 I llama_perf_context_print:       total time =    1086.36 ms /    70 tokens
0.02.450.213 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.108s
sys	0m0.272s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.049 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.010 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.014 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.556 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.557 I llama_model_loader: - type  f32:  194 tensors
0.00.025.557 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.558 I print_info: file format = GGUF V3 (latest)
0.00.025.558 I print_info: file type   = Q8_0
0.00.025.559 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.454 I load: special tokens cache size = 25
0.00.039.490 I load: token to piece cache size = 0.2984 MB
0.00.039.495 I print_info: arch             = gptneox
0.00.039.495 I print_info: vocab_only       = 0
0.00.039.495 I print_info: n_ctx_train      = 2048
0.00.039.495 I print_info: n_embd           = 2048
0.00.039.495 I print_info: n_layer          = 24
0.00.039.500 I print_info: n_head           = 16
0.00.039.501 I print_info: n_head_kv        = 16
0.00.039.501 I print_info: n_rot            = 32
0.00.039.501 I print_info: n_swa            = 0
0.00.039.502 I print_info: n_embd_head_k    = 128
0.00.039.502 I print_info: n_embd_head_v    = 128
0.00.039.502 I print_info: n_gqa            = 1
0.00.039.503 I print_info: n_embd_k_gqa     = 2048
0.00.039.504 I print_info: n_embd_v_gqa     = 2048
0.00.039.504 I print_info: f_norm_eps       = 1.0e-05
0.00.039.505 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.505 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.508 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.508 I print_info: f_logit_scale    = 0.0e+00
0.00.039.508 I print_info: n_ff             = 8192
0.00.039.509 I print_info: n_expert         = 0
0.00.039.509 I print_info: n_expert_used    = 0
0.00.039.509 I print_info: causal attn      = 1
0.00.039.509 I print_info: pooling type     = 0
0.00.039.510 I print_info: rope type        = 2
0.00.039.510 I print_info: rope scaling     = linear
0.00.039.510 I print_info: freq_base_train  = 10000.0
0.00.039.513 I print_info: freq_scale_train = 1
0.00.039.514 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.514 I print_info: rope_finetuned   = unknown
0.00.039.514 I print_info: ssm_d_conv       = 0
0.00.039.515 I print_info: ssm_d_inner      = 0
0.00.039.515 I print_info: ssm_d_state      = 0
0.00.039.515 I print_info: ssm_dt_rank      = 0
0.00.039.515 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.515 I print_info: model type       = 1.4B
0.00.039.515 I print_info: model params     = 1.41 B
0.00.039.516 I print_info: general.name     = 1.4B
0.00.039.516 I print_info: vocab type       = BPE
0.00.039.516 I print_info: n_vocab          = 50304
0.00.039.516 I print_info: n_merges         = 50009
0.00.039.517 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: LF token         = 187 ''
0.00.039.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: max token length = 1024
0.00.039.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.932.415 I load_tensors: offloading 24 repeating layers to GPU
0.00.932.420 I load_tensors: offloading output layer to GPU
0.00.932.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.932.439 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.932.440 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.933.348 I llama_init_from_model: n_seq_max     = 1
0.00.933.353 I llama_init_from_model: n_ctx         = 128
0.00.933.353 I llama_init_from_model: n_ctx_per_seq = 128
0.00.933.354 I llama_init_from_model: n_batch       = 128
0.00.933.354 I llama_init_from_model: n_ubatch      = 128
0.00.933.354 I llama_init_from_model: flash_attn    = 0
0.00.933.355 I llama_init_from_model: freq_base     = 10000.0
0.00.933.356 I llama_init_from_model: freq_scale    = 1
0.00.933.356 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.933.358 I ggml_metal_init: allocating
0.00.933.397 I ggml_metal_init: found device: Apple M4
0.00.933.407 I ggml_metal_init: picking default device: Apple M4
0.00.934.480 I ggml_metal_init: using embedded metal library
0.00.938.580 I ggml_metal_init: GPU name:   Apple M4
0.00.938.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.938.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.938.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.938.585 I ggml_metal_init: simdgroup reduction   = true
0.00.938.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.938.585 I ggml_metal_init: has residency sets    = true
0.00.938.585 I ggml_metal_init: has bfloat            = true
0.00.938.585 I ggml_metal_init: use bfloat            = true
0.00.938.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.938.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.949.293 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.950.887 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.950.891 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.950.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.952.601 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.952.602 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.952.602 I llama_init_from_model: graph nodes  = 967
0.00.952.602 I llama_init_from_model: graph splits = 2
0.00.952.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.952.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.977.143 I 
0.00.977.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.977.182 I perplexity: tokenizing the input ..
0.00.980.986 I perplexity: tokenization took 3.803 ms
0.00.980.992 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.118.172 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.119.563 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.119.586 I llama_perf_context_print:        load time =     967.09 ms
0.01.119.587 I llama_perf_context_print: prompt eval time =     136.95 ms /   128 tokens (    1.07 ms per token,   934.65 tokens per second)
0.01.119.588 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.119.588 I llama_perf_context_print:       total time =     142.44 ms /   129 tokens
0.01.119.941 I ggml_metal_free: deallocating

real	0m1.136s
user	0m0.066s
sys	0m0.138s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.011.231 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.031.956 I llama_model_loader: - type  f32:  194 tensors
0.00.031.956 I llama_model_loader: - type q4_0:   97 tensors
0.00.031.957 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.957 I print_info: file format = GGUF V3 (latest)
0.00.031.958 I print_info: file type   = Q4_0
0.00.031.959 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.041.887 I load: special tokens cache size = 25
0.00.049.880 I load: token to piece cache size = 0.2984 MB
0.00.049.884 I print_info: arch             = gptneox
0.00.049.884 I print_info: vocab_only       = 0
0.00.049.885 I print_info: n_ctx_train      = 2048
0.00.049.885 I print_info: n_embd           = 2048
0.00.049.885 I print_info: n_layer          = 24
0.00.049.890 I print_info: n_head           = 16
0.00.049.891 I print_info: n_head_kv        = 16
0.00.049.891 I print_info: n_rot            = 32
0.00.049.891 I print_info: n_swa            = 0
0.00.049.891 I print_info: n_embd_head_k    = 128
0.00.049.891 I print_info: n_embd_head_v    = 128
0.00.049.892 I print_info: n_gqa            = 1
0.00.049.893 I print_info: n_embd_k_gqa     = 2048
0.00.049.897 I print_info: n_embd_v_gqa     = 2048
0.00.049.897 I print_info: f_norm_eps       = 1.0e-05
0.00.049.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.900 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.900 I print_info: f_logit_scale    = 0.0e+00
0.00.049.901 I print_info: n_ff             = 8192
0.00.049.901 I print_info: n_expert         = 0
0.00.049.901 I print_info: n_expert_used    = 0
0.00.049.901 I print_info: causal attn      = 1
0.00.049.901 I print_info: pooling type     = 0
0.00.049.901 I print_info: rope type        = 2
0.00.049.902 I print_info: rope scaling     = linear
0.00.049.902 I print_info: freq_base_train  = 10000.0
0.00.049.902 I print_info: freq_scale_train = 1
0.00.049.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.903 I print_info: rope_finetuned   = unknown
0.00.049.903 I print_info: ssm_d_conv       = 0
0.00.049.903 I print_info: ssm_d_inner      = 0
0.00.049.903 I print_info: ssm_d_state      = 0
0.00.049.904 I print_info: ssm_dt_rank      = 0
0.00.049.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.908 I print_info: model type       = 1.4B
0.00.049.908 I print_info: model params     = 1.41 B
0.00.049.909 I print_info: general.name     = 1.4B
0.00.049.909 I print_info: vocab type       = BPE
0.00.049.909 I print_info: n_vocab          = 50304
0.00.049.909 I print_info: n_merges         = 50009
0.00.049.910 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.910 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.910 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.910 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.911 I print_info: LF token         = 187 ''
0.00.049.911 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.911 I print_info: max token length = 1024
0.00.049.912 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.717.107 I load_tensors: offloading 24 repeating layers to GPU
0.00.717.122 I load_tensors: offloading output layer to GPU
0.00.717.123 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.158 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.717.159 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.718.563 I llama_init_from_model: n_seq_max     = 1
0.00.718.565 I llama_init_from_model: n_ctx         = 2048
0.00.718.566 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.718.567 I llama_init_from_model: n_batch       = 2048
0.00.718.567 I llama_init_from_model: n_ubatch      = 512
0.00.718.567 I llama_init_from_model: flash_attn    = 0
0.00.718.570 I llama_init_from_model: freq_base     = 10000.0
0.00.718.570 I llama_init_from_model: freq_scale    = 1
0.00.718.573 I ggml_metal_init: allocating
0.00.718.653 I ggml_metal_init: found device: Apple M4
0.00.718.665 I ggml_metal_init: picking default device: Apple M4
0.00.720.553 I ggml_metal_init: using embedded metal library
0.00.726.747 I ggml_metal_init: GPU name:   Apple M4
0.00.726.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.758 I ggml_metal_init: simdgroup reduction   = true
0.00.726.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.726.758 I ggml_metal_init: has residency sets    = true
0.00.726.759 I ggml_metal_init: has bfloat            = true
0.00.726.759 I ggml_metal_init: use bfloat            = true
0.00.726.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.726.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.745.795 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.801.888 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.801.895 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.801.932 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.806.252 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.806.253 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.806.254 I llama_init_from_model: graph nodes  = 967
0.00.806.254 I llama_init_from_model: graph splits = 2
0.00.806.259 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.806.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.806.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.536 I main: llama threadpool init, n_threads = 4
0.00.863.582 I 
0.00.863.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.608 I 
0.00.863.788 I sampler seed: 1234
0.00.863.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.814 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.814 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.551.115 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.551.116 I llama_perf_context_print:        load time =     851.58 ms
0.01.551.117 I llama_perf_context_print: prompt eval time =      49.22 ms /     7 tokens (    7.03 ms per token,   142.21 tokens per second)
0.01.551.117 I llama_perf_context_print:        eval time =     635.16 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.551.118 I llama_perf_context_print:       total time =     688.30 ms /    70 tokens
0.01.551.388 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.115s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.266 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.307 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.511 I llama_model_loader: - type  f32:  194 tensors
0.00.028.512 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.513 I print_info: file format = GGUF V3 (latest)
0.00.028.513 I print_info: file type   = Q4_0
0.00.028.520 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.401 I load: special tokens cache size = 25
0.00.042.137 I load: token to piece cache size = 0.2984 MB
0.00.042.142 I print_info: arch             = gptneox
0.00.042.142 I print_info: vocab_only       = 0
0.00.042.142 I print_info: n_ctx_train      = 2048
0.00.042.143 I print_info: n_embd           = 2048
0.00.042.143 I print_info: n_layer          = 24
0.00.042.147 I print_info: n_head           = 16
0.00.042.148 I print_info: n_head_kv        = 16
0.00.042.148 I print_info: n_rot            = 32
0.00.042.148 I print_info: n_swa            = 0
0.00.042.151 I print_info: n_embd_head_k    = 128
0.00.042.151 I print_info: n_embd_head_v    = 128
0.00.042.152 I print_info: n_gqa            = 1
0.00.042.152 I print_info: n_embd_k_gqa     = 2048
0.00.042.153 I print_info: n_embd_v_gqa     = 2048
0.00.042.153 I print_info: f_norm_eps       = 1.0e-05
0.00.042.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.154 I print_info: f_logit_scale    = 0.0e+00
0.00.042.156 I print_info: n_ff             = 8192
0.00.042.156 I print_info: n_expert         = 0
0.00.042.156 I print_info: n_expert_used    = 0
0.00.042.159 I print_info: causal attn      = 1
0.00.042.160 I print_info: pooling type     = 0
0.00.042.160 I print_info: rope type        = 2
0.00.042.160 I print_info: rope scaling     = linear
0.00.042.161 I print_info: freq_base_train  = 10000.0
0.00.042.162 I print_info: freq_scale_train = 1
0.00.042.162 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.162 I print_info: rope_finetuned   = unknown
0.00.042.162 I print_info: ssm_d_conv       = 0
0.00.042.162 I print_info: ssm_d_inner      = 0
0.00.042.163 I print_info: ssm_d_state      = 0
0.00.042.163 I print_info: ssm_dt_rank      = 0
0.00.042.163 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.163 I print_info: model type       = 1.4B
0.00.042.163 I print_info: model params     = 1.41 B
0.00.042.164 I print_info: general.name     = 1.4B
0.00.042.164 I print_info: vocab type       = BPE
0.00.042.164 I print_info: n_vocab          = 50304
0.00.042.164 I print_info: n_merges         = 50009
0.00.042.165 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.165 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.165 I print_info: LF token         = 187 ''
0.00.042.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.166 I print_info: max token length = 1024
0.00.042.166 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.854 I load_tensors: offloading output layer to GPU
0.00.696.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.878 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.696.884 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.698.161 I llama_init_from_model: n_seq_max     = 1
0.00.698.166 I llama_init_from_model: n_ctx         = 128
0.00.698.167 I llama_init_from_model: n_ctx_per_seq = 128
0.00.698.167 I llama_init_from_model: n_batch       = 128
0.00.698.167 I llama_init_from_model: n_ubatch      = 128
0.00.698.168 I llama_init_from_model: flash_attn    = 0
0.00.698.169 I llama_init_from_model: freq_base     = 10000.0
0.00.698.170 I llama_init_from_model: freq_scale    = 1
0.00.698.170 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.698.172 I ggml_metal_init: allocating
0.00.698.274 I ggml_metal_init: found device: Apple M4
0.00.698.289 I ggml_metal_init: picking default device: Apple M4
0.00.699.726 I ggml_metal_init: using embedded metal library
0.00.708.975 I ggml_metal_init: GPU name:   Apple M4
0.00.708.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.708.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.708.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.708.982 I ggml_metal_init: simdgroup reduction   = true
0.00.708.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.708.982 I ggml_metal_init: has residency sets    = true
0.00.708.983 I ggml_metal_init: has bfloat            = true
0.00.708.983 I ggml_metal_init: use bfloat            = true
0.00.708.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.708.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.724.155 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.443 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.726.448 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.726.477 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.150 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.728.151 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.728.152 I llama_init_from_model: graph nodes  = 967
0.00.728.152 I llama_init_from_model: graph splits = 2
0.00.728.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.728.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.680 I 
0.00.753.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.719 I perplexity: tokenizing the input ..
0.00.757.608 I perplexity: tokenization took 3.888 ms
0.00.757.611 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.043 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.895.736 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.895.778 I llama_perf_context_print:        load time =     741.37 ms
0.00.895.779 I llama_perf_context_print: prompt eval time =     133.20 ms /   128 tokens (    1.04 ms per token,   960.96 tokens per second)
0.00.895.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.781 I llama_perf_context_print:       total time =     142.10 ms /   129 tokens
0.00.896.474 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.091s
sys	0m0.114s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.330 I llama_model_loader: - type  f32:  194 tensors
0.00.028.331 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.331 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.331 I print_info: file format = GGUF V3 (latest)
0.00.028.332 I print_info: file type   = Q4_1
0.00.028.332 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.288 I load: special tokens cache size = 25
0.00.042.238 I load: token to piece cache size = 0.2984 MB
0.00.042.240 I print_info: arch             = gptneox
0.00.042.241 I print_info: vocab_only       = 0
0.00.042.241 I print_info: n_ctx_train      = 2048
0.00.042.241 I print_info: n_embd           = 2048
0.00.042.241 I print_info: n_layer          = 24
0.00.042.244 I print_info: n_head           = 16
0.00.042.245 I print_info: n_head_kv        = 16
0.00.042.245 I print_info: n_rot            = 32
0.00.042.245 I print_info: n_swa            = 0
0.00.042.245 I print_info: n_embd_head_k    = 128
0.00.042.248 I print_info: n_embd_head_v    = 128
0.00.042.249 I print_info: n_gqa            = 1
0.00.042.249 I print_info: n_embd_k_gqa     = 2048
0.00.042.250 I print_info: n_embd_v_gqa     = 2048
0.00.042.251 I print_info: f_norm_eps       = 1.0e-05
0.00.042.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.251 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.251 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.252 I print_info: f_logit_scale    = 0.0e+00
0.00.042.252 I print_info: n_ff             = 8192
0.00.042.253 I print_info: n_expert         = 0
0.00.042.253 I print_info: n_expert_used    = 0
0.00.042.253 I print_info: causal attn      = 1
0.00.042.253 I print_info: pooling type     = 0
0.00.042.253 I print_info: rope type        = 2
0.00.042.253 I print_info: rope scaling     = linear
0.00.042.254 I print_info: freq_base_train  = 10000.0
0.00.042.256 I print_info: freq_scale_train = 1
0.00.042.256 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.256 I print_info: rope_finetuned   = unknown
0.00.042.256 I print_info: ssm_d_conv       = 0
0.00.042.256 I print_info: ssm_d_inner      = 0
0.00.042.257 I print_info: ssm_d_state      = 0
0.00.042.257 I print_info: ssm_dt_rank      = 0
0.00.042.257 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.257 I print_info: model type       = 1.4B
0.00.042.258 I print_info: model params     = 1.41 B
0.00.042.258 I print_info: general.name     = 1.4B
0.00.042.258 I print_info: vocab type       = BPE
0.00.042.258 I print_info: n_vocab          = 50304
0.00.042.258 I print_info: n_merges         = 50009
0.00.042.263 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.263 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.263 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.263 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.263 I print_info: LF token         = 187 ''
0.00.042.264 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.264 I print_info: max token length = 1024
0.00.042.264 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.060 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.073 I load_tensors: offloading output layer to GPU
0.00.660.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.115 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.660.116 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.661.689 I llama_init_from_model: n_seq_max     = 1
0.00.661.692 I llama_init_from_model: n_ctx         = 2048
0.00.661.692 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.693 I llama_init_from_model: n_batch       = 2048
0.00.661.693 I llama_init_from_model: n_ubatch      = 512
0.00.661.694 I llama_init_from_model: flash_attn    = 0
0.00.661.696 I llama_init_from_model: freq_base     = 10000.0
0.00.661.697 I llama_init_from_model: freq_scale    = 1
0.00.661.706 I ggml_metal_init: allocating
0.00.661.799 I ggml_metal_init: found device: Apple M4
0.00.661.811 I ggml_metal_init: picking default device: Apple M4
0.00.663.747 I ggml_metal_init: using embedded metal library
0.00.670.009 I ggml_metal_init: GPU name:   Apple M4
0.00.670.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.670.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.670.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.670.016 I ggml_metal_init: simdgroup reduction   = true
0.00.670.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.670.017 I ggml_metal_init: has residency sets    = true
0.00.670.017 I ggml_metal_init: has bfloat            = true
0.00.670.017 I ggml_metal_init: use bfloat            = true
0.00.670.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.670.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.978 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.298 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.305 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.339 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.646 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.649 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.649 I llama_init_from_model: graph nodes  = 967
0.00.756.649 I llama_init_from_model: graph splits = 2
0.00.756.654 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.779 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.824 I main: llama threadpool init, n_threads = 4
0.00.813.868 I 
0.00.813.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.892 I 
0.00.814.157 I sampler seed: 1234
0.00.814.165 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.180 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.184 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.184 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.546.707 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.546.707 I llama_perf_context_print:        load time =     804.31 ms
0.01.546.708 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.24 tokens per second)
0.01.546.709 I llama_perf_context_print:        eval time =     680.44 ms /    63 runs   (   10.80 ms per token,    92.59 tokens per second)
0.01.546.709 I llama_perf_context_print:       total time =     733.60 ms /    70 tokens
0.01.546.934 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.669 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.647 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.648 I llama_model_loader: - type  f32:  194 tensors
0.00.028.648 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.648 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.649 I print_info: file format = GGUF V3 (latest)
0.00.028.649 I print_info: file type   = Q4_1
0.00.028.650 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.037.028 I load: special tokens cache size = 25
0.00.043.167 I load: token to piece cache size = 0.2984 MB
0.00.043.171 I print_info: arch             = gptneox
0.00.043.172 I print_info: vocab_only       = 0
0.00.043.172 I print_info: n_ctx_train      = 2048
0.00.043.172 I print_info: n_embd           = 2048
0.00.043.172 I print_info: n_layer          = 24
0.00.043.176 I print_info: n_head           = 16
0.00.043.177 I print_info: n_head_kv        = 16
0.00.043.177 I print_info: n_rot            = 32
0.00.043.177 I print_info: n_swa            = 0
0.00.043.178 I print_info: n_embd_head_k    = 128
0.00.043.178 I print_info: n_embd_head_v    = 128
0.00.043.179 I print_info: n_gqa            = 1
0.00.043.179 I print_info: n_embd_k_gqa     = 2048
0.00.043.180 I print_info: n_embd_v_gqa     = 2048
0.00.043.181 I print_info: f_norm_eps       = 1.0e-05
0.00.043.181 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.181 I print_info: f_logit_scale    = 0.0e+00
0.00.043.184 I print_info: n_ff             = 8192
0.00.043.184 I print_info: n_expert         = 0
0.00.043.184 I print_info: n_expert_used    = 0
0.00.043.184 I print_info: causal attn      = 1
0.00.043.184 I print_info: pooling type     = 0
0.00.043.184 I print_info: rope type        = 2
0.00.043.184 I print_info: rope scaling     = linear
0.00.043.186 I print_info: freq_base_train  = 10000.0
0.00.043.187 I print_info: freq_scale_train = 1
0.00.043.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.188 I print_info: rope_finetuned   = unknown
0.00.043.188 I print_info: ssm_d_conv       = 0
0.00.043.188 I print_info: ssm_d_inner      = 0
0.00.043.188 I print_info: ssm_d_state      = 0
0.00.043.188 I print_info: ssm_dt_rank      = 0
0.00.043.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.188 I print_info: model type       = 1.4B
0.00.043.189 I print_info: model params     = 1.41 B
0.00.043.189 I print_info: general.name     = 1.4B
0.00.043.189 I print_info: vocab type       = BPE
0.00.043.189 I print_info: n_vocab          = 50304
0.00.043.190 I print_info: n_merges         = 50009
0.00.043.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.190 I print_info: LF token         = 187 ''
0.00.043.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.191 I print_info: max token length = 1024
0.00.043.191 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.970 I load_tensors: offloading output layer to GPU
0.00.649.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.006 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.650.008 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.651.513 I llama_init_from_model: n_seq_max     = 1
0.00.651.516 I llama_init_from_model: n_ctx         = 128
0.00.651.516 I llama_init_from_model: n_ctx_per_seq = 128
0.00.651.517 I llama_init_from_model: n_batch       = 128
0.00.651.517 I llama_init_from_model: n_ubatch      = 128
0.00.651.518 I llama_init_from_model: flash_attn    = 0
0.00.651.520 I llama_init_from_model: freq_base     = 10000.0
0.00.651.520 I llama_init_from_model: freq_scale    = 1
0.00.651.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.651.523 I ggml_metal_init: allocating
0.00.651.585 I ggml_metal_init: found device: Apple M4
0.00.651.598 I ggml_metal_init: picking default device: Apple M4
0.00.653.407 I ggml_metal_init: using embedded metal library
0.00.659.400 I ggml_metal_init: GPU name:   Apple M4
0.00.659.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.412 I ggml_metal_init: simdgroup reduction   = true
0.00.659.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.412 I ggml_metal_init: has residency sets    = true
0.00.659.413 I ggml_metal_init: has bfloat            = true
0.00.659.413 I ggml_metal_init: use bfloat            = true
0.00.659.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.417 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.682.421 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.959 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.959 I llama_init_from_model: graph nodes  = 967
0.00.685.960 I llama_init_from_model: graph splits = 2
0.00.685.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.596 I 
0.00.713.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.669 I perplexity: tokenizing the input ..
0.00.720.583 I perplexity: tokenization took 6.911 ms
0.00.720.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.562 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.857.065 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.857.088 I llama_perf_context_print:        load time =     703.92 ms
0.00.857.089 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   955.04 tokens per second)
0.00.857.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.090 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.857.415 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.081s
sys	0m0.111s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.015.536 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.546 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.552 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.755 I llama_model_loader: - type  f32:  194 tensors
0.00.045.755 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.756 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.756 I print_info: file format = GGUF V3 (latest)
0.00.045.757 I print_info: file type   = Q5_0
0.00.045.757 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.054.890 I load: special tokens cache size = 25
0.00.062.513 I load: token to piece cache size = 0.2984 MB
0.00.062.516 I print_info: arch             = gptneox
0.00.062.516 I print_info: vocab_only       = 0
0.00.062.516 I print_info: n_ctx_train      = 2048
0.00.062.517 I print_info: n_embd           = 2048
0.00.062.517 I print_info: n_layer          = 24
0.00.062.520 I print_info: n_head           = 16
0.00.062.521 I print_info: n_head_kv        = 16
0.00.062.521 I print_info: n_rot            = 32
0.00.062.521 I print_info: n_swa            = 0
0.00.062.521 I print_info: n_embd_head_k    = 128
0.00.062.522 I print_info: n_embd_head_v    = 128
0.00.062.522 I print_info: n_gqa            = 1
0.00.062.523 I print_info: n_embd_k_gqa     = 2048
0.00.062.524 I print_info: n_embd_v_gqa     = 2048
0.00.062.525 I print_info: f_norm_eps       = 1.0e-05
0.00.062.525 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.525 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.525 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.526 I print_info: f_logit_scale    = 0.0e+00
0.00.062.526 I print_info: n_ff             = 8192
0.00.062.526 I print_info: n_expert         = 0
0.00.062.527 I print_info: n_expert_used    = 0
0.00.062.527 I print_info: causal attn      = 1
0.00.062.527 I print_info: pooling type     = 0
0.00.062.529 I print_info: rope type        = 2
0.00.062.530 I print_info: rope scaling     = linear
0.00.062.531 I print_info: freq_base_train  = 10000.0
0.00.062.531 I print_info: freq_scale_train = 1
0.00.062.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.531 I print_info: rope_finetuned   = unknown
0.00.062.532 I print_info: ssm_d_conv       = 0
0.00.062.532 I print_info: ssm_d_inner      = 0
0.00.062.532 I print_info: ssm_d_state      = 0
0.00.062.532 I print_info: ssm_dt_rank      = 0
0.00.062.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.532 I print_info: model type       = 1.4B
0.00.062.533 I print_info: model params     = 1.41 B
0.00.062.533 I print_info: general.name     = 1.4B
0.00.062.533 I print_info: vocab type       = BPE
0.00.062.534 I print_info: n_vocab          = 50304
0.00.062.534 I print_info: n_merges         = 50009
0.00.062.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.540 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.541 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.542 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.542 I print_info: LF token         = 187 ''
0.00.062.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.545 I print_info: max token length = 1024
0.00.062.545 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.780.539 I load_tensors: offloading 24 repeating layers to GPU
0.00.780.555 I load_tensors: offloading output layer to GPU
0.00.780.556 I load_tensors: offloaded 25/25 layers to GPU
0.00.780.591 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.780.592 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.782.019 I llama_init_from_model: n_seq_max     = 1
0.00.782.022 I llama_init_from_model: n_ctx         = 2048
0.00.782.023 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.782.023 I llama_init_from_model: n_batch       = 2048
0.00.782.023 I llama_init_from_model: n_ubatch      = 512
0.00.782.024 I llama_init_from_model: flash_attn    = 0
0.00.782.026 I llama_init_from_model: freq_base     = 10000.0
0.00.782.026 I llama_init_from_model: freq_scale    = 1
0.00.782.032 I ggml_metal_init: allocating
0.00.782.103 I ggml_metal_init: found device: Apple M4
0.00.782.116 I ggml_metal_init: picking default device: Apple M4
0.00.783.986 I ggml_metal_init: using embedded metal library
0.00.790.656 I ggml_metal_init: GPU name:   Apple M4
0.00.790.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.790.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.790.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.790.665 I ggml_metal_init: simdgroup reduction   = true
0.00.790.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.790.665 I ggml_metal_init: has residency sets    = true
0.00.790.666 I ggml_metal_init: has bfloat            = true
0.00.790.666 I ggml_metal_init: use bfloat            = true
0.00.790.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.790.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.808.652 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.865.431 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.865.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.865.473 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.869.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.870.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.870.001 I llama_init_from_model: graph nodes  = 967
0.00.870.001 I llama_init_from_model: graph splits = 2
0.00.870.007 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.870.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.870.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.930.970 I main: llama threadpool init, n_threads = 4
0.00.931.015 I 
0.00.931.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.931.040 I 
0.00.931.180 I sampler seed: 1234
0.00.931.185 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.931.206 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.931.207 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.931.207 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.744.662 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.744.663 I llama_perf_context_print:        load time =     914.71 ms
0.01.744.666 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.28 tokens per second)
0.01.744.666 I llama_perf_context_print:        eval time =     757.79 ms /    63 runs   (   12.03 ms per token,    83.14 tokens per second)
0.01.744.667 I llama_perf_context_print:       total time =     814.41 ms /    70 tokens
0.01.744.912 I ggml_metal_free: deallocating

real	0m1.761s
user	0m0.114s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.946 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.634 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.303 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.304 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.306 I llama_model_loader: - type  f32:  194 tensors
0.00.025.306 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.307 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.307 I print_info: file format = GGUF V3 (latest)
0.00.025.308 I print_info: file type   = Q5_0
0.00.025.309 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.179 I load: special tokens cache size = 25
0.00.039.168 I load: token to piece cache size = 0.2984 MB
0.00.039.173 I print_info: arch             = gptneox
0.00.039.173 I print_info: vocab_only       = 0
0.00.039.174 I print_info: n_ctx_train      = 2048
0.00.039.174 I print_info: n_embd           = 2048
0.00.039.174 I print_info: n_layer          = 24
0.00.039.178 I print_info: n_head           = 16
0.00.039.179 I print_info: n_head_kv        = 16
0.00.039.179 I print_info: n_rot            = 32
0.00.039.179 I print_info: n_swa            = 0
0.00.039.179 I print_info: n_embd_head_k    = 128
0.00.039.180 I print_info: n_embd_head_v    = 128
0.00.039.180 I print_info: n_gqa            = 1
0.00.039.181 I print_info: n_embd_k_gqa     = 2048
0.00.039.182 I print_info: n_embd_v_gqa     = 2048
0.00.039.182 I print_info: f_norm_eps       = 1.0e-05
0.00.039.182 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.183 I print_info: f_logit_scale    = 0.0e+00
0.00.039.184 I print_info: n_ff             = 8192
0.00.039.184 I print_info: n_expert         = 0
0.00.039.184 I print_info: n_expert_used    = 0
0.00.039.184 I print_info: causal attn      = 1
0.00.039.184 I print_info: pooling type     = 0
0.00.039.184 I print_info: rope type        = 2
0.00.039.184 I print_info: rope scaling     = linear
0.00.039.185 I print_info: freq_base_train  = 10000.0
0.00.039.185 I print_info: freq_scale_train = 1
0.00.039.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.186 I print_info: rope_finetuned   = unknown
0.00.039.186 I print_info: ssm_d_conv       = 0
0.00.039.186 I print_info: ssm_d_inner      = 0
0.00.039.186 I print_info: ssm_d_state      = 0
0.00.039.186 I print_info: ssm_dt_rank      = 0
0.00.039.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.186 I print_info: model type       = 1.4B
0.00.039.187 I print_info: model params     = 1.41 B
0.00.039.187 I print_info: general.name     = 1.4B
0.00.039.187 I print_info: vocab type       = BPE
0.00.039.187 I print_info: n_vocab          = 50304
0.00.039.188 I print_info: n_merges         = 50009
0.00.039.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.191 I print_info: LF token         = 187 ''
0.00.039.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.191 I print_info: max token length = 1024
0.00.039.191 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.722.186 I load_tensors: offloading 24 repeating layers to GPU
0.00.722.194 I load_tensors: offloading output layer to GPU
0.00.722.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.722.230 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.722.232 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.723.687 I llama_init_from_model: n_seq_max     = 1
0.00.723.691 I llama_init_from_model: n_ctx         = 128
0.00.723.691 I llama_init_from_model: n_ctx_per_seq = 128
0.00.723.692 I llama_init_from_model: n_batch       = 128
0.00.723.692 I llama_init_from_model: n_ubatch      = 128
0.00.723.692 I llama_init_from_model: flash_attn    = 0
0.00.723.694 I llama_init_from_model: freq_base     = 10000.0
0.00.723.694 I llama_init_from_model: freq_scale    = 1
0.00.723.695 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.723.696 I ggml_metal_init: allocating
0.00.723.766 I ggml_metal_init: found device: Apple M4
0.00.723.779 I ggml_metal_init: picking default device: Apple M4
0.00.725.648 I ggml_metal_init: using embedded metal library
0.00.732.312 I ggml_metal_init: GPU name:   Apple M4
0.00.732.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.732.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.732.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.732.319 I ggml_metal_init: simdgroup reduction   = true
0.00.732.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.732.319 I ggml_metal_init: has residency sets    = true
0.00.732.319 I ggml_metal_init: has bfloat            = true
0.00.732.320 I ggml_metal_init: use bfloat            = true
0.00.732.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.732.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.749.684 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.753.220 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.753.286 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.458 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.756.460 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.756.461 I llama_init_from_model: graph nodes  = 967
0.00.756.461 I llama_init_from_model: graph splits = 2
0.00.756.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.756.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.158 I 
0.00.784.242 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.249 I perplexity: tokenizing the input ..
0.00.789.995 I perplexity: tokenization took 5.744 ms
0.00.790.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.924.226 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.925.561 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.925.587 I llama_perf_context_print:        load time =     773.99 ms
0.00.925.588 I llama_perf_context_print: prompt eval time =     133.68 ms /   128 tokens (    1.04 ms per token,   957.55 tokens per second)
0.00.925.589 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.925.590 I llama_perf_context_print:       total time =     141.43 ms /   129 tokens
0.00.925.945 I ggml_metal_free: deallocating

real	0m0.942s
user	0m0.077s
sys	0m0.144s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.011.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.549 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.552 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.029.372 I llama_model_loader: - type  f32:  194 tensors
0.00.029.372 I llama_model_loader: - type q5_1:   97 tensors
0.00.029.373 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.373 I print_info: file format = GGUF V3 (latest)
0.00.029.374 I print_info: file type   = Q5_1
0.00.029.375 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.037.666 I load: special tokens cache size = 25
0.00.043.811 I load: token to piece cache size = 0.2984 MB
0.00.043.817 I print_info: arch             = gptneox
0.00.043.821 I print_info: vocab_only       = 0
0.00.043.821 I print_info: n_ctx_train      = 2048
0.00.043.821 I print_info: n_embd           = 2048
0.00.043.822 I print_info: n_layer          = 24
0.00.043.826 I print_info: n_head           = 16
0.00.043.827 I print_info: n_head_kv        = 16
0.00.043.827 I print_info: n_rot            = 32
0.00.043.827 I print_info: n_swa            = 0
0.00.043.827 I print_info: n_embd_head_k    = 128
0.00.043.828 I print_info: n_embd_head_v    = 128
0.00.043.828 I print_info: n_gqa            = 1
0.00.043.829 I print_info: n_embd_k_gqa     = 2048
0.00.043.829 I print_info: n_embd_v_gqa     = 2048
0.00.043.830 I print_info: f_norm_eps       = 1.0e-05
0.00.043.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.830 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.830 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.831 I print_info: f_logit_scale    = 0.0e+00
0.00.043.831 I print_info: n_ff             = 8192
0.00.043.831 I print_info: n_expert         = 0
0.00.043.832 I print_info: n_expert_used    = 0
0.00.043.832 I print_info: causal attn      = 1
0.00.043.832 I print_info: pooling type     = 0
0.00.043.832 I print_info: rope type        = 2
0.00.043.832 I print_info: rope scaling     = linear
0.00.043.833 I print_info: freq_base_train  = 10000.0
0.00.043.833 I print_info: freq_scale_train = 1
0.00.043.833 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.833 I print_info: rope_finetuned   = unknown
0.00.043.834 I print_info: ssm_d_conv       = 0
0.00.043.834 I print_info: ssm_d_inner      = 0
0.00.043.834 I print_info: ssm_d_state      = 0
0.00.043.834 I print_info: ssm_dt_rank      = 0
0.00.043.834 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.835 I print_info: model type       = 1.4B
0.00.043.835 I print_info: model params     = 1.41 B
0.00.043.835 I print_info: general.name     = 1.4B
0.00.043.858 I print_info: vocab type       = BPE
0.00.043.863 I print_info: n_vocab          = 50304
0.00.043.863 I print_info: n_merges         = 50009
0.00.043.864 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.864 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.864 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.864 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.864 I print_info: LF token         = 187 ''
0.00.043.867 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.867 I print_info: max token length = 1024
0.00.043.867 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.320 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.328 I load_tensors: offloading output layer to GPU
0.00.604.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.362 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.365 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.605.682 I llama_init_from_model: n_seq_max     = 1
0.00.605.684 I llama_init_from_model: n_ctx         = 2048
0.00.605.685 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.685 I llama_init_from_model: n_batch       = 2048
0.00.605.686 I llama_init_from_model: n_ubatch      = 512
0.00.605.686 I llama_init_from_model: flash_attn    = 0
0.00.605.687 I llama_init_from_model: freq_base     = 10000.0
0.00.605.688 I llama_init_from_model: freq_scale    = 1
0.00.605.690 I ggml_metal_init: allocating
0.00.605.751 I ggml_metal_init: found device: Apple M4
0.00.605.764 I ggml_metal_init: picking default device: Apple M4
0.00.607.420 I ggml_metal_init: using embedded metal library
0.00.613.607 I ggml_metal_init: GPU name:   Apple M4
0.00.613.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.613 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.614 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.615 I ggml_metal_init: simdgroup reduction   = true
0.00.613.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.615 I ggml_metal_init: has residency sets    = true
0.00.613.615 I ggml_metal_init: has bfloat            = true
0.00.613.616 I ggml_metal_init: use bfloat            = true
0.00.613.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.925 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.334 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.656 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.659 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.659 I llama_init_from_model: graph nodes  = 967
0.00.689.660 I llama_init_from_model: graph splits = 2
0.00.689.666 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.689.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.689.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.840 I main: llama threadpool init, n_threads = 4
0.00.749.884 I 
0.00.749.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.907 I 
0.00.750.082 I sampler seed: 1234
0.00.750.086 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.107 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.107 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.108 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.600.827 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.600.828 I llama_perf_context_print:        load time =     737.35 ms
0.01.600.828 I llama_perf_context_print: prompt eval time =      51.96 ms /     7 tokens (    7.42 ms per token,   134.72 tokens per second)
0.01.600.829 I llama_perf_context_print:        eval time =     795.82 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.600.830 I llama_perf_context_print:       total time =     851.70 ms /    70 tokens
0.01.601.075 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.127 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.909 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.910 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.416 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.416 I llama_model_loader: - type  f32:  194 tensors
0.00.024.417 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.417 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.417 I print_info: file format = GGUF V3 (latest)
0.00.024.418 I print_info: file type   = Q5_1
0.00.024.419 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.886 I load: special tokens cache size = 25
0.00.038.996 I load: token to piece cache size = 0.2984 MB
0.00.039.000 I print_info: arch             = gptneox
0.00.039.000 I print_info: vocab_only       = 0
0.00.039.000 I print_info: n_ctx_train      = 2048
0.00.039.000 I print_info: n_embd           = 2048
0.00.039.000 I print_info: n_layer          = 24
0.00.039.005 I print_info: n_head           = 16
0.00.039.005 I print_info: n_head_kv        = 16
0.00.039.006 I print_info: n_rot            = 32
0.00.039.006 I print_info: n_swa            = 0
0.00.039.006 I print_info: n_embd_head_k    = 128
0.00.039.006 I print_info: n_embd_head_v    = 128
0.00.039.007 I print_info: n_gqa            = 1
0.00.039.008 I print_info: n_embd_k_gqa     = 2048
0.00.039.008 I print_info: n_embd_v_gqa     = 2048
0.00.039.009 I print_info: f_norm_eps       = 1.0e-05
0.00.039.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.010 I print_info: f_logit_scale    = 0.0e+00
0.00.039.011 I print_info: n_ff             = 8192
0.00.039.011 I print_info: n_expert         = 0
0.00.039.011 I print_info: n_expert_used    = 0
0.00.039.011 I print_info: causal attn      = 1
0.00.039.011 I print_info: pooling type     = 0
0.00.039.011 I print_info: rope type        = 2
0.00.039.012 I print_info: rope scaling     = linear
0.00.039.013 I print_info: freq_base_train  = 10000.0
0.00.039.013 I print_info: freq_scale_train = 1
0.00.039.013 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.013 I print_info: rope_finetuned   = unknown
0.00.039.014 I print_info: ssm_d_conv       = 0
0.00.039.014 I print_info: ssm_d_inner      = 0
0.00.039.014 I print_info: ssm_d_state      = 0
0.00.039.014 I print_info: ssm_dt_rank      = 0
0.00.039.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.014 I print_info: model type       = 1.4B
0.00.039.015 I print_info: model params     = 1.41 B
0.00.039.015 I print_info: general.name     = 1.4B
0.00.039.015 I print_info: vocab type       = BPE
0.00.039.016 I print_info: n_vocab          = 50304
0.00.039.016 I print_info: n_merges         = 50009
0.00.039.016 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.017 I print_info: LF token         = 187 ''
0.00.039.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.017 I print_info: max token length = 1024
0.00.039.018 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.423 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.435 I load_tensors: offloading output layer to GPU
0.00.619.435 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.468 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.619.470 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.621.168 I llama_init_from_model: n_seq_max     = 1
0.00.621.171 I llama_init_from_model: n_ctx         = 128
0.00.621.172 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.173 I llama_init_from_model: n_batch       = 128
0.00.621.173 I llama_init_from_model: n_ubatch      = 128
0.00.621.174 I llama_init_from_model: flash_attn    = 0
0.00.621.176 I llama_init_from_model: freq_base     = 10000.0
0.00.621.177 I llama_init_from_model: freq_scale    = 1
0.00.621.177 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.184 I ggml_metal_init: allocating
0.00.621.222 I ggml_metal_init: found device: Apple M4
0.00.621.234 I ggml_metal_init: picking default device: Apple M4
0.00.623.006 I ggml_metal_init: using embedded metal library
0.00.629.827 I ggml_metal_init: GPU name:   Apple M4
0.00.629.833 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.834 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.835 I ggml_metal_init: simdgroup reduction   = true
0.00.629.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.836 I ggml_metal_init: has residency sets    = true
0.00.629.836 I ggml_metal_init: has bfloat            = true
0.00.629.836 I ggml_metal_init: use bfloat            = true
0.00.629.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.808 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.420 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.765 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.654.767 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.654.767 I llama_init_from_model: graph nodes  = 967
0.00.654.768 I llama_init_from_model: graph splits = 2
0.00.654.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.771 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.495 I 
0.00.686.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.590 I perplexity: tokenizing the input ..
0.00.693.959 I perplexity: tokenization took 7.366 ms
0.00.693.967 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.918 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.843.222 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.843.250 I llama_perf_context_print:        load time =     677.36 ms
0.00.843.250 I llama_perf_context_print: prompt eval time =     147.03 ms /   128 tokens (    1.15 ms per token,   870.58 tokens per second)
0.00.843.251 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.252 I llama_perf_context_print:       total time =     156.76 ms /   129 tokens
0.00.843.660 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.080s
sys	0m0.145s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.563 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.905 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.907 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.907 I llama_model_loader: - type  f32:  194 tensors
0.00.024.907 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.908 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.908 I print_info: file format = GGUF V3 (latest)
0.00.024.909 I print_info: file type   = Q2_K - Medium
0.00.024.910 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.594 I load: special tokens cache size = 25
0.00.038.517 I load: token to piece cache size = 0.2984 MB
0.00.038.519 I print_info: arch             = gptneox
0.00.038.520 I print_info: vocab_only       = 0
0.00.038.520 I print_info: n_ctx_train      = 2048
0.00.038.520 I print_info: n_embd           = 2048
0.00.038.520 I print_info: n_layer          = 24
0.00.038.523 I print_info: n_head           = 16
0.00.038.524 I print_info: n_head_kv        = 16
0.00.038.524 I print_info: n_rot            = 32
0.00.038.525 I print_info: n_swa            = 0
0.00.038.525 I print_info: n_embd_head_k    = 128
0.00.038.525 I print_info: n_embd_head_v    = 128
0.00.038.526 I print_info: n_gqa            = 1
0.00.038.526 I print_info: n_embd_k_gqa     = 2048
0.00.038.527 I print_info: n_embd_v_gqa     = 2048
0.00.038.528 I print_info: f_norm_eps       = 1.0e-05
0.00.038.528 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.529 I print_info: f_logit_scale    = 0.0e+00
0.00.038.529 I print_info: n_ff             = 8192
0.00.038.529 I print_info: n_expert         = 0
0.00.038.529 I print_info: n_expert_used    = 0
0.00.038.530 I print_info: causal attn      = 1
0.00.038.530 I print_info: pooling type     = 0
0.00.038.530 I print_info: rope type        = 2
0.00.038.530 I print_info: rope scaling     = linear
0.00.038.533 I print_info: freq_base_train  = 10000.0
0.00.038.533 I print_info: freq_scale_train = 1
0.00.038.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.534 I print_info: rope_finetuned   = unknown
0.00.038.534 I print_info: ssm_d_conv       = 0
0.00.038.534 I print_info: ssm_d_inner      = 0
0.00.038.534 I print_info: ssm_d_state      = 0
0.00.038.534 I print_info: ssm_dt_rank      = 0
0.00.038.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.534 I print_info: model type       = 1.4B
0.00.038.535 I print_info: model params     = 1.41 B
0.00.038.535 I print_info: general.name     = 1.4B
0.00.038.536 I print_info: vocab type       = BPE
0.00.038.536 I print_info: n_vocab          = 50304
0.00.038.536 I print_info: n_merges         = 50009
0.00.038.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: LF token         = 187 ''
0.00.038.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.538 I print_info: max token length = 1024
0.00.038.538 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.355.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.583 I load_tensors: offloading output layer to GPU
0.00.355.583 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.621 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.622 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.357.328 I llama_init_from_model: n_seq_max     = 1
0.00.357.332 I llama_init_from_model: n_ctx         = 2048
0.00.357.333 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.357.333 I llama_init_from_model: n_batch       = 2048
0.00.357.334 I llama_init_from_model: n_ubatch      = 512
0.00.357.334 I llama_init_from_model: flash_attn    = 0
0.00.357.337 I llama_init_from_model: freq_base     = 10000.0
0.00.357.337 I llama_init_from_model: freq_scale    = 1
0.00.357.344 I ggml_metal_init: allocating
0.00.357.485 I ggml_metal_init: found device: Apple M4
0.00.357.502 I ggml_metal_init: picking default device: Apple M4
0.00.359.486 I ggml_metal_init: using embedded metal library
0.00.364.926 I ggml_metal_init: GPU name:   Apple M4
0.00.364.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.364.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.364.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.364.949 I ggml_metal_init: simdgroup reduction   = true
0.00.364.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.364.950 I ggml_metal_init: has residency sets    = true
0.00.364.950 I ggml_metal_init: has bfloat            = true
0.00.364.951 I ggml_metal_init: use bfloat            = true
0.00.364.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.364.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.386.666 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.445.884 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.445.894 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.445.931 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.450.485 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.450.487 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.450.487 I llama_init_from_model: graph nodes  = 967
0.00.450.488 I llama_init_from_model: graph splits = 2
0.00.450.496 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.450.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.450.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.960 I main: llama threadpool init, n_threads = 4
0.00.506.002 I 
0.00.506.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.025 I 
0.00.506.203 I sampler seed: 1234
0.00.506.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.228 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.229 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.177.382 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.177.382 I llama_perf_context_print:        load time =     495.34 ms
0.01.177.383 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.46 tokens per second)
0.01.177.384 I llama_perf_context_print:        eval time =     632.92 ms /    63 runs   (   10.05 ms per token,    99.54 tokens per second)
0.01.177.384 I llama_perf_context_print:       total time =     672.13 ms /    70 tokens
0.01.177.620 I ggml_metal_free: deallocating

real	0m1.196s
user	0m0.112s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.930 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.930 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.932 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.932 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.933 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.933 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.934 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.936 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.327 I llama_model_loader: - type  f32:  194 tensors
0.00.025.328 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.328 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.329 I print_info: file format = GGUF V3 (latest)
0.00.025.329 I print_info: file type   = Q2_K - Medium
0.00.025.331 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.267 I load: special tokens cache size = 25
0.00.039.371 I load: token to piece cache size = 0.2984 MB
0.00.039.374 I print_info: arch             = gptneox
0.00.039.375 I print_info: vocab_only       = 0
0.00.039.375 I print_info: n_ctx_train      = 2048
0.00.039.375 I print_info: n_embd           = 2048
0.00.039.375 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.383 I print_info: n_head_kv        = 16
0.00.039.383 I print_info: n_rot            = 32
0.00.039.383 I print_info: n_swa            = 0
0.00.039.384 I print_info: n_embd_head_k    = 128
0.00.039.384 I print_info: n_embd_head_v    = 128
0.00.039.385 I print_info: n_gqa            = 1
0.00.039.385 I print_info: n_embd_k_gqa     = 2048
0.00.039.386 I print_info: n_embd_v_gqa     = 2048
0.00.039.387 I print_info: f_norm_eps       = 1.0e-05
0.00.039.387 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.387 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.387 I print_info: f_logit_scale    = 0.0e+00
0.00.039.388 I print_info: n_ff             = 8192
0.00.039.388 I print_info: n_expert         = 0
0.00.039.388 I print_info: n_expert_used    = 0
0.00.039.389 I print_info: causal attn      = 1
0.00.039.389 I print_info: pooling type     = 0
0.00.039.389 I print_info: rope type        = 2
0.00.039.389 I print_info: rope scaling     = linear
0.00.039.390 I print_info: freq_base_train  = 10000.0
0.00.039.390 I print_info: freq_scale_train = 1
0.00.039.390 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.390 I print_info: rope_finetuned   = unknown
0.00.039.391 I print_info: ssm_d_conv       = 0
0.00.039.397 I print_info: ssm_d_inner      = 0
0.00.039.398 I print_info: ssm_d_state      = 0
0.00.039.399 I print_info: ssm_dt_rank      = 0
0.00.039.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.399 I print_info: model type       = 1.4B
0.00.039.400 I print_info: model params     = 1.41 B
0.00.039.400 I print_info: general.name     = 1.4B
0.00.039.400 I print_info: vocab type       = BPE
0.00.039.400 I print_info: n_vocab          = 50304
0.00.039.401 I print_info: n_merges         = 50009
0.00.039.401 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.403 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: LF token         = 187 ''
0.00.039.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: max token length = 1024
0.00.039.404 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.352.712 I load_tensors: offloading 24 repeating layers to GPU
0.00.352.726 I load_tensors: offloading output layer to GPU
0.00.352.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.352.756 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.352.763 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.354.413 I llama_init_from_model: n_seq_max     = 1
0.00.354.419 I llama_init_from_model: n_ctx         = 128
0.00.354.419 I llama_init_from_model: n_ctx_per_seq = 128
0.00.354.420 I llama_init_from_model: n_batch       = 128
0.00.354.420 I llama_init_from_model: n_ubatch      = 128
0.00.354.420 I llama_init_from_model: flash_attn    = 0
0.00.354.422 I llama_init_from_model: freq_base     = 10000.0
0.00.354.422 I llama_init_from_model: freq_scale    = 1
0.00.354.423 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.354.426 I ggml_metal_init: allocating
0.00.354.508 I ggml_metal_init: found device: Apple M4
0.00.354.521 I ggml_metal_init: picking default device: Apple M4
0.00.356.342 I ggml_metal_init: using embedded metal library
0.00.361.792 I ggml_metal_init: GPU name:   Apple M4
0.00.361.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.801 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.801 I ggml_metal_init: simdgroup reduction   = true
0.00.361.802 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.802 I ggml_metal_init: has residency sets    = true
0.00.361.802 I ggml_metal_init: has bfloat            = true
0.00.361.802 I ggml_metal_init: use bfloat            = true
0.00.361.804 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.809 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.549 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.387.124 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.387.127 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.387.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.390.566 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.390.568 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.390.569 I llama_init_from_model: graph nodes  = 967
0.00.390.569 I llama_init_from_model: graph splits = 2
0.00.390.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.390.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.424.031 I 
0.00.424.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.424.107 I perplexity: tokenizing the input ..
0.00.428.946 I perplexity: tokenization took 4.838 ms
0.00.428.950 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.539 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.323 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.365 I llama_perf_context_print:        load time =     413.92 ms
0.00.562.366 I llama_perf_context_print: prompt eval time =     131.35 ms /   128 tokens (    1.03 ms per token,   974.50 tokens per second)
0.00.562.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.369 I llama_perf_context_print:       total time =     138.34 ms /   129 tokens
0.00.562.730 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.076s
sys	0m0.101s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.470 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.646 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.646 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.647 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.049 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.050 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.051 I llama_model_loader: - type  f32:  194 tensors
0.00.026.051 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.052 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.052 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.052 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.053 I print_info: file format = GGUF V3 (latest)
0.00.026.053 I print_info: file type   = Q3_K - Medium
0.00.026.054 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.763 I load: special tokens cache size = 25
0.00.039.682 I load: token to piece cache size = 0.2984 MB
0.00.039.685 I print_info: arch             = gptneox
0.00.039.685 I print_info: vocab_only       = 0
0.00.039.685 I print_info: n_ctx_train      = 2048
0.00.039.686 I print_info: n_embd           = 2048
0.00.039.686 I print_info: n_layer          = 24
0.00.039.689 I print_info: n_head           = 16
0.00.039.689 I print_info: n_head_kv        = 16
0.00.039.690 I print_info: n_rot            = 32
0.00.039.692 I print_info: n_swa            = 0
0.00.039.692 I print_info: n_embd_head_k    = 128
0.00.039.692 I print_info: n_embd_head_v    = 128
0.00.039.693 I print_info: n_gqa            = 1
0.00.039.694 I print_info: n_embd_k_gqa     = 2048
0.00.039.694 I print_info: n_embd_v_gqa     = 2048
0.00.039.695 I print_info: f_norm_eps       = 1.0e-05
0.00.039.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.696 I print_info: f_logit_scale    = 0.0e+00
0.00.039.696 I print_info: n_ff             = 8192
0.00.039.697 I print_info: n_expert         = 0
0.00.039.697 I print_info: n_expert_used    = 0
0.00.039.698 I print_info: causal attn      = 1
0.00.039.699 I print_info: pooling type     = 0
0.00.039.699 I print_info: rope type        = 2
0.00.039.700 I print_info: rope scaling     = linear
0.00.039.700 I print_info: freq_base_train  = 10000.0
0.00.039.700 I print_info: freq_scale_train = 1
0.00.039.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.701 I print_info: rope_finetuned   = unknown
0.00.039.701 I print_info: ssm_d_conv       = 0
0.00.039.701 I print_info: ssm_d_inner      = 0
0.00.039.701 I print_info: ssm_d_state      = 0
0.00.039.701 I print_info: ssm_dt_rank      = 0
0.00.039.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.702 I print_info: model type       = 1.4B
0.00.039.702 I print_info: model params     = 1.41 B
0.00.039.702 I print_info: general.name     = 1.4B
0.00.039.703 I print_info: vocab type       = BPE
0.00.039.703 I print_info: n_vocab          = 50304
0.00.039.703 I print_info: n_merges         = 50009
0.00.039.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: LF token         = 187 ''
0.00.039.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.705 I print_info: max token length = 1024
0.00.039.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.453.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.641 I load_tensors: offloading output layer to GPU
0.00.453.641 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.677 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.678 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.455.325 I llama_init_from_model: n_seq_max     = 1
0.00.455.328 I llama_init_from_model: n_ctx         = 2048
0.00.455.329 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.455.330 I llama_init_from_model: n_batch       = 2048
0.00.455.330 I llama_init_from_model: n_ubatch      = 512
0.00.455.331 I llama_init_from_model: flash_attn    = 0
0.00.455.333 I llama_init_from_model: freq_base     = 10000.0
0.00.455.334 I llama_init_from_model: freq_scale    = 1
0.00.455.336 I ggml_metal_init: allocating
0.00.455.414 I ggml_metal_init: found device: Apple M4
0.00.455.427 I ggml_metal_init: picking default device: Apple M4
0.00.457.397 I ggml_metal_init: using embedded metal library
0.00.463.090 I ggml_metal_init: GPU name:   Apple M4
0.00.463.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.463.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.463.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.463.104 I ggml_metal_init: simdgroup reduction   = true
0.00.463.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.463.105 I ggml_metal_init: has residency sets    = true
0.00.463.105 I ggml_metal_init: has bfloat            = true
0.00.463.106 I ggml_metal_init: use bfloat            = true
0.00.463.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.463.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.230 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.542.931 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.542.976 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.792 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.547.793 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.547.793 I llama_init_from_model: graph nodes  = 967
0.00.547.794 I llama_init_from_model: graph splits = 2
0.00.547.798 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.416 I main: llama threadpool init, n_threads = 4
0.00.604.462 I 
0.00.604.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.486 I 
0.00.604.646 I sampler seed: 1234
0.00.604.650 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.604.661 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.604.662 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.604.662 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.349.063 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.349.064 I llama_perf_context_print:        load time =     593.24 ms
0.01.349.065 I llama_perf_context_print: prompt eval time =      49.69 ms /     7 tokens (    7.10 ms per token,   140.87 tokens per second)
0.01.349.066 I llama_perf_context_print:        eval time =     691.94 ms /    63 runs   (   10.98 ms per token,    91.05 tokens per second)
0.01.349.067 I llama_perf_context_print:       total time =     745.36 ms /    70 tokens
0.01.349.295 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.109s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.542 I llama_model_loader: - type  f32:  194 tensors
0.00.024.543 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.543 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.543 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.544 I print_info: file format = GGUF V3 (latest)
0.00.024.545 I print_info: file type   = Q3_K - Medium
0.00.024.546 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.403 I load: special tokens cache size = 25
0.00.038.451 I load: token to piece cache size = 0.2984 MB
0.00.038.455 I print_info: arch             = gptneox
0.00.038.455 I print_info: vocab_only       = 0
0.00.038.456 I print_info: n_ctx_train      = 2048
0.00.038.456 I print_info: n_embd           = 2048
0.00.038.456 I print_info: n_layer          = 24
0.00.038.460 I print_info: n_head           = 16
0.00.038.463 I print_info: n_head_kv        = 16
0.00.038.464 I print_info: n_rot            = 32
0.00.038.464 I print_info: n_swa            = 0
0.00.038.464 I print_info: n_embd_head_k    = 128
0.00.038.464 I print_info: n_embd_head_v    = 128
0.00.038.465 I print_info: n_gqa            = 1
0.00.038.465 I print_info: n_embd_k_gqa     = 2048
0.00.038.466 I print_info: n_embd_v_gqa     = 2048
0.00.038.466 I print_info: f_norm_eps       = 1.0e-05
0.00.038.467 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.467 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.467 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.467 I print_info: f_logit_scale    = 0.0e+00
0.00.038.468 I print_info: n_ff             = 8192
0.00.038.468 I print_info: n_expert         = 0
0.00.038.471 I print_info: n_expert_used    = 0
0.00.038.471 I print_info: causal attn      = 1
0.00.038.471 I print_info: pooling type     = 0
0.00.038.471 I print_info: rope type        = 2
0.00.038.472 I print_info: rope scaling     = linear
0.00.038.473 I print_info: freq_base_train  = 10000.0
0.00.038.473 I print_info: freq_scale_train = 1
0.00.038.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.473 I print_info: rope_finetuned   = unknown
0.00.038.473 I print_info: ssm_d_conv       = 0
0.00.038.474 I print_info: ssm_d_inner      = 0
0.00.038.474 I print_info: ssm_d_state      = 0
0.00.038.474 I print_info: ssm_dt_rank      = 0
0.00.038.476 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.476 I print_info: model type       = 1.4B
0.00.038.476 I print_info: model params     = 1.41 B
0.00.038.476 I print_info: general.name     = 1.4B
0.00.038.477 I print_info: vocab type       = BPE
0.00.038.477 I print_info: n_vocab          = 50304
0.00.038.477 I print_info: n_merges         = 50009
0.00.038.477 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.477 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.478 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.478 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.478 I print_info: LF token         = 187 ''
0.00.038.478 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.478 I print_info: max token length = 1024
0.00.038.479 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.448.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.448.129 I load_tensors: offloading output layer to GPU
0.00.448.130 I load_tensors: offloaded 25/25 layers to GPU
0.00.448.165 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.448.167 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.449.922 I llama_init_from_model: n_seq_max     = 1
0.00.449.926 I llama_init_from_model: n_ctx         = 128
0.00.449.927 I llama_init_from_model: n_ctx_per_seq = 128
0.00.449.927 I llama_init_from_model: n_batch       = 128
0.00.449.928 I llama_init_from_model: n_ubatch      = 128
0.00.449.928 I llama_init_from_model: flash_attn    = 0
0.00.449.931 I llama_init_from_model: freq_base     = 10000.0
0.00.449.932 I llama_init_from_model: freq_scale    = 1
0.00.449.932 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.449.935 I ggml_metal_init: allocating
0.00.450.025 I ggml_metal_init: found device: Apple M4
0.00.450.039 I ggml_metal_init: picking default device: Apple M4
0.00.451.992 I ggml_metal_init: using embedded metal library
0.00.457.513 I ggml_metal_init: GPU name:   Apple M4
0.00.457.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.457.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.457.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.457.524 I ggml_metal_init: simdgroup reduction   = true
0.00.457.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.457.525 I ggml_metal_init: has residency sets    = true
0.00.457.525 I ggml_metal_init: has bfloat            = true
0.00.457.525 I ggml_metal_init: use bfloat            = true
0.00.457.527 I ggml_metal_init: hasUnifiedMemory      = true
0.00.457.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.477.694 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.481.333 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.481.340 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.481.417 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.484.863 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.484.865 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.484.866 I llama_init_from_model: graph nodes  = 967
0.00.484.866 I llama_init_from_model: graph splits = 2
0.00.484.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.484.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.515.632 I 
0.00.515.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.515.715 I perplexity: tokenizing the input ..
0.00.522.701 I perplexity: tokenization took 6.983 ms
0.00.522.713 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.664.036 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.377 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.405 I llama_perf_context_print:        load time =     506.55 ms
0.00.665.410 I llama_perf_context_print: prompt eval time =     140.40 ms /   128 tokens (    1.10 ms per token,   911.71 tokens per second)
0.00.665.411 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.412 I llama_perf_context_print:       total time =     149.78 ms /   129 tokens
0.00.665.793 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.079s
sys	0m0.119s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.041 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.041 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.041 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.042 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.043 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.517 I llama_model_loader: - type  f32:  194 tensors
0.00.026.518 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.518 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.518 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.519 I print_info: file format = GGUF V3 (latest)
0.00.026.519 I print_info: file type   = Q4_K - Medium
0.00.026.520 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.547 I load: special tokens cache size = 25
0.00.040.651 I load: token to piece cache size = 0.2984 MB
0.00.040.653 I print_info: arch             = gptneox
0.00.040.653 I print_info: vocab_only       = 0
0.00.040.654 I print_info: n_ctx_train      = 2048
0.00.040.654 I print_info: n_embd           = 2048
0.00.040.654 I print_info: n_layer          = 24
0.00.040.657 I print_info: n_head           = 16
0.00.040.658 I print_info: n_head_kv        = 16
0.00.040.658 I print_info: n_rot            = 32
0.00.040.658 I print_info: n_swa            = 0
0.00.040.658 I print_info: n_embd_head_k    = 128
0.00.040.660 I print_info: n_embd_head_v    = 128
0.00.040.661 I print_info: n_gqa            = 1
0.00.040.661 I print_info: n_embd_k_gqa     = 2048
0.00.040.662 I print_info: n_embd_v_gqa     = 2048
0.00.040.667 I print_info: f_norm_eps       = 1.0e-05
0.00.040.667 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.668 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.668 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.668 I print_info: f_logit_scale    = 0.0e+00
0.00.040.669 I print_info: n_ff             = 8192
0.00.040.669 I print_info: n_expert         = 0
0.00.040.669 I print_info: n_expert_used    = 0
0.00.040.669 I print_info: causal attn      = 1
0.00.040.671 I print_info: pooling type     = 0
0.00.040.671 I print_info: rope type        = 2
0.00.040.675 I print_info: rope scaling     = linear
0.00.040.675 I print_info: freq_base_train  = 10000.0
0.00.040.678 I print_info: freq_scale_train = 1
0.00.040.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.680 I print_info: rope_finetuned   = unknown
0.00.040.680 I print_info: ssm_d_conv       = 0
0.00.040.680 I print_info: ssm_d_inner      = 0
0.00.040.680 I print_info: ssm_d_state      = 0
0.00.040.680 I print_info: ssm_dt_rank      = 0
0.00.040.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.680 I print_info: model type       = 1.4B
0.00.040.681 I print_info: model params     = 1.41 B
0.00.040.681 I print_info: general.name     = 1.4B
0.00.040.681 I print_info: vocab type       = BPE
0.00.040.681 I print_info: n_vocab          = 50304
0.00.040.682 I print_info: n_merges         = 50009
0.00.040.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: LF token         = 187 ''
0.00.040.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.690 I print_info: max token length = 1024
0.00.040.690 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.542.190 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.198 I load_tensors: offloading output layer to GPU
0.00.542.199 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.227 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.228 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.843 I llama_init_from_model: n_seq_max     = 1
0.00.543.846 I llama_init_from_model: n_ctx         = 2048
0.00.543.846 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.543.847 I llama_init_from_model: n_batch       = 2048
0.00.543.847 I llama_init_from_model: n_ubatch      = 512
0.00.543.848 I llama_init_from_model: flash_attn    = 0
0.00.543.849 I llama_init_from_model: freq_base     = 10000.0
0.00.543.849 I llama_init_from_model: freq_scale    = 1
0.00.543.852 I ggml_metal_init: allocating
0.00.543.905 I ggml_metal_init: found device: Apple M4
0.00.543.918 I ggml_metal_init: picking default device: Apple M4
0.00.546.118 I ggml_metal_init: using embedded metal library
0.00.553.241 I ggml_metal_init: GPU name:   Apple M4
0.00.553.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.553.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.553.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.553.249 I ggml_metal_init: simdgroup reduction   = true
0.00.553.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.553.250 I ggml_metal_init: has residency sets    = true
0.00.553.250 I ggml_metal_init: has bfloat            = true
0.00.553.250 I ggml_metal_init: use bfloat            = true
0.00.553.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.553.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.571.412 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.781 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.627.788 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.627.825 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.309 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.632.311 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.632.312 I llama_init_from_model: graph nodes  = 967
0.00.632.312 I llama_init_from_model: graph splits = 2
0.00.632.317 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.632.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.632.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.816 I main: llama threadpool init, n_threads = 4
0.00.689.862 I 
0.00.689.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.888 I 
0.00.690.053 I sampler seed: 1234
0.00.690.057 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.083 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.083 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.450.448 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.450.448 I llama_perf_context_print:        load time =     678.78 ms
0.01.450.449 I llama_perf_context_print: prompt eval time =      57.67 ms /     7 tokens (    8.24 ms per token,   121.38 tokens per second)
0.01.450.450 I llama_perf_context_print:        eval time =     699.76 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.450.450 I llama_perf_context_print:       total time =     761.34 ms /    70 tokens
0.01.450.679 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.794 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.804 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.804 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.805 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.806 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.806 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.807 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.807 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.807 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.808 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.810 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.811 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.208 I llama_model_loader: - type  f32:  194 tensors
0.00.026.209 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.209 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.209 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.210 I print_info: file format = GGUF V3 (latest)
0.00.026.211 I print_info: file type   = Q4_K - Medium
0.00.026.212 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.797 I load: special tokens cache size = 25
0.00.040.946 I load: token to piece cache size = 0.2984 MB
0.00.040.951 I print_info: arch             = gptneox
0.00.040.951 I print_info: vocab_only       = 0
0.00.040.951 I print_info: n_ctx_train      = 2048
0.00.040.951 I print_info: n_embd           = 2048
0.00.040.952 I print_info: n_layer          = 24
0.00.040.956 I print_info: n_head           = 16
0.00.040.957 I print_info: n_head_kv        = 16
0.00.040.957 I print_info: n_rot            = 32
0.00.040.957 I print_info: n_swa            = 0
0.00.040.957 I print_info: n_embd_head_k    = 128
0.00.040.960 I print_info: n_embd_head_v    = 128
0.00.040.960 I print_info: n_gqa            = 1
0.00.040.961 I print_info: n_embd_k_gqa     = 2048
0.00.040.962 I print_info: n_embd_v_gqa     = 2048
0.00.040.962 I print_info: f_norm_eps       = 1.0e-05
0.00.040.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.963 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.963 I print_info: f_logit_scale    = 0.0e+00
0.00.040.963 I print_info: n_ff             = 8192
0.00.040.964 I print_info: n_expert         = 0
0.00.040.964 I print_info: n_expert_used    = 0
0.00.040.964 I print_info: causal attn      = 1
0.00.040.964 I print_info: pooling type     = 0
0.00.040.964 I print_info: rope type        = 2
0.00.040.964 I print_info: rope scaling     = linear
0.00.040.965 I print_info: freq_base_train  = 10000.0
0.00.040.965 I print_info: freq_scale_train = 1
0.00.040.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.965 I print_info: rope_finetuned   = unknown
0.00.040.965 I print_info: ssm_d_conv       = 0
0.00.040.966 I print_info: ssm_d_inner      = 0
0.00.040.966 I print_info: ssm_d_state      = 0
0.00.040.966 I print_info: ssm_dt_rank      = 0
0.00.040.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.966 I print_info: model type       = 1.4B
0.00.040.967 I print_info: model params     = 1.41 B
0.00.040.967 I print_info: general.name     = 1.4B
0.00.040.967 I print_info: vocab type       = BPE
0.00.040.968 I print_info: n_vocab          = 50304
0.00.040.968 I print_info: n_merges         = 50009
0.00.040.968 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: LF token         = 187 ''
0.00.040.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.970 I print_info: max token length = 1024
0.00.040.970 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.340 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.355 I load_tensors: offloading output layer to GPU
0.00.521.356 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.394 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.395 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.523.004 I llama_init_from_model: n_seq_max     = 1
0.00.523.007 I llama_init_from_model: n_ctx         = 128
0.00.523.008 I llama_init_from_model: n_ctx_per_seq = 128
0.00.523.008 I llama_init_from_model: n_batch       = 128
0.00.523.009 I llama_init_from_model: n_ubatch      = 128
0.00.523.009 I llama_init_from_model: flash_attn    = 0
0.00.523.011 I llama_init_from_model: freq_base     = 10000.0
0.00.523.012 I llama_init_from_model: freq_scale    = 1
0.00.523.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.523.015 I ggml_metal_init: allocating
0.00.523.128 I ggml_metal_init: found device: Apple M4
0.00.523.144 I ggml_metal_init: picking default device: Apple M4
0.00.525.007 I ggml_metal_init: using embedded metal library
0.00.531.642 I ggml_metal_init: GPU name:   Apple M4
0.00.531.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.656 I ggml_metal_init: simdgroup reduction   = true
0.00.531.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.657 I ggml_metal_init: has residency sets    = true
0.00.531.657 I ggml_metal_init: has bfloat            = true
0.00.531.657 I ggml_metal_init: use bfloat            = true
0.00.531.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.525 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.553.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.553.081 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.553.151 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.303 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.556.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.556.305 I llama_init_from_model: graph nodes  = 967
0.00.556.305 I llama_init_from_model: graph splits = 2
0.00.556.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.556.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.790 I 
0.00.584.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.885 I perplexity: tokenizing the input ..
0.00.592.042 I perplexity: tokenization took 7.154 ms
0.00.592.054 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.856 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.737.187 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.737.207 I llama_perf_context_print:        load time =     575.86 ms
0.00.737.208 I llama_perf_context_print: prompt eval time =     142.90 ms /   128 tokens (    1.12 ms per token,   895.73 tokens per second)
0.00.737.209 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.209 I llama_perf_context_print:       total time =     152.42 ms /   129 tokens
0.00.737.586 I ggml_metal_free: deallocating

real	0m0.752s
user	0m0.081s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.886 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.887 I llama_model_loader: - type  f32:  194 tensors
0.00.026.887 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.887 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.888 I print_info: file format = GGUF V3 (latest)
0.00.026.888 I print_info: file type   = Q5_K - Medium
0.00.026.889 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.596 I load: special tokens cache size = 25
0.00.040.690 I load: token to piece cache size = 0.2984 MB
0.00.040.692 I print_info: arch             = gptneox
0.00.040.692 I print_info: vocab_only       = 0
0.00.040.693 I print_info: n_ctx_train      = 2048
0.00.040.693 I print_info: n_embd           = 2048
0.00.040.693 I print_info: n_layer          = 24
0.00.040.695 I print_info: n_head           = 16
0.00.040.696 I print_info: n_head_kv        = 16
0.00.040.696 I print_info: n_rot            = 32
0.00.040.696 I print_info: n_swa            = 0
0.00.040.697 I print_info: n_embd_head_k    = 128
0.00.040.699 I print_info: n_embd_head_v    = 128
0.00.040.700 I print_info: n_gqa            = 1
0.00.040.700 I print_info: n_embd_k_gqa     = 2048
0.00.040.701 I print_info: n_embd_v_gqa     = 2048
0.00.040.702 I print_info: f_norm_eps       = 1.0e-05
0.00.040.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.702 I print_info: f_logit_scale    = 0.0e+00
0.00.040.703 I print_info: n_ff             = 8192
0.00.040.703 I print_info: n_expert         = 0
0.00.040.703 I print_info: n_expert_used    = 0
0.00.040.703 I print_info: causal attn      = 1
0.00.040.703 I print_info: pooling type     = 0
0.00.040.705 I print_info: rope type        = 2
0.00.040.705 I print_info: rope scaling     = linear
0.00.040.705 I print_info: freq_base_train  = 10000.0
0.00.040.706 I print_info: freq_scale_train = 1
0.00.040.706 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.706 I print_info: rope_finetuned   = unknown
0.00.040.706 I print_info: ssm_d_conv       = 0
0.00.040.706 I print_info: ssm_d_inner      = 0
0.00.040.707 I print_info: ssm_d_state      = 0
0.00.040.707 I print_info: ssm_dt_rank      = 0
0.00.040.708 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.708 I print_info: model type       = 1.4B
0.00.040.708 I print_info: model params     = 1.41 B
0.00.040.709 I print_info: general.name     = 1.4B
0.00.040.709 I print_info: vocab type       = BPE
0.00.040.709 I print_info: n_vocab          = 50304
0.00.040.709 I print_info: n_merges         = 50009
0.00.040.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.711 I print_info: LF token         = 187 ''
0.00.040.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.711 I print_info: max token length = 1024
0.00.040.713 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.426 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.443 I load_tensors: offloading output layer to GPU
0.00.607.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.476 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.607.477 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.609.065 I llama_init_from_model: n_seq_max     = 1
0.00.609.069 I llama_init_from_model: n_ctx         = 2048
0.00.609.069 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.609.070 I llama_init_from_model: n_batch       = 2048
0.00.609.070 I llama_init_from_model: n_ubatch      = 512
0.00.609.070 I llama_init_from_model: flash_attn    = 0
0.00.609.073 I llama_init_from_model: freq_base     = 10000.0
0.00.609.073 I llama_init_from_model: freq_scale    = 1
0.00.609.076 I ggml_metal_init: allocating
0.00.609.150 I ggml_metal_init: found device: Apple M4
0.00.609.162 I ggml_metal_init: picking default device: Apple M4
0.00.611.219 I ggml_metal_init: using embedded metal library
0.00.617.665 I ggml_metal_init: GPU name:   Apple M4
0.00.617.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.671 I ggml_metal_init: simdgroup reduction   = true
0.00.617.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.672 I ggml_metal_init: has residency sets    = true
0.00.617.672 I ggml_metal_init: has bfloat            = true
0.00.617.672 I ggml_metal_init: use bfloat            = true
0.00.617.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.947 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.945 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.690.951 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.690.987 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.695.555 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.695.558 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.695.558 I llama_init_from_model: graph nodes  = 967
0.00.695.558 I llama_init_from_model: graph splits = 2
0.00.695.563 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.695.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.695.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.594 I main: llama threadpool init, n_threads = 4
0.00.746.632 I 
0.00.746.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.653 I 
0.00.746.764 I sampler seed: 1234
0.00.746.769 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.786 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.787 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.595.794 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.595.795 I llama_perf_context_print:        load time =     734.39 ms
0.01.595.796 I llama_perf_context_print: prompt eval time =      52.60 ms /     7 tokens (    7.51 ms per token,   133.09 tokens per second)
0.01.595.796 I llama_perf_context_print:        eval time =     793.59 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.595.797 I llama_perf_context_print:       total time =     849.97 ms /    70 tokens
0.01.596.078 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.476 I llama_model_loader: - type  f32:  194 tensors
0.00.025.477 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.477 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.478 I print_info: file format = GGUF V3 (latest)
0.00.025.478 I print_info: file type   = Q5_K - Medium
0.00.025.479 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.646 I load: special tokens cache size = 25
0.00.039.600 I load: token to piece cache size = 0.2984 MB
0.00.039.604 I print_info: arch             = gptneox
0.00.039.604 I print_info: vocab_only       = 0
0.00.039.605 I print_info: n_ctx_train      = 2048
0.00.039.605 I print_info: n_embd           = 2048
0.00.039.605 I print_info: n_layer          = 24
0.00.039.609 I print_info: n_head           = 16
0.00.039.610 I print_info: n_head_kv        = 16
0.00.039.610 I print_info: n_rot            = 32
0.00.039.610 I print_info: n_swa            = 0
0.00.039.613 I print_info: n_embd_head_k    = 128
0.00.039.613 I print_info: n_embd_head_v    = 128
0.00.039.614 I print_info: n_gqa            = 1
0.00.039.615 I print_info: n_embd_k_gqa     = 2048
0.00.039.615 I print_info: n_embd_v_gqa     = 2048
0.00.039.616 I print_info: f_norm_eps       = 1.0e-05
0.00.039.617 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.618 I print_info: f_logit_scale    = 0.0e+00
0.00.039.619 I print_info: n_ff             = 8192
0.00.039.620 I print_info: n_expert         = 0
0.00.039.620 I print_info: n_expert_used    = 0
0.00.039.620 I print_info: causal attn      = 1
0.00.039.621 I print_info: pooling type     = 0
0.00.039.621 I print_info: rope type        = 2
0.00.039.621 I print_info: rope scaling     = linear
0.00.039.625 I print_info: freq_base_train  = 10000.0
0.00.039.625 I print_info: freq_scale_train = 1
0.00.039.625 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.626 I print_info: rope_finetuned   = unknown
0.00.039.626 I print_info: ssm_d_conv       = 0
0.00.039.626 I print_info: ssm_d_inner      = 0
0.00.039.626 I print_info: ssm_d_state      = 0
0.00.039.626 I print_info: ssm_dt_rank      = 0
0.00.039.626 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.627 I print_info: model type       = 1.4B
0.00.039.627 I print_info: model params     = 1.41 B
0.00.039.627 I print_info: general.name     = 1.4B
0.00.039.628 I print_info: vocab type       = BPE
0.00.039.628 I print_info: n_vocab          = 50304
0.00.039.628 I print_info: n_merges         = 50009
0.00.039.633 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.633 I print_info: LF token         = 187 ''
0.00.039.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: max token length = 1024
0.00.039.634 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.235 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.241 I load_tensors: offloading output layer to GPU
0.00.601.242 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.267 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.268 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.511 I llama_init_from_model: n_seq_max     = 1
0.00.602.514 I llama_init_from_model: n_ctx         = 128
0.00.602.514 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.515 I llama_init_from_model: n_batch       = 128
0.00.602.515 I llama_init_from_model: n_ubatch      = 128
0.00.602.515 I llama_init_from_model: flash_attn    = 0
0.00.602.516 I llama_init_from_model: freq_base     = 10000.0
0.00.602.517 I llama_init_from_model: freq_scale    = 1
0.00.602.518 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.521 I ggml_metal_init: allocating
0.00.602.576 I ggml_metal_init: found device: Apple M4
0.00.602.591 I ggml_metal_init: picking default device: Apple M4
0.00.604.087 I ggml_metal_init: using embedded metal library
0.00.610.281 I ggml_metal_init: GPU name:   Apple M4
0.00.610.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.286 I ggml_metal_init: simdgroup reduction   = true
0.00.610.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.287 I ggml_metal_init: has residency sets    = true
0.00.610.287 I ggml_metal_init: has bfloat            = true
0.00.610.287 I ggml_metal_init: use bfloat            = true
0.00.610.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.180 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.599 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.605 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.652 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.724 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.726 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.726 I llama_init_from_model: graph nodes  = 967
0.00.633.727 I llama_init_from_model: graph splits = 2
0.00.633.730 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.041 I 
0.00.669.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.131 I perplexity: tokenizing the input ..
0.00.676.269 I perplexity: tokenization took 7.135 ms
0.00.676.279 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.672 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.816.079 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.816.106 I llama_perf_context_print:        load time =     658.98 ms
0.00.816.108 I llama_perf_context_print: prompt eval time =     137.51 ms /   128 tokens (    1.07 ms per token,   930.85 tokens per second)
0.00.816.109 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.109 I llama_perf_context_print:       total time =     147.07 ms /   129 tokens
0.00.816.457 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.078s
sys	0m0.141s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.420 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.421 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.765 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.766 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.767 I llama_model_loader: - type  f32:  194 tensors
0.00.025.767 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.767 I print_info: file format = GGUF V3 (latest)
0.00.025.768 I print_info: file type   = Q6_K
0.00.025.768 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.829 I load: special tokens cache size = 25
0.00.039.964 I load: token to piece cache size = 0.2984 MB
0.00.039.966 I print_info: arch             = gptneox
0.00.039.967 I print_info: vocab_only       = 0
0.00.039.967 I print_info: n_ctx_train      = 2048
0.00.039.967 I print_info: n_embd           = 2048
0.00.039.967 I print_info: n_layer          = 24
0.00.039.970 I print_info: n_head           = 16
0.00.039.970 I print_info: n_head_kv        = 16
0.00.039.971 I print_info: n_rot            = 32
0.00.039.971 I print_info: n_swa            = 0
0.00.039.971 I print_info: n_embd_head_k    = 128
0.00.039.971 I print_info: n_embd_head_v    = 128
0.00.039.972 I print_info: n_gqa            = 1
0.00.039.973 I print_info: n_embd_k_gqa     = 2048
0.00.039.973 I print_info: n_embd_v_gqa     = 2048
0.00.039.974 I print_info: f_norm_eps       = 1.0e-05
0.00.039.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.975 I print_info: f_logit_scale    = 0.0e+00
0.00.039.976 I print_info: n_ff             = 8192
0.00.039.976 I print_info: n_expert         = 0
0.00.039.976 I print_info: n_expert_used    = 0
0.00.039.976 I print_info: causal attn      = 1
0.00.039.976 I print_info: pooling type     = 0
0.00.039.976 I print_info: rope type        = 2
0.00.039.978 I print_info: rope scaling     = linear
0.00.039.981 I print_info: freq_base_train  = 10000.0
0.00.039.981 I print_info: freq_scale_train = 1
0.00.039.981 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.982 I print_info: rope_finetuned   = unknown
0.00.039.982 I print_info: ssm_d_conv       = 0
0.00.039.982 I print_info: ssm_d_inner      = 0
0.00.039.982 I print_info: ssm_d_state      = 0
0.00.039.982 I print_info: ssm_dt_rank      = 0
0.00.039.982 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.982 I print_info: model type       = 1.4B
0.00.039.983 I print_info: model params     = 1.41 B
0.00.039.983 I print_info: general.name     = 1.4B
0.00.039.983 I print_info: vocab type       = BPE
0.00.039.984 I print_info: n_vocab          = 50304
0.00.039.984 I print_info: n_merges         = 50009
0.00.039.984 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.984 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.984 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.985 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.985 I print_info: LF token         = 187 ''
0.00.039.985 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.986 I print_info: max token length = 1024
0.00.039.986 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.824 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.830 I load_tensors: offloading output layer to GPU
0.00.646.832 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.855 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.646.858 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.648.310 I llama_init_from_model: n_seq_max     = 1
0.00.648.312 I llama_init_from_model: n_ctx         = 2048
0.00.648.312 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.313 I llama_init_from_model: n_batch       = 2048
0.00.648.313 I llama_init_from_model: n_ubatch      = 512
0.00.648.313 I llama_init_from_model: flash_attn    = 0
0.00.648.314 I llama_init_from_model: freq_base     = 10000.0
0.00.648.315 I llama_init_from_model: freq_scale    = 1
0.00.648.316 I ggml_metal_init: allocating
0.00.648.335 I ggml_metal_init: found device: Apple M4
0.00.648.348 I ggml_metal_init: picking default device: Apple M4
0.00.649.865 I ggml_metal_init: using embedded metal library
0.00.655.845 I ggml_metal_init: GPU name:   Apple M4
0.00.655.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.851 I ggml_metal_init: simdgroup reduction   = true
0.00.655.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.851 I ggml_metal_init: has residency sets    = true
0.00.655.851 I ggml_metal_init: has bfloat            = true
0.00.655.852 I ggml_metal_init: use bfloat            = true
0.00.655.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.389 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.893 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.900 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.787 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.789 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.790 I llama_init_from_model: graph nodes  = 967
0.00.735.790 I llama_init_from_model: graph splits = 2
0.00.735.797 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.184 I main: llama threadpool init, n_threads = 4
0.00.799.227 I 
0.00.799.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.253 I 
0.00.799.393 I sampler seed: 1234
0.00.799.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.417 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.417 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.418 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.684.215 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.684.216 I llama_perf_context_print:        load time =     789.60 ms
0.01.684.217 I llama_perf_context_print: prompt eval time =      57.53 ms /     7 tokens (    8.22 ms per token,   121.68 tokens per second)
0.01.684.218 I llama_perf_context_print:        eval time =     824.30 ms /    63 runs   (   13.08 ms per token,    76.43 tokens per second)
0.01.684.218 I llama_perf_context_print:       total time =     885.78 ms /    70 tokens
0.01.684.442 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.109s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4761 (a28e0d5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.257 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.261 I llama_model_loader: - type  f32:  194 tensors
0.00.024.261 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.262 I print_info: file format = GGUF V3 (latest)
0.00.024.262 I print_info: file type   = Q6_K
0.00.024.263 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.097 I load: special tokens cache size = 25
0.00.038.187 I load: token to piece cache size = 0.2984 MB
0.00.038.191 I print_info: arch             = gptneox
0.00.038.191 I print_info: vocab_only       = 0
0.00.038.191 I print_info: n_ctx_train      = 2048
0.00.038.191 I print_info: n_embd           = 2048
0.00.038.192 I print_info: n_layer          = 24
0.00.038.196 I print_info: n_head           = 16
0.00.038.197 I print_info: n_head_kv        = 16
0.00.038.197 I print_info: n_rot            = 32
0.00.038.197 I print_info: n_swa            = 0
0.00.038.197 I print_info: n_embd_head_k    = 128
0.00.038.198 I print_info: n_embd_head_v    = 128
0.00.038.201 I print_info: n_gqa            = 1
0.00.038.202 I print_info: n_embd_k_gqa     = 2048
0.00.038.202 I print_info: n_embd_v_gqa     = 2048
0.00.038.203 I print_info: f_norm_eps       = 1.0e-05
0.00.038.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.203 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.204 I print_info: f_logit_scale    = 0.0e+00
0.00.038.204 I print_info: n_ff             = 8192
0.00.038.204 I print_info: n_expert         = 0
0.00.038.205 I print_info: n_expert_used    = 0
0.00.038.205 I print_info: causal attn      = 1
0.00.038.205 I print_info: pooling type     = 0
0.00.038.205 I print_info: rope type        = 2
0.00.038.205 I print_info: rope scaling     = linear
0.00.038.205 I print_info: freq_base_train  = 10000.0
0.00.038.206 I print_info: freq_scale_train = 1
0.00.038.206 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.206 I print_info: rope_finetuned   = unknown
0.00.038.206 I print_info: ssm_d_conv       = 0
0.00.038.206 I print_info: ssm_d_inner      = 0
0.00.038.206 I print_info: ssm_d_state      = 0
0.00.038.206 I print_info: ssm_dt_rank      = 0
0.00.038.207 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.207 I print_info: model type       = 1.4B
0.00.038.207 I print_info: model params     = 1.41 B
0.00.038.207 I print_info: general.name     = 1.4B
0.00.038.208 I print_info: vocab type       = BPE
0.00.038.209 I print_info: n_vocab          = 50304
0.00.038.210 I print_info: n_merges         = 50009
0.00.038.210 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.211 I print_info: LF token         = 187 ''
0.00.038.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.211 I print_info: max token length = 1024
0.00.038.212 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.937 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.950 I load_tensors: offloading output layer to GPU
0.00.611.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.987 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.611.993 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.613.702 I llama_init_from_model: n_seq_max     = 1
0.00.613.704 I llama_init_from_model: n_ctx         = 128
0.00.613.705 I llama_init_from_model: n_ctx_per_seq = 128
0.00.613.705 I llama_init_from_model: n_batch       = 128
0.00.613.706 I llama_init_from_model: n_ubatch      = 128
0.00.613.706 I llama_init_from_model: flash_attn    = 0
0.00.613.708 I llama_init_from_model: freq_base     = 10000.0
0.00.613.708 I llama_init_from_model: freq_scale    = 1
0.00.613.709 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.711 I ggml_metal_init: allocating
0.00.613.762 I ggml_metal_init: found device: Apple M4
0.00.613.775 I ggml_metal_init: picking default device: Apple M4
0.00.615.275 I ggml_metal_init: using embedded metal library
0.00.621.440 I ggml_metal_init: GPU name:   Apple M4
0.00.621.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.446 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.446 I ggml_metal_init: simdgroup reduction   = true
0.00.621.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.447 I ggml_metal_init: has residency sets    = true
0.00.621.447 I ggml_metal_init: has bfloat            = true
0.00.621.447 I ggml_metal_init: use bfloat            = true
0.00.621.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.910 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.347 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.351 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.545 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.547 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.548 I llama_init_from_model: graph nodes  = 967
0.00.644.548 I llama_init_from_model: graph splits = 2
0.00.644.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.371 I 
0.00.677.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.510 I perplexity: tokenizing the input ..
0.00.684.764 I perplexity: tokenization took 7.25 ms
0.00.684.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.411 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.818.753 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.818.780 I llama_perf_context_print:        load time =     668.43 ms
0.00.818.780 I llama_perf_context_print: prompt eval time =     131.72 ms /   128 tokens (    1.03 ms per token,   971.74 tokens per second)
0.00.818.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.782 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.819.179 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.078s
sys	0m0.140s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4761 (a28e0d5e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b305ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b306550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b3069c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b307110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b307580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b3079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b307fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b308550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b308b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b309000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b309500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b309a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b30a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b30acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b30b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b30bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b30c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b30ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b30d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b30d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b30e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b30e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b30ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b30f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b30fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b310110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b310720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b311390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b3118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b311b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b312030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b3122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b312b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b3130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b313380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b313820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b313cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b314160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b314600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b314f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b3153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b315880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b315d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b315fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b3165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b316c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b317520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b317b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b318140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b318750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b318d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b319370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b319980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b31a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b31a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b31aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b31ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b31b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b31bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b31be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b31c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b31c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b31cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b31d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b31d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b31d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b31de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b31e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b31e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b31ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b31f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b31f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b31fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b320050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b3205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b320af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b321040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b321590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b321ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b322580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b322ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b323020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b323570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b323ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b324010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b324560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b324ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b325000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b325550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b325aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b326540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b326a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b326fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b327530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b317210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b3279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b328150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b3286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b328bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b329140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b329690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b329be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b32a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b32a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b32abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b32b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b32b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b32bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b32c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b32c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b32cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b32cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b32d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b32d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b32dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b32e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b32e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b32eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b32f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b32f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b32f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b32fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b330280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b330720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b330bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b331060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b331500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b3319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b331e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b3322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b332780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b332c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b3330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b333560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b333a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b333ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b334340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b3347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b334c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b335120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b3355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b335a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b335f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b3363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b336840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b336ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b337180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b337620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b337ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b337f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b338400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b3388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b338d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b3391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b339680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b339b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b339fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b33a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b33a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b33ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b33b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b33b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b33bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b33c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b33c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b33c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b33ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b33d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b33d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b33dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b33e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b33e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b33e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b33ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b33f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b33f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b33fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b3400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b340580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b340a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b340ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b341360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b341800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b341ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b342140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b3425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b342a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b342f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b3433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b343860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b343db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b344300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b344850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b344da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b345060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b345670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b345c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b346290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b346a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b346f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b3471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b3477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b347e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b3485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b348a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b348f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b3493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b349b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b34a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b34a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b34ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b34b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b34b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b34bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b34c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b34c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b34cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b34d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b34d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b34db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b34e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b34e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b34eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b34f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b34f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b34fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b350070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b3505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b350b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b351060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b3515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b351b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b352050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b3525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b352af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b353040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b353590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b353ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b354030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b354580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b354ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b355020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b355570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b355ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b356010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b356560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b356ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b357000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b357550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b357aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b357ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b358540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b358a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b358fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b359530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b359a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b359fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b35a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b35aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b35afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b35b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b35ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b35bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b35c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b35c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b35ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b35d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b35d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b35dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b35e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b35e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b35ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b35eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b35f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b35f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b35fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b360120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b3605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b360a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b360fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b3616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b361df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b362510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b362c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b362ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b3636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b3639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b363fb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.755.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.755.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b206080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b2064f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b206960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b206dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b207240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b2076b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b207b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b207f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b208400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b208870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b208ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b2093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b209ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b20a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b20ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b20b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b20bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b20c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b20cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b20d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b20d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b20e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b20e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b20ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b20f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b20f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b20fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b210060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b2104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b210940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b210db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b2112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b211750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b211a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b211e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b2122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b212760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b212bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b213040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b2134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b213920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b213d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b214200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b214670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b214ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b214f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b2153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b215830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b215ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b216110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b216580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b2169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b216e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b2172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b217740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b217bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b218120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b218620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b218a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b218f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b219370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b2197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b219c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b21a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b21a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b21a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b21ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b21b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b21b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b21bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b21bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b21c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b21c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b21cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b21d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b21d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b21da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b21dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b21e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b21e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b21ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b21f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b21f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b21f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b21fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b220260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b2206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b220b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b220fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b221420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b221890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b221d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b222170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b2225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b222a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b222ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b223330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b2237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b223c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b224080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b2244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b224960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b224dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b225240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b2256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b225b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b225f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b226400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b226870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b226ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b227150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b2275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b227a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b227ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b228310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b228780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b228bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b229060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b2294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b229940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b229db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b22a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b22a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b22ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b22af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b22b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b22b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b22bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b22c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b22c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b22ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b22ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b22d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b22d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b22dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b22e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b22e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b22e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b22ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b22f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b22f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b22fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b22ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b2303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b230830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b230ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b231110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b231580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b2319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b231e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b2322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b232740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b232bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b233020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b233490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b233900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b233d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b2341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b234650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b234ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b234f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b2353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b235810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b235c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b2360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b236560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b237190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b237450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b237710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b237b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b237ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b238460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b2388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b238d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b2391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b239620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b239a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b239f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b23a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b23a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b23ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b23b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b23b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b23b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b23be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b23c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b23c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b23cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b23cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b23d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b23d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b23dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b23e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b23e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b23ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b23eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b23f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b23f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b23fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b2400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b240510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b240980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b240ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b2413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b241860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b241cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b242140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b2425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b242ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b242fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b243b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b243e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b2443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b244990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b244f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b245510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b245ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b246090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b246650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b246c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b2471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b247790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b247d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b248310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b2488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b248e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b249a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b249fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b24a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b24ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b24b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b24b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b24bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b24c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b24c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b24cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b24d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b24d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b24df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b24e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b24ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b24f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b24f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b24fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b250190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b250750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b250d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b2512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b251890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b251e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b252410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b2529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b252f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b253550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b253b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b2540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b254690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b254c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b255210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b2557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b255d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b256350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b256910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b256ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b257490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b257a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b258010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b258510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b258a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b258f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b259410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b259910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b259e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b25a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b25a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b25ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b25b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b25b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b25bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b25c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b25c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b25cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b25d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b25dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b25e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b25ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b25ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b25f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b25f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b25fe00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b406ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b407150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b4075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b407a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b407ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b408310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b408780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b408bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b409060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b4094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b409940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b40a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b40ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b40b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b40bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b40c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b40c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b40d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b40d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b40df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b40e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b40ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b40f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b40fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b4102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b410560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b410820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b410c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b411100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b411570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b4119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b411f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b412380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b412640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b412ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b412f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b413390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b413800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b413c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b4140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b414550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b4149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b414e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b4152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b415710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b415b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b415ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b416460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b4168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b416d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b4171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b417620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b417a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b417f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b418370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b4187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b418d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b419250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b4196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b419b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b419fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b41a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b41a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b41acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b41b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b41b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b41ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b41beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b41c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b41c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b41cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b41d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b41d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b41d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b41ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b41e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b41e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b41eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b41ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b41f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b41f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b41fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b420140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b4205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b420a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b420e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b421300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b421770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b421be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b422050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b4224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b422930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b422da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b423210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b423680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b423af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b423f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b4243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b424840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b424cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b425120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b425590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b425a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b425e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b4267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b426aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b426f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b427380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b4277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b427c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b4280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b428540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b4289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b428e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b429290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b429700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b429b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b429fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b42a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b42a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b42ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b42b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b42b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b42ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b42bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b42c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b42c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b42cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b42d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b42d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b42d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b42de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b42e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b42e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b42eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b42efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b42f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b42f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b42fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b430180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b4305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b430a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b430ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b431340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b4317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b431c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b432090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b432500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b432970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b432de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b433250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b4336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b433b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b433fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b434410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b434880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b434cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b435160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b4355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b435a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b435eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b436320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b436790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b436c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b437070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b4374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b437950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b437dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b438230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b4386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b438b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b438f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b4393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b439860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b439cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b43a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b43a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b43aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b43ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b43b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b43b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b43bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b43c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b43c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b43c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b43cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b43d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b43d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b43daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b43df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b43e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b43e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b43ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b43f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b43f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b43fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b43fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b4402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b440750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b440bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b441030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b4414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b441910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b441d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b4421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b442660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b442ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b443060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b4434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b443940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b444490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b444750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b444a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b444e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b4452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b445760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b445bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b446040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b4464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b446920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b446d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b447200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b447670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b447ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b447f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b4483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b448830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b448ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b449110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b449580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b4499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b449e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b44a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b44a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b44abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b44b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b44b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b44b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b44bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b44c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b44c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b44cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b44cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b44d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b44d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b44dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b44e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b44e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b44e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b44ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b44f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b44f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b44fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b450000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b450470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b4508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b450d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b4511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b451630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b451aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b451f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b452380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b4527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b452c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b4530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b453540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b4539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b453e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b454290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b454700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b454b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b454fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b455450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b4558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b455d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b4561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b456610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b456a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b456ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b457360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b4577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b457c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b4580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b458b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b459240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b459960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b45a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b45a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b45a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b45adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b45b3c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.792s
user	0m0.275s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4761 (a28e0d5e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e0c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e0ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e0d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e0e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e11cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e1b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e1f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e4a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e4dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e4eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e50fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e54500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e58f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e5a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e5d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e5d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e5df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e5ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e5f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e5f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e5fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e60440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e62370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e64870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e65f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e66980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e67ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e69370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e69980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1490053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1490069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1490072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1490090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14900a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14900a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14900ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14900b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14900bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14900c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14900cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14900d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14900d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14900e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14900e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14900e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14900eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14900ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14900f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14900f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14900fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1490101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1490111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1490123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1490130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1490139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1490142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1490158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1490161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1490170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1490186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14901a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14901a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14901aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14901aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14901b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14901b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14901bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14901c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14901c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14901c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14901cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14901d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14901d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14901db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14901df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14901e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14901e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14901ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14901f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14901f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14901fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14901fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1490214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1490233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1490245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1490252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1490264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1490283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1490299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14902a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14902a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14902abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14902b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14902b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14902b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14902bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14902c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14902c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14902cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14902cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14902d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14902d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14902dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14902e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14902e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14902e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14902ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14902f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14902f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14902fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1490308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1490311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1490327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1490330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1490339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1490377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1490380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1490396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14903a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14903a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14903ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14903b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14903b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14903ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14903bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14903c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14903c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14903cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14903d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14903d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14903d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14903ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14903e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14903e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14903eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14903ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14903f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14903f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14903fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1490402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1490425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1490439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1490450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1490467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1490495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14904a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14904a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14904acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14904b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14904b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14904be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14904c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14904c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14904cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14904d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14904dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14904e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14904e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14904ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14904f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14904f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14904fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1490508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1490536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1490564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14905a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14905a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14905ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14905b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14905b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14905bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14905c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14905cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14905d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14905d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14905dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14905e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14905e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f0ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f0bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f0c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f0d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f0f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f16430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147f16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147f17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147f17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147f18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147f18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147f19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147f19570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147f19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147f19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147f1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147f1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147f1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147f1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147f1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147f1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147f1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147f1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147f1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147f1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147f1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147f1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147f1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147f1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147f1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147f1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147f1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147f1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147f20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147f20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147f20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147f21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147f216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147f21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147f22020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147f22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147f22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147f23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147f23560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147f24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147f24aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147f24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147f25540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147f25a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147f25fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147f26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147f26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147f27520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147f27a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147f27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147f28510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147f28a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147f28fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147f29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147f29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147f29fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147f2a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147f2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147f2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147f2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147f2ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147f2bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147f2c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147f2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147f2cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147f2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147f2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147f2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147f2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147f2ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147f2f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147f2fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147f30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147f30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147f30bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147f31060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147f31500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147f319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147f31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147f322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147f32780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147f32c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147f330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147f33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147f33a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147f33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147f34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147f347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147f34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147f35120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147f355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147f35a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147f35f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147f363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147f36840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147f36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147f37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147f37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147f37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147f37f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147f38400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147f388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147f38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147f391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147f39b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147f39fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147f3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147f3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147f3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147f3b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147f3b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147f3bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147f3c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147f3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147f3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147f3ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147f3d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147f3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147f3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147f3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147f3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147f3e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147f3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147f3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147f3f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147f3fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147f400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147f40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147f40a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147f40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147f41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147f41800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147f41ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147f42140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147f425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147f42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147f42f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147f43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147f43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147f441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147f44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147f44ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147f44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147f45420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147f458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147f45d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147f46200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147f466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147f46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147f47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147f47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147f47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147f47ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147f484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147f48ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147f490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147f498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147f49d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147f4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147f4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147f4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147f4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147f4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147f4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147f4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147f4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147f4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147f4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147f4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147f4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147f4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147f4f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147f4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147f50430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147f50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147f50ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147f51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147f51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147f51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147f52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147f52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147f52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147f53400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147f53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147f53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147f543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147f54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147f553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147f55930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147f55e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147f563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147f56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147f56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147f573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147f57910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147f57e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147f583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147f58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147f58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147f593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147f598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147f59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147f5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147f5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147f5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147f5b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147f5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147f5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147f5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147f5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147f5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147f5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147f5d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147f5de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147f5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147f5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147f5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147f5f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147f5f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147f5fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147f60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147f605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147f60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147f60f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147f613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147f61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147f61ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147f62180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147f62620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147f62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147f62f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147f63400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147f638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147f63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147f64510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147f64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147f65350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147f65a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147f65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147f66520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147f667e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147f66df0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.230s
sys	0m0.188s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.43 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.86 sec*proc (2 tests)

Total Test time (real) =   1.88 sec
        1.90 real         0.51 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
