Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.518s
user	0m0.894s
sys	0m1.218s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-sampling
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-gguf
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-gguf
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-chat-template
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-backend-ops
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-rope
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-batched
[ 73%] Built target llama-bench
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-embedding
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Built target llama-gritlm
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-create
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-passkey
[ 83%] Built target llama-cli
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-perplexity
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-quantize
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.060s
user	0m6.243s
sys	0m9.323s

main: quantize time =  2958.39 ms
main:    total time =  2958.39 ms

main: quantize time =  1459.63 ms
main:    total time =  1459.63 ms

main: quantize time =  3710.83 ms
main:    total time =  3710.83 ms

main: quantize time =  2622.18 ms
main:    total time =  2622.18 ms

main: quantize time =  2468.70 ms
main:    total time =  2468.70 ms

main: quantize time =  4978.04 ms
main:    total time =  4978.04 ms

main: quantize time =  5719.80 ms
main:    total time =  5719.80 ms

main: quantize time =  7058.17 ms
main:    total time =  7058.17 ms

main: quantize time =  5794.86 ms
main:    total time =  5794.86 ms

main: quantize time =  4757.69 ms
main:    total time =  4757.69 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.313 I main: llama backend init
0.00.000.320 I main: load the model and apply lora adapter, if any
0.00.029.090 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.909 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.054 I llama_model_loader: - type  f32:  194 tensors
0.00.061.054 I llama_model_loader: - type  f16:   98 tensors
0.00.061.055 I print_info: file format = GGUF V3 (latest)
0.00.061.056 I print_info: file type   = all F32 (guessed)
0.00.061.058 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.766 I load: special tokens cache size = 25
0.00.094.612 I load: token to piece cache size = 0.2984 MB
0.00.094.614 I print_info: arch             = gptneox
0.00.094.615 I print_info: vocab_only       = 0
0.00.094.615 I print_info: n_ctx_train      = 2048
0.00.094.615 I print_info: n_embd           = 2048
0.00.094.615 I print_info: n_layer          = 24
0.00.094.619 I print_info: n_head           = 16
0.00.094.620 I print_info: n_head_kv        = 16
0.00.094.620 I print_info: n_rot            = 32
0.00.094.620 I print_info: n_swa            = 0
0.00.094.620 I print_info: n_embd_head_k    = 128
0.00.094.621 I print_info: n_embd_head_v    = 128
0.00.094.621 I print_info: n_gqa            = 1
0.00.094.622 I print_info: n_embd_k_gqa     = 2048
0.00.094.623 I print_info: n_embd_v_gqa     = 2048
0.00.094.624 I print_info: f_norm_eps       = 1.0e-05
0.00.094.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.094.624 I print_info: f_clamp_kqv      = 0.0e+00
0.00.094.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.094.625 I print_info: f_logit_scale    = 0.0e+00
0.00.094.625 I print_info: n_ff             = 8192
0.00.094.626 I print_info: n_expert         = 0
0.00.094.626 I print_info: n_expert_used    = 0
0.00.094.626 I print_info: causal attn      = 1
0.00.094.626 I print_info: pooling type     = 0
0.00.094.628 I print_info: rope type        = 2
0.00.094.628 I print_info: rope scaling     = linear
0.00.094.629 I print_info: freq_base_train  = 10000.0
0.00.094.629 I print_info: freq_scale_train = 1
0.00.094.629 I print_info: n_ctx_orig_yarn  = 2048
0.00.094.629 I print_info: rope_finetuned   = unknown
0.00.094.629 I print_info: ssm_d_conv       = 0
0.00.094.630 I print_info: ssm_d_inner      = 0
0.00.094.630 I print_info: ssm_d_state      = 0
0.00.094.630 I print_info: ssm_dt_rank      = 0
0.00.094.630 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.630 I print_info: model type       = 1.4B
0.00.094.631 I print_info: model params     = 1.41 B
0.00.094.631 I print_info: general.name     = 1.4B
0.00.094.631 I print_info: vocab type       = BPE
0.00.094.631 I print_info: n_vocab          = 50304
0.00.094.631 I print_info: n_merges         = 50009
0.00.094.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.632 I print_info: LF token         = 128 'Ä'
0.00.094.633 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.633 I print_info: max token length = 1024
0.00.097.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.061 I load_tensors: offloading output layer to GPU
0.00.097.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.080 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.081 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.097.339 I llama_init_from_model: n_seq_max     = 1
0.00.097.340 I llama_init_from_model: n_ctx         = 2048
0.00.097.340 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.097.340 I llama_init_from_model: n_batch       = 2048
0.00.097.340 I llama_init_from_model: n_ubatch      = 512
0.00.097.341 I llama_init_from_model: flash_attn    = 0
0.00.097.341 I llama_init_from_model: freq_base     = 10000.0
0.00.097.341 I llama_init_from_model: freq_scale    = 1
0.00.097.342 I ggml_metal_init: allocating
0.00.097.345 I ggml_metal_init: found device: Apple M4
0.00.097.347 I ggml_metal_init: picking default device: Apple M4
0.00.097.899 I ggml_metal_init: using embedded metal library
0.00.135.016 I ggml_metal_init: GPU name:   Apple M4
0.00.135.019 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.135.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.135.020 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.135.020 I ggml_metal_init: simdgroup reduction   = true
0.00.135.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.135.020 I ggml_metal_init: has bfloat            = true
0.00.135.020 I ggml_metal_init: use bfloat            = true
0.00.135.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.135.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.160.220 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.181.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.181.597 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.181.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.182.612 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.182.613 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.182.614 I llama_init_from_model: graph nodes  = 967
0.00.182.614 I llama_init_from_model: graph splits = 2
0.00.182.617 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.182.745 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.182.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.265.327 I main: llama threadpool init, n_threads = 4
0.00.265.372 I 
0.00.265.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.265.405 I 
0.00.265.627 I sampler seed: 1234
0.00.265.632 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.265.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.265.660 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.265.660 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.098.491 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.02.098.491 I llama_perf_context_print:        load time =     235.23 ms
0.02.098.492 I llama_perf_context_print: prompt eval time =      44.07 ms /     7 tokens (    6.30 ms per token,   158.82 tokens per second)
0.02.098.493 I llama_perf_context_print:        eval time =    1785.98 ms /    63 runs   (   28.35 ms per token,    35.27 tokens per second)
0.02.098.493 I llama_perf_context_print:       total time =    1834.16 ms /    70 tokens
0.02.098.778 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.146s
sys	0m0.105s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.973 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.556 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.024 I llama_model_loader: - type  f32:  194 tensors
0.00.040.024 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.025 I print_info: file format = GGUF V3 (latest)
0.00.040.026 I print_info: file type   = Q8_0
0.00.040.027 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.063.122 I load: special tokens cache size = 25
0.00.070.154 I load: token to piece cache size = 0.2984 MB
0.00.070.159 I print_info: arch             = gptneox
0.00.070.160 I print_info: vocab_only       = 0
0.00.070.161 I print_info: n_ctx_train      = 2048
0.00.070.161 I print_info: n_embd           = 2048
0.00.070.162 I print_info: n_layer          = 24
0.00.070.167 I print_info: n_head           = 16
0.00.070.168 I print_info: n_head_kv        = 16
0.00.070.168 I print_info: n_rot            = 32
0.00.070.168 I print_info: n_swa            = 0
0.00.070.168 I print_info: n_embd_head_k    = 128
0.00.070.168 I print_info: n_embd_head_v    = 128
0.00.070.169 I print_info: n_gqa            = 1
0.00.070.169 I print_info: n_embd_k_gqa     = 2048
0.00.070.170 I print_info: n_embd_v_gqa     = 2048
0.00.070.171 I print_info: f_norm_eps       = 1.0e-05
0.00.070.171 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.171 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.172 I print_info: f_logit_scale    = 0.0e+00
0.00.070.173 I print_info: n_ff             = 8192
0.00.070.173 I print_info: n_expert         = 0
0.00.070.173 I print_info: n_expert_used    = 0
0.00.070.173 I print_info: causal attn      = 1
0.00.070.173 I print_info: pooling type     = 0
0.00.070.173 I print_info: rope type        = 2
0.00.070.174 I print_info: rope scaling     = linear
0.00.070.174 I print_info: freq_base_train  = 10000.0
0.00.070.174 I print_info: freq_scale_train = 1
0.00.070.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.176 I print_info: rope_finetuned   = unknown
0.00.070.177 I print_info: ssm_d_conv       = 0
0.00.070.177 I print_info: ssm_d_inner      = 0
0.00.070.177 I print_info: ssm_d_state      = 0
0.00.070.177 I print_info: ssm_dt_rank      = 0
0.00.070.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.177 I print_info: model type       = 1.4B
0.00.070.178 I print_info: model params     = 1.41 B
0.00.070.178 I print_info: general.name     = 1.4B
0.00.070.178 I print_info: vocab type       = BPE
0.00.070.179 I print_info: n_vocab          = 50304
0.00.070.179 I print_info: n_merges         = 50009
0.00.070.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.180 I print_info: LF token         = 128 'Ä'
0.00.070.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.180 I print_info: max token length = 1024
0.00.072.727 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.727 I load_tensors: offloading output layer to GPU
0.00.072.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.739 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.740 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.073.096 I llama_init_from_model: n_seq_max     = 1
0.00.073.097 I llama_init_from_model: n_ctx         = 2048
0.00.073.097 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.073.097 I llama_init_from_model: n_batch       = 2048
0.00.073.098 I llama_init_from_model: n_ubatch      = 512
0.00.073.098 I llama_init_from_model: flash_attn    = 0
0.00.073.098 I llama_init_from_model: freq_base     = 10000.0
0.00.073.099 I llama_init_from_model: freq_scale    = 1
0.00.073.099 I ggml_metal_init: allocating
0.00.073.102 I ggml_metal_init: found device: Apple M4
0.00.073.104 I ggml_metal_init: picking default device: Apple M4
0.00.073.789 I ggml_metal_init: using embedded metal library
0.00.076.803 I ggml_metal_init: GPU name:   Apple M4
0.00.076.805 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.806 I ggml_metal_init: simdgroup reduction   = true
0.00.076.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.806 I ggml_metal_init: has bfloat            = true
0.00.076.806 I ggml_metal_init: use bfloat            = true
0.00.076.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.626 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.410 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.424 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.115.603 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.115.605 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.115.606 I llama_init_from_model: graph nodes  = 967
0.00.115.606 I llama_init_from_model: graph splits = 2
0.00.115.610 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.280.030 I main: llama threadpool init, n_threads = 4
0.01.280.113 I 
0.01.280.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.280.170 I 
0.01.280.723 I sampler seed: 1234
0.01.280.730 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.280.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.280.763 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.280.763 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.371.149 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.371.150 I llama_perf_context_print:        load time =    1268.67 ms
0.02.371.151 I llama_perf_context_print: prompt eval time =      49.80 ms /     7 tokens (    7.11 ms per token,   140.55 tokens per second)
0.02.371.151 I llama_perf_context_print:        eval time =    1037.48 ms /    63 runs   (   16.47 ms per token,    60.72 tokens per second)
0.02.371.153 I llama_perf_context_print:       total time =    1092.51 ms /    70 tokens
0.02.371.340 I ggml_metal_free: deallocating

real	0m2.388s
user	0m0.127s
sys	0m0.245s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.017.233 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.408 I llama_model_loader: - type  f32:  194 tensors
0.00.040.408 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.409 I print_info: file format = GGUF V3 (latest)
0.00.040.413 I print_info: file type   = Q4_0
0.00.040.414 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.063.107 I load: special tokens cache size = 25
0.00.069.942 I load: token to piece cache size = 0.2984 MB
0.00.069.945 I print_info: arch             = gptneox
0.00.069.945 I print_info: vocab_only       = 0
0.00.069.945 I print_info: n_ctx_train      = 2048
0.00.069.945 I print_info: n_embd           = 2048
0.00.069.946 I print_info: n_layer          = 24
0.00.069.949 I print_info: n_head           = 16
0.00.069.950 I print_info: n_head_kv        = 16
0.00.069.950 I print_info: n_rot            = 32
0.00.069.953 I print_info: n_swa            = 0
0.00.069.953 I print_info: n_embd_head_k    = 128
0.00.069.953 I print_info: n_embd_head_v    = 128
0.00.069.954 I print_info: n_gqa            = 1
0.00.069.955 I print_info: n_embd_k_gqa     = 2048
0.00.069.955 I print_info: n_embd_v_gqa     = 2048
0.00.069.956 I print_info: f_norm_eps       = 1.0e-05
0.00.069.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.958 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.958 I print_info: f_logit_scale    = 0.0e+00
0.00.069.959 I print_info: n_ff             = 8192
0.00.069.959 I print_info: n_expert         = 0
0.00.069.959 I print_info: n_expert_used    = 0
0.00.069.959 I print_info: causal attn      = 1
0.00.069.959 I print_info: pooling type     = 0
0.00.069.959 I print_info: rope type        = 2
0.00.069.959 I print_info: rope scaling     = linear
0.00.069.960 I print_info: freq_base_train  = 10000.0
0.00.069.960 I print_info: freq_scale_train = 1
0.00.069.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.960 I print_info: rope_finetuned   = unknown
0.00.069.960 I print_info: ssm_d_conv       = 0
0.00.069.960 I print_info: ssm_d_inner      = 0
0.00.069.961 I print_info: ssm_d_state      = 0
0.00.069.961 I print_info: ssm_dt_rank      = 0
0.00.069.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.961 I print_info: model type       = 1.4B
0.00.069.961 I print_info: model params     = 1.41 B
0.00.069.961 I print_info: general.name     = 1.4B
0.00.069.962 I print_info: vocab type       = BPE
0.00.069.965 I print_info: n_vocab          = 50304
0.00.069.966 I print_info: n_merges         = 50009
0.00.069.966 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.966 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.966 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.967 I print_info: LF token         = 128 'Ä'
0.00.069.967 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.967 I print_info: max token length = 1024
0.00.072.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.176 I load_tensors: offloading output layer to GPU
0.00.072.176 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.183 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.184 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.505 I llama_init_from_model: n_seq_max     = 1
0.00.072.506 I llama_init_from_model: n_ctx         = 2048
0.00.072.506 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.506 I llama_init_from_model: n_batch       = 2048
0.00.072.506 I llama_init_from_model: n_ubatch      = 512
0.00.072.506 I llama_init_from_model: flash_attn    = 0
0.00.072.507 I llama_init_from_model: freq_base     = 10000.0
0.00.072.507 I llama_init_from_model: freq_scale    = 1
0.00.072.508 I ggml_metal_init: allocating
0.00.072.511 I ggml_metal_init: found device: Apple M4
0.00.072.513 I ggml_metal_init: picking default device: Apple M4
0.00.073.152 I ggml_metal_init: using embedded metal library
0.00.075.973 I ggml_metal_init: GPU name:   Apple M4
0.00.075.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.976 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.976 I ggml_metal_init: simdgroup reduction   = true
0.00.075.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.976 I ggml_metal_init: has bfloat            = true
0.00.075.976 I ggml_metal_init: use bfloat            = true
0.00.075.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.222 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.115.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.115.745 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.765 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.116.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.116.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.116.947 I llama_init_from_model: graph nodes  = 967
0.00.116.947 I llama_init_from_model: graph splits = 2
0.00.116.951 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.963 I main: llama threadpool init, n_threads = 4
0.00.712.010 I 
0.00.712.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.035 I 
0.00.712.253 I sampler seed: 1234
0.00.712.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.295 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.307 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.307 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.394.804 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.394.805 I llama_perf_context_print:        load time =     693.87 ms
0.01.394.806 I llama_perf_context_print: prompt eval time =      46.71 ms /     7 tokens (    6.67 ms per token,   149.86 tokens per second)
0.01.394.806 I llama_perf_context_print:        eval time =     632.78 ms /    63 runs   (   10.04 ms per token,    99.56 tokens per second)
0.01.394.807 I llama_perf_context_print:       total time =     683.70 ms /    70 tokens
0.01.395.100 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.119s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.654 I llama_model_loader: - type  f32:  194 tensors
0.00.033.654 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.655 I print_info: file format = GGUF V3 (latest)
0.00.033.655 I print_info: file type   = Q4_1
0.00.033.656 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.052.404 I load: special tokens cache size = 25
0.00.058.325 I load: token to piece cache size = 0.2984 MB
0.00.058.328 I print_info: arch             = gptneox
0.00.058.328 I print_info: vocab_only       = 0
0.00.058.328 I print_info: n_ctx_train      = 2048
0.00.058.329 I print_info: n_embd           = 2048
0.00.058.329 I print_info: n_layer          = 24
0.00.058.332 I print_info: n_head           = 16
0.00.058.332 I print_info: n_head_kv        = 16
0.00.058.333 I print_info: n_rot            = 32
0.00.058.333 I print_info: n_swa            = 0
0.00.058.335 I print_info: n_embd_head_k    = 128
0.00.058.335 I print_info: n_embd_head_v    = 128
0.00.058.336 I print_info: n_gqa            = 1
0.00.058.337 I print_info: n_embd_k_gqa     = 2048
0.00.058.337 I print_info: n_embd_v_gqa     = 2048
0.00.058.343 I print_info: f_norm_eps       = 1.0e-05
0.00.058.343 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.343 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.343 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.343 I print_info: f_logit_scale    = 0.0e+00
0.00.058.344 I print_info: n_ff             = 8192
0.00.058.344 I print_info: n_expert         = 0
0.00.058.345 I print_info: n_expert_used    = 0
0.00.058.345 I print_info: causal attn      = 1
0.00.058.348 I print_info: pooling type     = 0
0.00.058.348 I print_info: rope type        = 2
0.00.058.348 I print_info: rope scaling     = linear
0.00.058.349 I print_info: freq_base_train  = 10000.0
0.00.058.349 I print_info: freq_scale_train = 1
0.00.058.350 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.351 I print_info: rope_finetuned   = unknown
0.00.058.351 I print_info: ssm_d_conv       = 0
0.00.058.351 I print_info: ssm_d_inner      = 0
0.00.058.351 I print_info: ssm_d_state      = 0
0.00.058.351 I print_info: ssm_dt_rank      = 0
0.00.058.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.351 I print_info: model type       = 1.4B
0.00.058.352 I print_info: model params     = 1.41 B
0.00.058.352 I print_info: general.name     = 1.4B
0.00.058.352 I print_info: vocab type       = BPE
0.00.058.353 I print_info: n_vocab          = 50304
0.00.058.353 I print_info: n_merges         = 50009
0.00.058.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.354 I print_info: LF token         = 128 'Ä'
0.00.058.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.354 I print_info: max token length = 1024
0.00.059.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.933 I load_tensors: offloading output layer to GPU
0.00.059.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.943 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.944 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.060.225 I llama_init_from_model: n_seq_max     = 1
0.00.060.226 I llama_init_from_model: n_ctx         = 2048
0.00.060.226 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.226 I llama_init_from_model: n_batch       = 2048
0.00.060.226 I llama_init_from_model: n_ubatch      = 512
0.00.060.226 I llama_init_from_model: flash_attn    = 0
0.00.060.227 I llama_init_from_model: freq_base     = 10000.0
0.00.060.227 I llama_init_from_model: freq_scale    = 1
0.00.060.227 I ggml_metal_init: allocating
0.00.060.230 I ggml_metal_init: found device: Apple M4
0.00.060.232 I ggml_metal_init: picking default device: Apple M4
0.00.060.754 I ggml_metal_init: using embedded metal library
0.00.063.080 I ggml_metal_init: GPU name:   Apple M4
0.00.063.082 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.083 I ggml_metal_init: simdgroup reduction   = true
0.00.063.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.083 I ggml_metal_init: has bfloat            = true
0.00.063.083 I ggml_metal_init: use bfloat            = true
0.00.063.083 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.635 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.553 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.559 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.588 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.682 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.683 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.684 I llama_init_from_model: graph nodes  = 967
0.00.092.684 I llama_init_from_model: graph splits = 2
0.00.092.687 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.160 I main: llama threadpool init, n_threads = 4
0.00.750.193 I 
0.00.750.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.215 I 
0.00.750.429 I sampler seed: 1234
0.00.750.435 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.446 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.446 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.446 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.469.126 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66479.40 tokens per second)
0.01.469.127 I llama_perf_context_print:        load time =     740.51 ms
0.01.469.127 I llama_perf_context_print: prompt eval time =      39.64 ms /     7 tokens (    5.66 ms per token,   176.59 tokens per second)
0.01.469.128 I llama_perf_context_print:        eval time =     676.22 ms /    63 runs   (   10.73 ms per token,    93.17 tokens per second)
0.01.469.128 I llama_perf_context_print:       total time =     719.83 ms /    70 tokens
0.01.469.347 I ggml_metal_free: deallocating

real	0m1.485s
user	0m0.110s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.684 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.680 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.555 I llama_model_loader: - type  f32:  194 tensors
0.00.025.555 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.556 I print_info: file format = GGUF V3 (latest)
0.00.025.556 I print_info: file type   = Q5_0
0.00.025.560 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.096 I load: special tokens cache size = 25
0.00.050.081 I load: token to piece cache size = 0.2984 MB
0.00.050.084 I print_info: arch             = gptneox
0.00.050.084 I print_info: vocab_only       = 0
0.00.050.084 I print_info: n_ctx_train      = 2048
0.00.050.085 I print_info: n_embd           = 2048
0.00.050.085 I print_info: n_layer          = 24
0.00.050.087 I print_info: n_head           = 16
0.00.050.088 I print_info: n_head_kv        = 16
0.00.050.088 I print_info: n_rot            = 32
0.00.050.088 I print_info: n_swa            = 0
0.00.050.088 I print_info: n_embd_head_k    = 128
0.00.050.088 I print_info: n_embd_head_v    = 128
0.00.050.089 I print_info: n_gqa            = 1
0.00.050.090 I print_info: n_embd_k_gqa     = 2048
0.00.050.091 I print_info: n_embd_v_gqa     = 2048
0.00.050.091 I print_info: f_norm_eps       = 1.0e-05
0.00.050.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.096 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.097 I print_info: f_logit_scale    = 0.0e+00
0.00.050.097 I print_info: n_ff             = 8192
0.00.050.098 I print_info: n_expert         = 0
0.00.050.098 I print_info: n_expert_used    = 0
0.00.050.098 I print_info: causal attn      = 1
0.00.050.098 I print_info: pooling type     = 0
0.00.050.099 I print_info: rope type        = 2
0.00.050.100 I print_info: rope scaling     = linear
0.00.050.100 I print_info: freq_base_train  = 10000.0
0.00.050.100 I print_info: freq_scale_train = 1
0.00.050.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.101 I print_info: rope_finetuned   = unknown
0.00.050.101 I print_info: ssm_d_conv       = 0
0.00.050.101 I print_info: ssm_d_inner      = 0
0.00.050.101 I print_info: ssm_d_state      = 0
0.00.050.101 I print_info: ssm_dt_rank      = 0
0.00.050.101 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.102 I print_info: model type       = 1.4B
0.00.050.102 I print_info: model params     = 1.41 B
0.00.050.102 I print_info: general.name     = 1.4B
0.00.050.103 I print_info: vocab type       = BPE
0.00.050.103 I print_info: n_vocab          = 50304
0.00.050.103 I print_info: n_merges         = 50009
0.00.050.103 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.106 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.106 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.106 I print_info: LF token         = 128 'Ä'
0.00.050.107 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.107 I print_info: max token length = 1024
0.00.052.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.104 I load_tensors: offloading output layer to GPU
0.00.052.104 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.115 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.116 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.387 I llama_init_from_model: n_seq_max     = 1
0.00.052.388 I llama_init_from_model: n_ctx         = 2048
0.00.052.388 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.388 I llama_init_from_model: n_batch       = 2048
0.00.052.388 I llama_init_from_model: n_ubatch      = 512
0.00.052.389 I llama_init_from_model: flash_attn    = 0
0.00.052.389 I llama_init_from_model: freq_base     = 10000.0
0.00.052.389 I llama_init_from_model: freq_scale    = 1
0.00.052.390 I ggml_metal_init: allocating
0.00.052.393 I ggml_metal_init: found device: Apple M4
0.00.052.395 I ggml_metal_init: picking default device: Apple M4
0.00.052.902 I ggml_metal_init: using embedded metal library
0.00.055.271 I ggml_metal_init: GPU name:   Apple M4
0.00.055.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.274 I ggml_metal_init: simdgroup reduction   = true
0.00.055.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.274 I ggml_metal_init: has bfloat            = true
0.00.055.274 I ggml_metal_init: use bfloat            = true
0.00.055.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.840 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.780 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.787 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.815 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.744 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.745 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.745 I llama_init_from_model: graph nodes  = 967
0.00.085.745 I llama_init_from_model: graph splits = 2
0.00.085.748 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.652 I main: llama threadpool init, n_threads = 4
0.00.809.702 I 
0.00.809.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.726 I 
0.00.810.017 I sampler seed: 1234
0.00.810.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.061 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.064 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.064 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.590.112 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.590.113 I llama_perf_context_print:        load time =     800.07 ms
0.01.590.116 I llama_perf_context_print: prompt eval time =      43.15 ms /     7 tokens (    6.16 ms per token,   162.24 tokens per second)
0.01.590.116 I llama_perf_context_print:        eval time =     734.23 ms /    63 runs   (   11.65 ms per token,    85.80 tokens per second)
0.01.590.116 I llama_perf_context_print:       total time =     781.35 ms /    70 tokens
0.01.590.416 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.211 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.823 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.830 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.831 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.083 I llama_model_loader: - type  f32:  194 tensors
0.00.027.084 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.084 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.084 I print_info: file format = GGUF V3 (latest)
0.00.027.085 I print_info: file type   = Q5_1
0.00.027.086 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.635 I load: special tokens cache size = 25
0.00.051.714 I load: token to piece cache size = 0.2984 MB
0.00.051.717 I print_info: arch             = gptneox
0.00.051.717 I print_info: vocab_only       = 0
0.00.051.718 I print_info: n_ctx_train      = 2048
0.00.051.718 I print_info: n_embd           = 2048
0.00.051.718 I print_info: n_layer          = 24
0.00.051.721 I print_info: n_head           = 16
0.00.051.722 I print_info: n_head_kv        = 16
0.00.051.722 I print_info: n_rot            = 32
0.00.051.722 I print_info: n_swa            = 0
0.00.051.722 I print_info: n_embd_head_k    = 128
0.00.051.722 I print_info: n_embd_head_v    = 128
0.00.051.723 I print_info: n_gqa            = 1
0.00.051.724 I print_info: n_embd_k_gqa     = 2048
0.00.051.725 I print_info: n_embd_v_gqa     = 2048
0.00.051.725 I print_info: f_norm_eps       = 1.0e-05
0.00.051.725 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.726 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.726 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.726 I print_info: f_logit_scale    = 0.0e+00
0.00.051.727 I print_info: n_ff             = 8192
0.00.051.727 I print_info: n_expert         = 0
0.00.051.727 I print_info: n_expert_used    = 0
0.00.051.727 I print_info: causal attn      = 1
0.00.051.727 I print_info: pooling type     = 0
0.00.051.729 I print_info: rope type        = 2
0.00.051.730 I print_info: rope scaling     = linear
0.00.051.731 I print_info: freq_base_train  = 10000.0
0.00.051.731 I print_info: freq_scale_train = 1
0.00.051.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.731 I print_info: rope_finetuned   = unknown
0.00.051.732 I print_info: ssm_d_conv       = 0
0.00.051.733 I print_info: ssm_d_inner      = 0
0.00.051.733 I print_info: ssm_d_state      = 0
0.00.051.733 I print_info: ssm_dt_rank      = 0
0.00.051.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.734 I print_info: model type       = 1.4B
0.00.051.734 I print_info: model params     = 1.41 B
0.00.051.734 I print_info: general.name     = 1.4B
0.00.051.735 I print_info: vocab type       = BPE
0.00.051.735 I print_info: n_vocab          = 50304
0.00.051.735 I print_info: n_merges         = 50009
0.00.051.735 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.736 I print_info: LF token         = 128 'Ä'
0.00.051.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.740 I print_info: max token length = 1024
0.00.053.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.741 I load_tensors: offloading output layer to GPU
0.00.053.741 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.752 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.753 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.054.022 I llama_init_from_model: n_seq_max     = 1
0.00.054.023 I llama_init_from_model: n_ctx         = 2048
0.00.054.023 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.023 I llama_init_from_model: n_batch       = 2048
0.00.054.023 I llama_init_from_model: n_ubatch      = 512
0.00.054.023 I llama_init_from_model: flash_attn    = 0
0.00.054.024 I llama_init_from_model: freq_base     = 10000.0
0.00.054.024 I llama_init_from_model: freq_scale    = 1
0.00.054.024 I ggml_metal_init: allocating
0.00.054.027 I ggml_metal_init: found device: Apple M4
0.00.054.029 I ggml_metal_init: picking default device: Apple M4
0.00.054.537 I ggml_metal_init: using embedded metal library
0.00.056.913 I ggml_metal_init: GPU name:   Apple M4
0.00.056.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.915 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.916 I ggml_metal_init: simdgroup reduction   = true
0.00.056.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.916 I ggml_metal_init: has bfloat            = true
0.00.056.916 I ggml_metal_init: use bfloat            = true
0.00.056.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.540 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.370 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.378 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.407 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.474 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.476 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.476 I llama_init_from_model: graph nodes  = 967
0.00.086.476 I llama_init_from_model: graph splits = 2
0.00.086.478 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.783 I main: llama threadpool init, n_threads = 4
0.00.698.822 I 
0.00.698.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.848 I 
0.00.699.067 I sampler seed: 1234
0.00.699.073 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.114 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.114 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.115 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.537.118 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.537.119 I llama_perf_context_print:        load time =     687.71 ms
0.01.537.120 I llama_perf_context_print: prompt eval time =      42.18 ms /     7 tokens (    6.03 ms per token,   165.96 tokens per second)
0.01.537.120 I llama_perf_context_print:        eval time =     792.83 ms /    63 runs   (   12.58 ms per token,    79.46 tokens per second)
0.01.537.121 I llama_perf_context_print:       total time =     839.19 ms /    70 tokens
0.01.537.378 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.520 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.522 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.522 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.523 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.581 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.582 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.583 I llama_model_loader: - type  f32:  194 tensors
0.00.024.583 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.583 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.583 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.584 I print_info: file format = GGUF V3 (latest)
0.00.024.584 I print_info: file type   = Q2_K - Medium
0.00.024.589 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.793 I load: special tokens cache size = 25
0.00.049.862 I load: token to piece cache size = 0.2984 MB
0.00.049.865 I print_info: arch             = gptneox
0.00.049.865 I print_info: vocab_only       = 0
0.00.049.865 I print_info: n_ctx_train      = 2048
0.00.049.866 I print_info: n_embd           = 2048
0.00.049.866 I print_info: n_layer          = 24
0.00.049.869 I print_info: n_head           = 16
0.00.049.869 I print_info: n_head_kv        = 16
0.00.049.869 I print_info: n_rot            = 32
0.00.049.870 I print_info: n_swa            = 0
0.00.049.870 I print_info: n_embd_head_k    = 128
0.00.049.870 I print_info: n_embd_head_v    = 128
0.00.049.871 I print_info: n_gqa            = 1
0.00.049.872 I print_info: n_embd_k_gqa     = 2048
0.00.049.872 I print_info: n_embd_v_gqa     = 2048
0.00.049.873 I print_info: f_norm_eps       = 1.0e-05
0.00.049.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.874 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.874 I print_info: f_logit_scale    = 0.0e+00
0.00.049.875 I print_info: n_ff             = 8192
0.00.049.875 I print_info: n_expert         = 0
0.00.049.875 I print_info: n_expert_used    = 0
0.00.049.875 I print_info: causal attn      = 1
0.00.049.875 I print_info: pooling type     = 0
0.00.049.875 I print_info: rope type        = 2
0.00.049.876 I print_info: rope scaling     = linear
0.00.049.878 I print_info: freq_base_train  = 10000.0
0.00.049.878 I print_info: freq_scale_train = 1
0.00.049.878 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.879 I print_info: rope_finetuned   = unknown
0.00.049.879 I print_info: ssm_d_conv       = 0
0.00.049.879 I print_info: ssm_d_inner      = 0
0.00.049.879 I print_info: ssm_d_state      = 0
0.00.049.879 I print_info: ssm_dt_rank      = 0
0.00.049.879 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.880 I print_info: model type       = 1.4B
0.00.049.880 I print_info: model params     = 1.41 B
0.00.049.880 I print_info: general.name     = 1.4B
0.00.049.881 I print_info: vocab type       = BPE
0.00.049.882 I print_info: n_vocab          = 50304
0.00.049.882 I print_info: n_merges         = 50009
0.00.049.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.882 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.884 I print_info: LF token         = 128 'Ä'
0.00.049.884 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.885 I print_info: max token length = 1024
0.00.051.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.850 I load_tensors: offloading output layer to GPU
0.00.051.850 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.861 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.862 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.165 I llama_init_from_model: n_seq_max     = 1
0.00.052.166 I llama_init_from_model: n_ctx         = 2048
0.00.052.166 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.166 I llama_init_from_model: n_batch       = 2048
0.00.052.166 I llama_init_from_model: n_ubatch      = 512
0.00.052.166 I llama_init_from_model: flash_attn    = 0
0.00.052.167 I llama_init_from_model: freq_base     = 10000.0
0.00.052.167 I llama_init_from_model: freq_scale    = 1
0.00.052.168 I ggml_metal_init: allocating
0.00.052.171 I ggml_metal_init: found device: Apple M4
0.00.052.173 I ggml_metal_init: picking default device: Apple M4
0.00.052.674 I ggml_metal_init: using embedded metal library
0.00.055.043 I ggml_metal_init: GPU name:   Apple M4
0.00.055.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.046 I ggml_metal_init: simdgroup reduction   = true
0.00.055.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.046 I ggml_metal_init: has bfloat            = true
0.00.055.046 I ggml_metal_init: use bfloat            = true
0.00.055.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.047 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.035 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.040 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.940 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.941 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.942 I llama_init_from_model: graph nodes  = 967
0.00.084.942 I llama_init_from_model: graph splits = 2
0.00.084.945 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.735 I main: llama threadpool init, n_threads = 4
0.00.438.779 I 
0.00.438.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.803 I 
0.00.439.032 I sampler seed: 1234
0.00.439.037 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.078 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.902 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.113.902 I llama_perf_context_print:        load time =     428.96 ms
0.01.113.903 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.113.904 I llama_perf_context_print:        eval time =     636.04 ms /    63 runs   (   10.10 ms per token,    99.05 tokens per second)
0.01.113.904 I llama_perf_context_print:       total time =     676.03 ms /    70 tokens
0.01.114.162 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.110s
sys	0m0.109s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.977 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.558 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.491 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.493 I llama_model_loader: - type  f32:  194 tensors
0.00.025.493 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.493 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.494 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.494 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.494 I print_info: file format = GGUF V3 (latest)
0.00.025.495 I print_info: file type   = Q3_K - Medium
0.00.025.495 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.146 I load: special tokens cache size = 25
0.00.050.156 I load: token to piece cache size = 0.2984 MB
0.00.050.159 I print_info: arch             = gptneox
0.00.050.159 I print_info: vocab_only       = 0
0.00.050.159 I print_info: n_ctx_train      = 2048
0.00.050.159 I print_info: n_embd           = 2048
0.00.050.160 I print_info: n_layer          = 24
0.00.050.163 I print_info: n_head           = 16
0.00.050.164 I print_info: n_head_kv        = 16
0.00.050.164 I print_info: n_rot            = 32
0.00.050.164 I print_info: n_swa            = 0
0.00.050.164 I print_info: n_embd_head_k    = 128
0.00.050.164 I print_info: n_embd_head_v    = 128
0.00.050.165 I print_info: n_gqa            = 1
0.00.050.166 I print_info: n_embd_k_gqa     = 2048
0.00.050.167 I print_info: n_embd_v_gqa     = 2048
0.00.050.167 I print_info: f_norm_eps       = 1.0e-05
0.00.050.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.168 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.168 I print_info: f_logit_scale    = 0.0e+00
0.00.050.169 I print_info: n_ff             = 8192
0.00.050.169 I print_info: n_expert         = 0
0.00.050.169 I print_info: n_expert_used    = 0
0.00.050.169 I print_info: causal attn      = 1
0.00.050.170 I print_info: pooling type     = 0
0.00.050.170 I print_info: rope type        = 2
0.00.050.170 I print_info: rope scaling     = linear
0.00.050.170 I print_info: freq_base_train  = 10000.0
0.00.050.171 I print_info: freq_scale_train = 1
0.00.050.171 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.171 I print_info: rope_finetuned   = unknown
0.00.050.171 I print_info: ssm_d_conv       = 0
0.00.050.171 I print_info: ssm_d_inner      = 0
0.00.050.172 I print_info: ssm_d_state      = 0
0.00.050.172 I print_info: ssm_dt_rank      = 0
0.00.050.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.172 I print_info: model type       = 1.4B
0.00.050.172 I print_info: model params     = 1.41 B
0.00.050.173 I print_info: general.name     = 1.4B
0.00.050.173 I print_info: vocab type       = BPE
0.00.050.173 I print_info: n_vocab          = 50304
0.00.050.176 I print_info: n_merges         = 50009
0.00.050.176 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: LF token         = 128 'Ä'
0.00.050.177 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: max token length = 1024
0.00.052.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.126 I load_tensors: offloading output layer to GPU
0.00.052.126 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.136 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.137 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.439 I llama_init_from_model: n_seq_max     = 1
0.00.052.439 I llama_init_from_model: n_ctx         = 2048
0.00.052.439 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.440 I llama_init_from_model: n_batch       = 2048
0.00.052.440 I llama_init_from_model: n_ubatch      = 512
0.00.052.440 I llama_init_from_model: flash_attn    = 0
0.00.052.440 I llama_init_from_model: freq_base     = 10000.0
0.00.052.441 I llama_init_from_model: freq_scale    = 1
0.00.052.441 I ggml_metal_init: allocating
0.00.052.444 I ggml_metal_init: found device: Apple M4
0.00.052.446 I ggml_metal_init: picking default device: Apple M4
0.00.052.933 I ggml_metal_init: using embedded metal library
0.00.055.304 I ggml_metal_init: GPU name:   Apple M4
0.00.055.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.307 I ggml_metal_init: simdgroup reduction   = true
0.00.055.307 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.307 I ggml_metal_init: has bfloat            = true
0.00.055.307 I ggml_metal_init: use bfloat            = true
0.00.055.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.938 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.100 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.109 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.166 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.167 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.167 I llama_init_from_model: graph nodes  = 967
0.00.085.167 I llama_init_from_model: graph splits = 2
0.00.085.170 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.563 I main: llama threadpool init, n_threads = 4
0.00.522.601 I 
0.00.522.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.644 I 
0.00.522.855 I sampler seed: 1234
0.00.522.859 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.893 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.893 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.261.978 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.261.979 I llama_perf_context_print:        load time =     512.73 ms
0.01.261.980 I llama_perf_context_print: prompt eval time =      40.44 ms /     7 tokens (    5.78 ms per token,   173.11 tokens per second)
0.01.261.980 I llama_perf_context_print:        eval time =     695.59 ms /    63 runs   (   11.04 ms per token,    90.57 tokens per second)
0.01.261.984 I llama_perf_context_print:       total time =     740.27 ms /    70 tokens
0.01.262.190 I ggml_metal_free: deallocating

real	0m1.282s
user	0m0.109s
sys	0m0.119s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.046 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.046 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.076 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.011 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.011 I llama_model_loader: - type  f32:  194 tensors
0.00.025.012 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.012 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.012 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.013 I print_info: file format = GGUF V3 (latest)
0.00.025.013 I print_info: file type   = Q4_K - Medium
0.00.025.014 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.588 I load: special tokens cache size = 25
0.00.049.342 I load: token to piece cache size = 0.2984 MB
0.00.049.345 I print_info: arch             = gptneox
0.00.049.345 I print_info: vocab_only       = 0
0.00.049.345 I print_info: n_ctx_train      = 2048
0.00.049.346 I print_info: n_embd           = 2048
0.00.049.346 I print_info: n_layer          = 24
0.00.049.349 I print_info: n_head           = 16
0.00.049.349 I print_info: n_head_kv        = 16
0.00.049.349 I print_info: n_rot            = 32
0.00.049.352 I print_info: n_swa            = 0
0.00.049.352 I print_info: n_embd_head_k    = 128
0.00.049.352 I print_info: n_embd_head_v    = 128
0.00.049.353 I print_info: n_gqa            = 1
0.00.049.354 I print_info: n_embd_k_gqa     = 2048
0.00.049.354 I print_info: n_embd_v_gqa     = 2048
0.00.049.355 I print_info: f_norm_eps       = 1.0e-05
0.00.049.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.357 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.358 I print_info: f_logit_scale    = 0.0e+00
0.00.049.359 I print_info: n_ff             = 8192
0.00.049.359 I print_info: n_expert         = 0
0.00.049.359 I print_info: n_expert_used    = 0
0.00.049.359 I print_info: causal attn      = 1
0.00.049.359 I print_info: pooling type     = 0
0.00.049.359 I print_info: rope type        = 2
0.00.049.360 I print_info: rope scaling     = linear
0.00.049.360 I print_info: freq_base_train  = 10000.0
0.00.049.360 I print_info: freq_scale_train = 1
0.00.049.360 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.361 I print_info: rope_finetuned   = unknown
0.00.049.361 I print_info: ssm_d_conv       = 0
0.00.049.361 I print_info: ssm_d_inner      = 0
0.00.049.361 I print_info: ssm_d_state      = 0
0.00.049.361 I print_info: ssm_dt_rank      = 0
0.00.049.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.362 I print_info: model type       = 1.4B
0.00.049.362 I print_info: model params     = 1.41 B
0.00.049.362 I print_info: general.name     = 1.4B
0.00.049.363 I print_info: vocab type       = BPE
0.00.049.363 I print_info: n_vocab          = 50304
0.00.049.363 I print_info: n_merges         = 50009
0.00.049.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.364 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.364 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.364 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.364 I print_info: LF token         = 128 'Ä'
0.00.049.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.365 I print_info: max token length = 1024
0.00.051.182 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.182 I load_tensors: offloading output layer to GPU
0.00.051.182 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.193 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.194 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.470 I llama_init_from_model: n_seq_max     = 1
0.00.051.471 I llama_init_from_model: n_ctx         = 2048
0.00.051.471 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.471 I llama_init_from_model: n_batch       = 2048
0.00.051.471 I llama_init_from_model: n_ubatch      = 512
0.00.051.472 I llama_init_from_model: flash_attn    = 0
0.00.051.472 I llama_init_from_model: freq_base     = 10000.0
0.00.051.472 I llama_init_from_model: freq_scale    = 1
0.00.051.473 I ggml_metal_init: allocating
0.00.051.476 I ggml_metal_init: found device: Apple M4
0.00.051.477 I ggml_metal_init: picking default device: Apple M4
0.00.052.006 I ggml_metal_init: using embedded metal library
0.00.054.382 I ggml_metal_init: GPU name:   Apple M4
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.385 I ggml_metal_init: simdgroup reduction   = true
0.00.054.385 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.385 I ggml_metal_init: has bfloat            = true
0.00.054.385 I ggml_metal_init: use bfloat            = true
0.00.054.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.551 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.569 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.685 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.686 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.686 I llama_init_from_model: graph nodes  = 967
0.00.086.686 I llama_init_from_model: graph splits = 2
0.00.086.689 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.292 I main: llama threadpool init, n_threads = 4
0.00.607.329 I 
0.00.607.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.351 I 
0.00.607.574 I sampler seed: 1234
0.00.607.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.597 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.598 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.598 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.364.238 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.364.239 I llama_perf_context_print:        load time =     598.28 ms
0.01.364.240 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.55 tokens per second)
0.01.364.240 I llama_perf_context_print:        eval time =     706.59 ms /    63 runs   (   11.22 ms per token,    89.16 tokens per second)
0.01.364.241 I llama_perf_context_print:       total time =     757.81 ms /    70 tokens
0.01.364.477 I ggml_metal_free: deallocating

real	0m1.383s
user	0m0.110s
sys	0m0.138s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.366 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.369 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.381 I llama_model_loader: - type  f32:  194 tensors
0.00.026.381 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.381 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.382 I print_info: file format = GGUF V3 (latest)
0.00.026.382 I print_info: file type   = Q5_K - Medium
0.00.026.383 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.790 I load: special tokens cache size = 25
0.00.051.778 I load: token to piece cache size = 0.2984 MB
0.00.051.781 I print_info: arch             = gptneox
0.00.051.781 I print_info: vocab_only       = 0
0.00.051.781 I print_info: n_ctx_train      = 2048
0.00.051.782 I print_info: n_embd           = 2048
0.00.051.782 I print_info: n_layer          = 24
0.00.051.785 I print_info: n_head           = 16
0.00.051.785 I print_info: n_head_kv        = 16
0.00.051.786 I print_info: n_rot            = 32
0.00.051.786 I print_info: n_swa            = 0
0.00.051.786 I print_info: n_embd_head_k    = 128
0.00.051.786 I print_info: n_embd_head_v    = 128
0.00.051.787 I print_info: n_gqa            = 1
0.00.051.788 I print_info: n_embd_k_gqa     = 2048
0.00.051.788 I print_info: n_embd_v_gqa     = 2048
0.00.051.789 I print_info: f_norm_eps       = 1.0e-05
0.00.051.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.789 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.790 I print_info: f_logit_scale    = 0.0e+00
0.00.051.790 I print_info: n_ff             = 8192
0.00.051.791 I print_info: n_expert         = 0
0.00.051.791 I print_info: n_expert_used    = 0
0.00.051.791 I print_info: causal attn      = 1
0.00.051.791 I print_info: pooling type     = 0
0.00.051.793 I print_info: rope type        = 2
0.00.051.795 I print_info: rope scaling     = linear
0.00.051.795 I print_info: freq_base_train  = 10000.0
0.00.051.796 I print_info: freq_scale_train = 1
0.00.051.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.796 I print_info: rope_finetuned   = unknown
0.00.051.796 I print_info: ssm_d_conv       = 0
0.00.051.796 I print_info: ssm_d_inner      = 0
0.00.051.797 I print_info: ssm_d_state      = 0
0.00.051.797 I print_info: ssm_dt_rank      = 0
0.00.051.797 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.797 I print_info: model type       = 1.4B
0.00.051.797 I print_info: model params     = 1.41 B
0.00.051.798 I print_info: general.name     = 1.4B
0.00.051.798 I print_info: vocab type       = BPE
0.00.051.798 I print_info: n_vocab          = 50304
0.00.051.798 I print_info: n_merges         = 50009
0.00.051.799 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.799 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.799 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.799 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.799 I print_info: LF token         = 128 'Ä'
0.00.051.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.804 I print_info: max token length = 1024
0.00.053.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.847 I load_tensors: offloading output layer to GPU
0.00.053.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.858 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.859 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.151 I llama_init_from_model: n_seq_max     = 1
0.00.054.151 I llama_init_from_model: n_ctx         = 2048
0.00.054.151 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.152 I llama_init_from_model: n_batch       = 2048
0.00.054.152 I llama_init_from_model: n_ubatch      = 512
0.00.054.152 I llama_init_from_model: flash_attn    = 0
0.00.054.152 I llama_init_from_model: freq_base     = 10000.0
0.00.054.153 I llama_init_from_model: freq_scale    = 1
0.00.054.153 I ggml_metal_init: allocating
0.00.054.156 I ggml_metal_init: found device: Apple M4
0.00.054.158 I ggml_metal_init: picking default device: Apple M4
0.00.054.672 I ggml_metal_init: using embedded metal library
0.00.057.019 I ggml_metal_init: GPU name:   Apple M4
0.00.057.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.021 I ggml_metal_init: simdgroup reduction   = true
0.00.057.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.022 I ggml_metal_init: has bfloat            = true
0.00.057.022 I ggml_metal_init: use bfloat            = true
0.00.057.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.903 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.779 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.799 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.891 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.893 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.893 I llama_init_from_model: graph nodes  = 967
0.00.087.893 I llama_init_from_model: graph splits = 2
0.00.087.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.420 I main: llama threadpool init, n_threads = 4
0.00.683.455 I 
0.00.683.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.478 I 
0.00.683.697 I sampler seed: 1234
0.00.683.702 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.742 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.742 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.532.421 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.532.421 I llama_perf_context_print:        load time =     673.76 ms
0.01.532.422 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.63 tokens per second)
0.01.532.423 I llama_perf_context_print:        eval time =     794.21 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.532.423 I llama_perf_context_print:       total time =     849.86 ms /    70 tokens
0.01.532.668 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.111s
sys	0m0.146s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.310 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.310 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.310 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.152 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.873 I llama_model_loader: - type  f32:  194 tensors
0.00.023.874 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.874 I print_info: file format = GGUF V3 (latest)
0.00.023.875 I print_info: file type   = Q6_K
0.00.023.875 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.504 I load: special tokens cache size = 25
0.00.048.508 I load: token to piece cache size = 0.2984 MB
0.00.048.511 I print_info: arch             = gptneox
0.00.048.511 I print_info: vocab_only       = 0
0.00.048.511 I print_info: n_ctx_train      = 2048
0.00.048.511 I print_info: n_embd           = 2048
0.00.048.511 I print_info: n_layer          = 24
0.00.048.514 I print_info: n_head           = 16
0.00.048.515 I print_info: n_head_kv        = 16
0.00.048.515 I print_info: n_rot            = 32
0.00.048.515 I print_info: n_swa            = 0
0.00.048.515 I print_info: n_embd_head_k    = 128
0.00.048.515 I print_info: n_embd_head_v    = 128
0.00.048.516 I print_info: n_gqa            = 1
0.00.048.517 I print_info: n_embd_k_gqa     = 2048
0.00.048.518 I print_info: n_embd_v_gqa     = 2048
0.00.048.519 I print_info: f_norm_eps       = 1.0e-05
0.00.048.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.522 I print_info: f_logit_scale    = 0.0e+00
0.00.048.523 I print_info: n_ff             = 8192
0.00.048.523 I print_info: n_expert         = 0
0.00.048.523 I print_info: n_expert_used    = 0
0.00.048.523 I print_info: causal attn      = 1
0.00.048.523 I print_info: pooling type     = 0
0.00.048.524 I print_info: rope type        = 2
0.00.048.524 I print_info: rope scaling     = linear
0.00.048.524 I print_info: freq_base_train  = 10000.0
0.00.048.525 I print_info: freq_scale_train = 1
0.00.048.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.525 I print_info: rope_finetuned   = unknown
0.00.048.525 I print_info: ssm_d_conv       = 0
0.00.048.530 I print_info: ssm_d_inner      = 0
0.00.048.530 I print_info: ssm_d_state      = 0
0.00.048.530 I print_info: ssm_dt_rank      = 0
0.00.048.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.532 I print_info: model type       = 1.4B
0.00.048.533 I print_info: model params     = 1.41 B
0.00.048.533 I print_info: general.name     = 1.4B
0.00.048.533 I print_info: vocab type       = BPE
0.00.048.534 I print_info: n_vocab          = 50304
0.00.048.534 I print_info: n_merges         = 50009
0.00.048.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.535 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.535 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.535 I print_info: LF token         = 128 'Ä'
0.00.048.535 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.536 I print_info: max token length = 1024
0.00.050.506 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.506 I load_tensors: offloading output layer to GPU
0.00.050.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.517 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.518 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.787 I llama_init_from_model: n_seq_max     = 1
0.00.050.788 I llama_init_from_model: n_ctx         = 2048
0.00.050.788 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.788 I llama_init_from_model: n_batch       = 2048
0.00.050.789 I llama_init_from_model: n_ubatch      = 512
0.00.050.789 I llama_init_from_model: flash_attn    = 0
0.00.050.789 I llama_init_from_model: freq_base     = 10000.0
0.00.050.789 I llama_init_from_model: freq_scale    = 1
0.00.050.790 I ggml_metal_init: allocating
0.00.050.793 I ggml_metal_init: found device: Apple M4
0.00.050.795 I ggml_metal_init: picking default device: Apple M4
0.00.051.264 I ggml_metal_init: using embedded metal library
0.00.053.621 I ggml_metal_init: GPU name:   Apple M4
0.00.053.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.624 I ggml_metal_init: simdgroup reduction   = true
0.00.053.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.624 I ggml_metal_init: has bfloat            = true
0.00.053.624 I ggml_metal_init: use bfloat            = true
0.00.053.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.235 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.190 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.197 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.233 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.234 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.234 I llama_init_from_model: graph nodes  = 967
0.00.084.235 I llama_init_from_model: graph splits = 2
0.00.084.237 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.195 I main: llama threadpool init, n_threads = 4
0.00.742.236 I 
0.00.742.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.286 I 
0.00.742.511 I sampler seed: 1234
0.00.742.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.572 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.574 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.574 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.635.058 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.635.059 I llama_perf_context_print:        load time =     732.63 ms
0.01.635.060 I llama_perf_context_print: prompt eval time =      60.29 ms /     7 tokens (    8.61 ms per token,   116.11 tokens per second)
0.01.635.061 I llama_perf_context_print:        eval time =     829.43 ms /    63 runs   (   13.17 ms per token,    75.96 tokens per second)
0.01.635.061 I llama_perf_context_print:       total time =     893.73 ms /    70 tokens
0.01.635.340 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.417 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.857 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.859 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.786 I llama_model_loader: - type  f32:  194 tensors
0.00.036.786 I llama_model_loader: - type  f16:   98 tensors
0.00.036.786 I print_info: file format = GGUF V3 (latest)
0.00.036.787 I print_info: file type   = all F32 (guessed)
0.00.036.792 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.317 I load: special tokens cache size = 25
0.00.062.436 I load: token to piece cache size = 0.2984 MB
0.00.062.439 I print_info: arch             = gptneox
0.00.062.439 I print_info: vocab_only       = 0
0.00.062.439 I print_info: n_ctx_train      = 2048
0.00.062.439 I print_info: n_embd           = 2048
0.00.062.440 I print_info: n_layer          = 24
0.00.062.444 I print_info: n_head           = 16
0.00.062.447 I print_info: n_head_kv        = 16
0.00.062.447 I print_info: n_rot            = 32
0.00.062.447 I print_info: n_swa            = 0
0.00.062.448 I print_info: n_embd_head_k    = 128
0.00.062.448 I print_info: n_embd_head_v    = 128
0.00.062.449 I print_info: n_gqa            = 1
0.00.062.453 I print_info: n_embd_k_gqa     = 2048
0.00.062.454 I print_info: n_embd_v_gqa     = 2048
0.00.062.454 I print_info: f_norm_eps       = 1.0e-05
0.00.062.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.455 I print_info: f_logit_scale    = 0.0e+00
0.00.062.456 I print_info: n_ff             = 8192
0.00.062.456 I print_info: n_expert         = 0
0.00.062.456 I print_info: n_expert_used    = 0
0.00.062.456 I print_info: causal attn      = 1
0.00.062.456 I print_info: pooling type     = 0
0.00.062.456 I print_info: rope type        = 2
0.00.062.457 I print_info: rope scaling     = linear
0.00.062.457 I print_info: freq_base_train  = 10000.0
0.00.062.457 I print_info: freq_scale_train = 1
0.00.062.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.458 I print_info: rope_finetuned   = unknown
0.00.062.458 I print_info: ssm_d_conv       = 0
0.00.062.458 I print_info: ssm_d_inner      = 0
0.00.062.459 I print_info: ssm_d_state      = 0
0.00.062.459 I print_info: ssm_dt_rank      = 0
0.00.062.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.459 I print_info: model type       = 1.4B
0.00.062.460 I print_info: model params     = 1.41 B
0.00.062.460 I print_info: general.name     = 1.4B
0.00.062.460 I print_info: vocab type       = BPE
0.00.062.460 I print_info: n_vocab          = 50304
0.00.062.461 I print_info: n_merges         = 50009
0.00.062.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.461 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.461 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.463 I print_info: LF token         = 128 'Ä'
0.00.062.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.464 I print_info: max token length = 1024
0.00.064.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.690 I load_tensors: offloading output layer to GPU
0.00.064.690 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.701 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.064.703 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.064.964 I llama_init_from_model: n_seq_max     = 1
0.00.064.965 I llama_init_from_model: n_ctx         = 128
0.00.064.965 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.965 I llama_init_from_model: n_batch       = 128
0.00.064.966 I llama_init_from_model: n_ubatch      = 128
0.00.064.966 I llama_init_from_model: flash_attn    = 0
0.00.064.966 I llama_init_from_model: freq_base     = 10000.0
0.00.064.966 I llama_init_from_model: freq_scale    = 1
0.00.064.967 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.967 I ggml_metal_init: allocating
0.00.064.970 I ggml_metal_init: found device: Apple M4
0.00.064.972 I ggml_metal_init: picking default device: Apple M4
0.00.065.473 I ggml_metal_init: using embedded metal library
0.00.067.919 I ggml_metal_init: GPU name:   Apple M4
0.00.067.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.921 I ggml_metal_init: simdgroup reduction   = true
0.00.067.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.922 I ggml_metal_init: has bfloat            = true
0.00.067.922 I ggml_metal_init: use bfloat            = true
0.00.067.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.994 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.605 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.607 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.583 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.585 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.585 I llama_init_from_model: graph nodes  = 967
0.00.080.585 I llama_init_from_model: graph splits = 2
0.00.080.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.475.160 I 
0.01.475.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.475.328 I perplexity: tokenizing the input ..
0.01.487.833 I perplexity: tokenization took 12.505 ms
0.01.487.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.609.237 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.610.833 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.610.863 I llama_perf_context_print:        load time =    1458.28 ms
0.01.610.865 I llama_perf_context_print: prompt eval time =     120.49 ms /   128 tokens (    0.94 ms per token,  1062.34 tokens per second)
0.01.610.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.610.866 I llama_perf_context_print:       total time =     135.72 ms /   129 tokens
0.01.611.493 I ggml_metal_free: deallocating

real	0m1.842s
user	0m0.099s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.799 I llama_model_loader: - type  f32:  194 tensors
0.00.035.799 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.800 I print_info: file format = GGUF V3 (latest)
0.00.035.800 I print_info: file type   = Q8_0
0.00.035.801 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.059.281 I load: special tokens cache size = 25
0.00.065.648 I load: token to piece cache size = 0.2984 MB
0.00.065.650 I print_info: arch             = gptneox
0.00.065.650 I print_info: vocab_only       = 0
0.00.065.651 I print_info: n_ctx_train      = 2048
0.00.065.651 I print_info: n_embd           = 2048
0.00.065.651 I print_info: n_layer          = 24
0.00.065.655 I print_info: n_head           = 16
0.00.065.656 I print_info: n_head_kv        = 16
0.00.065.656 I print_info: n_rot            = 32
0.00.065.656 I print_info: n_swa            = 0
0.00.065.656 I print_info: n_embd_head_k    = 128
0.00.065.658 I print_info: n_embd_head_v    = 128
0.00.065.659 I print_info: n_gqa            = 1
0.00.065.660 I print_info: n_embd_k_gqa     = 2048
0.00.065.660 I print_info: n_embd_v_gqa     = 2048
0.00.065.661 I print_info: f_norm_eps       = 1.0e-05
0.00.065.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.662 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.662 I print_info: f_logit_scale    = 0.0e+00
0.00.065.663 I print_info: n_ff             = 8192
0.00.065.663 I print_info: n_expert         = 0
0.00.065.663 I print_info: n_expert_used    = 0
0.00.065.663 I print_info: causal attn      = 1
0.00.065.663 I print_info: pooling type     = 0
0.00.065.666 I print_info: rope type        = 2
0.00.065.666 I print_info: rope scaling     = linear
0.00.065.666 I print_info: freq_base_train  = 10000.0
0.00.065.667 I print_info: freq_scale_train = 1
0.00.065.667 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.667 I print_info: rope_finetuned   = unknown
0.00.065.667 I print_info: ssm_d_conv       = 0
0.00.065.667 I print_info: ssm_d_inner      = 0
0.00.065.667 I print_info: ssm_d_state      = 0
0.00.065.667 I print_info: ssm_dt_rank      = 0
0.00.065.668 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.668 I print_info: model type       = 1.4B
0.00.065.668 I print_info: model params     = 1.41 B
0.00.065.668 I print_info: general.name     = 1.4B
0.00.065.669 I print_info: vocab type       = BPE
0.00.065.669 I print_info: n_vocab          = 50304
0.00.065.669 I print_info: n_merges         = 50009
0.00.065.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.670 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.671 I print_info: LF token         = 128 'Ä'
0.00.065.671 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.672 I print_info: max token length = 1024
0.00.067.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.000 I load_tensors: offloading output layer to GPU
0.00.068.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.011 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.012 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.298 I llama_init_from_model: n_seq_max     = 1
0.00.068.299 I llama_init_from_model: n_ctx         = 128
0.00.068.299 I llama_init_from_model: n_ctx_per_seq = 128
0.00.068.299 I llama_init_from_model: n_batch       = 128
0.00.068.299 I llama_init_from_model: n_ubatch      = 128
0.00.068.300 I llama_init_from_model: flash_attn    = 0
0.00.068.300 I llama_init_from_model: freq_base     = 10000.0
0.00.068.300 I llama_init_from_model: freq_scale    = 1
0.00.068.301 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.301 I ggml_metal_init: allocating
0.00.068.304 I ggml_metal_init: found device: Apple M4
0.00.068.306 I ggml_metal_init: picking default device: Apple M4
0.00.068.894 I ggml_metal_init: using embedded metal library
0.00.071.610 I ggml_metal_init: GPU name:   Apple M4
0.00.071.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.613 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.613 I ggml_metal_init: simdgroup reduction   = true
0.00.071.614 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.614 I ggml_metal_init: has bfloat            = true
0.00.071.614 I ggml_metal_init: use bfloat            = true
0.00.071.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.946 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.372 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.374 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.391 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.380 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.381 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.382 I llama_init_from_model: graph nodes  = 967
0.00.084.382 I llama_init_from_model: graph splits = 2
0.00.084.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.149 I 
0.00.935.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.189 I perplexity: tokenizing the input ..
0.00.943.054 I perplexity: tokenization took 7.863 ms
0.00.943.065 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.067.222 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.068.394 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.068.411 I llama_perf_context_print:        load time =     923.63 ms
0.01.068.412 I llama_perf_context_print: prompt eval time =     123.93 ms /   128 tokens (    0.97 ms per token,  1032.82 tokens per second)
0.01.068.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.068.413 I llama_perf_context_print:       total time =     133.26 ms /   129 tokens
0.01.068.892 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.094s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.084 I llama_model_loader: - type  f32:  194 tensors
0.00.027.085 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.085 I print_info: file format = GGUF V3 (latest)
0.00.027.086 I print_info: file type   = Q4_0
0.00.027.086 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.340 I load: special tokens cache size = 25
0.00.052.381 I load: token to piece cache size = 0.2984 MB
0.00.052.384 I print_info: arch             = gptneox
0.00.052.384 I print_info: vocab_only       = 0
0.00.052.384 I print_info: n_ctx_train      = 2048
0.00.052.384 I print_info: n_embd           = 2048
0.00.052.384 I print_info: n_layer          = 24
0.00.052.387 I print_info: n_head           = 16
0.00.052.388 I print_info: n_head_kv        = 16
0.00.052.388 I print_info: n_rot            = 32
0.00.052.391 I print_info: n_swa            = 0
0.00.052.391 I print_info: n_embd_head_k    = 128
0.00.052.391 I print_info: n_embd_head_v    = 128
0.00.052.392 I print_info: n_gqa            = 1
0.00.052.393 I print_info: n_embd_k_gqa     = 2048
0.00.052.399 I print_info: n_embd_v_gqa     = 2048
0.00.052.401 I print_info: f_norm_eps       = 1.0e-05
0.00.052.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.404 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.405 I print_info: f_logit_scale    = 0.0e+00
0.00.052.407 I print_info: n_ff             = 8192
0.00.052.407 I print_info: n_expert         = 0
0.00.052.407 I print_info: n_expert_used    = 0
0.00.052.407 I print_info: causal attn      = 1
0.00.052.408 I print_info: pooling type     = 0
0.00.052.408 I print_info: rope type        = 2
0.00.052.408 I print_info: rope scaling     = linear
0.00.052.408 I print_info: freq_base_train  = 10000.0
0.00.052.409 I print_info: freq_scale_train = 1
0.00.052.409 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.409 I print_info: rope_finetuned   = unknown
0.00.052.409 I print_info: ssm_d_conv       = 0
0.00.052.409 I print_info: ssm_d_inner      = 0
0.00.052.409 I print_info: ssm_d_state      = 0
0.00.052.410 I print_info: ssm_dt_rank      = 0
0.00.052.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.410 I print_info: model type       = 1.4B
0.00.052.410 I print_info: model params     = 1.41 B
0.00.052.410 I print_info: general.name     = 1.4B
0.00.052.411 I print_info: vocab type       = BPE
0.00.052.411 I print_info: n_vocab          = 50304
0.00.052.411 I print_info: n_merges         = 50009
0.00.052.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: LF token         = 128 'Ä'
0.00.052.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.413 I print_info: max token length = 1024
0.00.054.299 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.299 I load_tensors: offloading output layer to GPU
0.00.054.300 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.310 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.312 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.582 I llama_init_from_model: n_seq_max     = 1
0.00.054.582 I llama_init_from_model: n_ctx         = 128
0.00.054.583 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.583 I llama_init_from_model: n_batch       = 128
0.00.054.583 I llama_init_from_model: n_ubatch      = 128
0.00.054.583 I llama_init_from_model: flash_attn    = 0
0.00.054.583 I llama_init_from_model: freq_base     = 10000.0
0.00.054.584 I llama_init_from_model: freq_scale    = 1
0.00.054.584 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.584 I ggml_metal_init: allocating
0.00.054.587 I ggml_metal_init: found device: Apple M4
0.00.054.589 I ggml_metal_init: picking default device: Apple M4
0.00.055.056 I ggml_metal_init: using embedded metal library
0.00.057.487 I ggml_metal_init: GPU name:   Apple M4
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.490 I ggml_metal_init: simdgroup reduction   = true
0.00.057.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.490 I ggml_metal_init: has bfloat            = true
0.00.057.490 I ggml_metal_init: use bfloat            = true
0.00.057.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.491 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.048 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.333 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.276 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.277 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.277 I llama_init_from_model: graph nodes  = 967
0.00.069.277 I llama_init_from_model: graph splits = 2
0.00.069.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.297 I 
0.00.599.341 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.353 I perplexity: tokenizing the input ..
0.00.607.380 I perplexity: tokenization took 8.026 ms
0.00.607.391 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.982 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.731.106 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.731.119 I llama_perf_context_print:        load time =     588.47 ms
0.00.731.120 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.05 tokens per second)
0.00.731.122 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.123 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.731.593 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.971 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.977 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.071 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.043 I llama_model_loader: - type  f32:  194 tensors
0.00.025.043 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.044 I print_info: file format = GGUF V3 (latest)
0.00.025.044 I print_info: file type   = Q4_1
0.00.025.045 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.514 I load: special tokens cache size = 25
0.00.049.599 I load: token to piece cache size = 0.2984 MB
0.00.049.601 I print_info: arch             = gptneox
0.00.049.602 I print_info: vocab_only       = 0
0.00.049.602 I print_info: n_ctx_train      = 2048
0.00.049.602 I print_info: n_embd           = 2048
0.00.049.602 I print_info: n_layer          = 24
0.00.049.605 I print_info: n_head           = 16
0.00.049.606 I print_info: n_head_kv        = 16
0.00.049.606 I print_info: n_rot            = 32
0.00.049.606 I print_info: n_swa            = 0
0.00.049.607 I print_info: n_embd_head_k    = 128
0.00.049.607 I print_info: n_embd_head_v    = 128
0.00.049.607 I print_info: n_gqa            = 1
0.00.049.608 I print_info: n_embd_k_gqa     = 2048
0.00.049.609 I print_info: n_embd_v_gqa     = 2048
0.00.049.610 I print_info: f_norm_eps       = 1.0e-05
0.00.049.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.610 I print_info: f_logit_scale    = 0.0e+00
0.00.049.611 I print_info: n_ff             = 8192
0.00.049.611 I print_info: n_expert         = 0
0.00.049.611 I print_info: n_expert_used    = 0
0.00.049.611 I print_info: causal attn      = 1
0.00.049.612 I print_info: pooling type     = 0
0.00.049.612 I print_info: rope type        = 2
0.00.049.612 I print_info: rope scaling     = linear
0.00.049.614 I print_info: freq_base_train  = 10000.0
0.00.049.614 I print_info: freq_scale_train = 1
0.00.049.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.615 I print_info: rope_finetuned   = unknown
0.00.049.615 I print_info: ssm_d_conv       = 0
0.00.049.615 I print_info: ssm_d_inner      = 0
0.00.049.615 I print_info: ssm_d_state      = 0
0.00.049.615 I print_info: ssm_dt_rank      = 0
0.00.049.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.616 I print_info: model type       = 1.4B
0.00.049.616 I print_info: model params     = 1.41 B
0.00.049.616 I print_info: general.name     = 1.4B
0.00.049.617 I print_info: vocab type       = BPE
0.00.049.617 I print_info: n_vocab          = 50304
0.00.049.617 I print_info: n_merges         = 50009
0.00.049.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: LF token         = 128 'Ä'
0.00.049.618 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: max token length = 1024
0.00.051.512 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.512 I load_tensors: offloading output layer to GPU
0.00.051.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.523 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.524 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.795 I llama_init_from_model: n_seq_max     = 1
0.00.051.796 I llama_init_from_model: n_ctx         = 128
0.00.051.796 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.796 I llama_init_from_model: n_batch       = 128
0.00.051.797 I llama_init_from_model: n_ubatch      = 128
0.00.051.797 I llama_init_from_model: flash_attn    = 0
0.00.051.797 I llama_init_from_model: freq_base     = 10000.0
0.00.051.797 I llama_init_from_model: freq_scale    = 1
0.00.051.798 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.798 I ggml_metal_init: allocating
0.00.051.801 I ggml_metal_init: found device: Apple M4
0.00.051.803 I ggml_metal_init: picking default device: Apple M4
0.00.052.279 I ggml_metal_init: using embedded metal library
0.00.054.640 I ggml_metal_init: GPU name:   Apple M4
0.00.054.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.642 I ggml_metal_init: simdgroup reduction   = true
0.00.054.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.643 I ggml_metal_init: has bfloat            = true
0.00.054.643 I ggml_metal_init: use bfloat            = true
0.00.054.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.234 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.497 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.501 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.385 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.386 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.386 I llama_init_from_model: graph nodes  = 967
0.00.066.386 I llama_init_from_model: graph splits = 2
0.00.066.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.984 I 
0.00.686.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.031 I perplexity: tokenizing the input ..
0.00.694.103 I perplexity: tokenization took 8.07 ms
0.00.694.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.870 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.818.030 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.818.051 I llama_perf_context_print:        load time =     677.08 ms
0.00.818.052 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.68 tokens per second)
0.00.818.053 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.053 I llama_perf_context_print:       total time =     132.07 ms /   129 tokens
0.00.818.501 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.086 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.537 I llama_model_loader: - type  f32:  194 tensors
0.00.024.538 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.538 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.539 I print_info: file format = GGUF V3 (latest)
0.00.024.539 I print_info: file type   = Q5_0
0.00.024.540 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.058 I load: special tokens cache size = 25
0.00.049.163 I load: token to piece cache size = 0.2984 MB
0.00.049.166 I print_info: arch             = gptneox
0.00.049.166 I print_info: vocab_only       = 0
0.00.049.166 I print_info: n_ctx_train      = 2048
0.00.049.167 I print_info: n_embd           = 2048
0.00.049.167 I print_info: n_layer          = 24
0.00.049.170 I print_info: n_head           = 16
0.00.049.171 I print_info: n_head_kv        = 16
0.00.049.171 I print_info: n_rot            = 32
0.00.049.171 I print_info: n_swa            = 0
0.00.049.171 I print_info: n_embd_head_k    = 128
0.00.049.171 I print_info: n_embd_head_v    = 128
0.00.049.172 I print_info: n_gqa            = 1
0.00.049.173 I print_info: n_embd_k_gqa     = 2048
0.00.049.174 I print_info: n_embd_v_gqa     = 2048
0.00.049.174 I print_info: f_norm_eps       = 1.0e-05
0.00.049.175 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.175 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.175 I print_info: f_logit_scale    = 0.0e+00
0.00.049.176 I print_info: n_ff             = 8192
0.00.049.176 I print_info: n_expert         = 0
0.00.049.176 I print_info: n_expert_used    = 0
0.00.049.176 I print_info: causal attn      = 1
0.00.049.176 I print_info: pooling type     = 0
0.00.049.177 I print_info: rope type        = 2
0.00.049.177 I print_info: rope scaling     = linear
0.00.049.177 I print_info: freq_base_train  = 10000.0
0.00.049.178 I print_info: freq_scale_train = 1
0.00.049.178 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.178 I print_info: rope_finetuned   = unknown
0.00.049.178 I print_info: ssm_d_conv       = 0
0.00.049.178 I print_info: ssm_d_inner      = 0
0.00.049.179 I print_info: ssm_d_state      = 0
0.00.049.179 I print_info: ssm_dt_rank      = 0
0.00.049.179 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.179 I print_info: model type       = 1.4B
0.00.049.180 I print_info: model params     = 1.41 B
0.00.049.180 I print_info: general.name     = 1.4B
0.00.049.180 I print_info: vocab type       = BPE
0.00.049.180 I print_info: n_vocab          = 50304
0.00.049.181 I print_info: n_merges         = 50009
0.00.049.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.181 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.182 I print_info: LF token         = 128 'Ä'
0.00.049.182 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.182 I print_info: max token length = 1024
0.00.051.132 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.133 I load_tensors: offloading output layer to GPU
0.00.051.133 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.144 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.145 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.422 I llama_init_from_model: n_seq_max     = 1
0.00.051.423 I llama_init_from_model: n_ctx         = 128
0.00.051.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.423 I llama_init_from_model: n_batch       = 128
0.00.051.424 I llama_init_from_model: n_ubatch      = 128
0.00.051.424 I llama_init_from_model: flash_attn    = 0
0.00.051.424 I llama_init_from_model: freq_base     = 10000.0
0.00.051.424 I llama_init_from_model: freq_scale    = 1
0.00.051.425 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.425 I ggml_metal_init: allocating
0.00.051.428 I ggml_metal_init: found device: Apple M4
0.00.051.430 I ggml_metal_init: picking default device: Apple M4
0.00.051.907 I ggml_metal_init: using embedded metal library
0.00.054.220 I ggml_metal_init: GPU name:   Apple M4
0.00.054.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.222 I ggml_metal_init: simdgroup reduction   = true
0.00.054.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.222 I ggml_metal_init: has bfloat            = true
0.00.054.222 I ggml_metal_init: use bfloat            = true
0.00.054.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.748 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.037 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.926 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.927 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.927 I llama_init_from_model: graph nodes  = 967
0.00.065.927 I llama_init_from_model: graph splits = 2
0.00.065.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.525 I 
0.00.712.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.578 I perplexity: tokenizing the input ..
0.00.720.863 I perplexity: tokenization took 8.284 ms
0.00.720.874 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.236 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.857.426 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.857.440 I llama_perf_context_print:        load time =     703.61 ms
0.00.857.442 I llama_perf_context_print: prompt eval time =     135.13 ms /   128 tokens (    1.06 ms per token,   947.20 tokens per second)
0.00.857.443 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.443 I llama_perf_context_print:       total time =     144.92 ms /   129 tokens
0.00.857.932 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.077s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.208 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.987 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.893 I llama_model_loader: - type  f32:  194 tensors
0.00.025.893 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.893 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.894 I print_info: file format = GGUF V3 (latest)
0.00.025.894 I print_info: file type   = Q5_1
0.00.025.895 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.180 I load: special tokens cache size = 25
0.00.051.166 I load: token to piece cache size = 0.2984 MB
0.00.051.169 I print_info: arch             = gptneox
0.00.051.169 I print_info: vocab_only       = 0
0.00.051.170 I print_info: n_ctx_train      = 2048
0.00.051.170 I print_info: n_embd           = 2048
0.00.051.170 I print_info: n_layer          = 24
0.00.051.173 I print_info: n_head           = 16
0.00.051.174 I print_info: n_head_kv        = 16
0.00.051.174 I print_info: n_rot            = 32
0.00.051.174 I print_info: n_swa            = 0
0.00.051.174 I print_info: n_embd_head_k    = 128
0.00.051.174 I print_info: n_embd_head_v    = 128
0.00.051.175 I print_info: n_gqa            = 1
0.00.051.176 I print_info: n_embd_k_gqa     = 2048
0.00.051.179 I print_info: n_embd_v_gqa     = 2048
0.00.051.179 I print_info: f_norm_eps       = 1.0e-05
0.00.051.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.180 I print_info: f_logit_scale    = 0.0e+00
0.00.051.182 I print_info: n_ff             = 8192
0.00.051.182 I print_info: n_expert         = 0
0.00.051.182 I print_info: n_expert_used    = 0
0.00.051.182 I print_info: causal attn      = 1
0.00.051.183 I print_info: pooling type     = 0
0.00.051.184 I print_info: rope type        = 2
0.00.051.184 I print_info: rope scaling     = linear
0.00.051.185 I print_info: freq_base_train  = 10000.0
0.00.051.185 I print_info: freq_scale_train = 1
0.00.051.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.185 I print_info: rope_finetuned   = unknown
0.00.051.186 I print_info: ssm_d_conv       = 0
0.00.051.186 I print_info: ssm_d_inner      = 0
0.00.051.186 I print_info: ssm_d_state      = 0
0.00.051.186 I print_info: ssm_dt_rank      = 0
0.00.051.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.186 I print_info: model type       = 1.4B
0.00.051.187 I print_info: model params     = 1.41 B
0.00.051.187 I print_info: general.name     = 1.4B
0.00.051.191 I print_info: vocab type       = BPE
0.00.051.191 I print_info: n_vocab          = 50304
0.00.051.196 I print_info: n_merges         = 50009
0.00.051.196 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.196 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: LF token         = 128 'Ä'
0.00.051.197 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.198 I print_info: max token length = 1024
0.00.053.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.179 I load_tensors: offloading output layer to GPU
0.00.053.179 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.189 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.191 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.491 I llama_init_from_model: n_seq_max     = 1
0.00.053.492 I llama_init_from_model: n_ctx         = 128
0.00.053.492 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.492 I llama_init_from_model: n_batch       = 128
0.00.053.492 I llama_init_from_model: n_ubatch      = 128
0.00.053.492 I llama_init_from_model: flash_attn    = 0
0.00.053.492 I llama_init_from_model: freq_base     = 10000.0
0.00.053.493 I llama_init_from_model: freq_scale    = 1
0.00.053.493 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.493 I ggml_metal_init: allocating
0.00.053.496 I ggml_metal_init: found device: Apple M4
0.00.053.498 I ggml_metal_init: picking default device: Apple M4
0.00.053.978 I ggml_metal_init: using embedded metal library
0.00.056.346 I ggml_metal_init: GPU name:   Apple M4
0.00.056.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.349 I ggml_metal_init: simdgroup reduction   = true
0.00.056.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.349 I ggml_metal_init: has bfloat            = true
0.00.056.349 I ggml_metal_init: use bfloat            = true
0.00.056.350 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.913 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.143 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.148 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.163 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.003 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.004 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.004 I llama_init_from_model: graph nodes  = 967
0.00.068.004 I llama_init_from_model: graph splits = 2
0.00.068.006 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.028 I 
0.00.655.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.071 I perplexity: tokenizing the input ..
0.00.662.669 I perplexity: tokenization took 7.596 ms
0.00.662.679 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.426 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.798.605 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.798.622 I llama_perf_context_print:        load time =     644.81 ms
0.00.798.623 I llama_perf_context_print: prompt eval time =     134.52 ms /   128 tokens (    1.05 ms per token,   951.54 tokens per second)
0.00.798.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.624 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.799.160 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.027 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.505 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.353 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.200 I llama_model_loader: - type  f32:  194 tensors
0.00.024.200 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.200 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.200 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.201 I print_info: file format = GGUF V3 (latest)
0.00.024.201 I print_info: file type   = Q2_K - Medium
0.00.024.202 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.582 I load: special tokens cache size = 25
0.00.048.430 I load: token to piece cache size = 0.2984 MB
0.00.048.432 I print_info: arch             = gptneox
0.00.048.433 I print_info: vocab_only       = 0
0.00.048.433 I print_info: n_ctx_train      = 2048
0.00.048.433 I print_info: n_embd           = 2048
0.00.048.433 I print_info: n_layer          = 24
0.00.048.436 I print_info: n_head           = 16
0.00.048.437 I print_info: n_head_kv        = 16
0.00.048.437 I print_info: n_rot            = 32
0.00.048.437 I print_info: n_swa            = 0
0.00.048.437 I print_info: n_embd_head_k    = 128
0.00.048.438 I print_info: n_embd_head_v    = 128
0.00.048.438 I print_info: n_gqa            = 1
0.00.048.439 I print_info: n_embd_k_gqa     = 2048
0.00.048.440 I print_info: n_embd_v_gqa     = 2048
0.00.048.440 I print_info: f_norm_eps       = 1.0e-05
0.00.048.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.441 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.441 I print_info: f_logit_scale    = 0.0e+00
0.00.048.442 I print_info: n_ff             = 8192
0.00.048.442 I print_info: n_expert         = 0
0.00.048.442 I print_info: n_expert_used    = 0
0.00.048.442 I print_info: causal attn      = 1
0.00.048.443 I print_info: pooling type     = 0
0.00.048.443 I print_info: rope type        = 2
0.00.048.443 I print_info: rope scaling     = linear
0.00.048.443 I print_info: freq_base_train  = 10000.0
0.00.048.444 I print_info: freq_scale_train = 1
0.00.048.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.444 I print_info: rope_finetuned   = unknown
0.00.048.444 I print_info: ssm_d_conv       = 0
0.00.048.444 I print_info: ssm_d_inner      = 0
0.00.048.445 I print_info: ssm_d_state      = 0
0.00.048.445 I print_info: ssm_dt_rank      = 0
0.00.048.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.445 I print_info: model type       = 1.4B
0.00.048.446 I print_info: model params     = 1.41 B
0.00.048.446 I print_info: general.name     = 1.4B
0.00.048.446 I print_info: vocab type       = BPE
0.00.048.447 I print_info: n_vocab          = 50304
0.00.048.447 I print_info: n_merges         = 50009
0.00.048.447 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.447 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.448 I print_info: LF token         = 128 'Ä'
0.00.048.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.448 I print_info: max token length = 1024
0.00.050.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.261 I load_tensors: offloading output layer to GPU
0.00.050.261 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.272 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.273 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.050.540 I llama_init_from_model: n_seq_max     = 1
0.00.050.541 I llama_init_from_model: n_ctx         = 128
0.00.050.541 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.541 I llama_init_from_model: n_batch       = 128
0.00.050.541 I llama_init_from_model: n_ubatch      = 128
0.00.050.541 I llama_init_from_model: flash_attn    = 0
0.00.050.541 I llama_init_from_model: freq_base     = 10000.0
0.00.050.542 I llama_init_from_model: freq_scale    = 1
0.00.050.542 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.542 I ggml_metal_init: allocating
0.00.050.545 I ggml_metal_init: found device: Apple M4
0.00.050.547 I ggml_metal_init: picking default device: Apple M4
0.00.051.022 I ggml_metal_init: using embedded metal library
0.00.053.355 I ggml_metal_init: GPU name:   Apple M4
0.00.053.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.357 I ggml_metal_init: simdgroup reduction   = true
0.00.053.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.357 I ggml_metal_init: has bfloat            = true
0.00.053.358 I ggml_metal_init: use bfloat            = true
0.00.053.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.629 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.936 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.940 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.956 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.825 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.826 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.826 I llama_init_from_model: graph nodes  = 967
0.00.064.827 I llama_init_from_model: graph splits = 2
0.00.064.827 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.377.383 I 
0.00.377.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.377.442 I perplexity: tokenizing the input ..
0.00.385.030 I perplexity: tokenization took 7.587 ms
0.00.385.042 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.517.514 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.518.654 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.518.665 I llama_perf_context_print:        load time =     368.35 ms
0.00.518.666 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.89 tokens per second)
0.00.518.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.518.667 I llama_perf_context_print:       total time =     141.29 ms /   129 tokens
0.00.518.966 I ggml_metal_free: deallocating

real	0m0.534s
user	0m0.076s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.676 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.767 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.701 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.701 I llama_model_loader: - type  f32:  194 tensors
0.00.024.701 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.701 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.702 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.702 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.702 I print_info: file format = GGUF V3 (latest)
0.00.024.703 I print_info: file type   = Q3_K - Medium
0.00.024.704 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.161 I load: special tokens cache size = 25
0.00.049.194 I load: token to piece cache size = 0.2984 MB
0.00.049.197 I print_info: arch             = gptneox
0.00.049.197 I print_info: vocab_only       = 0
0.00.049.197 I print_info: n_ctx_train      = 2048
0.00.049.197 I print_info: n_embd           = 2048
0.00.049.198 I print_info: n_layer          = 24
0.00.049.200 I print_info: n_head           = 16
0.00.049.201 I print_info: n_head_kv        = 16
0.00.049.201 I print_info: n_rot            = 32
0.00.049.201 I print_info: n_swa            = 0
0.00.049.202 I print_info: n_embd_head_k    = 128
0.00.049.203 I print_info: n_embd_head_v    = 128
0.00.049.203 I print_info: n_gqa            = 1
0.00.049.204 I print_info: n_embd_k_gqa     = 2048
0.00.049.206 I print_info: n_embd_v_gqa     = 2048
0.00.049.207 I print_info: f_norm_eps       = 1.0e-05
0.00.049.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.208 I print_info: f_logit_scale    = 0.0e+00
0.00.049.209 I print_info: n_ff             = 8192
0.00.049.209 I print_info: n_expert         = 0
0.00.049.209 I print_info: n_expert_used    = 0
0.00.049.209 I print_info: causal attn      = 1
0.00.049.209 I print_info: pooling type     = 0
0.00.049.210 I print_info: rope type        = 2
0.00.049.210 I print_info: rope scaling     = linear
0.00.049.210 I print_info: freq_base_train  = 10000.0
0.00.049.211 I print_info: freq_scale_train = 1
0.00.049.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.211 I print_info: rope_finetuned   = unknown
0.00.049.211 I print_info: ssm_d_conv       = 0
0.00.049.212 I print_info: ssm_d_inner      = 0
0.00.049.212 I print_info: ssm_d_state      = 0
0.00.049.212 I print_info: ssm_dt_rank      = 0
0.00.049.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.212 I print_info: model type       = 1.4B
0.00.049.213 I print_info: model params     = 1.41 B
0.00.049.213 I print_info: general.name     = 1.4B
0.00.049.213 I print_info: vocab type       = BPE
0.00.049.214 I print_info: n_vocab          = 50304
0.00.049.214 I print_info: n_merges         = 50009
0.00.049.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.216 I print_info: LF token         = 128 'Ä'
0.00.049.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: max token length = 1024
0.00.051.054 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.054 I load_tensors: offloading output layer to GPU
0.00.051.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.065 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.066 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.344 I llama_init_from_model: n_seq_max     = 1
0.00.051.345 I llama_init_from_model: n_ctx         = 128
0.00.051.345 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.345 I llama_init_from_model: n_batch       = 128
0.00.051.346 I llama_init_from_model: n_ubatch      = 128
0.00.051.346 I llama_init_from_model: flash_attn    = 0
0.00.051.346 I llama_init_from_model: freq_base     = 10000.0
0.00.051.346 I llama_init_from_model: freq_scale    = 1
0.00.051.347 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.347 I ggml_metal_init: allocating
0.00.051.350 I ggml_metal_init: found device: Apple M4
0.00.051.352 I ggml_metal_init: picking default device: Apple M4
0.00.051.819 I ggml_metal_init: using embedded metal library
0.00.054.176 I ggml_metal_init: GPU name:   Apple M4
0.00.054.178 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.179 I ggml_metal_init: simdgroup reduction   = true
0.00.054.179 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.179 I ggml_metal_init: has bfloat            = true
0.00.054.179 I ggml_metal_init: use bfloat            = true
0.00.054.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.729 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.987 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.989 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.014 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.913 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.914 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.914 I llama_init_from_model: graph nodes  = 967
0.00.065.914 I llama_init_from_model: graph splits = 2
0.00.065.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.904 I 
0.00.473.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.956 I perplexity: tokenizing the input ..
0.00.481.650 I perplexity: tokenization took 7.692 ms
0.00.481.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.769 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.986 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.002 I llama_perf_context_print:        load time =     465.12 ms
0.00.615.003 I llama_perf_context_print: prompt eval time =     131.88 ms /   128 tokens (    1.03 ms per token,   970.58 tokens per second)
0.00.615.004 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.004 I llama_perf_context_print:       total time =     141.10 ms /   129 tokens
0.00.615.518 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.077s
sys	0m0.082s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.466 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.467 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.316 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.317 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.318 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.318 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.318 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.319 I print_info: file format = GGUF V3 (latest)
0.00.025.320 I print_info: file type   = Q4_K - Medium
0.00.025.320 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.653 I load: special tokens cache size = 25
0.00.049.641 I load: token to piece cache size = 0.2984 MB
0.00.049.644 I print_info: arch             = gptneox
0.00.049.644 I print_info: vocab_only       = 0
0.00.049.645 I print_info: n_ctx_train      = 2048
0.00.049.645 I print_info: n_embd           = 2048
0.00.049.645 I print_info: n_layer          = 24
0.00.049.649 I print_info: n_head           = 16
0.00.049.649 I print_info: n_head_kv        = 16
0.00.049.649 I print_info: n_rot            = 32
0.00.049.650 I print_info: n_swa            = 0
0.00.049.650 I print_info: n_embd_head_k    = 128
0.00.049.650 I print_info: n_embd_head_v    = 128
0.00.049.653 I print_info: n_gqa            = 1
0.00.049.654 I print_info: n_embd_k_gqa     = 2048
0.00.049.654 I print_info: n_embd_v_gqa     = 2048
0.00.049.655 I print_info: f_norm_eps       = 1.0e-05
0.00.049.655 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.656 I print_info: f_logit_scale    = 0.0e+00
0.00.049.657 I print_info: n_ff             = 8192
0.00.049.657 I print_info: n_expert         = 0
0.00.049.657 I print_info: n_expert_used    = 0
0.00.049.657 I print_info: causal attn      = 1
0.00.049.657 I print_info: pooling type     = 0
0.00.049.657 I print_info: rope type        = 2
0.00.049.659 I print_info: rope scaling     = linear
0.00.049.661 I print_info: freq_base_train  = 10000.0
0.00.049.661 I print_info: freq_scale_train = 1
0.00.049.661 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.661 I print_info: rope_finetuned   = unknown
0.00.049.661 I print_info: ssm_d_conv       = 0
0.00.049.662 I print_info: ssm_d_inner      = 0
0.00.049.662 I print_info: ssm_d_state      = 0
0.00.049.662 I print_info: ssm_dt_rank      = 0
0.00.049.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.662 I print_info: model type       = 1.4B
0.00.049.663 I print_info: model params     = 1.41 B
0.00.049.663 I print_info: general.name     = 1.4B
0.00.049.664 I print_info: vocab type       = BPE
0.00.049.664 I print_info: n_vocab          = 50304
0.00.049.664 I print_info: n_merges         = 50009
0.00.049.665 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: LF token         = 128 'Ä'
0.00.049.666 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.666 I print_info: max token length = 1024
0.00.051.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.617 I load_tensors: offloading output layer to GPU
0.00.051.617 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.628 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.629 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.897 I llama_init_from_model: n_seq_max     = 1
0.00.051.898 I llama_init_from_model: n_ctx         = 128
0.00.051.898 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.898 I llama_init_from_model: n_batch       = 128
0.00.051.898 I llama_init_from_model: n_ubatch      = 128
0.00.051.898 I llama_init_from_model: flash_attn    = 0
0.00.051.899 I llama_init_from_model: freq_base     = 10000.0
0.00.051.899 I llama_init_from_model: freq_scale    = 1
0.00.051.899 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.900 I ggml_metal_init: allocating
0.00.051.903 I ggml_metal_init: found device: Apple M4
0.00.051.905 I ggml_metal_init: picking default device: Apple M4
0.00.052.377 I ggml_metal_init: using embedded metal library
0.00.054.712 I ggml_metal_init: GPU name:   Apple M4
0.00.054.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.714 I ggml_metal_init: simdgroup reduction   = true
0.00.054.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.714 I ggml_metal_init: has bfloat            = true
0.00.054.714 I ggml_metal_init: use bfloat            = true
0.00.054.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.260 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.473 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.476 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.489 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.398 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.399 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.399 I llama_init_from_model: graph nodes  = 967
0.00.066.400 I llama_init_from_model: graph splits = 2
0.00.066.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.732 I 
0.00.536.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.773 I perplexity: tokenizing the input ..
0.00.544.547 I perplexity: tokenization took 7.772 ms
0.00.544.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.679.003 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.680.161 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.680.177 I llama_perf_context_print:        load time =     526.73 ms
0.00.680.179 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.69 tokens per second)
0.00.680.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.680.180 I llama_perf_context_print:       total time =     143.45 ms /   129 tokens
0.00.680.532 I ggml_metal_free: deallocating

real	0m0.695s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.720 I llama_model_loader: - type  f32:  194 tensors
0.00.024.720 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.720 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.721 I print_info: file format = GGUF V3 (latest)
0.00.024.721 I print_info: file type   = Q5_K - Medium
0.00.024.722 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.670 I load: special tokens cache size = 25
0.00.049.778 I load: token to piece cache size = 0.2984 MB
0.00.049.781 I print_info: arch             = gptneox
0.00.049.782 I print_info: vocab_only       = 0
0.00.049.782 I print_info: n_ctx_train      = 2048
0.00.049.782 I print_info: n_embd           = 2048
0.00.049.782 I print_info: n_layer          = 24
0.00.049.785 I print_info: n_head           = 16
0.00.049.786 I print_info: n_head_kv        = 16
0.00.049.786 I print_info: n_rot            = 32
0.00.049.786 I print_info: n_swa            = 0
0.00.049.786 I print_info: n_embd_head_k    = 128
0.00.049.786 I print_info: n_embd_head_v    = 128
0.00.049.787 I print_info: n_gqa            = 1
0.00.049.788 I print_info: n_embd_k_gqa     = 2048
0.00.049.790 I print_info: n_embd_v_gqa     = 2048
0.00.049.791 I print_info: f_norm_eps       = 1.0e-05
0.00.049.791 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.791 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.792 I print_info: f_logit_scale    = 0.0e+00
0.00.049.792 I print_info: n_ff             = 8192
0.00.049.794 I print_info: n_expert         = 0
0.00.049.794 I print_info: n_expert_used    = 0
0.00.049.794 I print_info: causal attn      = 1
0.00.049.794 I print_info: pooling type     = 0
0.00.049.794 I print_info: rope type        = 2
0.00.049.795 I print_info: rope scaling     = linear
0.00.049.795 I print_info: freq_base_train  = 10000.0
0.00.049.795 I print_info: freq_scale_train = 1
0.00.049.795 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.796 I print_info: rope_finetuned   = unknown
0.00.049.796 I print_info: ssm_d_conv       = 0
0.00.049.796 I print_info: ssm_d_inner      = 0
0.00.049.796 I print_info: ssm_d_state      = 0
0.00.049.796 I print_info: ssm_dt_rank      = 0
0.00.049.796 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.796 I print_info: model type       = 1.4B
0.00.049.797 I print_info: model params     = 1.41 B
0.00.049.797 I print_info: general.name     = 1.4B
0.00.049.798 I print_info: vocab type       = BPE
0.00.049.803 I print_info: n_vocab          = 50304
0.00.049.803 I print_info: n_merges         = 50009
0.00.049.803 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.805 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.805 I print_info: LF token         = 128 'Ä'
0.00.049.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.805 I print_info: max token length = 1024
0.00.051.805 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.805 I load_tensors: offloading output layer to GPU
0.00.051.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.816 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.818 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.094 I llama_init_from_model: n_seq_max     = 1
0.00.052.094 I llama_init_from_model: n_ctx         = 128
0.00.052.095 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.095 I llama_init_from_model: n_batch       = 128
0.00.052.095 I llama_init_from_model: n_ubatch      = 128
0.00.052.095 I llama_init_from_model: flash_attn    = 0
0.00.052.095 I llama_init_from_model: freq_base     = 10000.0
0.00.052.095 I llama_init_from_model: freq_scale    = 1
0.00.052.096 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.096 I ggml_metal_init: allocating
0.00.052.099 I ggml_metal_init: found device: Apple M4
0.00.052.101 I ggml_metal_init: picking default device: Apple M4
0.00.052.582 I ggml_metal_init: using embedded metal library
0.00.054.903 I ggml_metal_init: GPU name:   Apple M4
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.906 I ggml_metal_init: simdgroup reduction   = true
0.00.054.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.906 I ggml_metal_init: has bfloat            = true
0.00.054.906 I ggml_metal_init: use bfloat            = true
0.00.054.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.751 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.756 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.780 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.640 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.641 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.641 I llama_init_from_model: graph nodes  = 967
0.00.066.641 I llama_init_from_model: graph splits = 2
0.00.066.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.543 I 
0.00.637.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.588 I perplexity: tokenizing the input ..
0.00.645.597 I perplexity: tokenization took 8.008 ms
0.00.645.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.261 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.684 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.697 I llama_perf_context_print:        load time =     628.55 ms
0.00.786.698 I llama_perf_context_print: prompt eval time =     139.41 ms /   128 tokens (    1.09 ms per token,   918.15 tokens per second)
0.00.786.699 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.699 I llama_perf_context_print:       total time =     149.15 ms /   129 tokens
0.00.787.046 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.694 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.386 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.667 I llama_model_loader: - type  f32:  194 tensors
0.00.024.667 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.668 I print_info: file format = GGUF V3 (latest)
0.00.024.668 I print_info: file type   = Q6_K
0.00.024.669 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.444 I load: special tokens cache size = 25
0.00.050.522 I load: token to piece cache size = 0.2984 MB
0.00.050.526 I print_info: arch             = gptneox
0.00.050.526 I print_info: vocab_only       = 0
0.00.050.526 I print_info: n_ctx_train      = 2048
0.00.050.527 I print_info: n_embd           = 2048
0.00.050.527 I print_info: n_layer          = 24
0.00.050.532 I print_info: n_head           = 16
0.00.050.533 I print_info: n_head_kv        = 16
0.00.050.534 I print_info: n_rot            = 32
0.00.050.534 I print_info: n_swa            = 0
0.00.050.536 I print_info: n_embd_head_k    = 128
0.00.050.536 I print_info: n_embd_head_v    = 128
0.00.050.537 I print_info: n_gqa            = 1
0.00.050.537 I print_info: n_embd_k_gqa     = 2048
0.00.050.538 I print_info: n_embd_v_gqa     = 2048
0.00.050.538 I print_info: f_norm_eps       = 1.0e-05
0.00.050.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.539 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.539 I print_info: f_logit_scale    = 0.0e+00
0.00.050.540 I print_info: n_ff             = 8192
0.00.050.542 I print_info: n_expert         = 0
0.00.050.543 I print_info: n_expert_used    = 0
0.00.050.543 I print_info: causal attn      = 1
0.00.050.543 I print_info: pooling type     = 0
0.00.050.543 I print_info: rope type        = 2
0.00.050.543 I print_info: rope scaling     = linear
0.00.050.544 I print_info: freq_base_train  = 10000.0
0.00.050.544 I print_info: freq_scale_train = 1
0.00.050.544 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.544 I print_info: rope_finetuned   = unknown
0.00.050.544 I print_info: ssm_d_conv       = 0
0.00.050.545 I print_info: ssm_d_inner      = 0
0.00.050.545 I print_info: ssm_d_state      = 0
0.00.050.545 I print_info: ssm_dt_rank      = 0
0.00.050.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.545 I print_info: model type       = 1.4B
0.00.050.545 I print_info: model params     = 1.41 B
0.00.050.546 I print_info: general.name     = 1.4B
0.00.050.546 I print_info: vocab type       = BPE
0.00.050.546 I print_info: n_vocab          = 50304
0.00.050.546 I print_info: n_merges         = 50009
0.00.050.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.547 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.547 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.547 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.547 I print_info: LF token         = 128 'Ä'
0.00.050.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.548 I print_info: max token length = 1024
0.00.052.656 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.656 I load_tensors: offloading output layer to GPU
0.00.052.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.667 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.668 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.944 I llama_init_from_model: n_seq_max     = 1
0.00.052.944 I llama_init_from_model: n_ctx         = 128
0.00.052.945 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.945 I llama_init_from_model: n_batch       = 128
0.00.052.945 I llama_init_from_model: n_ubatch      = 128
0.00.052.945 I llama_init_from_model: flash_attn    = 0
0.00.052.946 I llama_init_from_model: freq_base     = 10000.0
0.00.052.946 I llama_init_from_model: freq_scale    = 1
0.00.052.946 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.947 I ggml_metal_init: allocating
0.00.052.950 I ggml_metal_init: found device: Apple M4
0.00.052.952 I ggml_metal_init: picking default device: Apple M4
0.00.053.475 I ggml_metal_init: using embedded metal library
0.00.055.857 I ggml_metal_init: GPU name:   Apple M4
0.00.055.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.860 I ggml_metal_init: simdgroup reduction   = true
0.00.055.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.860 I ggml_metal_init: has bfloat            = true
0.00.055.861 I ggml_metal_init: use bfloat            = true
0.00.055.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.453 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.470 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.340 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.341 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.342 I llama_init_from_model: graph nodes  = 967
0.00.068.342 I llama_init_from_model: graph splits = 2
0.00.068.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.797 I 
0.00.638.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.838 I perplexity: tokenizing the input ..
0.00.647.170 I perplexity: tokenization took 8.33 ms
0.00.647.182 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.823 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.788.057 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.788.074 I llama_perf_context_print:        load time =     630.10 ms
0.00.788.075 I llama_perf_context_print: prompt eval time =     139.42 ms /   128 tokens (    1.09 ms per token,   918.12 tokens per second)
0.00.788.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.076 I llama_perf_context_print:       total time =     149.28 ms /   129 tokens
0.00.788.420 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.080s
sys	0m0.107s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.310 I build: 4557 (f35726c2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.838 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.388 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.404 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.405 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.406 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.180 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.194 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.602 I llama_model_loader: - type  f32:  194 tensors
0.00.053.602 I llama_model_loader: - type  f16:   98 tensors
0.00.053.603 I print_info: file format = GGUF V3 (latest)
0.00.053.604 I print_info: file type   = all F32 (guessed)
0.00.053.605 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.318 I load: special tokens cache size = 25
0.00.085.953 I load: token to piece cache size = 0.2984 MB
0.00.085.956 I print_info: arch             = gptneox
0.00.085.957 I print_info: vocab_only       = 0
0.00.085.957 I print_info: n_ctx_train      = 2048
0.00.085.957 I print_info: n_embd           = 2048
0.00.085.957 I print_info: n_layer          = 24
0.00.085.960 I print_info: n_head           = 16
0.00.085.961 I print_info: n_head_kv        = 16
0.00.085.961 I print_info: n_rot            = 32
0.00.085.961 I print_info: n_swa            = 0
0.00.085.961 I print_info: n_embd_head_k    = 128
0.00.085.963 I print_info: n_embd_head_v    = 128
0.00.085.964 I print_info: n_gqa            = 1
0.00.085.965 I print_info: n_embd_k_gqa     = 2048
0.00.085.965 I print_info: n_embd_v_gqa     = 2048
0.00.085.966 I print_info: f_norm_eps       = 1.0e-05
0.00.085.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.967 I print_info: f_logit_scale    = 0.0e+00
0.00.085.968 I print_info: n_ff             = 8192
0.00.085.968 I print_info: n_expert         = 0
0.00.085.968 I print_info: n_expert_used    = 0
0.00.085.968 I print_info: causal attn      = 1
0.00.085.970 I print_info: pooling type     = 0
0.00.085.970 I print_info: rope type        = 2
0.00.085.970 I print_info: rope scaling     = linear
0.00.085.970 I print_info: freq_base_train  = 10000.0
0.00.085.971 I print_info: freq_scale_train = 1
0.00.085.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.971 I print_info: rope_finetuned   = unknown
0.00.085.971 I print_info: ssm_d_conv       = 0
0.00.085.971 I print_info: ssm_d_inner      = 0
0.00.085.971 I print_info: ssm_d_state      = 0
0.00.085.971 I print_info: ssm_dt_rank      = 0
0.00.085.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.972 I print_info: model type       = 1.4B
0.00.085.972 I print_info: model params     = 1.41 B
0.00.085.972 I print_info: general.name     = 1.4B
0.00.085.973 I print_info: vocab type       = BPE
0.00.085.973 I print_info: n_vocab          = 50304
0.00.085.976 I print_info: n_merges         = 50009
0.00.085.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.978 I print_info: LF token         = 128 'Ä'
0.00.085.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.978 I print_info: max token length = 1024
0.00.088.577 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.577 I load_tensors: offloading output layer to GPU
0.00.088.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.588 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.590 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.920 I llama_init_from_model: n_seq_max     = 1
0.00.088.921 I llama_init_from_model: n_ctx         = 128
0.00.088.921 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.921 I llama_init_from_model: n_batch       = 128
0.00.088.921 I llama_init_from_model: n_ubatch      = 128
0.00.088.922 I llama_init_from_model: flash_attn    = 0
0.00.088.922 I llama_init_from_model: freq_base     = 10000.0
0.00.088.922 I llama_init_from_model: freq_scale    = 1
0.00.088.923 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.923 I ggml_metal_init: allocating
0.00.088.926 I ggml_metal_init: found device: Apple M4
0.00.088.928 I ggml_metal_init: picking default device: Apple M4
0.00.089.442 I ggml_metal_init: using embedded metal library
0.00.091.978 I ggml_metal_init: GPU name:   Apple M4
0.00.091.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.981 I ggml_metal_init: simdgroup reduction   = true
0.00.091.981 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.981 I ggml_metal_init: has bfloat            = true
0.00.091.981 I ggml_metal_init: use bfloat            = true
0.00.091.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.470 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.473 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.426 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.427 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.428 I llama_init_from_model: graph nodes  = 967
0.00.103.428 I llama_init_from_model: graph splits = 2
0.00.103.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.430 I 
0.00.103.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.464 I compute_imatrix: tokenizing the input ..
0.00.110.062 I compute_imatrix: tokenization took 6.598 ms
0.00.110.064 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.628.200 I compute_imatrix: 1.52 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.630.913 I llama_perf_context_print:        load time =    1606.36 ms
0.01.630.914 I llama_perf_context_print: prompt eval time =    1517.51 ms /   128 tokens (   11.86 ms per token,    84.35 tokens per second)
0.01.630.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.630.916 I llama_perf_context_print:       total time =    1609.07 ms /   129 tokens
0.01.631.497 I ggml_metal_free: deallocating

real	0m1.820s
user	0m0.169s
sys	0m0.229s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4557 (f35726c2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c50a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c50a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c50ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c50b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c50b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c50beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c50c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c50ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c50cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c50d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c50d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c50dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c50e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c50f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c50f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c5100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c5107e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c510f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c511620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c511df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c512510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c512c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c513350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c513bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c514310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c5145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c514be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c515850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c515d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c516050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c5164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c5167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c517040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c517580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c517840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c517ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c518180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c518620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c518ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c518f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c519400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c5198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c519d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c51a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c51a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c51aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c51b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c51b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c51bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c51c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c51cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c51d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c51d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c51de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c51e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c51ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c51ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c51f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c51f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c520030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c5202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c520790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c520c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c5210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c521570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c521a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c521eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c522350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c5227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c522c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c523130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c5235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c523a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c523fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c524510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c524a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c524fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c525500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c525a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c525fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c5264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c526a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c526f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c5274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c527a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c527f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c5284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c528a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c528f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c5294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c529a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c529f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c52a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c52aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c52af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c52b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c52b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c51b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c52be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c52c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c52cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c52d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c52d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c52db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c52e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c52e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c52eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c52f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c52f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c52fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c530080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c5305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c530b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c530fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c531460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c531900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c531da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c532240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c5326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c532b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c533020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c5334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c533960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c533e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c5342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c534740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c534be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c535080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c535520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c5359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c535e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c536300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c5367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c536c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c5370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c537580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c537a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c537ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c538360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c538800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c538ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c539140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c5395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c539a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c539f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c53a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c53a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c53ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c53b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c53b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c53bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c53bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c53c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c53c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c53cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c53d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c53d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c53db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c53dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c53e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c53e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c53edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c53f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c53f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c53fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c540040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c5404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c540980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c540e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c5412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c541760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c541c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c5420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c542540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c5429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c542e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c543320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c5437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c543c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c544100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c5445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c544a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c544ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c545380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c545820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c545cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c546160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c546600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c546aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c546f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c5473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c547880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c547d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c548270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c5487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c548d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c549260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c549520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c549b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c54a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c54a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c54af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c54b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c54b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c54bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c54c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c54cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c54cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c54d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c54d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c54e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c54e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c54eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c54f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c54f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c54fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c550020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c550570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c550ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c551010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c551560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c551ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c552000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c552550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c552aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c552ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c553540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c553a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c553fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c554530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c554a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c554fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c555520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c555a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c555fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c556510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c556a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c556fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c557500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c557a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c557fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c5584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c558a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c558f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c5594e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c559a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c559f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c55a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c55aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c55af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c55b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c55ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c55bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c55c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c55ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c55cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c55d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c55d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c55df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c55e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c55e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c55ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c55f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c55f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c55ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c560470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c5609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c560e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c561300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c5617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c561c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c5620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c562580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c562a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c562ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c563360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c563800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c563ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c564140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c5645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c564a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c564f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c565470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c565b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c5662b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c5669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c5670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c5673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c567ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c567e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c568470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.150.102 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.150.105 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c306dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c305220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c309060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c3094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c309940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c309db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c30a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c30a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c30ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c30af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c30b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c30bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c30c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c30c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c30c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c30ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c30d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c30d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c30db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c30dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c30e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c30e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c30ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c30f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c30f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c30fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c30fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c310340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c3107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c310c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c311090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c311500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c311970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c311de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c312250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c3126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c312b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c312fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c313410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c313880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c313cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c314160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c3145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c314a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c314eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c315320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c315790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c315c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c316070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c3164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c316950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c316dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c317230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c3176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c317b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c317f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c3183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c318860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c318cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c319140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c3195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c319a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c319e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c31a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c31a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c31abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c31b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c31b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c31b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c31bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c31c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c31c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c31caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c31cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c31d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c31d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c31dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c31e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c31e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c31ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c31ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c31f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c31f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c31fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c320630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c320d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c321470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c321b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c321e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c3222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c3228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c322ed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c31fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c322b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c308d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c30b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c323190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c323450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c323710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c3239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c323d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c324030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c3242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c3245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c324b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c325150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c325780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c325cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c326200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c326740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c326c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c327450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c327990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c327ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c328410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c328950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c328e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c3293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c329690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c329950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c329c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c329ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c32a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c32a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c32a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c32a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c32ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c32af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c32b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c32b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c32b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c32ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c32bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c32bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c32c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c32c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c32c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c32cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c32cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c32d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c32d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c32d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c32d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c32db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c32de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c32e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c32e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c32e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c32e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c32ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c32ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c32f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c32f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c32f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c32f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c32fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c32ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c3301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c330490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c330750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c330a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c330cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c330f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c331250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c331510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c3317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c331a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c331d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c332010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c3322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c332590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c332850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c332b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c332dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c333090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c3335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c333b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c3345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c335070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c3355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c335b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c336060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c3365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c336b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c337050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c3375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c337af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c338040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c338590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c338ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c339030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c339580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c339ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c33a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c33a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c33aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c33b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c33b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c33bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c33c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c33c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c33caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c33cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c33d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c33d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c33d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c33dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c33e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c33e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c33eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c33efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c33f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c33f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c33fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c340160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c3405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c340a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c340eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c341320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c341790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c341c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c342070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c3424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c342950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c342dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c343230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c3436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c343b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c343f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c3443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c344860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c344cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c345140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c3455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c345a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c345e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c346300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c346770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c346be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c347050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c3474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c347930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c347da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c348210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c348680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c348af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c348f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c3493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c349840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c349cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c34a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c34a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c34aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c34ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c34b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c34b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c34bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c34c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c34c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c34c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c34cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c34d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c34d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c34dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c34df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c34e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c34e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c34ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c34f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c34f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c34f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c34fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c3502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c350730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c350ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c351010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c351480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c3518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c351d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c3521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c352640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c352ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c352f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c353390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c353800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c353c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c3540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c354550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c3549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c354e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c3552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c355710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c355cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c3561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c356640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c356ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c356f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c357390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c3578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c357dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c358930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c358bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c3591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c359770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c359d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c35a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c35a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c35ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c35b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c35b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c35bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c35c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c35cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c35d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c35d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c35dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c35e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c35e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c35edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c35f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c35f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c35fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c3604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c360a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c361030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c3615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c361bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c362170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c362730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c362cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c3632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c363870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c363e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c3643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c3649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c364f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c365530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c365af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c3660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c366670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c366c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c3671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c3677b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c367d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c368330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c3688f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c368eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c369470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c369a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c369ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c36a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c36ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c36b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c36b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c36bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c36c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c36c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c36cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c36d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c36d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c36dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c36e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c36e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c36ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c36f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c36f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c36faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c36fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c3704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c3709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c370ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c3713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c3718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c372300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c372a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c373140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c373860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c373b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c374310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c3745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c374be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.807s
user	0m0.296s
sys	0m0.304s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4557 (f35726c2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f70cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f70d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f70dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f70e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f70e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f70ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f70f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f70f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f70fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f710300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f710800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f710d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f711820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f711fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f7127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f713620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f713d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f714460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f714c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f715350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f715a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f716190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f716a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f717150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f717410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f718690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f718bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f718e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f719330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f7195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f719e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f71a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f71a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f71ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f71afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f71b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f71b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f71bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f71c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f71c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f71cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f71d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f71d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f71d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f71df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f71e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f71ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f71f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f71fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f721470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f721910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f721db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f722070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f722e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f723130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f7235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f723a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f723f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f7243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f724850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f724cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f725190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f725ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f725f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f726410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f7268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f726e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f727350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f7278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f727df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f728340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f728de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f729330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f729880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f729dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f72a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f72a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f72adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f72b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f72b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f72bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f72c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f72c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f72cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f72d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f72d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f72dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f72e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f72e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f71e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f72eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f72f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f72f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f72fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f730440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f730990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f730ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f731430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f731980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f732420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f732970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f733410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f733e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f7342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f734740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f734be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f735080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f735520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f7359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f735e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f7367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f736c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f7370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f737580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f737a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f738360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f738800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f738ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f739140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f7395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f739a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f739f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f73a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f73a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f73ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f73b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f73b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f73bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f73bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f73c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f73c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f73cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f73d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f73d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f73db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f73dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f73e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f73e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f73edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f73f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f73f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f73fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f7404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f740980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f7412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f741760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f741c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f7420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f7429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f742e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f743320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f7437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f743c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f744100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f7445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f744a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f744ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f745380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f745820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f745cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f746160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f746600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f746aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f746f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f7473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f747880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f747d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f7481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f748660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f748fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f749440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f7498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f749d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f74a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f74a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f74ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f74b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f74b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f74bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f74c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f74c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f74c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f74cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f74d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f74dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f74e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f74e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f74eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f74f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f74f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f74fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f750230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f7506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f750e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f7513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f751920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f751e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f7523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f752910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f752e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f7533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f753900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f753e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f7543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f7548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f754e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f755390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f7558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f755e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f756380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f7568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f756e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f757370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f7578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f757e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f758360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f7588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f758e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f759350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f7598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f759df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f75a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f75a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f75ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f75b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f75b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f75bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f75c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f75c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f75cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f75d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f75d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f75ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f75e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f75e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f75eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f75f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f75f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f75fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f7602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f760830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f760d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f7612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f761820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f7622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f762810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f762d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f7632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f763800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f763ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f764140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f7645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f764a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f764f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f7653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f765860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f765d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f7661a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f766640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f766ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f766f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f767420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f7678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f767d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f7682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f7689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f7690f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f769810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f769f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f76a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f76a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f76aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f76b2b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.461 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f608d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f6091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f609650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f609ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f60a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f60a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f60add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f60b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f60b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f60bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f60c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f60c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f60cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f60d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f60dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f60e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f60ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f60f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f60f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f6103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f610ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f6111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f611900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f612020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f612740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f612a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f613010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f614420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f6148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f614b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f615410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f615c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f6160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f616550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f6169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f616e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f617330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f617c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f618110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f6185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f618e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f619490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f61a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f61a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f61acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f61b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f61b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f61bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f61c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f61cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f61d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f61d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f61e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f61e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f61ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f61eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f61f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f6205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f6213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f621d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f622260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f6227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f622d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f623250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f6237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f623cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f625230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f625780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f625cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f626220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f626770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f626cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f627210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f627760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f627cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f628200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f628750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f628ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f6291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f629740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f629c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f62a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f62a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f62ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f62b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f62b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f62bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f62c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f62c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f62cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f62d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f62d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f62dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f62e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f62e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f62ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f62f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f62f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f62fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f62ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f6308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f6311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f631690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f631b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f631fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f632470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f632910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f632db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f633250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f6336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f633b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f634030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f6344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f634970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f6352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f635bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f636530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f6369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f637310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f6377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f6380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f638a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f638ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f639810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f639cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f63a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f63a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f63aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f63af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f63b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f63b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f63bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f63c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f63c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f63caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f63cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f63d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f63d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f63dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f63e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f63e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f63eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f63eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f63f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f63f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f63fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f640270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f640710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f640bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f641050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f6414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f641e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f6422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f642c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f6430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f6439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f644330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f6447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f644c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f645110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f6455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f645a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f645ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f646390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f6468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f646e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f647380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f6478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f647b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f6481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f6487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f648dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f6495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f649a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f649d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f64a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f64a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f64b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f64b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f64ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f64c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f64cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f64d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f64d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f64dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f64e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f64e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f64ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f64f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f64f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f64fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f650120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f650670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f650bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f651660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f651bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f652100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f652650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f652ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f6530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f653640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f653b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f6540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f654630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f6550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f655620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f655b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f6560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f656610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f656b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f6570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f657600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f657b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f6580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f6585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f658b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f659090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f6595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f659b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f65a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f65a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f65ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f65b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f65b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f65bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f65c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f65c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f65cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f65d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f65d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f65daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f65e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f65e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f65eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f65f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f65f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f65f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f65fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f6602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f660750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f660bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f661090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f661530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f6619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f661e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f662310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f6627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f6630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f663590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f663ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f664200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f664920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f665040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f665760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f666210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f6664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f666ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f76af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f74e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f74c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f74d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f71fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f722330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f74edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f7176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f71e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f71eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f71f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f71f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f7166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f72ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f76a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f7198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f719b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f74f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f74d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f717fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f718260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f76b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f76b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f76bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f76bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f76c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f76c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f76c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f76ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f76cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f76cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f76d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f76d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f76d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f76dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f76dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f76e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f76e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f76e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f76e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f76eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f76ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f76f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f76f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f76f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f76f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f76fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f76fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f770150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f770410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f7706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f770990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f770c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f770f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f7711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f771490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f771750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f771a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f771cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f771f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f772250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f772510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f7727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f772a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f772d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f773010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f7732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f773590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f773850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f773b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f773dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f774090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f774350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f774610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f7748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f774b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f774e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f775110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f7753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f775690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f775950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f775c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f775ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f776190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f776450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f776710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f7769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f776c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f776f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f777210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f7774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f777790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f777a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f777d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f777fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f778290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f778550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f778810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f778ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f778d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f779050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f779310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f7795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f779890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f779b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f779e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f77a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f77a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f77a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f77a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f77abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f77ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f77b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f77b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f77b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f77b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f77bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f77bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f77c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f77c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f77c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f77ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f77ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f77cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f77d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f77d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f77d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f77da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f77dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f77e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f77e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f77e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f77e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f77eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f77edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f77f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f77f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f77f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f77f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f77fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f77fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f780110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f7803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f780690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f780950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f780c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f780ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f781190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f781450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f781710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f7819d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f781c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f781f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f782210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f7824d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f782790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f782a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f782d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f782fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f783290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f783550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f783810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f783ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f783d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f784050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f784310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f7845d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f784890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f784b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f784e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f7850d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f785390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f785650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f785910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f785bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f785e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f786150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f786410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f7866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f786990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f786c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f786f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f7871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f787490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f787750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f787a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f787cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f787f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f788250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f788510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f7887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f788a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f788d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f789010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f7892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f789590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f789850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f789b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f789dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f78a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f78a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f78a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f78a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f78ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f78ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f78b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f78b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f78b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f78bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f78c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f78c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f78cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f78d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f78d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f78ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f78e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f78e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f78eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f78f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f78f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f78feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f790400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f790950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f790ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f7913f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f791940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f791e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f7923e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f792930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f792e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f7933d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f793920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f793e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f7943c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f794910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f794e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f7953b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f795900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f795e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f7963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f7968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f796e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f797390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f7978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f797e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f798380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f7988d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f798e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f799370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f7998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f799e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f79a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f79a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f79ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f79b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f79b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f79bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f79c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f79c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f79cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f79d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f79d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f79ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f79e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f79e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f79e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f79ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f79eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f79f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f79f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f79fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f7a00b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f7a0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f7a0990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f7a0e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f7a1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f7a16e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f7a1b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f7a1fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f7a2430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f7a3120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f7a3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f7a3f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f7a4220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f7a4690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f7a4c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f7a52a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.913s
user	0m0.243s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
