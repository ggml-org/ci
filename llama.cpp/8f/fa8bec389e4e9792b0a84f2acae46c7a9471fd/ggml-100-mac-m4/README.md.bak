### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.12 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.50 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.68 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.77 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.94 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.01 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.53 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.88 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.04 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.96 sec*proc (29 tests)

Total Test time (real) = 165.97 sec

real	2m45.977s
user	4m39.989s
sys	0m5.689s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.37 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.36 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.09 sec*proc (29 tests)

Total Test time (real) =  48.10 sec

real	0m48.111s
user	0m54.422s
sys	0m5.137s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.107 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.617 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.110 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.117 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.123 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.123 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.124 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.125 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.125 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.126 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.126 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.127 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.129 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.129 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.130 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.130 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.130 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.131 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.131 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.928 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.931 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.931 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.932 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.932 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.933 I llama_model_loader: - type  f32:  124 tensors
0.00.024.934 I llama_model_loader: - type  f16:   73 tensors
0.00.024.934 I print_info: file format = GGUF V3 (latest)
0.00.024.935 I print_info: file type   = F16
0.00.024.936 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.189 I load: special tokens cache size = 5
0.00.031.401 I load: token to piece cache size = 0.2032 MB
0.00.031.405 I print_info: arch             = bert
0.00.031.406 I print_info: vocab_only       = 0
0.00.031.406 I print_info: n_ctx_train      = 512
0.00.031.406 I print_info: n_embd           = 384
0.00.031.407 I print_info: n_layer          = 12
0.00.031.410 I print_info: n_head           = 12
0.00.031.411 I print_info: n_head_kv        = 12
0.00.031.412 I print_info: n_rot            = 32
0.00.031.412 I print_info: n_swa            = 0
0.00.031.412 I print_info: n_embd_head_k    = 32
0.00.031.412 I print_info: n_embd_head_v    = 32
0.00.031.413 I print_info: n_gqa            = 1
0.00.031.414 I print_info: n_embd_k_gqa     = 384
0.00.031.417 I print_info: n_embd_v_gqa     = 384
0.00.031.418 I print_info: f_norm_eps       = 1.0e-12
0.00.031.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.421 I print_info: f_logit_scale    = 0.0e+00
0.00.031.422 I print_info: n_ff             = 1536
0.00.031.422 I print_info: n_expert         = 0
0.00.031.422 I print_info: n_expert_used    = 0
0.00.031.423 I print_info: causal attn      = 0
0.00.031.423 I print_info: pooling type     = 2
0.00.031.423 I print_info: rope type        = 2
0.00.031.423 I print_info: rope scaling     = linear
0.00.031.424 I print_info: freq_base_train  = 10000.0
0.00.031.424 I print_info: freq_scale_train = 1
0.00.031.424 I print_info: n_ctx_orig_yarn  = 512
0.00.031.425 I print_info: rope_finetuned   = unknown
0.00.031.425 I print_info: ssm_d_conv       = 0
0.00.031.425 I print_info: ssm_d_inner      = 0
0.00.031.425 I print_info: ssm_d_state      = 0
0.00.031.426 I print_info: ssm_dt_rank      = 0
0.00.031.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.427 I print_info: model type       = 33M
0.00.031.428 I print_info: model params     = 33.21 M
0.00.031.428 I print_info: general.name     = Bge Small
0.00.031.428 I print_info: vocab type       = WPM
0.00.031.429 I print_info: n_vocab          = 30522
0.00.031.429 I print_info: n_merges         = 0
0.00.031.429 I print_info: BOS token        = 101 '[CLS]'
0.00.031.429 I print_info: UNK token        = 100 '[UNK]'
0.00.031.430 I print_info: SEP token        = 102 '[SEP]'
0.00.031.430 I print_info: PAD token        = 0 '[PAD]'
0.00.031.430 I print_info: MASK token       = 103 '[MASK]'
0.00.031.430 I print_info: LF token         = 0 '[PAD]'
0.00.031.431 I print_info: max token length = 21
0.00.031.431 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.034.570 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.573 I load_tensors: offloading output layer to GPU
0.00.034.573 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.599 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.601 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.904 I llama_init_from_model: n_seq_max     = 1
0.00.034.905 I llama_init_from_model: n_ctx         = 512
0.00.034.905 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.906 I llama_init_from_model: n_batch       = 2048
0.00.034.906 I llama_init_from_model: n_ubatch      = 2048
0.00.034.906 I llama_init_from_model: flash_attn    = 0
0.00.034.907 I llama_init_from_model: freq_base     = 10000.0
0.00.034.907 I llama_init_from_model: freq_scale    = 1
0.00.034.908 I ggml_metal_init: allocating
0.00.034.921 I ggml_metal_init: found device: Apple M4
0.00.034.930 I ggml_metal_init: picking default device: Apple M4
0.00.035.738 I ggml_metal_init: using embedded metal library
0.00.039.913 I ggml_metal_init: GPU name:   Apple M4
0.00.039.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.917 I ggml_metal_init: simdgroup reduction   = true
0.00.039.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.917 I ggml_metal_init: has residency sets    = true
0.00.039.917 I ggml_metal_init: has bfloat            = true
0.00.039.918 I ggml_metal_init: use bfloat            = true
0.00.039.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.190 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.904 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.907 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.928 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.054.120 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.054.122 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.054.122 I llama_init_from_model: graph nodes  = 429
0.00.054.122 I llama_init_from_model: graph splits = 2
0.00.054.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.684 I 
0.00.059.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.356 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.425 I llama_perf_context_print:        load time =      45.06 ms
0.00.065.426 I llama_perf_context_print: prompt eval time =       4.92 ms /     9 tokens (    0.55 ms per token,  1829.64 tokens per second)
0.00.065.427 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.427 I llama_perf_context_print:       total time =       5.74 ms /    10 tokens
0.00.065.576 I ggml_metal_free: deallocating

real	0m0.241s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.526 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.244 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.249 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.250 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.250 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.250 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.251 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.251 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.252 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.252 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.252 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.255 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.256 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.258 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.258 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.258 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.259 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.621 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.294 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.296 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.296 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.296 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.297 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.297 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.297 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.298 I llama_model_loader: - type  f32:  124 tensors
0.00.015.298 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.298 I print_info: file format = GGUF V3 (latest)
0.00.015.299 I print_info: file type   = Q8_0
0.00.015.299 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.829 I load: special tokens cache size = 5
0.00.019.118 I load: token to piece cache size = 0.2032 MB
0.00.019.121 I print_info: arch             = bert
0.00.019.122 I print_info: vocab_only       = 0
0.00.019.122 I print_info: n_ctx_train      = 512
0.00.019.122 I print_info: n_embd           = 384
0.00.019.122 I print_info: n_layer          = 12
0.00.019.126 I print_info: n_head           = 12
0.00.019.126 I print_info: n_head_kv        = 12
0.00.019.126 I print_info: n_rot            = 32
0.00.019.126 I print_info: n_swa            = 0
0.00.019.127 I print_info: n_embd_head_k    = 32
0.00.019.128 I print_info: n_embd_head_v    = 32
0.00.019.129 I print_info: n_gqa            = 1
0.00.019.129 I print_info: n_embd_k_gqa     = 384
0.00.019.130 I print_info: n_embd_v_gqa     = 384
0.00.019.130 I print_info: f_norm_eps       = 1.0e-12
0.00.019.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.131 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.131 I print_info: f_logit_scale    = 0.0e+00
0.00.019.132 I print_info: n_ff             = 1536
0.00.019.132 I print_info: n_expert         = 0
0.00.019.132 I print_info: n_expert_used    = 0
0.00.019.132 I print_info: causal attn      = 0
0.00.019.133 I print_info: pooling type     = 2
0.00.019.133 I print_info: rope type        = 2
0.00.019.133 I print_info: rope scaling     = linear
0.00.019.134 I print_info: freq_base_train  = 10000.0
0.00.019.136 I print_info: freq_scale_train = 1
0.00.019.136 I print_info: n_ctx_orig_yarn  = 512
0.00.019.136 I print_info: rope_finetuned   = unknown
0.00.019.136 I print_info: ssm_d_conv       = 0
0.00.019.136 I print_info: ssm_d_inner      = 0
0.00.019.137 I print_info: ssm_d_state      = 0
0.00.019.138 I print_info: ssm_dt_rank      = 0
0.00.019.138 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.138 I print_info: model type       = 33M
0.00.019.139 I print_info: model params     = 33.21 M
0.00.019.139 I print_info: general.name     = Bge Small
0.00.019.139 I print_info: vocab type       = WPM
0.00.019.140 I print_info: n_vocab          = 30522
0.00.019.140 I print_info: n_merges         = 0
0.00.019.140 I print_info: BOS token        = 101 '[CLS]'
0.00.019.140 I print_info: UNK token        = 100 '[UNK]'
0.00.019.140 I print_info: SEP token        = 102 '[SEP]'
0.00.019.140 I print_info: PAD token        = 0 '[PAD]'
0.00.019.140 I print_info: MASK token       = 103 '[MASK]'
0.00.019.141 I print_info: LF token         = 0 '[PAD]'
0.00.019.141 I print_info: max token length = 21
0.00.019.141 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.870 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.870 I load_tensors: offloading output layer to GPU
0.00.020.871 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.877 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.877 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.071 I llama_init_from_model: n_seq_max     = 1
0.00.021.071 I llama_init_from_model: n_ctx         = 512
0.00.021.072 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.072 I llama_init_from_model: n_batch       = 2048
0.00.021.072 I llama_init_from_model: n_ubatch      = 2048
0.00.021.072 I llama_init_from_model: flash_attn    = 0
0.00.021.073 I llama_init_from_model: freq_base     = 10000.0
0.00.021.073 I llama_init_from_model: freq_scale    = 1
0.00.021.073 I ggml_metal_init: allocating
0.00.021.080 I ggml_metal_init: found device: Apple M4
0.00.021.085 I ggml_metal_init: picking default device: Apple M4
0.00.021.598 I ggml_metal_init: using embedded metal library
0.00.024.241 I ggml_metal_init: GPU name:   Apple M4
0.00.024.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.244 I ggml_metal_init: simdgroup reduction   = true
0.00.024.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.245 I ggml_metal_init: has residency sets    = true
0.00.024.245 I ggml_metal_init: has bfloat            = true
0.00.024.245 I ggml_metal_init: use bfloat            = true
0.00.024.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.944 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.562 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.564 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.578 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.670 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.671 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.671 I llama_init_from_model: graph nodes  = 429
0.00.036.671 I llama_init_from_model: graph splits = 2
0.00.036.673 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.673 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.803 I 
0.00.040.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.375 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.780 I llama_perf_context_print:        load time =      31.27 ms
0.00.045.782 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2102.31 tokens per second)
0.00.045.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.783 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.045.956 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.235 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.054 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.059 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.062 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.066 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.067 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.068 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.071 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.071 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.072 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.073 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.073 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.077 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.077 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.078 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.340 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.341 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.341 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.341 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.342 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.342 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.342 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.343 I llama_model_loader: - type  f32:   40 tensors
0.00.052.343 I llama_model_loader: - type  f16:   30 tensors
0.00.052.344 I print_info: file format = GGUF V3 (latest)
0.00.052.345 I print_info: file type   = F16
0.00.052.346 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.726 W load: empty token at index 5
0.00.061.964 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.463 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.498 I load: special tokens cache size = 5
0.00.327.911 I load: token to piece cache size = 1.5060 MB
0.00.327.919 I print_info: arch             = jina-bert-v2
0.00.327.919 I print_info: vocab_only       = 0
0.00.327.919 I print_info: n_ctx_train      = 8192
0.00.327.919 I print_info: n_embd           = 384
0.00.327.920 I print_info: n_layer          = 4
0.00.327.924 I print_info: n_head           = 12
0.00.327.924 I print_info: n_head_kv        = 12
0.00.327.924 I print_info: n_rot            = 32
0.00.327.924 I print_info: n_swa            = 0
0.00.327.925 I print_info: n_embd_head_k    = 32
0.00.327.925 I print_info: n_embd_head_v    = 32
0.00.327.925 I print_info: n_gqa            = 1
0.00.327.927 I print_info: n_embd_k_gqa     = 384
0.00.327.927 I print_info: n_embd_v_gqa     = 384
0.00.327.928 I print_info: f_norm_eps       = 1.0e-12
0.00.327.931 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.931 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.931 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.931 I print_info: f_logit_scale    = 0.0e+00
0.00.327.932 I print_info: n_ff             = 1536
0.00.327.932 I print_info: n_expert         = 0
0.00.327.932 I print_info: n_expert_used    = 0
0.00.327.934 I print_info: causal attn      = 0
0.00.327.934 I print_info: pooling type     = -1
0.00.327.934 I print_info: rope type        = -1
0.00.327.934 I print_info: rope scaling     = linear
0.00.327.935 I print_info: freq_base_train  = 10000.0
0.00.327.935 I print_info: freq_scale_train = 1
0.00.327.935 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.935 I print_info: rope_finetuned   = unknown
0.00.327.936 I print_info: ssm_d_conv       = 0
0.00.327.936 I print_info: ssm_d_inner      = 0
0.00.327.936 I print_info: ssm_d_state      = 0
0.00.327.936 I print_info: ssm_dt_rank      = 0
0.00.327.936 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.936 I print_info: model type       = 33M
0.00.327.938 I print_info: model params     = 32.90 M
0.00.327.938 I print_info: general.name     = Jina Bert Implementation
0.00.327.939 I print_info: vocab type       = BPE
0.00.327.940 I print_info: n_vocab          = 61056
0.00.327.940 I print_info: n_merges         = 39382
0.00.327.941 I print_info: BOS token        = 0 '<s>'
0.00.327.941 I print_info: EOS token        = 2 '</s>'
0.00.327.941 I print_info: UNK token        = 3 '<unk>'
0.00.327.943 I print_info: SEP token        = 2 '</s>'
0.00.327.943 I print_info: PAD token        = 1 '<pad>'
0.00.327.943 I print_info: MASK token       = 4 '<mask>'
0.00.327.943 I print_info: EOG token        = 2 '</s>'
0.00.327.944 I print_info: max token length = 45
0.00.327.944 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.313 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.314 I load_tensors: offloading output layer to GPU
0.00.329.314 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.333 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.334 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.329.566 I llama_init_from_model: n_seq_max     = 1
0.00.329.567 I llama_init_from_model: n_ctx         = 8192
0.00.329.567 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.567 I llama_init_from_model: n_batch       = 2048
0.00.329.567 I llama_init_from_model: n_ubatch      = 2048
0.00.329.567 I llama_init_from_model: flash_attn    = 0
0.00.329.568 I llama_init_from_model: freq_base     = 10000.0
0.00.329.568 I llama_init_from_model: freq_scale    = 1
0.00.329.568 I ggml_metal_init: allocating
0.00.329.572 I ggml_metal_init: found device: Apple M4
0.00.329.576 I ggml_metal_init: picking default device: Apple M4
0.00.330.158 I ggml_metal_init: using embedded metal library
0.00.332.948 I ggml_metal_init: GPU name:   Apple M4
0.00.332.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.951 I ggml_metal_init: simdgroup reduction   = true
0.00.332.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.952 I ggml_metal_init: has residency sets    = true
0.00.332.952 I ggml_metal_init: has bfloat            = true
0.00.332.952 I ggml_metal_init: use bfloat            = true
0.00.332.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.520 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.346.599 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.601 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.623 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.545 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.547 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.547 I llama_init_from_model: graph nodes  = 154
0.00.353.547 I llama_init_from_model: graph splits = 2
0.00.353.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.032 I 
0.00.361.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.260 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.261 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.272 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.272 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.278 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.278 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.776 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.323 I llama_perf_context_print:        load time =     336.39 ms
0.00.365.324 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17519.07 tokens per second)
0.00.365.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.326 I llama_perf_context_print:       total time =       4.29 ms /    63 tokens
0.00.365.595 I ggml_metal_free: deallocating

real	0m1.061s
user	0m0.333s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.184 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.361 I main: llama backend init
0.00.000.368 I main: load the model and apply lora adapter, if any
0.00.067.281 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.079.647 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.672 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.088.658 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.095.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.582 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.583 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.585 I llama_model_loader: - type  f32:  194 tensors
0.00.095.585 I llama_model_loader: - type  f16:   98 tensors
0.00.095.587 I print_info: file format = GGUF V3 (latest)
0.00.095.588 I print_info: file type   = all F32 (guessed)
0.00.095.590 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.768 I load: special tokens cache size = 25
0.00.124.024 I load: token to piece cache size = 0.2984 MB
0.00.124.028 I print_info: arch             = gptneox
0.00.124.028 I print_info: vocab_only       = 0
0.00.124.029 I print_info: n_ctx_train      = 2048
0.00.124.029 I print_info: n_embd           = 2048
0.00.124.029 I print_info: n_layer          = 24
0.00.124.035 I print_info: n_head           = 16
0.00.124.036 I print_info: n_head_kv        = 16
0.00.124.036 I print_info: n_rot            = 32
0.00.124.036 I print_info: n_swa            = 0
0.00.124.036 I print_info: n_embd_head_k    = 128
0.00.124.037 I print_info: n_embd_head_v    = 128
0.00.124.037 I print_info: n_gqa            = 1
0.00.124.038 I print_info: n_embd_k_gqa     = 2048
0.00.124.039 I print_info: n_embd_v_gqa     = 2048
0.00.124.040 I print_info: f_norm_eps       = 1.0e-05
0.00.124.040 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.124.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.124.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.124.041 I print_info: f_logit_scale    = 0.0e+00
0.00.124.042 I print_info: n_ff             = 8192
0.00.124.042 I print_info: n_expert         = 0
0.00.124.043 I print_info: n_expert_used    = 0
0.00.124.043 I print_info: causal attn      = 1
0.00.124.043 I print_info: pooling type     = 0
0.00.124.043 I print_info: rope type        = 2
0.00.124.043 I print_info: rope scaling     = linear
0.00.124.044 I print_info: freq_base_train  = 10000.0
0.00.124.044 I print_info: freq_scale_train = 1
0.00.124.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.124.045 I print_info: rope_finetuned   = unknown
0.00.124.045 I print_info: ssm_d_conv       = 0
0.00.124.046 I print_info: ssm_d_inner      = 0
0.00.124.047 I print_info: ssm_d_state      = 0
0.00.124.047 I print_info: ssm_dt_rank      = 0
0.00.124.049 I print_info: ssm_dt_b_c_rms   = 0
0.00.124.049 I print_info: model type       = 1.4B
0.00.124.050 I print_info: model params     = 1.41 B
0.00.124.050 I print_info: general.name     = 1.4B
0.00.124.050 I print_info: vocab type       = BPE
0.00.124.051 I print_info: n_vocab          = 50304
0.00.124.051 I print_info: n_merges         = 50009
0.00.124.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.124.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.124.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.124.054 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.124.054 I print_info: LF token         = 187 ''
0.00.124.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.124.055 I print_info: max token length = 1024
0.00.124.055 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.184.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.184.381 I load_tensors: offloading output layer to GPU
0.00.184.381 I load_tensors: offloaded 25/25 layers to GPU
0.00.184.407 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.184.408 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.185.051 I llama_init_from_model: n_seq_max     = 1
0.00.185.051 I llama_init_from_model: n_ctx         = 2048
0.00.185.052 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.185.052 I llama_init_from_model: n_batch       = 2048
0.00.185.052 I llama_init_from_model: n_ubatch      = 512
0.00.185.052 I llama_init_from_model: flash_attn    = 0
0.00.185.053 I llama_init_from_model: freq_base     = 10000.0
0.00.185.053 I llama_init_from_model: freq_scale    = 1
0.00.185.054 I ggml_metal_init: allocating
0.00.185.101 I ggml_metal_init: found device: Apple M4
0.00.185.107 I ggml_metal_init: picking default device: Apple M4
0.00.185.752 I ggml_metal_init: using embedded metal library
0.00.201.541 I ggml_metal_init: GPU name:   Apple M4
0.00.201.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.201.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.201.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.201.544 I ggml_metal_init: simdgroup reduction   = true
0.00.201.544 I ggml_metal_init: simdgroup matrix mul. = true
0.00.201.544 I ggml_metal_init: has residency sets    = true
0.00.201.544 I ggml_metal_init: has bfloat            = true
0.00.201.544 I ggml_metal_init: use bfloat            = true
0.00.201.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.201.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.235.096 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.265.615 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.265.621 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.265.663 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.269.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.269.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.269.321 I llama_init_from_model: graph nodes  = 967
0.00.269.321 I llama_init_from_model: graph splits = 2
0.00.269.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.269.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.269.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.412 I main: llama threadpool init, n_threads = 4
0.00.336.453 I 
0.00.336.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.485 I 
0.00.336.661 I sampler seed: 1234
0.00.336.666 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.336.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.336.692 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.336.692 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.180.485 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.02.180.487 I llama_perf_context_print:        load time =     268.23 ms
0.02.180.488 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.12 tokens per second)
0.02.180.489 I llama_perf_context_print:        eval time =    1797.23 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.180.489 I llama_perf_context_print:       total time =    1844.96 ms /    70 tokens
0.02.180.768 I ggml_metal_free: deallocating

real	0m2.493s
user	0m0.134s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.497 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.204 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.220 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.221 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.221 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.222 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.222 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.235 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.492 I llama_model_loader: - type  f32:  194 tensors
0.00.054.492 I llama_model_loader: - type  f16:   98 tensors
0.00.054.493 I print_info: file format = GGUF V3 (latest)
0.00.054.494 I print_info: file type   = all F32 (guessed)
0.00.054.495 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.310 I load: special tokens cache size = 25
0.00.075.386 I load: token to piece cache size = 0.2984 MB
0.00.075.389 I print_info: arch             = gptneox
0.00.075.390 I print_info: vocab_only       = 0
0.00.075.390 I print_info: n_ctx_train      = 2048
0.00.075.390 I print_info: n_embd           = 2048
0.00.075.390 I print_info: n_layer          = 24
0.00.075.393 I print_info: n_head           = 16
0.00.075.394 I print_info: n_head_kv        = 16
0.00.075.394 I print_info: n_rot            = 32
0.00.075.394 I print_info: n_swa            = 0
0.00.075.394 I print_info: n_embd_head_k    = 128
0.00.075.394 I print_info: n_embd_head_v    = 128
0.00.075.395 I print_info: n_gqa            = 1
0.00.075.396 I print_info: n_embd_k_gqa     = 2048
0.00.075.397 I print_info: n_embd_v_gqa     = 2048
0.00.075.397 I print_info: f_norm_eps       = 1.0e-05
0.00.075.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.400 I print_info: f_logit_scale    = 0.0e+00
0.00.075.400 I print_info: n_ff             = 8192
0.00.075.401 I print_info: n_expert         = 0
0.00.075.401 I print_info: n_expert_used    = 0
0.00.075.401 I print_info: causal attn      = 1
0.00.075.401 I print_info: pooling type     = 0
0.00.075.401 I print_info: rope type        = 2
0.00.075.402 I print_info: rope scaling     = linear
0.00.075.402 I print_info: freq_base_train  = 10000.0
0.00.075.402 I print_info: freq_scale_train = 1
0.00.075.402 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.403 I print_info: rope_finetuned   = unknown
0.00.075.403 I print_info: ssm_d_conv       = 0
0.00.075.403 I print_info: ssm_d_inner      = 0
0.00.075.403 I print_info: ssm_d_state      = 0
0.00.075.403 I print_info: ssm_dt_rank      = 0
0.00.075.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.404 I print_info: model type       = 1.4B
0.00.075.404 I print_info: model params     = 1.41 B
0.00.075.404 I print_info: general.name     = 1.4B
0.00.075.405 I print_info: vocab type       = BPE
0.00.075.405 I print_info: n_vocab          = 50304
0.00.075.405 I print_info: n_merges         = 50009
0.00.075.405 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.406 I print_info: LF token         = 187 ''
0.00.075.406 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.407 I print_info: max token length = 1024
0.00.075.408 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.361.287 I load_tensors: offloading 24 repeating layers to GPU
0.01.361.291 I load_tensors: offloading output layer to GPU
0.01.361.291 I load_tensors: offloaded 25/25 layers to GPU
0.01.361.312 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.361.315 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.362.473 I llama_init_from_model: n_seq_max     = 1
0.01.362.475 I llama_init_from_model: n_ctx         = 128
0.01.362.475 I llama_init_from_model: n_ctx_per_seq = 128
0.01.362.475 I llama_init_from_model: n_batch       = 128
0.01.362.475 I llama_init_from_model: n_ubatch      = 128
0.01.362.476 I llama_init_from_model: flash_attn    = 0
0.01.362.476 I llama_init_from_model: freq_base     = 10000.0
0.01.362.476 I llama_init_from_model: freq_scale    = 1
0.01.362.477 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.362.478 I ggml_metal_init: allocating
0.01.362.551 I ggml_metal_init: found device: Apple M4
0.01.362.562 I ggml_metal_init: picking default device: Apple M4
0.01.363.673 I ggml_metal_init: using embedded metal library
0.01.367.646 I ggml_metal_init: GPU name:   Apple M4
0.01.367.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.367.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.367.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.367.649 I ggml_metal_init: simdgroup reduction   = true
0.01.367.649 I ggml_metal_init: simdgroup matrix mul. = true
0.01.367.649 I ggml_metal_init: has residency sets    = true
0.01.367.650 I ggml_metal_init: has bfloat            = true
0.01.367.650 I ggml_metal_init: use bfloat            = true
0.01.367.650 I ggml_metal_init: hasUnifiedMemory      = true
0.01.367.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.378.409 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.380.128 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.380.130 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.380.157 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.381.863 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.381.864 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.381.864 I llama_init_from_model: graph nodes  = 967
0.01.381.865 I llama_init_from_model: graph splits = 2
0.01.381.866 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.381.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.415.405 I 
0.01.415.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.415.444 I perplexity: tokenizing the input ..
0.01.420.526 I perplexity: tokenization took 5.08 ms
0.01.420.531 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.539.758 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.542.723 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.542.790 I llama_perf_context_print:        load time =    1392.48 ms
0.01.542.791 I llama_perf_context_print: prompt eval time =     118.96 ms /   128 tokens (    0.93 ms per token,  1076.02 tokens per second)
0.01.542.792 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.542.792 I llama_perf_context_print:       total time =     127.38 ms /   129 tokens
0.01.543.237 I ggml_metal_free: deallocating

real	0m1.731s
user	0m0.104s
sys	0m0.247s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.694 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.561 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.570 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.454 I llama_model_loader: - type  f32:  194 tensors
0.00.036.454 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.455 I print_info: file format = GGUF V3 (latest)
0.00.036.456 I print_info: file type   = Q8_0
0.00.036.456 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.419 I load: special tokens cache size = 25
0.00.053.159 I load: token to piece cache size = 0.2984 MB
0.00.053.162 I print_info: arch             = gptneox
0.00.053.162 I print_info: vocab_only       = 0
0.00.053.163 I print_info: n_ctx_train      = 2048
0.00.053.163 I print_info: n_embd           = 2048
0.00.053.163 I print_info: n_layer          = 24
0.00.053.167 I print_info: n_head           = 16
0.00.053.168 I print_info: n_head_kv        = 16
0.00.053.168 I print_info: n_rot            = 32
0.00.053.168 I print_info: n_swa            = 0
0.00.053.169 I print_info: n_embd_head_k    = 128
0.00.053.169 I print_info: n_embd_head_v    = 128
0.00.053.170 I print_info: n_gqa            = 1
0.00.053.170 I print_info: n_embd_k_gqa     = 2048
0.00.053.171 I print_info: n_embd_v_gqa     = 2048
0.00.053.172 I print_info: f_norm_eps       = 1.0e-05
0.00.053.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.173 I print_info: f_logit_scale    = 0.0e+00
0.00.053.173 I print_info: n_ff             = 8192
0.00.053.174 I print_info: n_expert         = 0
0.00.053.174 I print_info: n_expert_used    = 0
0.00.053.174 I print_info: causal attn      = 1
0.00.053.175 I print_info: pooling type     = 0
0.00.053.175 I print_info: rope type        = 2
0.00.053.175 I print_info: rope scaling     = linear
0.00.053.176 I print_info: freq_base_train  = 10000.0
0.00.053.176 I print_info: freq_scale_train = 1
0.00.053.176 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.177 I print_info: rope_finetuned   = unknown
0.00.053.177 I print_info: ssm_d_conv       = 0
0.00.053.177 I print_info: ssm_d_inner      = 0
0.00.053.177 I print_info: ssm_d_state      = 0
0.00.053.177 I print_info: ssm_dt_rank      = 0
0.00.053.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.178 I print_info: model type       = 1.4B
0.00.053.179 I print_info: model params     = 1.41 B
0.00.053.179 I print_info: general.name     = 1.4B
0.00.053.180 I print_info: vocab type       = BPE
0.00.053.180 I print_info: n_vocab          = 50304
0.00.053.180 I print_info: n_merges         = 50009
0.00.053.180 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.181 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.183 I print_info: LF token         = 187 ''
0.00.053.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.183 I print_info: max token length = 1024
0.00.053.184 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.165.254 I load_tensors: offloading 24 repeating layers to GPU
0.01.165.259 I load_tensors: offloading output layer to GPU
0.01.165.260 I load_tensors: offloaded 25/25 layers to GPU
0.01.165.282 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.165.283 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.166.615 I llama_init_from_model: n_seq_max     = 1
0.01.166.616 I llama_init_from_model: n_ctx         = 2048
0.01.166.616 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.166.616 I llama_init_from_model: n_batch       = 2048
0.01.166.617 I llama_init_from_model: n_ubatch      = 512
0.01.166.617 I llama_init_from_model: flash_attn    = 0
0.01.166.618 I llama_init_from_model: freq_base     = 10000.0
0.01.166.618 I llama_init_from_model: freq_scale    = 1
0.01.166.619 I ggml_metal_init: allocating
0.01.166.628 I ggml_metal_init: found device: Apple M4
0.01.166.635 I ggml_metal_init: picking default device: Apple M4
0.01.167.853 I ggml_metal_init: using embedded metal library
0.01.173.180 I ggml_metal_init: GPU name:   Apple M4
0.01.173.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.173.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.173.184 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.173.185 I ggml_metal_init: simdgroup reduction   = true
0.01.173.185 I ggml_metal_init: simdgroup matrix mul. = true
0.01.173.185 I ggml_metal_init: has residency sets    = true
0.01.173.185 I ggml_metal_init: has bfloat            = true
0.01.173.186 I ggml_metal_init: use bfloat            = true
0.01.173.186 I ggml_metal_init: hasUnifiedMemory      = true
0.01.173.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.188.552 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.234.098 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.234.104 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.234.138 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.238.367 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.238.369 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.238.369 I llama_init_from_model: graph nodes  = 967
0.01.238.370 I llama_init_from_model: graph splits = 2
0.01.238.376 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.238.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.238.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.294.182 I main: llama threadpool init, n_threads = 4
0.01.294.222 I 
0.01.294.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.294.246 I 
0.01.294.408 I sampler seed: 1234
0.01.294.413 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.294.425 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.294.426 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.294.426 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.392.990 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.02.392.991 I llama_perf_context_print:        load time =    1283.73 ms
0.02.392.992 I llama_perf_context_print: prompt eval time =      48.80 ms /     7 tokens (    6.97 ms per token,   143.46 tokens per second)
0.02.392.993 I llama_perf_context_print:        eval time =    1046.95 ms /    63 runs   (   16.62 ms per token,    60.17 tokens per second)
0.02.392.993 I llama_perf_context_print:       total time =    1099.56 ms /    70 tokens
0.02.393.247 I ggml_metal_free: deallocating

real	0m2.411s
user	0m0.110s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.268 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.624 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.640 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.494 I llama_model_loader: - type  f32:  194 tensors
0.00.025.494 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.495 I print_info: file format = GGUF V3 (latest)
0.00.025.495 I print_info: file type   = Q8_0
0.00.025.497 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.726 I load: special tokens cache size = 25
0.00.039.725 I load: token to piece cache size = 0.2984 MB
0.00.039.731 I print_info: arch             = gptneox
0.00.039.731 I print_info: vocab_only       = 0
0.00.039.731 I print_info: n_ctx_train      = 2048
0.00.039.732 I print_info: n_embd           = 2048
0.00.039.732 I print_info: n_layer          = 24
0.00.039.737 I print_info: n_head           = 16
0.00.039.738 I print_info: n_head_kv        = 16
0.00.039.738 I print_info: n_rot            = 32
0.00.039.738 I print_info: n_swa            = 0
0.00.039.740 I print_info: n_embd_head_k    = 128
0.00.039.740 I print_info: n_embd_head_v    = 128
0.00.039.741 I print_info: n_gqa            = 1
0.00.039.741 I print_info: n_embd_k_gqa     = 2048
0.00.039.742 I print_info: n_embd_v_gqa     = 2048
0.00.039.742 I print_info: f_norm_eps       = 1.0e-05
0.00.039.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.743 I print_info: f_logit_scale    = 0.0e+00
0.00.039.744 I print_info: n_ff             = 8192
0.00.039.744 I print_info: n_expert         = 0
0.00.039.744 I print_info: n_expert_used    = 0
0.00.039.744 I print_info: causal attn      = 1
0.00.039.744 I print_info: pooling type     = 0
0.00.039.744 I print_info: rope type        = 2
0.00.039.746 I print_info: rope scaling     = linear
0.00.039.746 I print_info: freq_base_train  = 10000.0
0.00.039.747 I print_info: freq_scale_train = 1
0.00.039.747 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.747 I print_info: rope_finetuned   = unknown
0.00.039.747 I print_info: ssm_d_conv       = 0
0.00.039.747 I print_info: ssm_d_inner      = 0
0.00.039.747 I print_info: ssm_d_state      = 0
0.00.039.748 I print_info: ssm_dt_rank      = 0
0.00.039.748 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.748 I print_info: model type       = 1.4B
0.00.039.748 I print_info: model params     = 1.41 B
0.00.039.748 I print_info: general.name     = 1.4B
0.00.039.749 I print_info: vocab type       = BPE
0.00.039.749 I print_info: n_vocab          = 50304
0.00.039.749 I print_info: n_merges         = 50009
0.00.039.750 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.752 I print_info: LF token         = 187 ''
0.00.039.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.752 I print_info: max token length = 1024
0.00.039.754 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.894.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.894.270 I load_tensors: offloading output layer to GPU
0.00.894.271 I load_tensors: offloaded 25/25 layers to GPU
0.00.894.300 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.894.301 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.895.718 I llama_init_from_model: n_seq_max     = 1
0.00.895.720 I llama_init_from_model: n_ctx         = 128
0.00.895.720 I llama_init_from_model: n_ctx_per_seq = 128
0.00.895.720 I llama_init_from_model: n_batch       = 128
0.00.895.721 I llama_init_from_model: n_ubatch      = 128
0.00.895.721 I llama_init_from_model: flash_attn    = 0
0.00.895.721 I llama_init_from_model: freq_base     = 10000.0
0.00.895.722 I llama_init_from_model: freq_scale    = 1
0.00.895.722 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.895.723 I ggml_metal_init: allocating
0.00.895.812 I ggml_metal_init: found device: Apple M4
0.00.895.822 I ggml_metal_init: picking default device: Apple M4
0.00.897.317 I ggml_metal_init: using embedded metal library
0.00.902.522 I ggml_metal_init: GPU name:   Apple M4
0.00.902.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.902.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.902.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.902.527 I ggml_metal_init: simdgroup reduction   = true
0.00.902.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.902.528 I ggml_metal_init: has residency sets    = true
0.00.902.528 I ggml_metal_init: has bfloat            = true
0.00.902.528 I ggml_metal_init: use bfloat            = true
0.00.902.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.902.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.917.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.920.762 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.920.766 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.920.808 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.924.107 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.924.108 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.924.108 I llama_init_from_model: graph nodes  = 967
0.00.924.109 I llama_init_from_model: graph splits = 2
0.00.924.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.924.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.952.811 I 
0.00.952.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.952.866 I perplexity: tokenizing the input ..
0.00.958.949 I perplexity: tokenization took 6.081 ms
0.00.958.962 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.097.370 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.098.731 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.098.755 I llama_perf_context_print:        load time =     943.54 ms
0.01.098.756 I llama_perf_context_print: prompt eval time =     138.01 ms /   128 tokens (    1.08 ms per token,   927.48 tokens per second)
0.01.098.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.098.757 I llama_perf_context_print:       total time =     145.94 ms /   129 tokens
0.01.099.096 I ggml_metal_free: deallocating

real	0m1.113s
user	0m0.075s
sys	0m0.178s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.018.064 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.917 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.918 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.920 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.921 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.921 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.925 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.925 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.297 I llama_model_loader: - type  f32:  194 tensors
0.00.044.297 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.298 I print_info: file format = GGUF V3 (latest)
0.00.044.299 I print_info: file type   = Q4_0
0.00.044.300 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.055.028 I load: special tokens cache size = 25
0.00.063.538 I load: token to piece cache size = 0.2984 MB
0.00.063.542 I print_info: arch             = gptneox
0.00.063.543 I print_info: vocab_only       = 0
0.00.063.543 I print_info: n_ctx_train      = 2048
0.00.063.543 I print_info: n_embd           = 2048
0.00.063.544 I print_info: n_layer          = 24
0.00.063.548 I print_info: n_head           = 16
0.00.063.549 I print_info: n_head_kv        = 16
0.00.063.549 I print_info: n_rot            = 32
0.00.063.550 I print_info: n_swa            = 0
0.00.063.550 I print_info: n_embd_head_k    = 128
0.00.063.550 I print_info: n_embd_head_v    = 128
0.00.063.551 I print_info: n_gqa            = 1
0.00.063.552 I print_info: n_embd_k_gqa     = 2048
0.00.063.553 I print_info: n_embd_v_gqa     = 2048
0.00.063.554 I print_info: f_norm_eps       = 1.0e-05
0.00.063.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.555 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.556 I print_info: f_logit_scale    = 0.0e+00
0.00.063.557 I print_info: n_ff             = 8192
0.00.063.557 I print_info: n_expert         = 0
0.00.063.557 I print_info: n_expert_used    = 0
0.00.063.557 I print_info: causal attn      = 1
0.00.063.557 I print_info: pooling type     = 0
0.00.063.558 I print_info: rope type        = 2
0.00.063.558 I print_info: rope scaling     = linear
0.00.063.559 I print_info: freq_base_train  = 10000.0
0.00.063.559 I print_info: freq_scale_train = 1
0.00.063.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.559 I print_info: rope_finetuned   = unknown
0.00.063.560 I print_info: ssm_d_conv       = 0
0.00.063.560 I print_info: ssm_d_inner      = 0
0.00.063.564 I print_info: ssm_d_state      = 0
0.00.063.565 I print_info: ssm_dt_rank      = 0
0.00.063.565 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.565 I print_info: model type       = 1.4B
0.00.063.565 I print_info: model params     = 1.41 B
0.00.063.566 I print_info: general.name     = 1.4B
0.00.063.567 I print_info: vocab type       = BPE
0.00.063.567 I print_info: n_vocab          = 50304
0.00.063.567 I print_info: n_merges         = 50009
0.00.063.568 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.568 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.568 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.568 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.569 I print_info: LF token         = 187 ''
0.00.063.569 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.571 I print_info: max token length = 1024
0.00.063.572 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.696 I load_tensors: offloading output layer to GPU
0.00.621.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.730 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.621.731 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.623.109 I llama_init_from_model: n_seq_max     = 1
0.00.623.111 I llama_init_from_model: n_ctx         = 2048
0.00.623.112 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.623.112 I llama_init_from_model: n_batch       = 2048
0.00.623.113 I llama_init_from_model: n_ubatch      = 512
0.00.623.113 I llama_init_from_model: flash_attn    = 0
0.00.623.115 I llama_init_from_model: freq_base     = 10000.0
0.00.623.116 I llama_init_from_model: freq_scale    = 1
0.00.623.121 I ggml_metal_init: allocating
0.00.623.200 I ggml_metal_init: found device: Apple M4
0.00.623.214 I ggml_metal_init: picking default device: Apple M4
0.00.625.001 I ggml_metal_init: using embedded metal library
0.00.630.494 I ggml_metal_init: GPU name:   Apple M4
0.00.630.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.509 I ggml_metal_init: simdgroup reduction   = true
0.00.630.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.510 I ggml_metal_init: has residency sets    = true
0.00.630.510 I ggml_metal_init: has bfloat            = true
0.00.630.511 I ggml_metal_init: use bfloat            = true
0.00.630.516 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.175 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.149 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.156 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.855 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.858 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.859 I llama_init_from_model: graph nodes  = 967
0.00.711.859 I llama_init_from_model: graph splits = 2
0.00.711.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.711.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.573 I main: llama threadpool init, n_threads = 4
0.00.769.623 I 
0.00.769.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.648 I 
0.00.769.807 I sampler seed: 1234
0.00.769.811 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.822 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.822 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.822 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.453.050 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.453.050 I llama_perf_context_print:        load time =     750.78 ms
0.01.453.052 I llama_perf_context_print: prompt eval time =      49.36 ms /     7 tokens (    7.05 ms per token,   141.83 tokens per second)
0.01.453.052 I llama_perf_context_print:        eval time =     630.90 ms /    63 runs   (   10.01 ms per token,    99.86 tokens per second)
0.01.453.053 I llama_perf_context_print:       total time =     684.20 ms /    70 tokens
0.01.453.274 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.119s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.238 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.031 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.034 I llama_model_loader: - type  f32:  194 tensors
0.00.026.035 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.035 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.036 I print_info: file format = GGUF V3 (latest)
0.00.026.036 I print_info: file type   = Q4_0
0.00.026.037 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.667 I load: special tokens cache size = 25
0.00.040.891 I load: token to piece cache size = 0.2984 MB
0.00.040.897 I print_info: arch             = gptneox
0.00.040.897 I print_info: vocab_only       = 0
0.00.040.897 I print_info: n_ctx_train      = 2048
0.00.040.899 I print_info: n_embd           = 2048
0.00.040.899 I print_info: n_layer          = 24
0.00.040.903 I print_info: n_head           = 16
0.00.040.904 I print_info: n_head_kv        = 16
0.00.040.904 I print_info: n_rot            = 32
0.00.040.904 I print_info: n_swa            = 0
0.00.040.904 I print_info: n_embd_head_k    = 128
0.00.040.905 I print_info: n_embd_head_v    = 128
0.00.040.905 I print_info: n_gqa            = 1
0.00.040.906 I print_info: n_embd_k_gqa     = 2048
0.00.040.906 I print_info: n_embd_v_gqa     = 2048
0.00.040.907 I print_info: f_norm_eps       = 1.0e-05
0.00.040.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.908 I print_info: f_logit_scale    = 0.0e+00
0.00.040.910 I print_info: n_ff             = 8192
0.00.040.911 I print_info: n_expert         = 0
0.00.040.911 I print_info: n_expert_used    = 0
0.00.040.911 I print_info: causal attn      = 1
0.00.040.911 I print_info: pooling type     = 0
0.00.040.912 I print_info: rope type        = 2
0.00.040.912 I print_info: rope scaling     = linear
0.00.040.912 I print_info: freq_base_train  = 10000.0
0.00.040.913 I print_info: freq_scale_train = 1
0.00.040.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.913 I print_info: rope_finetuned   = unknown
0.00.040.914 I print_info: ssm_d_conv       = 0
0.00.040.914 I print_info: ssm_d_inner      = 0
0.00.040.914 I print_info: ssm_d_state      = 0
0.00.040.914 I print_info: ssm_dt_rank      = 0
0.00.040.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.914 I print_info: model type       = 1.4B
0.00.040.915 I print_info: model params     = 1.41 B
0.00.040.915 I print_info: general.name     = 1.4B
0.00.040.916 I print_info: vocab type       = BPE
0.00.040.916 I print_info: n_vocab          = 50304
0.00.040.917 I print_info: n_merges         = 50009
0.00.040.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.917 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.917 I print_info: LF token         = 187 ''
0.00.040.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.918 I print_info: max token length = 1024
0.00.040.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.575.654 I load_tensors: offloading 24 repeating layers to GPU
0.00.575.666 I load_tensors: offloading output layer to GPU
0.00.575.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.575.702 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.575.703 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.576.820 I llama_init_from_model: n_seq_max     = 1
0.00.576.825 I llama_init_from_model: n_ctx         = 128
0.00.576.826 I llama_init_from_model: n_ctx_per_seq = 128
0.00.576.826 I llama_init_from_model: n_batch       = 128
0.00.576.827 I llama_init_from_model: n_ubatch      = 128
0.00.576.827 I llama_init_from_model: flash_attn    = 0
0.00.576.829 I llama_init_from_model: freq_base     = 10000.0
0.00.576.830 I llama_init_from_model: freq_scale    = 1
0.00.576.831 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.576.833 I ggml_metal_init: allocating
0.00.576.912 I ggml_metal_init: found device: Apple M4
0.00.576.927 I ggml_metal_init: picking default device: Apple M4
0.00.578.641 I ggml_metal_init: using embedded metal library
0.00.584.358 I ggml_metal_init: GPU name:   Apple M4
0.00.584.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.584.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.584.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.584.370 I ggml_metal_init: simdgroup reduction   = true
0.00.584.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.584.371 I ggml_metal_init: has residency sets    = true
0.00.584.371 I ggml_metal_init: has bfloat            = true
0.00.584.372 I ggml_metal_init: use bfloat            = true
0.00.584.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.584.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.600.485 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.059 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.603.062 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.603.091 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.872 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.604.873 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.604.874 I llama_init_from_model: graph nodes  = 967
0.00.604.874 I llama_init_from_model: graph splits = 2
0.00.604.875 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.604.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.699 I 
0.00.626.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.737 I perplexity: tokenizing the input ..
0.00.630.702 I perplexity: tokenization took 3.963 ms
0.00.630.707 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.758 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.771.268 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.771.301 I llama_perf_context_print:        load time =     616.62 ms
0.00.771.303 I llama_perf_context_print: prompt eval time =     135.82 ms /   128 tokens (    1.06 ms per token,   942.44 tokens per second)
0.00.771.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.304 I llama_perf_context_print:       total time =     144.60 ms /   129 tokens
0.00.772.064 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.092s
sys	0m0.121s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.946 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.990 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.793 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.482 I llama_model_loader: - type  f32:  194 tensors
0.00.025.482 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.483 I print_info: file format = GGUF V3 (latest)
0.00.025.484 I print_info: file type   = Q4_1
0.00.025.485 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.836 I load: special tokens cache size = 25
0.00.039.900 I load: token to piece cache size = 0.2984 MB
0.00.039.903 I print_info: arch             = gptneox
0.00.039.903 I print_info: vocab_only       = 0
0.00.039.903 I print_info: n_ctx_train      = 2048
0.00.039.904 I print_info: n_embd           = 2048
0.00.039.904 I print_info: n_layer          = 24
0.00.039.906 I print_info: n_head           = 16
0.00.039.907 I print_info: n_head_kv        = 16
0.00.039.907 I print_info: n_rot            = 32
0.00.039.908 I print_info: n_swa            = 0
0.00.039.908 I print_info: n_embd_head_k    = 128
0.00.039.908 I print_info: n_embd_head_v    = 128
0.00.039.909 I print_info: n_gqa            = 1
0.00.039.909 I print_info: n_embd_k_gqa     = 2048
0.00.039.912 I print_info: n_embd_v_gqa     = 2048
0.00.039.913 I print_info: f_norm_eps       = 1.0e-05
0.00.039.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.913 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.913 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.914 I print_info: f_logit_scale    = 0.0e+00
0.00.039.914 I print_info: n_ff             = 8192
0.00.039.914 I print_info: n_expert         = 0
0.00.039.914 I print_info: n_expert_used    = 0
0.00.039.916 I print_info: causal attn      = 1
0.00.039.916 I print_info: pooling type     = 0
0.00.039.916 I print_info: rope type        = 2
0.00.039.916 I print_info: rope scaling     = linear
0.00.039.917 I print_info: freq_base_train  = 10000.0
0.00.039.917 I print_info: freq_scale_train = 1
0.00.039.917 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.918 I print_info: rope_finetuned   = unknown
0.00.039.918 I print_info: ssm_d_conv       = 0
0.00.039.918 I print_info: ssm_d_inner      = 0
0.00.039.918 I print_info: ssm_d_state      = 0
0.00.039.918 I print_info: ssm_dt_rank      = 0
0.00.039.918 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.919 I print_info: model type       = 1.4B
0.00.039.919 I print_info: model params     = 1.41 B
0.00.039.919 I print_info: general.name     = 1.4B
0.00.039.920 I print_info: vocab type       = BPE
0.00.039.920 I print_info: n_vocab          = 50304
0.00.039.920 I print_info: n_merges         = 50009
0.00.039.920 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: LF token         = 187 ''
0.00.039.922 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.922 I print_info: max token length = 1024
0.00.039.922 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.408 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.424 I load_tensors: offloading output layer to GPU
0.00.637.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.459 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.637.461 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.638.915 I llama_init_from_model: n_seq_max     = 1
0.00.638.917 I llama_init_from_model: n_ctx         = 2048
0.00.638.918 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.918 I llama_init_from_model: n_batch       = 2048
0.00.638.919 I llama_init_from_model: n_ubatch      = 512
0.00.638.919 I llama_init_from_model: flash_attn    = 0
0.00.638.921 I llama_init_from_model: freq_base     = 10000.0
0.00.638.921 I llama_init_from_model: freq_scale    = 1
0.00.638.924 I ggml_metal_init: allocating
0.00.639.004 I ggml_metal_init: found device: Apple M4
0.00.639.017 I ggml_metal_init: picking default device: Apple M4
0.00.640.920 I ggml_metal_init: using embedded metal library
0.00.647.586 I ggml_metal_init: GPU name:   Apple M4
0.00.647.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.592 I ggml_metal_init: simdgroup reduction   = true
0.00.647.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.593 I ggml_metal_init: has residency sets    = true
0.00.647.593 I ggml_metal_init: has bfloat            = true
0.00.647.593 I ggml_metal_init: use bfloat            = true
0.00.647.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.857 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.067 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.075 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.113 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.280 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.723.283 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.723.283 I llama_init_from_model: graph nodes  = 967
0.00.723.283 I llama_init_from_model: graph splits = 2
0.00.723.289 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.723.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.487 I main: llama threadpool init, n_threads = 4
0.00.780.531 I 
0.00.780.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.555 I 
0.00.780.705 I sampler seed: 1234
0.00.780.710 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.728 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.729 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.729 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.506.875 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.506.876 I llama_perf_context_print:        load time =     770.80 ms
0.01.506.876 I llama_perf_context_print: prompt eval time =      48.96 ms /     7 tokens (    6.99 ms per token,   142.97 tokens per second)
0.01.506.878 I llama_perf_context_print:        eval time =     674.37 ms /    63 runs   (   10.70 ms per token,    93.42 tokens per second)
0.01.506.879 I llama_perf_context_print:       total time =     727.12 ms /    70 tokens
0.01.507.166 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.505 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.449 I llama_model_loader: - type  f32:  194 tensors
0.00.026.449 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.450 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.450 I print_info: file format = GGUF V3 (latest)
0.00.026.451 I print_info: file type   = Q4_1
0.00.026.452 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.733 I load: special tokens cache size = 25
0.00.040.621 I load: token to piece cache size = 0.2984 MB
0.00.040.625 I print_info: arch             = gptneox
0.00.040.625 I print_info: vocab_only       = 0
0.00.040.626 I print_info: n_ctx_train      = 2048
0.00.040.626 I print_info: n_embd           = 2048
0.00.040.626 I print_info: n_layer          = 24
0.00.040.631 I print_info: n_head           = 16
0.00.040.633 I print_info: n_head_kv        = 16
0.00.040.634 I print_info: n_rot            = 32
0.00.040.634 I print_info: n_swa            = 0
0.00.040.634 I print_info: n_embd_head_k    = 128
0.00.040.634 I print_info: n_embd_head_v    = 128
0.00.040.635 I print_info: n_gqa            = 1
0.00.040.636 I print_info: n_embd_k_gqa     = 2048
0.00.040.636 I print_info: n_embd_v_gqa     = 2048
0.00.040.637 I print_info: f_norm_eps       = 1.0e-05
0.00.040.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.639 I print_info: f_logit_scale    = 0.0e+00
0.00.040.640 I print_info: n_ff             = 8192
0.00.040.641 I print_info: n_expert         = 0
0.00.040.641 I print_info: n_expert_used    = 0
0.00.040.641 I print_info: causal attn      = 1
0.00.040.641 I print_info: pooling type     = 0
0.00.040.641 I print_info: rope type        = 2
0.00.040.642 I print_info: rope scaling     = linear
0.00.040.642 I print_info: freq_base_train  = 10000.0
0.00.040.642 I print_info: freq_scale_train = 1
0.00.040.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.643 I print_info: rope_finetuned   = unknown
0.00.040.643 I print_info: ssm_d_conv       = 0
0.00.040.643 I print_info: ssm_d_inner      = 0
0.00.040.643 I print_info: ssm_d_state      = 0
0.00.040.643 I print_info: ssm_dt_rank      = 0
0.00.040.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.643 I print_info: model type       = 1.4B
0.00.040.644 I print_info: model params     = 1.41 B
0.00.040.644 I print_info: general.name     = 1.4B
0.00.040.644 I print_info: vocab type       = BPE
0.00.040.644 I print_info: n_vocab          = 50304
0.00.040.645 I print_info: n_merges         = 50009
0.00.040.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: LF token         = 187 ''
0.00.040.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.647 I print_info: max token length = 1024
0.00.040.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.651.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.298 I load_tensors: offloading output layer to GPU
0.00.651.299 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.321 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.651.322 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.652.230 I llama_init_from_model: n_seq_max     = 1
0.00.652.233 I llama_init_from_model: n_ctx         = 128
0.00.652.233 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.234 I llama_init_from_model: n_batch       = 128
0.00.652.234 I llama_init_from_model: n_ubatch      = 128
0.00.652.234 I llama_init_from_model: flash_attn    = 0
0.00.652.236 I llama_init_from_model: freq_base     = 10000.0
0.00.652.236 I llama_init_from_model: freq_scale    = 1
0.00.652.236 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.238 I ggml_metal_init: allocating
0.00.652.284 I ggml_metal_init: found device: Apple M4
0.00.652.295 I ggml_metal_init: picking default device: Apple M4
0.00.653.456 I ggml_metal_init: using embedded metal library
0.00.657.814 I ggml_metal_init: GPU name:   Apple M4
0.00.657.819 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.821 I ggml_metal_init: simdgroup reduction   = true
0.00.657.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.821 I ggml_metal_init: has residency sets    = true
0.00.657.821 I ggml_metal_init: has bfloat            = true
0.00.657.822 I ggml_metal_init: use bfloat            = true
0.00.657.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.680 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.300 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.303 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.331 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.933 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.934 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.934 I llama_init_from_model: graph nodes  = 967
0.00.676.935 I llama_init_from_model: graph splits = 2
0.00.676.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.908 I 
0.00.697.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.945 I perplexity: tokenizing the input ..
0.00.701.877 I perplexity: tokenization took 3.931 ms
0.00.701.880 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.407 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.824.808 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.824.831 I llama_perf_context_print:        load time =     689.08 ms
0.00.824.832 I llama_perf_context_print: prompt eval time =     121.30 ms /   128 tokens (    0.95 ms per token,  1055.25 tokens per second)
0.00.824.832 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.833 I llama_perf_context_print:       total time =     126.92 ms /   129 tokens
0.00.825.189 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.073s
sys	0m0.100s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.569 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.058 I llama_model_loader: - type  f32:  194 tensors
0.00.025.059 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.059 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.059 I print_info: file format = GGUF V3 (latest)
0.00.025.059 I print_info: file type   = Q5_0
0.00.025.060 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.858 I load: special tokens cache size = 25
0.00.038.890 I load: token to piece cache size = 0.2984 MB
0.00.038.893 I print_info: arch             = gptneox
0.00.038.893 I print_info: vocab_only       = 0
0.00.038.893 I print_info: n_ctx_train      = 2048
0.00.038.893 I print_info: n_embd           = 2048
0.00.038.894 I print_info: n_layer          = 24
0.00.038.896 I print_info: n_head           = 16
0.00.038.897 I print_info: n_head_kv        = 16
0.00.038.897 I print_info: n_rot            = 32
0.00.038.897 I print_info: n_swa            = 0
0.00.038.897 I print_info: n_embd_head_k    = 128
0.00.038.897 I print_info: n_embd_head_v    = 128
0.00.038.898 I print_info: n_gqa            = 1
0.00.038.899 I print_info: n_embd_k_gqa     = 2048
0.00.038.899 I print_info: n_embd_v_gqa     = 2048
0.00.038.900 I print_info: f_norm_eps       = 1.0e-05
0.00.038.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.901 I print_info: f_logit_scale    = 0.0e+00
0.00.038.902 I print_info: n_ff             = 8192
0.00.038.902 I print_info: n_expert         = 0
0.00.038.904 I print_info: n_expert_used    = 0
0.00.038.904 I print_info: causal attn      = 1
0.00.038.904 I print_info: pooling type     = 0
0.00.038.906 I print_info: rope type        = 2
0.00.038.907 I print_info: rope scaling     = linear
0.00.038.907 I print_info: freq_base_train  = 10000.0
0.00.038.908 I print_info: freq_scale_train = 1
0.00.038.908 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.908 I print_info: rope_finetuned   = unknown
0.00.038.908 I print_info: ssm_d_conv       = 0
0.00.038.908 I print_info: ssm_d_inner      = 0
0.00.038.909 I print_info: ssm_d_state      = 0
0.00.038.909 I print_info: ssm_dt_rank      = 0
0.00.038.909 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.909 I print_info: model type       = 1.4B
0.00.038.910 I print_info: model params     = 1.41 B
0.00.038.910 I print_info: general.name     = 1.4B
0.00.038.910 I print_info: vocab type       = BPE
0.00.038.910 I print_info: n_vocab          = 50304
0.00.038.910 I print_info: n_merges         = 50009
0.00.038.911 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.915 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.915 I print_info: LF token         = 187 ''
0.00.038.915 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.915 I print_info: max token length = 1024
0.00.038.916 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.023 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.043 I load_tensors: offloading output layer to GPU
0.00.642.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.077 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.642.083 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.643.710 I llama_init_from_model: n_seq_max     = 1
0.00.643.716 I llama_init_from_model: n_ctx         = 2048
0.00.643.717 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.717 I llama_init_from_model: n_batch       = 2048
0.00.643.718 I llama_init_from_model: n_ubatch      = 512
0.00.643.718 I llama_init_from_model: flash_attn    = 0
0.00.643.719 I llama_init_from_model: freq_base     = 10000.0
0.00.643.719 I llama_init_from_model: freq_scale    = 1
0.00.643.722 I ggml_metal_init: allocating
0.00.643.771 I ggml_metal_init: found device: Apple M4
0.00.643.784 I ggml_metal_init: picking default device: Apple M4
0.00.645.602 I ggml_metal_init: using embedded metal library
0.00.652.158 I ggml_metal_init: GPU name:   Apple M4
0.00.652.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.164 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.165 I ggml_metal_init: simdgroup reduction   = true
0.00.652.165 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.166 I ggml_metal_init: has residency sets    = true
0.00.652.166 I ggml_metal_init: has bfloat            = true
0.00.652.166 I ggml_metal_init: use bfloat            = true
0.00.652.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.180 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.922 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.929 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.264 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.265 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.265 I llama_init_from_model: graph nodes  = 967
0.00.725.265 I llama_init_from_model: graph splits = 2
0.00.725.274 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.977 I main: llama threadpool init, n_threads = 4
0.00.783.020 I 
0.00.783.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.044 I 
0.00.783.188 I sampler seed: 1234
0.00.783.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.232 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.564.660 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.564.661 I llama_perf_context_print:        load time =     773.51 ms
0.01.564.662 I llama_perf_context_print: prompt eval time =      43.14 ms /     7 tokens (    6.16 ms per token,   162.25 tokens per second)
0.01.564.662 I llama_perf_context_print:        eval time =     735.39 ms /    63 runs   (   11.67 ms per token,    85.67 tokens per second)
0.01.564.663 I llama_perf_context_print:       total time =     782.40 ms /    70 tokens
0.01.564.922 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.110s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.397 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.398 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.230 I llama_model_loader: - type  f32:  194 tensors
0.00.025.230 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.231 I print_info: file format = GGUF V3 (latest)
0.00.025.231 I print_info: file type   = Q5_0
0.00.025.233 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.702 I load: special tokens cache size = 25
0.00.039.534 I load: token to piece cache size = 0.2984 MB
0.00.039.538 I print_info: arch             = gptneox
0.00.039.539 I print_info: vocab_only       = 0
0.00.039.539 I print_info: n_ctx_train      = 2048
0.00.039.539 I print_info: n_embd           = 2048
0.00.039.539 I print_info: n_layer          = 24
0.00.039.544 I print_info: n_head           = 16
0.00.039.545 I print_info: n_head_kv        = 16
0.00.039.545 I print_info: n_rot            = 32
0.00.039.545 I print_info: n_swa            = 0
0.00.039.545 I print_info: n_embd_head_k    = 128
0.00.039.545 I print_info: n_embd_head_v    = 128
0.00.039.546 I print_info: n_gqa            = 1
0.00.039.547 I print_info: n_embd_k_gqa     = 2048
0.00.039.547 I print_info: n_embd_v_gqa     = 2048
0.00.039.548 I print_info: f_norm_eps       = 1.0e-05
0.00.039.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.548 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.548 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.549 I print_info: f_logit_scale    = 0.0e+00
0.00.039.549 I print_info: n_ff             = 8192
0.00.039.549 I print_info: n_expert         = 0
0.00.039.550 I print_info: n_expert_used    = 0
0.00.039.550 I print_info: causal attn      = 1
0.00.039.550 I print_info: pooling type     = 0
0.00.039.550 I print_info: rope type        = 2
0.00.039.550 I print_info: rope scaling     = linear
0.00.039.553 I print_info: freq_base_train  = 10000.0
0.00.039.554 I print_info: freq_scale_train = 1
0.00.039.555 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.555 I print_info: rope_finetuned   = unknown
0.00.039.555 I print_info: ssm_d_conv       = 0
0.00.039.555 I print_info: ssm_d_inner      = 0
0.00.039.555 I print_info: ssm_d_state      = 0
0.00.039.555 I print_info: ssm_dt_rank      = 0
0.00.039.555 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.556 I print_info: model type       = 1.4B
0.00.039.556 I print_info: model params     = 1.41 B
0.00.039.556 I print_info: general.name     = 1.4B
0.00.039.557 I print_info: vocab type       = BPE
0.00.039.557 I print_info: n_vocab          = 50304
0.00.039.557 I print_info: n_merges         = 50009
0.00.039.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.558 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.558 I print_info: LF token         = 187 ''
0.00.039.559 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.560 I print_info: max token length = 1024
0.00.039.560 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.937 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.955 I load_tensors: offloading output layer to GPU
0.00.640.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.987 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.640.988 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.642.207 I llama_init_from_model: n_seq_max     = 1
0.00.642.212 I llama_init_from_model: n_ctx         = 128
0.00.642.213 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.213 I llama_init_from_model: n_batch       = 128
0.00.642.214 I llama_init_from_model: n_ubatch      = 128
0.00.642.214 I llama_init_from_model: flash_attn    = 0
0.00.642.216 I llama_init_from_model: freq_base     = 10000.0
0.00.642.216 I llama_init_from_model: freq_scale    = 1
0.00.642.217 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.219 I ggml_metal_init: allocating
0.00.642.286 I ggml_metal_init: found device: Apple M4
0.00.642.301 I ggml_metal_init: picking default device: Apple M4
0.00.644.100 I ggml_metal_init: using embedded metal library
0.00.650.231 I ggml_metal_init: GPU name:   Apple M4
0.00.650.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.242 I ggml_metal_init: simdgroup reduction   = true
0.00.650.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.242 I ggml_metal_init: has residency sets    = true
0.00.650.243 I ggml_metal_init: has bfloat            = true
0.00.650.243 I ggml_metal_init: use bfloat            = true
0.00.650.244 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.247 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.705 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.015 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.064 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.447 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.449 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.450 I llama_init_from_model: graph nodes  = 967
0.00.676.450 I llama_init_from_model: graph splits = 2
0.00.676.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.660 I 
0.00.705.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.725 I perplexity: tokenizing the input ..
0.00.710.529 I perplexity: tokenization took 4.803 ms
0.00.710.533 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.030 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.859.370 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.859.396 I llama_perf_context_print:        load time =     696.75 ms
0.00.859.397 I llama_perf_context_print: prompt eval time =     147.27 ms /   128 tokens (    1.15 ms per token,   869.17 tokens per second)
0.00.859.397 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.398 I llama_perf_context_print:       total time =     153.74 ms /   129 tokens
0.00.859.800 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.078s
sys	0m0.115s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.231 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.760 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.262 I llama_model_loader: - type  f32:  194 tensors
0.00.026.263 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.264 I print_info: file format = GGUF V3 (latest)
0.00.026.264 I print_info: file type   = Q5_1
0.00.026.265 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.411 I load: special tokens cache size = 25
0.00.040.392 I load: token to piece cache size = 0.2984 MB
0.00.040.395 I print_info: arch             = gptneox
0.00.040.395 I print_info: vocab_only       = 0
0.00.040.395 I print_info: n_ctx_train      = 2048
0.00.040.395 I print_info: n_embd           = 2048
0.00.040.395 I print_info: n_layer          = 24
0.00.040.398 I print_info: n_head           = 16
0.00.040.399 I print_info: n_head_kv        = 16
0.00.040.399 I print_info: n_rot            = 32
0.00.040.399 I print_info: n_swa            = 0
0.00.040.399 I print_info: n_embd_head_k    = 128
0.00.040.401 I print_info: n_embd_head_v    = 128
0.00.040.402 I print_info: n_gqa            = 1
0.00.040.403 I print_info: n_embd_k_gqa     = 2048
0.00.040.403 I print_info: n_embd_v_gqa     = 2048
0.00.040.404 I print_info: f_norm_eps       = 1.0e-05
0.00.040.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.405 I print_info: f_logit_scale    = 0.0e+00
0.00.040.406 I print_info: n_ff             = 8192
0.00.040.406 I print_info: n_expert         = 0
0.00.040.406 I print_info: n_expert_used    = 0
0.00.040.406 I print_info: causal attn      = 1
0.00.040.406 I print_info: pooling type     = 0
0.00.040.406 I print_info: rope type        = 2
0.00.040.406 I print_info: rope scaling     = linear
0.00.040.411 I print_info: freq_base_train  = 10000.0
0.00.040.411 I print_info: freq_scale_train = 1
0.00.040.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.412 I print_info: rope_finetuned   = unknown
0.00.040.412 I print_info: ssm_d_conv       = 0
0.00.040.412 I print_info: ssm_d_inner      = 0
0.00.040.412 I print_info: ssm_d_state      = 0
0.00.040.412 I print_info: ssm_dt_rank      = 0
0.00.040.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.413 I print_info: model type       = 1.4B
0.00.040.414 I print_info: model params     = 1.41 B
0.00.040.414 I print_info: general.name     = 1.4B
0.00.040.415 I print_info: vocab type       = BPE
0.00.040.415 I print_info: n_vocab          = 50304
0.00.040.415 I print_info: n_merges         = 50009
0.00.040.415 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: LF token         = 187 ''
0.00.040.416 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.417 I print_info: max token length = 1024
0.00.040.417 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.337 I load_tensors: offloading output layer to GPU
0.00.596.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.372 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.596.373 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.597.997 I llama_init_from_model: n_seq_max     = 1
0.00.598.000 I llama_init_from_model: n_ctx         = 2048
0.00.598.001 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.598.002 I llama_init_from_model: n_batch       = 2048
0.00.598.002 I llama_init_from_model: n_ubatch      = 512
0.00.598.002 I llama_init_from_model: flash_attn    = 0
0.00.598.005 I llama_init_from_model: freq_base     = 10000.0
0.00.598.005 I llama_init_from_model: freq_scale    = 1
0.00.598.008 I ggml_metal_init: allocating
0.00.598.086 I ggml_metal_init: found device: Apple M4
0.00.598.099 I ggml_metal_init: picking default device: Apple M4
0.00.600.051 I ggml_metal_init: using embedded metal library
0.00.606.698 I ggml_metal_init: GPU name:   Apple M4
0.00.606.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.704 I ggml_metal_init: simdgroup reduction   = true
0.00.606.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.704 I ggml_metal_init: has residency sets    = true
0.00.606.705 I ggml_metal_init: has bfloat            = true
0.00.606.705 I ggml_metal_init: use bfloat            = true
0.00.606.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.216 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.563 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.570 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.614 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.353 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.356 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.356 I llama_init_from_model: graph nodes  = 967
0.00.682.356 I llama_init_from_model: graph splits = 2
0.00.682.363 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.483 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.844 I main: llama threadpool init, n_threads = 4
0.00.741.891 I 
0.00.741.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.916 I 
0.00.742.064 I sampler seed: 1234
0.00.742.069 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.090 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.090 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.090 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.591.552 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.591.554 I llama_perf_context_print:        load time =     730.88 ms
0.01.591.554 I llama_perf_context_print: prompt eval time =      49.79 ms /     7 tokens (    7.11 ms per token,   140.60 tokens per second)
0.01.591.556 I llama_perf_context_print:        eval time =     796.79 ms /    63 runs   (   12.65 ms per token,    79.07 tokens per second)
0.01.591.556 I llama_perf_context_print:       total time =     850.44 ms /    70 tokens
0.01.591.774 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.109s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.917 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.918 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.918 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.502 I llama_model_loader: - type  f32:  194 tensors
0.00.025.502 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.503 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.503 I print_info: file format = GGUF V3 (latest)
0.00.025.504 I print_info: file type   = Q5_1
0.00.025.506 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.956 I load: special tokens cache size = 25
0.00.039.755 I load: token to piece cache size = 0.2984 MB
0.00.039.759 I print_info: arch             = gptneox
0.00.039.760 I print_info: vocab_only       = 0
0.00.039.760 I print_info: n_ctx_train      = 2048
0.00.039.760 I print_info: n_embd           = 2048
0.00.039.760 I print_info: n_layer          = 24
0.00.039.765 I print_info: n_head           = 16
0.00.039.765 I print_info: n_head_kv        = 16
0.00.039.766 I print_info: n_rot            = 32
0.00.039.766 I print_info: n_swa            = 0
0.00.039.766 I print_info: n_embd_head_k    = 128
0.00.039.766 I print_info: n_embd_head_v    = 128
0.00.039.767 I print_info: n_gqa            = 1
0.00.039.768 I print_info: n_embd_k_gqa     = 2048
0.00.039.768 I print_info: n_embd_v_gqa     = 2048
0.00.039.769 I print_info: f_norm_eps       = 1.0e-05
0.00.039.769 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.770 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.770 I print_info: f_logit_scale    = 0.0e+00
0.00.039.771 I print_info: n_ff             = 8192
0.00.039.771 I print_info: n_expert         = 0
0.00.039.771 I print_info: n_expert_used    = 0
0.00.039.771 I print_info: causal attn      = 1
0.00.039.771 I print_info: pooling type     = 0
0.00.039.771 I print_info: rope type        = 2
0.00.039.772 I print_info: rope scaling     = linear
0.00.039.772 I print_info: freq_base_train  = 10000.0
0.00.039.773 I print_info: freq_scale_train = 1
0.00.039.773 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.773 I print_info: rope_finetuned   = unknown
0.00.039.773 I print_info: ssm_d_conv       = 0
0.00.039.773 I print_info: ssm_d_inner      = 0
0.00.039.773 I print_info: ssm_d_state      = 0
0.00.039.773 I print_info: ssm_dt_rank      = 0
0.00.039.773 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.776 I print_info: model type       = 1.4B
0.00.039.776 I print_info: model params     = 1.41 B
0.00.039.776 I print_info: general.name     = 1.4B
0.00.039.777 I print_info: vocab type       = BPE
0.00.039.777 I print_info: n_vocab          = 50304
0.00.039.777 I print_info: n_merges         = 50009
0.00.039.777 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: LF token         = 187 ''
0.00.039.778 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: max token length = 1024
0.00.039.779 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.816 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.833 I load_tensors: offloading output layer to GPU
0.00.608.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.872 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.874 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.576 I llama_init_from_model: n_seq_max     = 1
0.00.610.578 I llama_init_from_model: n_ctx         = 128
0.00.610.579 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.579 I llama_init_from_model: n_batch       = 128
0.00.610.580 I llama_init_from_model: n_ubatch      = 128
0.00.610.580 I llama_init_from_model: flash_attn    = 0
0.00.610.582 I llama_init_from_model: freq_base     = 10000.0
0.00.610.583 I llama_init_from_model: freq_scale    = 1
0.00.610.584 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.593 I ggml_metal_init: allocating
0.00.610.678 I ggml_metal_init: found device: Apple M4
0.00.610.691 I ggml_metal_init: picking default device: Apple M4
0.00.612.244 I ggml_metal_init: using embedded metal library
0.00.618.523 I ggml_metal_init: GPU name:   Apple M4
0.00.618.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.527 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.528 I ggml_metal_init: simdgroup reduction   = true
0.00.618.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.529 I ggml_metal_init: has residency sets    = true
0.00.618.529 I ggml_metal_init: has bfloat            = true
0.00.618.529 I ggml_metal_init: use bfloat            = true
0.00.618.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.592 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.998 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.004 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.048 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.311 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.311 I llama_init_from_model: graph nodes  = 967
0.00.642.312 I llama_init_from_model: graph splits = 2
0.00.642.314 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.314 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.884 I 
0.00.675.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.979 I perplexity: tokenizing the input ..
0.00.682.812 I perplexity: tokenization took 6.828 ms
0.00.682.819 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.641 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.825.969 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.825.994 I llama_perf_context_print:        load time =     666.27 ms
0.00.825.995 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.37 tokens per second)
0.00.825.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.996 I llama_perf_context_print:       total time =     150.11 ms /   129 tokens
0.00.826.372 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.153s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.566 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.568 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.125 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.127 I llama_model_loader: - type  f32:  194 tensors
0.00.024.127 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.127 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.128 I print_info: file format = GGUF V3 (latest)
0.00.024.129 I print_info: file type   = Q2_K - Medium
0.00.024.129 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.381 I load: special tokens cache size = 25
0.00.038.452 I load: token to piece cache size = 0.2984 MB
0.00.038.455 I print_info: arch             = gptneox
0.00.038.455 I print_info: vocab_only       = 0
0.00.038.455 I print_info: n_ctx_train      = 2048
0.00.038.455 I print_info: n_embd           = 2048
0.00.038.456 I print_info: n_layer          = 24
0.00.038.458 I print_info: n_head           = 16
0.00.038.459 I print_info: n_head_kv        = 16
0.00.038.460 I print_info: n_rot            = 32
0.00.038.460 I print_info: n_swa            = 0
0.00.038.460 I print_info: n_embd_head_k    = 128
0.00.038.460 I print_info: n_embd_head_v    = 128
0.00.038.461 I print_info: n_gqa            = 1
0.00.038.462 I print_info: n_embd_k_gqa     = 2048
0.00.038.463 I print_info: n_embd_v_gqa     = 2048
0.00.038.463 I print_info: f_norm_eps       = 1.0e-05
0.00.038.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.464 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.464 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.464 I print_info: f_logit_scale    = 0.0e+00
0.00.038.465 I print_info: n_ff             = 8192
0.00.038.465 I print_info: n_expert         = 0
0.00.038.465 I print_info: n_expert_used    = 0
0.00.038.465 I print_info: causal attn      = 1
0.00.038.465 I print_info: pooling type     = 0
0.00.038.465 I print_info: rope type        = 2
0.00.038.468 I print_info: rope scaling     = linear
0.00.038.468 I print_info: freq_base_train  = 10000.0
0.00.038.469 I print_info: freq_scale_train = 1
0.00.038.469 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.470 I print_info: rope_finetuned   = unknown
0.00.038.470 I print_info: ssm_d_conv       = 0
0.00.038.470 I print_info: ssm_d_inner      = 0
0.00.038.471 I print_info: ssm_d_state      = 0
0.00.038.471 I print_info: ssm_dt_rank      = 0
0.00.038.471 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.471 I print_info: model type       = 1.4B
0.00.038.472 I print_info: model params     = 1.41 B
0.00.038.472 I print_info: general.name     = 1.4B
0.00.038.472 I print_info: vocab type       = BPE
0.00.038.472 I print_info: n_vocab          = 50304
0.00.038.472 I print_info: n_merges         = 50009
0.00.038.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.473 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.477 I print_info: LF token         = 187 ''
0.00.038.478 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.479 I print_info: max token length = 1024
0.00.038.480 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.014 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.029 I load_tensors: offloading output layer to GPU
0.00.336.030 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.066 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.068 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.737 I llama_init_from_model: n_seq_max     = 1
0.00.337.740 I llama_init_from_model: n_ctx         = 2048
0.00.337.741 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.741 I llama_init_from_model: n_batch       = 2048
0.00.337.741 I llama_init_from_model: n_ubatch      = 512
0.00.337.742 I llama_init_from_model: flash_attn    = 0
0.00.337.744 I llama_init_from_model: freq_base     = 10000.0
0.00.337.744 I llama_init_from_model: freq_scale    = 1
0.00.337.747 I ggml_metal_init: allocating
0.00.337.861 I ggml_metal_init: found device: Apple M4
0.00.337.875 I ggml_metal_init: picking default device: Apple M4
0.00.339.800 I ggml_metal_init: using embedded metal library
0.00.345.490 I ggml_metal_init: GPU name:   Apple M4
0.00.345.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.502 I ggml_metal_init: simdgroup reduction   = true
0.00.345.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.502 I ggml_metal_init: has residency sets    = true
0.00.345.502 I ggml_metal_init: has bfloat            = true
0.00.345.503 I ggml_metal_init: use bfloat            = true
0.00.345.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.492 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.423.363 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.423.370 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.423.411 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.427.652 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.427.654 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.427.655 I llama_init_from_model: graph nodes  = 967
0.00.427.655 I llama_init_from_model: graph splits = 2
0.00.427.662 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.427.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.427.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.367 I main: llama threadpool init, n_threads = 4
0.00.487.415 I 
0.00.487.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.440 I 
0.00.487.620 I sampler seed: 1234
0.00.487.625 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.677 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.681 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.681 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.170.536 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.170.537 I llama_perf_context_print:        load time =     477.82 ms
0.01.170.537 I llama_perf_context_print: prompt eval time =      44.53 ms /     7 tokens (    6.36 ms per token,   157.21 tokens per second)
0.01.170.538 I llama_perf_context_print:        eval time =     635.42 ms /    63 runs   (   10.09 ms per token,    99.15 tokens per second)
0.01.170.538 I llama_perf_context_print:       total time =     683.90 ms /    70 tokens
0.01.170.761 I ggml_metal_free: deallocating

real	0m1.187s
user	0m0.113s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.903 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.917 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.924 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.680 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.493 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.495 I llama_model_loader: - type  f32:  194 tensors
0.00.024.495 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.495 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.496 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.496 I print_info: file format = GGUF V3 (latest)
0.00.024.497 I print_info: file type   = Q2_K - Medium
0.00.024.498 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.577 I load: special tokens cache size = 25
0.00.038.645 I load: token to piece cache size = 0.2984 MB
0.00.038.649 I print_info: arch             = gptneox
0.00.038.650 I print_info: vocab_only       = 0
0.00.038.650 I print_info: n_ctx_train      = 2048
0.00.038.650 I print_info: n_embd           = 2048
0.00.038.650 I print_info: n_layer          = 24
0.00.038.655 I print_info: n_head           = 16
0.00.038.656 I print_info: n_head_kv        = 16
0.00.038.656 I print_info: n_rot            = 32
0.00.038.656 I print_info: n_swa            = 0
0.00.038.656 I print_info: n_embd_head_k    = 128
0.00.038.656 I print_info: n_embd_head_v    = 128
0.00.038.657 I print_info: n_gqa            = 1
0.00.038.658 I print_info: n_embd_k_gqa     = 2048
0.00.038.658 I print_info: n_embd_v_gqa     = 2048
0.00.038.659 I print_info: f_norm_eps       = 1.0e-05
0.00.038.659 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.660 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.660 I print_info: f_logit_scale    = 0.0e+00
0.00.038.660 I print_info: n_ff             = 8192
0.00.038.660 I print_info: n_expert         = 0
0.00.038.661 I print_info: n_expert_used    = 0
0.00.038.661 I print_info: causal attn      = 1
0.00.038.661 I print_info: pooling type     = 0
0.00.038.661 I print_info: rope type        = 2
0.00.038.661 I print_info: rope scaling     = linear
0.00.038.662 I print_info: freq_base_train  = 10000.0
0.00.038.662 I print_info: freq_scale_train = 1
0.00.038.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.662 I print_info: rope_finetuned   = unknown
0.00.038.662 I print_info: ssm_d_conv       = 0
0.00.038.663 I print_info: ssm_d_inner      = 0
0.00.038.663 I print_info: ssm_d_state      = 0
0.00.038.663 I print_info: ssm_dt_rank      = 0
0.00.038.663 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.663 I print_info: model type       = 1.4B
0.00.038.663 I print_info: model params     = 1.41 B
0.00.038.664 I print_info: general.name     = 1.4B
0.00.038.664 I print_info: vocab type       = BPE
0.00.038.664 I print_info: n_vocab          = 50304
0.00.038.664 I print_info: n_merges         = 50009
0.00.038.665 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.665 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.666 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.666 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.667 I print_info: LF token         = 187 ''
0.00.038.667 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.667 I print_info: max token length = 1024
0.00.038.667 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.903 I load_tensors: offloading 24 repeating layers to GPU
0.00.329.918 I load_tensors: offloading output layer to GPU
0.00.329.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.329.955 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.329.957 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.331.772 I llama_init_from_model: n_seq_max     = 1
0.00.331.776 I llama_init_from_model: n_ctx         = 128
0.00.331.776 I llama_init_from_model: n_ctx_per_seq = 128
0.00.331.777 I llama_init_from_model: n_batch       = 128
0.00.331.777 I llama_init_from_model: n_ubatch      = 128
0.00.331.777 I llama_init_from_model: flash_attn    = 0
0.00.331.780 I llama_init_from_model: freq_base     = 10000.0
0.00.331.780 I llama_init_from_model: freq_scale    = 1
0.00.331.781 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.331.783 I ggml_metal_init: allocating
0.00.331.868 I ggml_metal_init: found device: Apple M4
0.00.331.882 I ggml_metal_init: picking default device: Apple M4
0.00.333.677 I ggml_metal_init: using embedded metal library
0.00.339.109 I ggml_metal_init: GPU name:   Apple M4
0.00.339.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.126 I ggml_metal_init: simdgroup reduction   = true
0.00.339.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.127 I ggml_metal_init: has residency sets    = true
0.00.339.127 I ggml_metal_init: has bfloat            = true
0.00.339.128 I ggml_metal_init: use bfloat            = true
0.00.339.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.133 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.361.474 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.365.290 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.365.301 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.365.402 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.368.729 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.368.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.368.732 I llama_init_from_model: graph nodes  = 967
0.00.368.732 I llama_init_from_model: graph splits = 2
0.00.368.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.368.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.703 I 
0.00.401.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.803 I perplexity: tokenizing the input ..
0.00.408.255 I perplexity: tokenization took 6.454 ms
0.00.408.262 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.553.913 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.254 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.276 I llama_perf_context_print:        load time =     392.67 ms
0.00.555.277 I llama_perf_context_print: prompt eval time =     145.11 ms /   128 tokens (    1.13 ms per token,   882.11 tokens per second)
0.00.555.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.283 I llama_perf_context_print:       total time =     153.58 ms /   129 tokens
0.00.555.651 I ggml_metal_free: deallocating

real	0m0.569s
user	0m0.081s
sys	0m0.086s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.500 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.501 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.502 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.502 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.907 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.908 I llama_model_loader: - type  f32:  194 tensors
0.00.026.908 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.908 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.908 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.909 I print_info: file format = GGUF V3 (latest)
0.00.026.909 I print_info: file type   = Q3_K - Medium
0.00.026.910 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.814 I load: special tokens cache size = 25
0.00.040.816 I load: token to piece cache size = 0.2984 MB
0.00.040.819 I print_info: arch             = gptneox
0.00.040.819 I print_info: vocab_only       = 0
0.00.040.819 I print_info: n_ctx_train      = 2048
0.00.040.820 I print_info: n_embd           = 2048
0.00.040.820 I print_info: n_layer          = 24
0.00.040.823 I print_info: n_head           = 16
0.00.040.823 I print_info: n_head_kv        = 16
0.00.040.824 I print_info: n_rot            = 32
0.00.040.824 I print_info: n_swa            = 0
0.00.040.824 I print_info: n_embd_head_k    = 128
0.00.040.824 I print_info: n_embd_head_v    = 128
0.00.040.825 I print_info: n_gqa            = 1
0.00.040.825 I print_info: n_embd_k_gqa     = 2048
0.00.040.826 I print_info: n_embd_v_gqa     = 2048
0.00.040.827 I print_info: f_norm_eps       = 1.0e-05
0.00.040.827 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.827 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.827 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.828 I print_info: f_logit_scale    = 0.0e+00
0.00.040.828 I print_info: n_ff             = 8192
0.00.040.829 I print_info: n_expert         = 0
0.00.040.829 I print_info: n_expert_used    = 0
0.00.040.829 I print_info: causal attn      = 1
0.00.040.829 I print_info: pooling type     = 0
0.00.040.829 I print_info: rope type        = 2
0.00.040.829 I print_info: rope scaling     = linear
0.00.040.830 I print_info: freq_base_train  = 10000.0
0.00.040.830 I print_info: freq_scale_train = 1
0.00.040.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.830 I print_info: rope_finetuned   = unknown
0.00.040.830 I print_info: ssm_d_conv       = 0
0.00.040.835 I print_info: ssm_d_inner      = 0
0.00.040.835 I print_info: ssm_d_state      = 0
0.00.040.835 I print_info: ssm_dt_rank      = 0
0.00.040.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.835 I print_info: model type       = 1.4B
0.00.040.836 I print_info: model params     = 1.41 B
0.00.040.836 I print_info: general.name     = 1.4B
0.00.040.836 I print_info: vocab type       = BPE
0.00.040.837 I print_info: n_vocab          = 50304
0.00.040.837 I print_info: n_merges         = 50009
0.00.040.837 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.838 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.839 I print_info: LF token         = 187 ''
0.00.040.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: max token length = 1024
0.00.040.840 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.693 I load_tensors: offloading output layer to GPU
0.00.443.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.727 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.728 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.279 I llama_init_from_model: n_seq_max     = 1
0.00.445.281 I llama_init_from_model: n_ctx         = 2048
0.00.445.282 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.445.283 I llama_init_from_model: n_batch       = 2048
0.00.445.283 I llama_init_from_model: n_ubatch      = 512
0.00.445.283 I llama_init_from_model: flash_attn    = 0
0.00.445.286 I llama_init_from_model: freq_base     = 10000.0
0.00.445.286 I llama_init_from_model: freq_scale    = 1
0.00.445.288 I ggml_metal_init: allocating
0.00.445.361 I ggml_metal_init: found device: Apple M4
0.00.445.375 I ggml_metal_init: picking default device: Apple M4
0.00.447.249 I ggml_metal_init: using embedded metal library
0.00.453.145 I ggml_metal_init: GPU name:   Apple M4
0.00.453.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.152 I ggml_metal_init: simdgroup reduction   = true
0.00.453.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.152 I ggml_metal_init: has residency sets    = true
0.00.453.153 I ggml_metal_init: has bfloat            = true
0.00.453.153 I ggml_metal_init: use bfloat            = true
0.00.453.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.398 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.824 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.527.830 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.527.865 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.532.486 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.532.488 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.532.488 I llama_init_from_model: graph nodes  = 967
0.00.532.488 I llama_init_from_model: graph splits = 2
0.00.532.494 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.532.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.532.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.731 I main: llama threadpool init, n_threads = 4
0.00.586.771 I 
0.00.586.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.793 I 
0.00.586.945 I sampler seed: 1234
0.00.586.949 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.586.959 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.586.960 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.586.960 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.320.480 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.320.481 I llama_perf_context_print:        load time =     575.22 ms
0.01.320.483 I llama_perf_context_print: prompt eval time =      40.21 ms /     7 tokens (    5.74 ms per token,   174.07 tokens per second)
0.01.320.484 I llama_perf_context_print:        eval time =     690.47 ms /    63 runs   (   10.96 ms per token,    91.24 tokens per second)
0.01.320.484 I llama_perf_context_print:       total time =     734.53 ms /    70 tokens
0.01.320.736 I ggml_metal_free: deallocating

real	0m1.336s
user	0m0.110s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.061 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.825 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.825 I llama_model_loader: - type  f32:  194 tensors
0.00.024.825 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.826 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.826 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.826 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.827 I print_info: file format = GGUF V3 (latest)
0.00.024.827 I print_info: file type   = Q3_K - Medium
0.00.024.828 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.878 I load: special tokens cache size = 25
0.00.039.107 I load: token to piece cache size = 0.2984 MB
0.00.039.111 I print_info: arch             = gptneox
0.00.039.111 I print_info: vocab_only       = 0
0.00.039.112 I print_info: n_ctx_train      = 2048
0.00.039.112 I print_info: n_embd           = 2048
0.00.039.112 I print_info: n_layer          = 24
0.00.039.116 I print_info: n_head           = 16
0.00.039.117 I print_info: n_head_kv        = 16
0.00.039.120 I print_info: n_rot            = 32
0.00.039.120 I print_info: n_swa            = 0
0.00.039.121 I print_info: n_embd_head_k    = 128
0.00.039.121 I print_info: n_embd_head_v    = 128
0.00.039.121 I print_info: n_gqa            = 1
0.00.039.122 I print_info: n_embd_k_gqa     = 2048
0.00.039.122 I print_info: n_embd_v_gqa     = 2048
0.00.039.123 I print_info: f_norm_eps       = 1.0e-05
0.00.039.124 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.124 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.125 I print_info: f_logit_scale    = 0.0e+00
0.00.039.126 I print_info: n_ff             = 8192
0.00.039.126 I print_info: n_expert         = 0
0.00.039.126 I print_info: n_expert_used    = 0
0.00.039.126 I print_info: causal attn      = 1
0.00.039.127 I print_info: pooling type     = 0
0.00.039.127 I print_info: rope type        = 2
0.00.039.128 I print_info: rope scaling     = linear
0.00.039.128 I print_info: freq_base_train  = 10000.0
0.00.039.133 I print_info: freq_scale_train = 1
0.00.039.135 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.136 I print_info: rope_finetuned   = unknown
0.00.039.136 I print_info: ssm_d_conv       = 0
0.00.039.136 I print_info: ssm_d_inner      = 0
0.00.039.136 I print_info: ssm_d_state      = 0
0.00.039.136 I print_info: ssm_dt_rank      = 0
0.00.039.136 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.137 I print_info: model type       = 1.4B
0.00.039.137 I print_info: model params     = 1.41 B
0.00.039.137 I print_info: general.name     = 1.4B
0.00.039.138 I print_info: vocab type       = BPE
0.00.039.138 I print_info: n_vocab          = 50304
0.00.039.138 I print_info: n_merges         = 50009
0.00.039.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: LF token         = 187 ''
0.00.039.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.140 I print_info: max token length = 1024
0.00.039.140 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.610 I load_tensors: offloading output layer to GPU
0.00.441.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.629 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.630 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.556 I llama_init_from_model: n_seq_max     = 1
0.00.442.559 I llama_init_from_model: n_ctx         = 128
0.00.442.559 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.560 I llama_init_from_model: n_batch       = 128
0.00.442.560 I llama_init_from_model: n_ubatch      = 128
0.00.442.560 I llama_init_from_model: flash_attn    = 0
0.00.442.562 I llama_init_from_model: freq_base     = 10000.0
0.00.442.562 I llama_init_from_model: freq_scale    = 1
0.00.442.563 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.564 I ggml_metal_init: allocating
0.00.442.602 I ggml_metal_init: found device: Apple M4
0.00.442.613 I ggml_metal_init: picking default device: Apple M4
0.00.443.623 I ggml_metal_init: using embedded metal library
0.00.447.727 I ggml_metal_init: GPU name:   Apple M4
0.00.447.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.738 I ggml_metal_init: simdgroup reduction   = true
0.00.447.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.738 I ggml_metal_init: has residency sets    = true
0.00.447.738 I ggml_metal_init: has bfloat            = true
0.00.447.739 I ggml_metal_init: use bfloat            = true
0.00.447.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.465.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.465.665 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.465.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.320 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.467.321 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.467.322 I llama_init_from_model: graph nodes  = 967
0.00.467.322 I llama_init_from_model: graph splits = 2
0.00.467.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.586 I 
0.00.494.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.627 I perplexity: tokenizing the input ..
0.00.498.569 I perplexity: tokenization took 3.94 ms
0.00.498.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.653 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.639.849 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.639.871 I llama_perf_context_print:        load time =     485.67 ms
0.00.639.872 I llama_perf_context_print: prompt eval time =     139.85 ms /   128 tokens (    1.09 ms per token,   915.27 tokens per second)
0.00.639.873 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.873 I llama_perf_context_print:       total time =     145.29 ms /   129 tokens
0.00.640.201 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.070s
sys	0m0.093s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.596 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.003 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.638 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.639 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.639 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.640 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.640 I llama_model_loader: - type  f32:  194 tensors
0.00.025.641 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.641 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.641 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.642 I print_info: file format = GGUF V3 (latest)
0.00.025.642 I print_info: file type   = Q4_K - Medium
0.00.025.644 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.424 I load: special tokens cache size = 25
0.00.039.464 I load: token to piece cache size = 0.2984 MB
0.00.039.467 I print_info: arch             = gptneox
0.00.039.467 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.472 I print_info: n_head_kv        = 16
0.00.039.472 I print_info: n_rot            = 32
0.00.039.472 I print_info: n_swa            = 0
0.00.039.472 I print_info: n_embd_head_k    = 128
0.00.039.472 I print_info: n_embd_head_v    = 128
0.00.039.473 I print_info: n_gqa            = 1
0.00.039.474 I print_info: n_embd_k_gqa     = 2048
0.00.039.475 I print_info: n_embd_v_gqa     = 2048
0.00.039.477 I print_info: f_norm_eps       = 1.0e-05
0.00.039.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.478 I print_info: f_logit_scale    = 0.0e+00
0.00.039.479 I print_info: n_ff             = 8192
0.00.039.479 I print_info: n_expert         = 0
0.00.039.479 I print_info: n_expert_used    = 0
0.00.039.479 I print_info: causal attn      = 1
0.00.039.480 I print_info: pooling type     = 0
0.00.039.481 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.482 I print_info: freq_base_train  = 10000.0
0.00.039.482 I print_info: freq_scale_train = 1
0.00.039.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.483 I print_info: ssm_d_conv       = 0
0.00.039.483 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.483 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.485 I print_info: vocab type       = BPE
0.00.039.485 I print_info: n_vocab          = 50304
0.00.039.485 I print_info: n_merges         = 50009
0.00.039.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: LF token         = 187 ''
0.00.039.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: max token length = 1024
0.00.039.491 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.879 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.895 I load_tensors: offloading output layer to GPU
0.00.533.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.923 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.925 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.535.174 I llama_init_from_model: n_seq_max     = 1
0.00.535.178 I llama_init_from_model: n_ctx         = 2048
0.00.535.178 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.535.179 I llama_init_from_model: n_batch       = 2048
0.00.535.179 I llama_init_from_model: n_ubatch      = 512
0.00.535.179 I llama_init_from_model: flash_attn    = 0
0.00.535.182 I llama_init_from_model: freq_base     = 10000.0
0.00.535.182 I llama_init_from_model: freq_scale    = 1
0.00.535.186 I ggml_metal_init: allocating
0.00.535.251 I ggml_metal_init: found device: Apple M4
0.00.535.265 I ggml_metal_init: picking default device: Apple M4
0.00.537.149 I ggml_metal_init: using embedded metal library
0.00.542.836 I ggml_metal_init: GPU name:   Apple M4
0.00.542.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.850 I ggml_metal_init: simdgroup reduction   = true
0.00.542.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.850 I ggml_metal_init: has residency sets    = true
0.00.542.850 I ggml_metal_init: has bfloat            = true
0.00.542.851 I ggml_metal_init: use bfloat            = true
0.00.542.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.860 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.564.208 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.356 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.619.365 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.619.409 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.931 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.623.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.623.933 I llama_init_from_model: graph nodes  = 967
0.00.623.933 I llama_init_from_model: graph splits = 2
0.00.623.938 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.553 I main: llama threadpool init, n_threads = 4
0.00.679.594 I 
0.00.679.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.620 I 
0.00.679.759 I sampler seed: 1234
0.00.679.764 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.775 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.776 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.776 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.430.866 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48135.59 tokens per second)
0.01.430.867 I llama_perf_context_print:        load time =     669.19 ms
0.01.430.867 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.20 tokens per second)
0.01.430.868 I llama_perf_context_print:        eval time =     700.87 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.430.868 I llama_perf_context_print:       total time =     752.08 ms /    70 tokens
0.01.431.097 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.111s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.735 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.819 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.827 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.828 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.546 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.276 I llama_model_loader: - type  f32:  194 tensors
0.00.025.277 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.277 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.277 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.278 I print_info: file format = GGUF V3 (latest)
0.00.025.278 I print_info: file type   = Q4_K - Medium
0.00.025.279 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.486 I load: special tokens cache size = 25
0.00.039.351 I load: token to piece cache size = 0.2984 MB
0.00.039.354 I print_info: arch             = gptneox
0.00.039.354 I print_info: vocab_only       = 0
0.00.039.355 I print_info: n_ctx_train      = 2048
0.00.039.355 I print_info: n_embd           = 2048
0.00.039.355 I print_info: n_layer          = 24
0.00.039.357 I print_info: n_head           = 16
0.00.039.358 I print_info: n_head_kv        = 16
0.00.039.358 I print_info: n_rot            = 32
0.00.039.358 I print_info: n_swa            = 0
0.00.039.359 I print_info: n_embd_head_k    = 128
0.00.039.359 I print_info: n_embd_head_v    = 128
0.00.039.360 I print_info: n_gqa            = 1
0.00.039.360 I print_info: n_embd_k_gqa     = 2048
0.00.039.361 I print_info: n_embd_v_gqa     = 2048
0.00.039.362 I print_info: f_norm_eps       = 1.0e-05
0.00.039.362 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.363 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.363 I print_info: f_logit_scale    = 0.0e+00
0.00.039.363 I print_info: n_ff             = 8192
0.00.039.363 I print_info: n_expert         = 0
0.00.039.364 I print_info: n_expert_used    = 0
0.00.039.364 I print_info: causal attn      = 1
0.00.039.364 I print_info: pooling type     = 0
0.00.039.364 I print_info: rope type        = 2
0.00.039.364 I print_info: rope scaling     = linear
0.00.039.365 I print_info: freq_base_train  = 10000.0
0.00.039.365 I print_info: freq_scale_train = 1
0.00.039.367 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.368 I print_info: rope_finetuned   = unknown
0.00.039.368 I print_info: ssm_d_conv       = 0
0.00.039.368 I print_info: ssm_d_inner      = 0
0.00.039.368 I print_info: ssm_d_state      = 0
0.00.039.368 I print_info: ssm_dt_rank      = 0
0.00.039.368 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.369 I print_info: model type       = 1.4B
0.00.039.369 I print_info: model params     = 1.41 B
0.00.039.369 I print_info: general.name     = 1.4B
0.00.039.370 I print_info: vocab type       = BPE
0.00.039.370 I print_info: n_vocab          = 50304
0.00.039.370 I print_info: n_merges         = 50009
0.00.039.370 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: LF token         = 187 ''
0.00.039.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.373 I print_info: max token length = 1024
0.00.039.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.540.142 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.158 I load_tensors: offloading output layer to GPU
0.00.540.158 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.193 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.540.194 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.541.892 I llama_init_from_model: n_seq_max     = 1
0.00.541.895 I llama_init_from_model: n_ctx         = 128
0.00.541.896 I llama_init_from_model: n_ctx_per_seq = 128
0.00.541.896 I llama_init_from_model: n_batch       = 128
0.00.541.896 I llama_init_from_model: n_ubatch      = 128
0.00.541.897 I llama_init_from_model: flash_attn    = 0
0.00.541.899 I llama_init_from_model: freq_base     = 10000.0
0.00.541.900 I llama_init_from_model: freq_scale    = 1
0.00.541.900 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.541.904 I ggml_metal_init: allocating
0.00.541.982 I ggml_metal_init: found device: Apple M4
0.00.541.994 I ggml_metal_init: picking default device: Apple M4
0.00.543.786 I ggml_metal_init: using embedded metal library
0.00.549.864 I ggml_metal_init: GPU name:   Apple M4
0.00.549.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.871 I ggml_metal_init: simdgroup reduction   = true
0.00.549.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.872 I ggml_metal_init: has residency sets    = true
0.00.549.872 I ggml_metal_init: has bfloat            = true
0.00.549.872 I ggml_metal_init: use bfloat            = true
0.00.549.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.568.926 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.572.517 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.572.525 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.572.574 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.575.701 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.575.703 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.575.703 I llama_init_from_model: graph nodes  = 967
0.00.575.704 I llama_init_from_model: graph splits = 2
0.00.575.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.575.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.581 I 
0.00.608.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.677 I perplexity: tokenizing the input ..
0.00.615.811 I perplexity: tokenization took 7.131 ms
0.00.615.823 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.821 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.762.172 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.762.199 I llama_perf_context_print:        load time =     598.84 ms
0.00.762.199 I llama_perf_context_print: prompt eval time =     144.31 ms /   128 tokens (    1.13 ms per token,   886.97 tokens per second)
0.00.762.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.201 I llama_perf_context_print:       total time =     153.62 ms /   129 tokens
0.00.762.617 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.079s
sys	0m0.140s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.598 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.194 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.934 I llama_model_loader: - type  f32:  194 tensors
0.00.024.935 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.935 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.936 I print_info: file format = GGUF V3 (latest)
0.00.024.936 I print_info: file type   = Q5_K - Medium
0.00.024.937 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.912 I load: special tokens cache size = 25
0.00.038.737 I load: token to piece cache size = 0.2984 MB
0.00.038.740 I print_info: arch             = gptneox
0.00.038.740 I print_info: vocab_only       = 0
0.00.038.740 I print_info: n_ctx_train      = 2048
0.00.038.740 I print_info: n_embd           = 2048
0.00.038.741 I print_info: n_layer          = 24
0.00.038.743 I print_info: n_head           = 16
0.00.038.744 I print_info: n_head_kv        = 16
0.00.038.744 I print_info: n_rot            = 32
0.00.038.744 I print_info: n_swa            = 0
0.00.038.744 I print_info: n_embd_head_k    = 128
0.00.038.745 I print_info: n_embd_head_v    = 128
0.00.038.745 I print_info: n_gqa            = 1
0.00.038.746 I print_info: n_embd_k_gqa     = 2048
0.00.038.747 I print_info: n_embd_v_gqa     = 2048
0.00.038.747 I print_info: f_norm_eps       = 1.0e-05
0.00.038.750 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.750 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.750 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.750 I print_info: f_logit_scale    = 0.0e+00
0.00.038.751 I print_info: n_ff             = 8192
0.00.038.751 I print_info: n_expert         = 0
0.00.038.751 I print_info: n_expert_used    = 0
0.00.038.751 I print_info: causal attn      = 1
0.00.038.752 I print_info: pooling type     = 0
0.00.038.753 I print_info: rope type        = 2
0.00.038.754 I print_info: rope scaling     = linear
0.00.038.754 I print_info: freq_base_train  = 10000.0
0.00.038.755 I print_info: freq_scale_train = 1
0.00.038.755 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.755 I print_info: rope_finetuned   = unknown
0.00.038.755 I print_info: ssm_d_conv       = 0
0.00.038.756 I print_info: ssm_d_inner      = 0
0.00.038.756 I print_info: ssm_d_state      = 0
0.00.038.756 I print_info: ssm_dt_rank      = 0
0.00.038.756 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.756 I print_info: model type       = 1.4B
0.00.038.757 I print_info: model params     = 1.41 B
0.00.038.757 I print_info: general.name     = 1.4B
0.00.038.757 I print_info: vocab type       = BPE
0.00.038.757 I print_info: n_vocab          = 50304
0.00.038.757 I print_info: n_merges         = 50009
0.00.038.758 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: LF token         = 187 ''
0.00.038.762 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.763 I print_info: max token length = 1024
0.00.038.763 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.693 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.708 I load_tensors: offloading output layer to GPU
0.00.597.708 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.744 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.746 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.351 I llama_init_from_model: n_seq_max     = 1
0.00.599.353 I llama_init_from_model: n_ctx         = 2048
0.00.599.353 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.354 I llama_init_from_model: n_batch       = 2048
0.00.599.354 I llama_init_from_model: n_ubatch      = 512
0.00.599.355 I llama_init_from_model: flash_attn    = 0
0.00.599.356 I llama_init_from_model: freq_base     = 10000.0
0.00.599.357 I llama_init_from_model: freq_scale    = 1
0.00.599.358 I ggml_metal_init: allocating
0.00.599.380 I ggml_metal_init: found device: Apple M4
0.00.599.390 I ggml_metal_init: picking default device: Apple M4
0.00.600.930 I ggml_metal_init: using embedded metal library
0.00.607.108 I ggml_metal_init: GPU name:   Apple M4
0.00.607.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.113 I ggml_metal_init: simdgroup reduction   = true
0.00.607.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.114 I ggml_metal_init: has residency sets    = true
0.00.607.114 I ggml_metal_init: has bfloat            = true
0.00.607.114 I ggml_metal_init: use bfloat            = true
0.00.607.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.130 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.120 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.676.126 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.676.161 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.371 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.680.373 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.680.373 I llama_init_from_model: graph nodes  = 967
0.00.680.374 I llama_init_from_model: graph splits = 2
0.00.680.381 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.285 I main: llama threadpool init, n_threads = 4
0.00.736.327 I 
0.00.736.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.345 I 
0.00.736.451 I sampler seed: 1234
0.00.736.455 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.465 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.586.865 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.586.865 I llama_perf_context_print:        load time =     726.93 ms
0.01.586.866 I llama_perf_context_print: prompt eval time =      52.68 ms /     7 tokens (    7.53 ms per token,   132.88 tokens per second)
0.01.586.867 I llama_perf_context_print:        eval time =     794.89 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.586.867 I llama_perf_context_print:       total time =     851.34 ms /    70 tokens
0.01.587.076 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.108s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.960 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.028 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.633 I llama_model_loader: - type  f32:  194 tensors
0.00.024.633 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.633 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.634 I print_info: file format = GGUF V3 (latest)
0.00.024.635 I print_info: file type   = Q5_K - Medium
0.00.024.636 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.724 I load: special tokens cache size = 25
0.00.038.559 I load: token to piece cache size = 0.2984 MB
0.00.038.563 I print_info: arch             = gptneox
0.00.038.563 I print_info: vocab_only       = 0
0.00.038.564 I print_info: n_ctx_train      = 2048
0.00.038.564 I print_info: n_embd           = 2048
0.00.038.564 I print_info: n_layer          = 24
0.00.038.568 I print_info: n_head           = 16
0.00.038.569 I print_info: n_head_kv        = 16
0.00.038.569 I print_info: n_rot            = 32
0.00.038.569 I print_info: n_swa            = 0
0.00.038.569 I print_info: n_embd_head_k    = 128
0.00.038.570 I print_info: n_embd_head_v    = 128
0.00.038.570 I print_info: n_gqa            = 1
0.00.038.571 I print_info: n_embd_k_gqa     = 2048
0.00.038.572 I print_info: n_embd_v_gqa     = 2048
0.00.038.572 I print_info: f_norm_eps       = 1.0e-05
0.00.038.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.573 I print_info: f_logit_scale    = 0.0e+00
0.00.038.574 I print_info: n_ff             = 8192
0.00.038.574 I print_info: n_expert         = 0
0.00.038.574 I print_info: n_expert_used    = 0
0.00.038.574 I print_info: causal attn      = 1
0.00.038.574 I print_info: pooling type     = 0
0.00.038.574 I print_info: rope type        = 2
0.00.038.575 I print_info: rope scaling     = linear
0.00.038.575 I print_info: freq_base_train  = 10000.0
0.00.038.575 I print_info: freq_scale_train = 1
0.00.038.575 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.576 I print_info: rope_finetuned   = unknown
0.00.038.576 I print_info: ssm_d_conv       = 0
0.00.038.576 I print_info: ssm_d_inner      = 0
0.00.038.576 I print_info: ssm_d_state      = 0
0.00.038.576 I print_info: ssm_dt_rank      = 0
0.00.038.576 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.577 I print_info: model type       = 1.4B
0.00.038.577 I print_info: model params     = 1.41 B
0.00.038.577 I print_info: general.name     = 1.4B
0.00.038.578 I print_info: vocab type       = BPE
0.00.038.578 I print_info: n_vocab          = 50304
0.00.038.578 I print_info: n_merges         = 50009
0.00.038.578 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.578 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.578 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.579 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.579 I print_info: LF token         = 187 ''
0.00.038.579 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.579 I print_info: max token length = 1024
0.00.038.580 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.150 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.164 I load_tensors: offloading output layer to GPU
0.00.585.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.200 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.585.201 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.586.893 I llama_init_from_model: n_seq_max     = 1
0.00.586.896 I llama_init_from_model: n_ctx         = 128
0.00.586.896 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.897 I llama_init_from_model: n_batch       = 128
0.00.586.897 I llama_init_from_model: n_ubatch      = 128
0.00.586.898 I llama_init_from_model: flash_attn    = 0
0.00.586.900 I llama_init_from_model: freq_base     = 10000.0
0.00.586.901 I llama_init_from_model: freq_scale    = 1
0.00.586.901 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.904 I ggml_metal_init: allocating
0.00.586.990 I ggml_metal_init: found device: Apple M4
0.00.587.004 I ggml_metal_init: picking default device: Apple M4
0.00.588.559 I ggml_metal_init: using embedded metal library
0.00.595.071 I ggml_metal_init: GPU name:   Apple M4
0.00.595.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.077 I ggml_metal_init: simdgroup reduction   = true
0.00.595.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.078 I ggml_metal_init: has residency sets    = true
0.00.595.078 I ggml_metal_init: has bfloat            = true
0.00.595.078 I ggml_metal_init: use bfloat            = true
0.00.595.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.398 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.907 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.912 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.975 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.360 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.362 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.363 I llama_init_from_model: graph nodes  = 967
0.00.619.363 I llama_init_from_model: graph splits = 2
0.00.619.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.929 I 
0.00.654.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.009 I perplexity: tokenizing the input ..
0.00.660.802 I perplexity: tokenization took 6.789 ms
0.00.660.816 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.366 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.714 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.735 I llama_perf_context_print:        load time =     644.96 ms
0.00.799.735 I llama_perf_context_print: prompt eval time =     136.60 ms /   128 tokens (    1.07 ms per token,   937.06 tokens per second)
0.00.799.736 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.736 I llama_perf_context_print:       total time =     145.81 ms /   129 tokens
0.00.800.079 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.079s
sys	0m0.138s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.216 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.218 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.219 I llama_model_loader: - type  f32:  194 tensors
0.00.024.219 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.220 I print_info: file format = GGUF V3 (latest)
0.00.024.220 I print_info: file type   = Q6_K
0.00.024.221 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.405 I load: special tokens cache size = 25
0.00.038.489 I load: token to piece cache size = 0.2984 MB
0.00.038.492 I print_info: arch             = gptneox
0.00.038.492 I print_info: vocab_only       = 0
0.00.038.492 I print_info: n_ctx_train      = 2048
0.00.038.492 I print_info: n_embd           = 2048
0.00.038.493 I print_info: n_layer          = 24
0.00.038.495 I print_info: n_head           = 16
0.00.038.496 I print_info: n_head_kv        = 16
0.00.038.496 I print_info: n_rot            = 32
0.00.038.496 I print_info: n_swa            = 0
0.00.038.496 I print_info: n_embd_head_k    = 128
0.00.038.496 I print_info: n_embd_head_v    = 128
0.00.038.499 I print_info: n_gqa            = 1
0.00.038.500 I print_info: n_embd_k_gqa     = 2048
0.00.038.501 I print_info: n_embd_v_gqa     = 2048
0.00.038.501 I print_info: f_norm_eps       = 1.0e-05
0.00.038.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.507 I print_info: f_logit_scale    = 0.0e+00
0.00.038.507 I print_info: n_ff             = 8192
0.00.038.508 I print_info: n_expert         = 0
0.00.038.508 I print_info: n_expert_used    = 0
0.00.038.508 I print_info: causal attn      = 1
0.00.038.508 I print_info: pooling type     = 0
0.00.038.508 I print_info: rope type        = 2
0.00.038.509 I print_info: rope scaling     = linear
0.00.038.509 I print_info: freq_base_train  = 10000.0
0.00.038.509 I print_info: freq_scale_train = 1
0.00.038.510 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.510 I print_info: rope_finetuned   = unknown
0.00.038.511 I print_info: ssm_d_conv       = 0
0.00.038.511 I print_info: ssm_d_inner      = 0
0.00.038.511 I print_info: ssm_d_state      = 0
0.00.038.512 I print_info: ssm_dt_rank      = 0
0.00.038.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.512 I print_info: model type       = 1.4B
0.00.038.514 I print_info: model params     = 1.41 B
0.00.038.514 I print_info: general.name     = 1.4B
0.00.038.514 I print_info: vocab type       = BPE
0.00.038.514 I print_info: n_vocab          = 50304
0.00.038.515 I print_info: n_merges         = 50009
0.00.038.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: LF token         = 187 ''
0.00.038.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.516 I print_info: max token length = 1024
0.00.038.517 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.675 I load_tensors: offloading output layer to GPU
0.00.630.676 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.700 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.630.702 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.632.163 I llama_init_from_model: n_seq_max     = 1
0.00.632.165 I llama_init_from_model: n_ctx         = 2048
0.00.632.166 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.632.166 I llama_init_from_model: n_batch       = 2048
0.00.632.166 I llama_init_from_model: n_ubatch      = 512
0.00.632.167 I llama_init_from_model: flash_attn    = 0
0.00.632.168 I llama_init_from_model: freq_base     = 10000.0
0.00.632.168 I llama_init_from_model: freq_scale    = 1
0.00.632.169 I ggml_metal_init: allocating
0.00.632.223 I ggml_metal_init: found device: Apple M4
0.00.632.235 I ggml_metal_init: picking default device: Apple M4
0.00.633.726 I ggml_metal_init: using embedded metal library
0.00.639.641 I ggml_metal_init: GPU name:   Apple M4
0.00.639.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.646 I ggml_metal_init: simdgroup reduction   = true
0.00.639.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.647 I ggml_metal_init: has residency sets    = true
0.00.639.647 I ggml_metal_init: has bfloat            = true
0.00.639.647 I ggml_metal_init: use bfloat            = true
0.00.639.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.193 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.706.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.706.526 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.706.564 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.216 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.219 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.219 I llama_init_from_model: graph nodes  = 967
0.00.711.220 I llama_init_from_model: graph splits = 2
0.00.711.225 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.711.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.431 I main: llama threadpool init, n_threads = 4
0.00.778.478 I 
0.00.778.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.503 I 
0.00.778.683 I sampler seed: 1234
0.00.778.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.709 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.709 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.709 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.665.593 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.665.593 I llama_perf_context_print:        load time =     768.95 ms
0.01.665.594 I llama_perf_context_print: prompt eval time =      57.58 ms /     7 tokens (    8.23 ms per token,   121.57 tokens per second)
0.01.665.595 I llama_perf_context_print:        eval time =     826.32 ms /    63 runs   (   13.12 ms per token,    76.24 tokens per second)
0.01.665.595 I llama_perf_context_print:       total time =     887.88 ms /    70 tokens
0.01.665.851 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4794 (8ffa8bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.217 I llama_model_loader: - type  f32:  194 tensors
0.00.024.218 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.218 I print_info: file format = GGUF V3 (latest)
0.00.024.219 I print_info: file type   = Q6_K
0.00.024.220 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.249 I load: special tokens cache size = 25
0.00.038.298 I load: token to piece cache size = 0.2984 MB
0.00.038.302 I print_info: arch             = gptneox
0.00.038.302 I print_info: vocab_only       = 0
0.00.038.303 I print_info: n_ctx_train      = 2048
0.00.038.303 I print_info: n_embd           = 2048
0.00.038.303 I print_info: n_layer          = 24
0.00.038.308 I print_info: n_head           = 16
0.00.038.308 I print_info: n_head_kv        = 16
0.00.038.309 I print_info: n_rot            = 32
0.00.038.309 I print_info: n_swa            = 0
0.00.038.309 I print_info: n_embd_head_k    = 128
0.00.038.309 I print_info: n_embd_head_v    = 128
0.00.038.310 I print_info: n_gqa            = 1
0.00.038.311 I print_info: n_embd_k_gqa     = 2048
0.00.038.312 I print_info: n_embd_v_gqa     = 2048
0.00.038.313 I print_info: f_norm_eps       = 1.0e-05
0.00.038.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.316 I print_info: f_logit_scale    = 0.0e+00
0.00.038.316 I print_info: n_ff             = 8192
0.00.038.316 I print_info: n_expert         = 0
0.00.038.316 I print_info: n_expert_used    = 0
0.00.038.317 I print_info: causal attn      = 1
0.00.038.317 I print_info: pooling type     = 0
0.00.038.317 I print_info: rope type        = 2
0.00.038.317 I print_info: rope scaling     = linear
0.00.038.317 I print_info: freq_base_train  = 10000.0
0.00.038.318 I print_info: freq_scale_train = 1
0.00.038.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.318 I print_info: rope_finetuned   = unknown
0.00.038.318 I print_info: ssm_d_conv       = 0
0.00.038.318 I print_info: ssm_d_inner      = 0
0.00.038.318 I print_info: ssm_d_state      = 0
0.00.038.318 I print_info: ssm_dt_rank      = 0
0.00.038.319 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.320 I print_info: model type       = 1.4B
0.00.038.321 I print_info: model params     = 1.41 B
0.00.038.321 I print_info: general.name     = 1.4B
0.00.038.321 I print_info: vocab type       = BPE
0.00.038.321 I print_info: n_vocab          = 50304
0.00.038.321 I print_info: n_merges         = 50009
0.00.038.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: LF token         = 187 ''
0.00.038.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.324 I print_info: max token length = 1024
0.00.038.324 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.570.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.959 I load_tensors: offloading output layer to GPU
0.00.570.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.990 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.570.993 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.572.535 I llama_init_from_model: n_seq_max     = 1
0.00.572.537 I llama_init_from_model: n_ctx         = 128
0.00.572.538 I llama_init_from_model: n_ctx_per_seq = 128
0.00.572.538 I llama_init_from_model: n_batch       = 128
0.00.572.538 I llama_init_from_model: n_ubatch      = 128
0.00.572.539 I llama_init_from_model: flash_attn    = 0
0.00.572.540 I llama_init_from_model: freq_base     = 10000.0
0.00.572.541 I llama_init_from_model: freq_scale    = 1
0.00.572.541 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.572.543 I ggml_metal_init: allocating
0.00.572.605 I ggml_metal_init: found device: Apple M4
0.00.572.618 I ggml_metal_init: picking default device: Apple M4
0.00.574.096 I ggml_metal_init: using embedded metal library
0.00.580.235 I ggml_metal_init: GPU name:   Apple M4
0.00.580.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.241 I ggml_metal_init: simdgroup reduction   = true
0.00.580.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.242 I ggml_metal_init: has residency sets    = true
0.00.580.242 I ggml_metal_init: has bfloat            = true
0.00.580.242 I ggml_metal_init: use bfloat            = true
0.00.580.244 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.245 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.597.649 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.168 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.601.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.601.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.316 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.604.317 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.604.318 I llama_init_from_model: graph nodes  = 967
0.00.604.318 I llama_init_from_model: graph splits = 2
0.00.604.321 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.604.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.254 I 
0.00.640.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.349 I perplexity: tokenizing the input ..
0.00.647.819 I perplexity: tokenization took 7.467 ms
0.00.647.827 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.193 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.781.689 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.781.713 I llama_perf_context_print:        load time =     631.29 ms
0.00.781.716 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.16 tokens per second)
0.00.781.717 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.719 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.782.080 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.079s
sys	0m0.126s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4794 (8ffa8bec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127708f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127709640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127709bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12770a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12770a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12770ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12770b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12770b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12770be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12770c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12770c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12770cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12770d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12770dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12770e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12770ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12770f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12770fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127710470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127710c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127711360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127711a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1277121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127712a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127713160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127713420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127713a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1277146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127714be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127715340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127715600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127715e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1277163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127716690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127716b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127716fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127717910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127717db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127718250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1277186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127718b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127719030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1277192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127719900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127719f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12771a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12771ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12771b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12771ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12771c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12771c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12771cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12771d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12771d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12771ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12771e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12771e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12771ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12771f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12771f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12771fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12771ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1277203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127720860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127720d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1277211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127721640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127721ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127721f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1277228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127722e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127723360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1277238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127723e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127724350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1277248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127724df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127725340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127725890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127725de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127726330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127726880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127726dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127727320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127727870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127727dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127728310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127728860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127728db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127729300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127729850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127729da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12772a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12772a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12771a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12772acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12772b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12772b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12772bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12772c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12772c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12772cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12772d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12772d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12772dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12772e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12772e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12772eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12772f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12772f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12772fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1277302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127730750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127730bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127731090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127731530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1277319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127731e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127732310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1277327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127732c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1277330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127733590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127733a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127734370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127734810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127734cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127735150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1277355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127735a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127735f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1277363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127736870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127736d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1277371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127737650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127737af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127737f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127738430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1277388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127738d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1277396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12773a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12773a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12773add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12773b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12773b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12773bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12773c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12773c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12773c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12773ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12773d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12773d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12773dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12773e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12773e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12773e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12773ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12773f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12773f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12773fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127740110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1277405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127740ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127741390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127741830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127742170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127742610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127742ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127742f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1277433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1176048f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117604d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1176051d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1176082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117608740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117608bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117609020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117609900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117609d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11760a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11760a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11760aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11760af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11760b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11760b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11760bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11760c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11760c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11760c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11760ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11760d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11760d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11760dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11760e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11760e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11760e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11760ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11760f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11760ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117610240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117610dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117611380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117611940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117611f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1176124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117612a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117613040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117613600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117613bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117614180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117614740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117614d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1176152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117615880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117615e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117616400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1176169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1176180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117618c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1176197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117619d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11761a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11761a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11761aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11761b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11761ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11761c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11761c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11761cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11761d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11761d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11761dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11761e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11761e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11761ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11761f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11761f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11761ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117620ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117621080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117621c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1176221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117622780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117622d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1176238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117623e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117625840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117626240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117627640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117627b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117628040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117628a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x117628f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x117629440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x117629940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x117629e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11762a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11762a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11762ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11762b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11762b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11762bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11762c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11762cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11762d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11762d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11762e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11762e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11762eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11762ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11762f430 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.667.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.667.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107c04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107c05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107c056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107c05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107c05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107c06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107c06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107c06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107c07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107c075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107c07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107c08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107c08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107c093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107c09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107c0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107c0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107c0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107c0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107c0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107c0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107c0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107c0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107c0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107c0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107c0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107c0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107c0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107c0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107c0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107c0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107c0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107c10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107c106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107c10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107c10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107c11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107c118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107c11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107c12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107c12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107c12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107c12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107c13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107c137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107c13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107c140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107c14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107c14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107c14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107c15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107c156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107c15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107c15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107c16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107c16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107c16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107c17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107c17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107c17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107c18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107c184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107c18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107c18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107c19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107c19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107c19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107c19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107c1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107c1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107c1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107c1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107c1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107c1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107c1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107c1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107c1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107c1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107c1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107c1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107c1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107c1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107c1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107c1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107c1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107c1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107c1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107c1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107c1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107c20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107c20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107c209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107c20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107c212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107c21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107c21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107c22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107c22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107c228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107c22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107c231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107c23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107c23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107c23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107c24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107c24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107c24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107c250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107c25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107c259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107c25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107c262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107c26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107c26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107c26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107c27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107c278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107c27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107c281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107c28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107c28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107c28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107c29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107c297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107c29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107c2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107c2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107c2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107c2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107c2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107c2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107c2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117504230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1175046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117504b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117504f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1175053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117505860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117505cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117506140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1175065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117506a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117506e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117507300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117507770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117507be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117508050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1175084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117508930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117508da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117509210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117509680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117509af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117509f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11750a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11750a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11750acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11750b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11750b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11750ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11750be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11750c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11750c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11750cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11750d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11750d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11750e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11750e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11750e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11750eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11750ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11750f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11750f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11750fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1175100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117510540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1175109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117510e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117511290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117511700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117511b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117511fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117512450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1175128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117512d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1175131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117513610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117513a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117513ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117514360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1175147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117514c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1175150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117515520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117515990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117515e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117516270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1175166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117516b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117516fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117517430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1175178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117517e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117518310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117518780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117518bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117519060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1175194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1175199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117519f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11751aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11751ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11751b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11751b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11751be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11751c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11751c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11751cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11751d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11751db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11751e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11751e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11751ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11751f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11751f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11751fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117520370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117520930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117520ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1175214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117521a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117522030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1175225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117522bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117523170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117523730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117523cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1175242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117524870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117524e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1175253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1175259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117525f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117526530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117526af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1175270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117527670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117527c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1175281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1175287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117528d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117529330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1175298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117529eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11752a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11752aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11752aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11752b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11752bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11752c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11752c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11752ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11752d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11752d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11752ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11752e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11752e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11752ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11752f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11752f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11752fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117530330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117530830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117530d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117531230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117531730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117531c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117532130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117532630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117532b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117533030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117533530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x117533a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x117533f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x117534430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x117534930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x117534e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x117535330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x117535830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x117535d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x117536230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x117536730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117536c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117537640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117537d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117538480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117538ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117538e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117539650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117539910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117539f20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107c2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107c083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107c04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107c0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107c2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107c2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107c2c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107c2c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107c2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107c2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107c2d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107c2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107c2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107c2e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107c2e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107c2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107c2ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107c2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107c2f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107c30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107c305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107c30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107c31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107c31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107c31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107c32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107c322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107c32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107c32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107c32b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107c32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107c33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107c33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107c33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107c338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107c33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107c33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107c34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107c343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107c34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107c34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107c34c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107c34ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107c35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107c35440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107c35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107c359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107c35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107c35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107c36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107c364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107c36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107c36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107c36fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107c37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107c37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107c37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107c37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107c37d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107c38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107c38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107c385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107c38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107c38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107c38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107c390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107c39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107c39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107c39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107c39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107c39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107c3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107c3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107c3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107c3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107c3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107c3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107c3b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107c3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107c3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107c3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107c3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107c3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107c3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107c3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107c3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107c3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107c3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107c3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107c3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107c3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107c3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107c3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107c3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107c3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107c3e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107c3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107c3e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107c3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107c3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107c3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107c3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107c3f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107c3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107c3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107c3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107c40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107c40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107c40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107c409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107c40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107c40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107c41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107c414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107c41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107c41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107c41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107c41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107c42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107c42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107c42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107c42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107c42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107c43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107c43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107c43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107c43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107c43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107c44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107c448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107c44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107c451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107c45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107c45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107c45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107c46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107c467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107c46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107c470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107c47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107c47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107c47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107c48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107c486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107c48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107c48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107c49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107c498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107c49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107c4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107c4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107c4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107c4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107c4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107c4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107c4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107c4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107c4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107c4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107c4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107c4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107c4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107c4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107c4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107c4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107c4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107c4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107c4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107c4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107c4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107c4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107c50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107c50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107c50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107c51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107c514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107c51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107c51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107c52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107c526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107c52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107c52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107c533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107c53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107c53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107c54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107c545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107c54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107c54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107c55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107c55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107c55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107c56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107c564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107c56930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107c56da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107c57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107c57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107c57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107c57f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107c583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107c58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107c58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107c59490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107c59aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107c5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107c5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107c5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107c5b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107c5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107c5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107c5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107c5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107c5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107c5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107c5d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107c5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107c5e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107c5e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107c5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107c5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107c5f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107c5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107c60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107c607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107c60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107c61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107c617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107c61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107c62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107c627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107c62d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107c63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107c637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107c63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107c64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107c64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107c64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107c65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107c65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107c65cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107c66220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107c66770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107c66cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107c67210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107c67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107c67cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107c68200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107c68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107c68ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107c691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107c69740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107c69c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107c6a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107c6a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107c6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107c6b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107c6b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107c6bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107c6c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107c6c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107c6cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107c6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107c6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107c6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107c6e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107c6e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107c6eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107c6ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107c6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107c6f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107c6fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107c70200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107c706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107c70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107c70fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107c71480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107c71920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107c71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107c72260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107c72700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x107c72ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x107c73040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x107c734e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x107c73980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x107c73e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x107c742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x107c74760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x107c74c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x107c750a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x107c75540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107c75a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107c761b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107c768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107c76ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107c77710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107c779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107c781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107c78480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107c78a90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.721s
user	0m0.280s
sys	0m0.286s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4794 (8ffa8bec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121e0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121e0fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121e0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121e105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121e10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121e11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121e116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121e11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121e12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121e12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121e13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121e13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121e143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121e14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121e15310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121e15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121e16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121e16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121e17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121e17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121e17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121e185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121e18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121e19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121e19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121e19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121e1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121e1afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121e1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121e1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121e1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121e1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121e1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121e1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121e1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121e1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121e1d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121e1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121e1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121e1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121e1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121e1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121e1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121e1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121e20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121e20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121e21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121e21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121e21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121e22470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121e22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121e23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121e23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121e23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121e241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121e24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121e24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121e25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121e25540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121e259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121e25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121e26320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121e267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121e26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121e27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121e275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121e27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121e27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121e28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121e28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121e28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121e29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121e29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121e29cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121e2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121e2a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121e2aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121e2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121e2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121e2bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121e2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121e2c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121e2cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121e2d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121e2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121e2e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121e2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121e2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121e2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121e2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121e301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121e306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121e30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121e20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121e310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121e31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121e31db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121e32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121e32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121e32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121e332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121e33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121e33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121e342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121e34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121e352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121e35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121e35d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121e36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121e36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121e36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121e37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121e37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121e37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121e38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121e38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121e38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121e39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121e394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121e39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121e39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121e3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121e3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121e3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121e3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121e3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121e3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121e3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121e3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121e3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121e3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121e3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121e3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121e3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121e3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121e3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121e3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121e3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121e3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121e3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121e3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121e403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121e40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121e40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121e411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121e41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121e41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121e41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121e42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121e428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121e42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121e43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121e436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121e43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121e44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121e444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121e44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121e44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121e45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121e45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121e45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121e46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121e46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121e469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121e47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121e47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121e480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121e48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121e48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121e48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121e49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121e497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121e49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121e4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121e4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121e4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121e4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121e4b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121e4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121e4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121e4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121e4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121e4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121e4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121e4df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121e4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121e4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121e4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121e4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121e4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121e50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121e508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121e50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121e51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121e51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121e521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121e52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121e52ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121e53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121e537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121e53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121e54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121e547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121e54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121e55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121e557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121e56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121e567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121e56d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121e57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121e57cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121e58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121e58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121e59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121e59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121e59cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121e5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121e5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121e5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121e5b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121e5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121e5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121e5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121e5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121e5cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121e5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121e5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121e5dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121e5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121e5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121e5ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121e5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121e5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121e5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121e601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121e60710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121e60c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121e611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121e61700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121e61c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121e621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121e626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121e62c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121e63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121e636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121e63c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121e64180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121e646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121e64c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121e65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121e656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121e65c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121e660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121e66550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121e669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121e66e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121e67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121e677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121e67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121e68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121e685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121e68a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121e68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121e69390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121e69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121e69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121e6a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x121e6a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x121e6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x121e6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x121e6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x121e6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x121e6bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x121e6c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x121e6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x121e6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x121e6cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121e6d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121e6dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121e6e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121e6ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121e6f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121e6f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121e6fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121e6fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121e70500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1230053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1230069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1230072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1230090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12300a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12300a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12300ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12300b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12300bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12300c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12300cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12300d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12300d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12300e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12300e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12300e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12300eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12300ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12300f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12300f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12300fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1230101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1230111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1230123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1230130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1230139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1230142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1230158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1230161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1230170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1230186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12301a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12301a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12301aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12301aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12301b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12301b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12301bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12301c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12301c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12301c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12301cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12301d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12301d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12301db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12301df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12301e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12301e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12301ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12301f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12301f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12301fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12301fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1230214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1230233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1230245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1230252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1230264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1230283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1230299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12302a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12302a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12302abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12302b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12302b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12302b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12302bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12302c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12302c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12302cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12302cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12302d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12302d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12302dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12302e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12302e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12302e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12302ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12302f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12302f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12302fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1230308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1230311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1230327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1230330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1230339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1230377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1230380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1230396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12303a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12303a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12303ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12303b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12303b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12303ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12303bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12303c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12303c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12303cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12303d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12303d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12303d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12303ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12303e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12303e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12303eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12303ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12303f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12303f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12303fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1230402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1230425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1230439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1230450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1230467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1230495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12304a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12304a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12304acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12304b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12304b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12304be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12304c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12304c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12304cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12304d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12304dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12304e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12304e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12304ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12304f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12304f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12304fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1230508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1230536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1230564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12305a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12305a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12305ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12305b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12305b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12305ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12305bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12305c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12305c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12305ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12305d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12305d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12305dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12305e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12305e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12305f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12305f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12305ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123060700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1230609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1230611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123061470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123061a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f09320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f09790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f09e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f0a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f14230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f19cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f1e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f21b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f22b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f28590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f29030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f29ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f2a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f2b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f2c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f2cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f30d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f32460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f32900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f36080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f36520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f38ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f3af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f3e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f3f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f41040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f42cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f43200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f44570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f44b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f45980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f4afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f4ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f4c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f4df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f4ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f4ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f4fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f4ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f53480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f53f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f55f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f57990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f57ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f58430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f58980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f59ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f5a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f5a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f5aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f5b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f5c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f5cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f5d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f5d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f5dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f5f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f5f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f5f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x121f5fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x121f602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x121f60740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x121f60be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x121f61080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x121f61520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x121f619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x121f61e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x121f62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x121f627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f62cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f63410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f64250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f64970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f65420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f65cf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.232s
sys	0m0.174s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.12 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.55 sec*proc (2 tests)

Total Test time (real) =   1.56 sec
        1.58 real         0.51 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.12 user         0.08 sys
```
